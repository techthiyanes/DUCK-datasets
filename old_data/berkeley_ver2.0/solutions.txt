\title{
Real Analysis
}

\section{$1.1$ Elementary Calculus}

Solution to 1.1.1: Let $f(\theta)=\cos p \theta-(\cos \theta)^{p}$. We have $f(0)=0$ and, for $0<\theta<\pi / 2$,

$$
\begin{aligned}
f^{\prime}(\theta) &=-p \sin p \theta+p \cos ^{p-1} \theta \sin \theta \\
&=p\left(-\sin p \theta+\frac{\sin \theta}{\cos ^{1-p} \theta}\right) \\
&>0
\end{aligned}
$$

since sin is an increasing function on $[0, \pi / 2]$ and $\cos ^{1-p} \theta \in(0,1)$. We conclude that $f(\theta) \geqslant 0$ for $0 \leqslant \theta \leqslant \pi / 2$, which is equivalent to the inequality we wanted to establish.

Solution to 1.1.2: Let $x \in[0,1]$. Using the fact that $f(0)=0$ and the CauchySchwarz Inequality [MH93, p. 69] we have,

$$
\begin{aligned}
|f(x)| &=\left|\int_{0}^{x} f^{\prime}(t) d t\right| \\
& \leqslant \sqrt{\int_{0}^{x}\left|f^{\prime}(t)\right|^{2} d t} \sqrt{\int_{0}^{x} 1^{2} d t} \\
& \leqslant \sqrt{\int_{0}^{x}\left|f^{\prime}(t)\right|^{2} d t}
\end{aligned}
$$

and the conclusion follows.

Solution to 1.1.3: As $f^{\prime}$ is positive, $f$ is an increasing function, so we have, for $t>1, f(t)>f(1)=1$. Therefore, for $t>1$,

$$
f^{\prime}(t)=\frac{1}{t^{2}+f^{2}(t)}<\frac{1}{t^{2}+1},
$$

so

$$
\begin{aligned}
f(x) &=1+\int_{1}^{x} f^{\prime}(t) d t \\
&<1+\int_{1}^{x} \frac{1}{t^{2}+1} d t \\
&<1+\int_{1}^{\infty} \frac{1}{t^{2}+1} d t \\
&=1+\frac{\pi}{4}
\end{aligned}
$$

hence, $\lim _{x \rightarrow \infty} f(x)$ exists and is, at most, $1+\frac{\pi}{4}$. The strict inequality holds because

$$
\lim _{x \rightarrow \infty} f(x)=1+\int_{1}^{\infty} f^{\prime}(t) d t<1+\int_{1}^{\infty} \frac{1}{t^{2}+1} d t=1+\frac{\pi}{4}
$$

Solution to 1.1.4: Denote the common supremum of $f$ and $g$ by $M$. Since $f$ and $g$ are continuous and $[0,1]$ is compact, there exist $\alpha, \beta \in[0,1]$ with $f(\alpha)=g(\beta)=M$. The function $h$ defined by $h(x)=f(x)-g(x)$ satisfies $h(\alpha)=M-g(\alpha) \geqslant 0, h(\beta)=f(\beta)-M \leq 0$. Since $h$ is continuous, it has a zero $t \in[\alpha, \beta]$. We have $f(t)=g(t)$, so $f(t)^{2}+3 f(t)=g(t)^{2}+3 g(t)$.

Solution to 1.1.5: Call a function of the desired form a periodic polynomial, and call its degree the largest $k$ such that $x^{k}$ occurs with a nonzero coefficient.

If $a$ is 1-periodic, then $\Delta(a f)=a \Delta f$ for any function $f$, so, by the Induction Principle [MH93, p. 7], $\Delta^{n}(a f)=a \Delta^{n} f$ for all $n$.

We will use Complete Induction [MH93, p. 32]. For $n=1$, the result holds: $\Delta f=0$ if and only if $f$ is 1-periodic. Assume it is true for $1, \ldots, n-1$. If

$$
f=a_{0}+a_{1} x+\cdots+a_{n-1} x^{n-1}
$$

is a periodic polynomial of degree, at most, $n-1$, then

$$
\Delta^{n} f=a_{1} \Delta^{n} x+\cdots+a_{n-1} \Delta x^{n-1}
$$

and the induction hypothesis implies that all the terms vanish except, maybe, the last. We have $\Delta^{n}\left(x^{n-1}\right)=\Delta^{n-1} \Delta\left(x^{n-1}\right)$, a polynomial of degree $n-2$ by the Binomial Theorem [BML97, p. 15]. So the induction hypothesis also implies $\Delta^{n}\left(x^{n-1}\right)=0$ and the first half of the statement is established. For the other half, assume $\Delta^{n} f=0$. By the induction hypothesis, $\Delta f$ is a periodic polynomial of degree, at most, $n-2$. Suppose we can find a periodic polynomial $g$, of degree, at most, $n-1$, such that $\Delta g=\Delta f$. Then, as $\Delta(f-g)=$ 0 , the function $f-g$ will be 1 -periodic, implying that $f$ is a periodic polynomial of degree, at most, $n-1$, as desired. Thus, it is enough to prove the following claim: If $h$ is a periodic polynomial of degree $n(n=0,1, \ldots)$, then there is a periodic polynomial $g$ of degree $n+1$ such that $\Delta g=h$.

If $n=0$, we can take $g=h x$. Assume $h$ has degree $n>0$ and, as an induction hypothesis, that the claim is true for lower degrees than $n$. We can then, without loss of generality, assume $h=a x^{n}$, where $a$ is 1-periodic. By the Binomial Theorem,

$$
h-\Delta\left(\frac{a x^{n+1}}{n+1}\right)
$$

is a periodic polynomial of degree $n-1$, so it equals $\Delta g_{1}$, for some periodic polynomial $g_{1}$ of degree $n$, and we have $h=\Delta g$, where

$$
g=\frac{a x^{n+1}}{n+1}+g_{1}
$$

as desired.

Solution to 1.1.6: Since $f$ is increasing and nonconstant, there exist $r_{0}<s_{0}$ such that $f\left(r_{0}\right)<f\left(s_{0}\right)$. Increasing $s_{0}$ if necessary, we may assume $\delta=s_{0}-r_{0}>1$. Let $\Delta=f\left(s_{0}\right)-f\left(r_{0}\right)$.

Choose $\left[r_{1}, s_{1}\right]$ to be either $\left[r_{0},\left(r_{0}+s_{0}\right) / 2\right]$ or $\left[\left(r_{0}+s_{0}\right) / 2, s_{0}\right]$, whichever makes $f\left(s_{1}\right)-f\left(r_{1}\right)$ larger. (Choose either if they are equal.) Then $f\left(s_{1}\right)-$ $f\left(r_{1}\right) \geqslant \Delta / 2$. Similarly choose $\left[r_{2}, s_{2}\right]$ as the left or right half of $\left[r_{1}, s_{1}\right]$, to $\max -$ imize $f\left(s_{2}\right)-f\left(r_{2}\right)$, and so on, so that $s_{i}-r_{i}=2^{-i} \delta$, and $f\left(s_{i}\right)-f\left(r_{i}\right) \geqslant 2^{-i} \Delta$.

Let $a=\lim r_{i}$, and set $c=\Delta /(2 \delta)$. Given $x \in(0,1]$, choose the first integer $i \geqslant 0$ such that $2^{-i} \delta \leqslant x$. Then $2^{-i} \delta \geqslant x / 2$. Also $\left[r_{i}, s_{i}\right]$ contains $a$ and is of length $\leqslant x$, so $\left[r_{i}, s_{i}\right] \subseteq[a-x, a+x]$. Since $f$ is increasing,

$$
f(a+x)-f(a-x) \geqslant f\left(s_{i}\right)-f\left(r_{i}\right) \geqslant 2^{-i} \Delta=2 c 2^{-i} \delta \geqslant c x .
$$

Finally, the inequality $f(a+x)-f(a-x) \geqslant c x$ is immediate for $x=0$.

\section{Solution to 1.1.7:}

1. This is false. For

$$
f(t)=g(t)=\left\{\begin{array}{ll}
0 & \text { for } t \neq 0 \\
1 & \text { for } t=0
\end{array} .\right.
$$

we have $\lim _{t \rightarrow 0} g(t)=\lim _{t \rightarrow 0} f(t)=0$ but $\lim _{t \rightarrow 0} f(g(t))=1$.

2. This is false. $f(t)=t^{2}$ maps the open interval $(-1,1)$ onto $[0,1)$, which is not open. 3. This is true. Let $x, x_{0} \in(-1,1)$. By Taylor's Theorem [Rud87, p. 110], there is $\xi \in(-1,1)$ such that

$$
f(x)=\sum_{k=0}^{n-1} \frac{f^{(k)}\left(x_{0}\right)}{k !}\left(x-x_{0}\right)^{k}+\frac{f^{(n)}(\xi)}{n !}\left(x-x_{0}\right)^{n} \quad(n \in \mathbb{N}) .
$$

We have

$$
\lim _{n \rightarrow \infty}\left|\frac{f^{(n)}(\xi)}{n !}\left(x-x_{0}\right)^{n}\right| \leqslant \lim _{n \rightarrow \infty} \frac{\left|x-x_{0}\right|^{n}}{n !}=0,
$$

so

$$
f(x)=\sum_{k=0}^{\infty} \frac{f^{(k)}\left(x_{0}\right)}{k !}\left(x-x_{0}\right)^{k}
$$

for any $x_{0} \in(-1,1)$ and $f$ is real analytic.

Solution to 1.1.8: Let $f(x)=\sin x-x+x^{3} / 6$; then $f(0)=f^{\prime}(0)=f^{\prime \prime}(0)=0$ and $f^{\prime \prime \prime}(x)=1-\cos x$, so $f^{\prime \prime \prime}(x) \geqslant 0$ for all $x$, and $f^{\prime \prime \prime}(x)>0$ for $0<x<2 \pi$. Hence, for $x>0, f^{\prime \prime}(x)=\int_{0}^{x} f^{\prime \prime \prime}(t) d t>0$; similarly $f^{\prime}(x)=\int_{0}^{x} f^{\prime \prime}(t) d t>0$, and finally $f(x)=\int_{0}^{x} f^{\prime}(t) d t>0$.

Solution 2. The function $f(x)=\sin x-x+\frac{x^{3}}{6}$ has derivatives

$$
\begin{aligned}
f^{\prime}(x) &=\cos x-1+\frac{x^{2}}{2} \\
f^{\prime \prime}(x) &=-\sin x+x \\
f^{\prime \prime \prime}(x) &=-\cos x+1 .
\end{aligned}
$$

Now $f^{\prime \prime \prime}(x) \geqslant 0$ with equality at the discrete set of points $2 \pi n, n \in \mathbb{Z}$. Therefore $f^{\prime \prime}$ is strictly increasing on $\mathbb{R}$, and since $f^{\prime \prime}(0)=0$, we obtain $f^{\prime \prime}(x)>0$ for $x>0$. Therefore $f^{\prime}$ is strictly increasing on $\mathbb{R}^{+} ;$since $f^{\prime}(0)=0$, we obtain $f^{\prime}(x)>0$ for $x>0$. Therefore $f$ is strictly increasing on $\mathbb{R}^{+}$; since $f(0)=0$ we obtain $f(x)>0$ for $x>0$, as required.

Solution 3. Consider the Taylor expansion of $\sin x$ around 0 , of order 5 , with the Lagrange remainder. For $x>0$ we have

$$
\sin x=x-\frac{x^{3}}{6}+\frac{x^{5}}{5 !}+\frac{\cos ^{6} \xi}{6 !}
$$

for some $0<\xi<x$. Since $\frac{x^{5}}{5 !}>0$ and $\frac{\cos ^{6} \xi}{6 !} \geqslant 0$ we conclude that the result follows for positive $x$.

Solution to 1.1.9: The Maclaurin expansion of $\sin x$ of order 3 is

$$
\sin x=x-\frac{x^{3}}{6}+O\left(x^{5}\right), \quad(x \rightarrow 0),
$$

therefore

$$
\sin ^{2} x=x^{2}+O\left(x^{4}\right) \quad(x \rightarrow 0) .
$$

So, for $h$ near 0 , we have $y(h)=1-8 \pi^{2} h^{2}+O\left(h^{4}\right)$. (Alternatively, observe that $y(h)=\cos 4$ pih and use the expansion for cosine.) Thus,

$$
f(y(h))=\frac{2}{1+\sqrt{8 \pi^{2} h^{2}+O\left(h^{4}\right)}}=\frac{2}{1+2 \sqrt{2} \pi|h|+O\left(h^{2}\right)} \quad(h \rightarrow 0) .
$$

Using the Maclaurin expansion $\frac{2}{1+x}=2-2 x+2 x^{2}+O\left(x^{2}\right)$, we get

$$
f(y(h))=2-4 \sqrt{2} \pi|h|+O\left(h^{2}\right), \quad(h \rightarrow 0) \text {. }
$$

Solution to 1.1.10: 1. Suppose $f:[0,1] \rightarrow(0,1)$ is a continuous surjection. Consider the sequence $\left(x_{n}\right)$ such that $x_{n} \in f^{-1}((0,1 / n))$. By the BolzanoWeierstrass Theorem [Rud87, p. 40], [MH93, p. 153], we may assume that $\left(x_{n}\right)$ converges, to $x \in[0,1]$, say. By continuity, we have $f(x)=0$, which is absurd. Therefore, no such a function can exist.

2. $g(x)=|\sin 2 \pi x|$.

3. Suppose $g:(0,1) \rightarrow[0,1]$ is a continuous bijection. Let $x_{0}=g^{-1}(0)$ and $x_{1}=g^{-1}(1)$. Without loss of generality, assume $x_{0}<x_{1}$ (otherwise consider $1-g)$. By the Intermediate Value Theorem [Rud87, p. 93], we have $g\left(\left[x_{0}, x_{1}\right]\right)=$ $[0,1]$. As $x_{0}, x_{1} \in(0,1), g$ is not an injection, which contradicts our assumption.

Solution to 1.1.11: Let $A(c)$ denote the area the problem refers. The condition on $f^{\prime \prime}$ implies the convexity of $f$, so the graph of $f$ is always above any tangent to it, and we have

$$
A(c)=\int_{a}^{b}\left(f(x)-f(c)-f^{\prime}(c)(x-c)\right) d x .
$$

The derivative of $A$ is given by

$$
\begin{aligned}
A^{\prime}(c) &=-\int_{a}^{b} f^{\prime \prime}(c)(x-c) d x \\
&=-f^{\prime \prime}(c) \frac{b^{2}-a^{2}}{2}+(b-a) c f^{\prime}(c) \\
&=f^{\prime \prime}(c)(b-a)\left(c-\frac{a+b}{2}\right)
\end{aligned}
$$

so the minimum occurs at $c=(a+b) / 2$. As $A^{\prime}$ is an increasing function, $A$ is convex, so its minimum in $[a, b]$ corresponds to the only critical point in $(a, b)$.

Solution to 1.1.12: Using the parameterization

$$
x=a \cos t, y=b \sin t,
$$

a triple of points on the ellipse is given by

$$
\left(a \cos t_{i}, b \sin t_{i}\right), \quad i=1,2,3 .
$$

So the area of an inscribed triangle is given by

$$
\frac{1}{2}\left|\begin{array}{lll}
1 & a \cos t_{1} & b \sin t_{1} \\
1 & a \cos t_{2} & b \sin t_{2} \\
1 & a \cos t_{3} & b \sin t_{3}
\end{array}\right|=\frac{a b}{2}\left|\begin{array}{lll}
1 & \cos t_{1} & \sin t_{1} \\
1 & \cos t_{2} & \sin t_{2} \\
1 & \cos t_{3} & \sin t_{3}
\end{array}\right|
$$

which is $a b$ times the area of a triangle inscribed in the unit circle. In the case of the circle, among all inscribed triangles with a given base $2 w(0<w \leqslant 1)$, the one of maximum area is an isosceles triangle whose area equals

$$
g(w)=w\left(1+\sqrt{1-w^{2}}\right) .
$$

Using elementary calculus one finds that the maximum of $g$ on the interval $0 \leqslant w \leqslant 1$ occurs at $w=\sqrt{3} / 2$, corresponding to an equilateral triangle, and equals $3 \sqrt{3} / 4$. Alternatively, fixing one side of the triangle as the basis, we easily see that among all the inscribed triangles the one with the greatest area is isosceles because of the maximum height, showing that the angle at the basis is the same. Fixing another side we see that the triangle is indeed equilateral. Hence, the area is maximal when

$$
t_{2}=t_{1}+\frac{2 \pi}{3} \text { and } t_{3}=t_{2}+\frac{2 \pi}{3}
$$

that is, when the corresponding triangle inscribed in the unit circle is regular.

For the ellipse with semiaxes $a, b$, this corresponds to an inscribed triangle with maximum area equals $3 a b \sqrt{3} / 4$.

Solution 2. Let $f: \mathbb{R}^{2} \rightarrow \mathbb{R}^{2}$ be the stretch function $f(x, y)=(a x, b y)$. By a well know lemma in the proof of the Change of Variable for Integration [MH93, p. 524, Lemma 1], or the theorem itself when proven in its whole generality,

$$
\operatorname{vol}(f(T))=|\operatorname{det} f| \cdot \operatorname{vol} A
$$

since the determinant of $f$ is constant, following on the steps of the previous proof, the maximum for the area is achieved over the image of an equilateral triangle and it is equal to

$$
\operatorname{vol}(f(T))=a b \cdot \operatorname{vol}(T)=3 a b \frac{\sqrt{3}}{4} .
$$

Solution to 1.1.13: Assume that $a$ and $b$ are in $A$ and that $a<b$. Suppose $a<c<b$. Let $\left(x_{n}\right)$ and $\left(y_{n}\right)$ be sequences in $[0, \infty)$ tending to $+\infty$ such that $a=\lim _{n \rightarrow \infty} f\left(x_{n}\right)$ and $b=\lim _{n \rightarrow \infty} f\left(y_{n}\right)$. Deleting finitely many terms from each sequence, if necessary, we can assume $f\left(x_{n}\right)<c$ and $f\left(y_{n}\right)>c$ for every $n$. Then, by the Intermediate Value Theorem [Rud87, p. 93], there is for each $n$ a 
point $z_{n}$ between $x_{n}$ and $y_{n}$ such that $f\left(z_{n}\right)=c$. Since obviously $\lim _{n \rightarrow \infty} z_{n}=+\infty$,
it follows that $c$ is in $A$, as desired.

Solution to 1.1.14: For each $x>0$, the equation $x(1+\log (1 / \varepsilon \sqrt{x}))=1$ may be solved for $\varepsilon$ to obtain $\varepsilon=e /\left(x^{1 / 2} e^{1 / x}\right)$ that we will call $f(x)$.

For $x>0$ define $g(x)=x^{1 / 2} e^{1 / x}=e / f(x)$. Then $g^{\prime}(x)=\frac{1}{2} x^{-3 / 2} e^{1 / x}(x-2)$. Thus $g$ is strictly decreasing on $(0,2]$ and strictly increasing on $[2, \infty)$. Moreover $\lim _{x \rightarrow 0+} g(x)=\infty=\lim _{x \rightarrow \infty} g(x)$, and $g(2)>0$. Thus $\lim _{x \rightarrow 0+} f(x)=0=$ $\lim _{x \rightarrow \infty} f(x), f$ is strictly increasing on $(0,2]$ and strictly decreasing on $[2, \infty)$, $f$ is continuous on $(0, \infty)$, and $f(2)=\sqrt{e / 2}$. Denote the restrictions of $f$ to $(0,2)$ and $(2, \infty)$ by $f_{1}, f_{2}$ respectively. Then for each $0<\varepsilon<\sqrt{e / 2}$, the given equation has exactly two solutions, namely $x=f_{1}^{-1}(\varepsilon), f_{2}^{-1}(\varepsilon)$. The smaller solution is then $x=f_{1}^{-1}(\varepsilon)$. As $f_{1}$ is strictly increasing and continuous, with $f_{1}(0+)=0$, we deduce that $x(\varepsilon) \rightarrow 0$ as $\varepsilon \rightarrow 0+$.

Fix $s>0$. Now $\varepsilon^{-s} x(\varepsilon)=\left(e / x^{1 / 2} e^{1 / x}\right)^{-s} x=e^{-s} x^{1+s / 2} e^{s / x}$, where $x=$ $x(\varepsilon)=f_{1}^{-1}(\varepsilon)$. As $\varepsilon \rightarrow 0+, x \rightarrow 0+$. The function $x \mapsto e^{-s} x^{1+(s / 2)} e^{s / x}$ has limit $\infty$ as $x \rightarrow 0+$. This proves the second part.

Solution to 1.1.15: Let $g$ be a polynomial,

$$
g(x)=a_{0}+a_{1}(x-a)+a_{2}(x-a)^{2}+\cdots+a_{n}(x-a)^{n} .
$$

If we take

$$
a_{0}=\frac{1}{f(a)}, \quad a_{1}=-\frac{f^{\prime}(a) g(a)}{f(a)}, \quad a_{2}=-\frac{f^{\prime \prime}(a) g(a)+f^{\prime}(a) g^{\prime}(a)}{f(a)},
$$

a calculation shows that the requirements on $g$ are met.

Solution to 1.1.16: Suppose that all the roots of $p$ are real and let $\operatorname{deg} p=n$. We have $p(z)=\left(z-r_{1}\right)^{n_{1}}\left(z-r_{2}\right)^{n_{2}} \cdots\left(z-r_{k}\right)^{n_{k}}$, where $r_{1}<r_{2}<\cdots<r_{k}$ and $\sum n_{i}=n$. By differentiating this expression, we see that the $r_{i}$ 's are roots of $p^{\prime}$ of order $n_{i}-1$ when $n_{i}>1$. Summing these orders, we see that we have accounted for $n-k$ of the possible $n-1$ roots of $p^{\prime}$. Now by Rolle's Theorem [MH93, p. 200], for each $i, 1 \leqslant i \leqslant k-1$, there is a point $s_{i}, r_{i}<s_{i}<r_{i+1}$, such that $p^{\prime}\left(s_{i}\right)=0$. Thus, we have found the remaining $k-1$ roots of $p^{\prime}$, and they are distinct. Now we know that $a$ is a root of $p^{\prime}$ but not of $p$, so $a \neq r_{i}$ for all $i$. But $a$ is a root of $p^{\prime \prime}$, so $a$ is a multiple root of $p^{\prime}$; hence, $a \neq s_{i}$ for all $i$. Therefore, $a$ is not a root of $p^{\prime}$, a contradiction.

Solution to 1.1.17: Let $x \in \mathbb{R}$ and $h>0$. By the Taylor's Theorem [Rud87, pag. 110], there is a $w \in(x, x+2 h)$ such that

$$
f(x+2 h)=f(x)+2 h f^{\prime}(x)+2 h^{2} f^{\prime \prime}(w),
$$

or rewriting

$$
f^{\prime}(x)=\frac{f(x+2 h)-f(x)}{2 h}-h f^{\prime \prime}(w)
$$

Taking absolute values and applying our hypotheses, we get

$$
\left|f^{\prime}(x)\right| \leqslant \frac{A}{h}+h B .
$$

Now making $h=\sqrt{\frac{A}{B}}$ the conclusion follows:

$$
\left|f^{\prime}(x)\right| \leqslant 2 \sqrt{A B},
$$

Solution to 1.1.18: Consider the function $f(x)=\log x / x$. We have $a^{b}=b^{a}$ iff $f(a)=f(b)$. Now $f^{\prime}(x)=(1-\log x) / x^{2}$, so $f$ is increasing for $x<e$ and decreasing for $x>e$. For the above equality to hold, we must have $0<a<e$, so $a$ is either 1 or 2 , and $b>e$. For $a=1$, clearly there are no solutions, and for $a=2$ and $b=4$ works; since $f$ is decreasing for $x>e$, this is the only solution. Solution 2. Clearly, $a$ and $b$ have the same prime factors. Let $b=a+t$, for some positive integer $t$. Then $a^{a} a^{t}=b^{a}$ which implies that $a^{a} \mid b^{a}$, therefore $a \mid b$. We have then $b=k a$, for some integer $k>1$. Now $b^{a}=(k a)^{a}=a^{b}$ implies that $k$ is a power of $a$, so $b=a^{m}$ for some $m>1$. Now $b^{a}=a^{m a}=a^{a^{m}}$ exactly when $m a=a^{m}$, which can easily be seen to have the unique solution $a=m=2$. So $a=2$ and $b=2^{2}=4$.

Solution 3. Let $b=a(1+t)$, for some positive $t$. Then the equation $a^{b}=b^{a}$ is equivalent to any of the following

$$
\begin{aligned}
a^{a(1+t)} &=(a(1+t))^{a} \\
\left(a^{a}\right)^{1+t} &=a^{a}(1+t)^{a} \\
\left(a^{a}\right)^{t} &=(1+t)^{a} \\
a^{t} &=1+t .
\end{aligned}
$$

We have, by the power series expansion of the exponential function, that $e^{t}>1+t$ for positive $t$, so $a<e$. As $a=1$ is impossible, we conclude $a=2$. The original equation now becomes

$$
2^{b}=b^{2}
$$

which, considering the prime decomposition of $b$, clearly implies $b=4$.

Solution to 1.1.19: The equation can be rewritten as $a^{x^{b}}=x$, or

$$
\frac{\log x}{x^{b}}=\log a .
$$

There is thus a solution for $x$ if and only if $\log a$ is in the range of $x \mapsto(\log x) / x^{b}$. Using elementary calculus, we get that the range of this function is $(-\infty, 1 / b e]$. We conclude then that the original equation has a positive solution for $x$ if and only if $\log a \leqslant 1 / b e$, that is, if and only if $1<a<e^{1 / b e}$. Solution to 1.1.20: Let $f(x)=3^{x} x^{-3}$ for $x>0$. We have

$$
f^{\prime}(x)=\frac{3^{x}(x \log 3-3)}{x^{4}}>0 \text { for } x>\frac{3}{\log 3} .
$$

As $3 / \log 3<3<\pi$, we have $f(3)=1<f(\pi)=3^{\pi} / \pi^{3}$, that is, $\pi^{3}<3^{\pi}$.

Solution 2 . The same kind of analysis can be performed to the function $f(x)=$ $\ln (x) / x$, which is decreasing for $x>e$, as well as others like $g(x)=x^{3}-3^{x}$ and $h(x)=(3+x)^{(\pi-x)}$.

Solution to 1.1.21: Fix $a$ in $(1, \infty)$, and consider the function $f(x)=a^{x} x^{-a}$ on $(1, \infty)$, which we try to minimize. Since $\log f(x)=x \log a-a \log x$, we have

$$
\frac{f^{\prime}(x)}{f(x)}=\log a-\frac{a}{x},
$$

showing that $f^{\prime}(x)$ is negative on $\left(1, \frac{a}{\log a}\right)$ and positive on $\left(\frac{a}{\log a}, \infty\right)$. Hence, $f$ attains its minimum on $(1, \infty)$ at the point $x_{a}=\frac{a}{\log a}$, and

$$
\log f\left(x_{a}\right)=a-a \log \left(\frac{a}{\log a}\right)=a \log \left(\frac{e \log a}{a}\right) .
$$

The number $a$ thus has the required property if and only if $\frac{e \log a}{a} \geqslant 1$. To see which numbers $a$ in $(1, \infty)$ satisfy this condition, we consider the function $g(y)=\frac{\log y}{y}$ on $(1, \infty)$. We have

$$
g^{\prime}(y)=\frac{1-\log y}{y^{2}},
$$

from which we conclude that $g$ attains its maximum on $(1, \infty)$ at $y=e$, the maximum value being $g(e)=\frac{1}{e}$. Since $g(y)<\frac{1}{e}$ on $(1, \infty) \backslash\{e\}$, we conclude that $\frac{e \log a}{a}<1$ for $a$ in $(1, \infty)$, except for $a=e$. The number $a=e$ is thus the only number in $(1, \infty)$ with the required property.

Solution to 1.1.22: Let $g(x)=e^{x} / x^{t}$ for $x>0$. Since $g(x) \rightarrow \infty$ as $x \rightarrow 0$ and as $x \rightarrow \infty$, there must be a minimum value in between. At the minimum,

$$
g^{\prime}(x)=e^{x} x^{-t}(1-t / x)=0,
$$

so the minimum must occur at $x=t$, where

$$
g(x)=g(t)=e^{t} / t^{t}=(e / t)^{t} .
$$

Thus,

$$
e^{x} \geqslant\left(\frac{x e}{t}\right)^{t}
$$

and the right-hand side is strictly larger than $x^{t}$ if and only if $t<e$. Solution to 1.1.23: $f$ can be written as its second degree Maclaurin polynomial [PMJ85, p. 127] on this interval:

$$
f(x)=f(0)+f^{\prime}(0) x+\frac{f^{\prime \prime}(0)}{2} x^{2}+\frac{f^{(3)}(\xi)}{6} x^{3}
$$

where $\xi$ is between 0 and $x$. Letting $x=\pm \frac{1}{n}$ in this formula and combining the results, we get, for $n \geqslant 1$,

$$
\begin{aligned}
n\left(f\left(\frac{1}{n}\right)-f\left(-\frac{1}{n}\right)\right)-2 f^{\prime}(0) &=n\left(\frac{2 f^{\prime}(0)}{n}+\frac{f^{(3)}\left(\alpha_{n}\right)}{6 n^{3}}+\frac{f^{(3)}\left(\beta_{n}\right)}{6 n^{3}}\right)-2 f^{\prime}(0) \\
&=\frac{f^{(3)}\left(\alpha_{n}\right)+f^{(3)}\left(\beta_{n}\right)}{6 n^{2}}
\end{aligned}
$$

for some $\alpha_{n}, \beta_{n} \in[-1,1]$. As $f^{\prime \prime \prime}$ is continuous, there is some $M>0$ such that $\left|f^{\prime \prime \prime}(x)\right|<M$ for all $x \in[-1,1]$. Hence,

$$
\left|\sum_{n=1}^{\infty} n\left(f\left(\frac{1}{n}\right)-f\left(-\frac{1}{n}\right)\right)-2 f^{\prime}(0)\right| \leqslant \frac{M}{3} \sum_{n=1}^{\infty} \frac{1}{n^{2}}<\infty .
$$

Solution to 1.1.24: Using Taylor's Theorem [Rud87, p. 110],

$$
f(x+h)-f(x)=f^{\prime}(x) h+\frac{f^{\prime \prime}(z)}{2} h^{2} \quad \text { for some } \quad z \in(x, x+h)
$$

and similarly

$$
f(x-h)-f(x)=-f^{\prime}(x) h+\frac{f^{\prime \prime}(w)}{2} h^{2} \quad \text { for some } w \in(x-h, x) .
$$

The result follows by adding the two expressions, dividing by $h^{2}$, and taking the limit when $h \rightarrow 0$.

Solution to 1.1.25: 1 . The geometric construction

![](https://cdn.mathpix.com/cropped/2022_10_26_8d6ad07375e689af3a1cg-010.jpg?height=569&width=827&top_left_y=1941&top_left_x=627)

shows that

$$
\sin \theta \geqslant \frac{2}{\pi} \theta \text { for } 0 \leqslant \theta \leqslant \frac{\pi}{2} \text {. }
$$

An analytic proof can be written down from the fact that the sin function is concave down (second derivative negative) in the interval $0 \leqslant \theta \leqslant \frac{\pi}{2}$.

It can also be seen from the following geometric construction due to József Sándor [Sán88] and later rediscovered by Feng Yuefeng [Yue96]:

![](https://cdn.mathpix.com/cropped/2022_10_26_8d6ad07375e689af3a1cg-011.jpg?height=504&width=823&top_left_y=680&top_left_x=651)

$$
\begin{aligned}
O B=O M+M P \geqslant O A & \Longrightarrow P \widehat{B} Q \geqslant P \overparen{A A} Q \\
& \Longrightarrow \pi \sin \theta \geqslant 2 \theta \\
& \Longrightarrow \sin \theta \geqslant \frac{2 \theta}{\pi}
\end{aligned}
$$

\section{The integral inequality}

$$
\begin{aligned}
J &=\int_{0}^{\pi / 2} e^{-R \sin \theta} R d \theta \\
& \leqslant \int_{0}^{\pi / 2} e^{-2 R \theta / \pi} R d \theta \\
&=-\left.\pi e^{-2 R \theta / \pi}\right|_{0} ^{\pi / 2} \\
&<\pi
\end{aligned}
$$

is called Jordan's Lemma [MH87, p. 301]. Our limit is then

$$
\begin{aligned}
\lim _{R \rightarrow \infty} R^{\lambda} \int_{0}^{\pi / 2} e^{-R \sin \theta} d \theta &=\lim _{R \rightarrow \infty} R^{\lambda-1} \int_{0}^{\pi / 2} e^{-R \sin \theta} R d \theta \\
&<\lim _{R \rightarrow \infty} R^{\lambda-1} \pi=0 .
\end{aligned}
$$

Solution 2. We have

$$
R^{\lambda} \int_{0}^{\frac{\pi}{2}} e^{-R \sin \theta} d \theta=R^{\lambda} \int_{0}^{\frac{\pi}{3}} e^{-R \sin \theta} d \theta+R^{\lambda} \int_{\frac{\pi}{3}}^{\frac{\pi}{2}} e^{-R \sin \theta} d \theta
$$

As $\cos \theta \geqslant 1 / 2$ for $0 \leqslant \theta \leqslant \pi / 3$, and $\sin \theta$ is an increasing function on $[0, \pi / 2]$, we have

$$
\begin{aligned}
R^{\lambda} \int_{0}^{\frac{\pi}{2}} e^{-R \sin \theta} d \theta & \leqslant 2 R^{\lambda} \int_{0}^{\frac{\pi}{3}} e^{-R \sin \theta} \cos \theta d \theta+R^{\lambda} \int_{\frac{\pi}{3}}^{\frac{\pi}{2}} e^{-R \sin (\pi / 3)} d \theta \\
&=2 R^{\lambda-1}\left(1-e^{-R \sin (\pi / 3)}\right)+\frac{R^{\lambda} \pi}{6} e^{-R \sin (\pi / 3)} \\
&=o(1) \quad(R \rightarrow \infty)
\end{aligned}
$$

Solution to 1.1.26: Let $T=\mathbb{R} \backslash S, T$ is dense in $\mathbb{R}$ because each nonempty interval contains uncountably many numbers.

Fix $p \in T$ and define $F: \mathbb{R} \rightarrow \mathbb{R}$ by

$$
F(x)=\int_{p}^{x} f(t) d t .
$$

$F$ vanishes on $T$, so, as it is continuous, $F$ vanishes on $\mathbb{R}$. Therefore, we have $F^{\prime}=f \equiv 0$.

Solution to 1.1.27: $f^{\prime}(c)=0$ for some $c \in(0,1)$, by Rolle's Theorem [MH93, p. 200]. The concavity of $f$ shows that $f$ is increasing on $(0, c)$ and decreasing on $(c, 1)$. The arc length of the graph of $f$ on $[0, c]$ is

$$
L_{(0, c)}=\int_{0}^{c} \sqrt{1+f^{\prime}(x)^{2}} d x=\lim _{n \rightarrow \infty} \frac{c}{n} \sum_{k=0}^{n-1} \sqrt{1+f^{\prime}\left(\xi_{k}\right)^{2}}
$$

where $\xi_{k} \in(k c / n,(k+1) c / n)$. By the Mean Value Theorem [Rud87, p. 108] we can assume the $\xi_{k}$ 's satisfy

$$
f^{\prime}\left(\xi_{k}\right)=\frac{f((k+1) c / n)-f(k c / n)}{c / n} .
$$

We get

$$
\begin{aligned}
L_{(0, c)} &=\lim _{n \rightarrow \infty} \sum_{k=0}^{n-1} \sqrt{(c / n)^{2}+(f((k+1) c / n)-f(k c / n))^{2}} \\
& \leqslant \lim _{n \rightarrow \infty} \sum_{k=0}^{n-1}(c / n)+(f((k+1) c / n)-f(k c / n)) \\
&=c+f(c)
\end{aligned}
$$

since $f$ is increasing. A similar reasoning shows that $L_{(c, 1)} \leqslant 1-c+f(c)$. So $L_{[0,1]} \leqslant c+f(c)+1-c+f(c) \leqslant 3$.

Solution to 1.1.28: The convergence of $\int_{1}^{\infty}\left|f^{\prime}(x)\right| d x$ implies the convergence of $\int_{1}^{\infty} f^{\prime}(x) d x$, which implies that $\lim _{x \rightarrow \infty} f(x)$ exists. If that limit is not 0 , then $\sum_{n=1}^{\infty} f(n)$ and $\int_{1}^{\infty} f(x) d x$ both diverge. We may therefore, assume that $\lim _{x \rightarrow \infty} f(x)=0$. Then $\int_{\lfloor r\rfloor}^{r} f(x) d x \rightarrow 0$ as $r \rightarrow \infty$ (where $\lfloor r\rfloor$ is the greatest integer $\leqslant r$ ), implying that $\int_{1}^{\infty} f(x) d x$ converges if and only if $\lim _{n \rightarrow \infty} \int_{1}^{n} f(x) d x$ exists (where $n$ here tends to $\infty$ through integer values). In other words, the convergence of $\int_{1}^{\infty} f(x) d x$ is equivalent to the convergence of $\sum_{n=1}^{\infty} \int_{n}^{n+1} f(x) d x$. It will therefore, suffice to prove that

$$
\sum_{n=1}^{\infty}\left|\int_{n}^{n+1} f(x) d x-f(n)\right|<\infty .
$$

We have

$$
\begin{aligned}
\left|\int_{n}^{n+1} f(x) d x-f(n)\right| &=\left|\int_{n}^{n+1}(f(x)-f(n)) d x\right| \\
&=\left|\int_{n}^{n+1} \int_{n}^{x} f^{\prime}(t) d t d x\right| \leqslant \int_{n}^{n+1} \int_{n}^{n+1}\left|f^{\prime}(t)\right| d t d x \\
&=\int_{n}^{n+1}\left|f^{\prime}(t)\right| d t .
\end{aligned}
$$

Hence, $\sum_{n=1}^{\infty}\left|\int_{n}^{n+1} f(x) d x-f(n)\right| \leqslant \int_{1}^{\infty}\left|f^{\prime}(t)\right| d t<\infty$, as desired.

Solution to 1.1.29: We have

$$
|u(x)|=|u(x)-u(0)| \leqslant|x|
$$

and

$$
\left|u^{2}(x)-u(x)\right|=|u(x)||u(x)-1| \leqslant|x|(|x|+1)
$$

so

$$
|\varphi(u)| \leqslant \int_{0}^{1}\left|u(x)^{2}-u(x)\right| \leqslant \int_{0}^{1} x(x+1) d x=\frac{5}{6} .
$$

Equality can be achieved if $|u(x)|=x$ and $|u(x)-1|=x+1$. This is the case for $u(x)=-x$ which is in $E$.

Solution to 1.1.31: Let

$$
u(t)=1+2 \int_{0}^{t} f(s) d s .
$$

We have

$$
u^{\prime}(t)=2 f(t) \leqslant 2 \sqrt{u(t)},
$$

so

$$
\sqrt{u(t)}-1=\int_{0}^{t} \frac{u^{\prime}(s)}{2 \sqrt{u(s)}} d s \leqslant \int_{0}^{t} d s=t
$$

therefore,

$$
f(t) \leqslant \sqrt{u(t)} \leqslant 1+t .
$$

Solution to 1.1.32: We will show $b$ must be zero. By subtracting and multiplying by constants, we can assume $a=0 \leqslant b$. Given $\varepsilon>0$, choose $R \geqslant 1$ such that

$$
|\varphi(x)| \leqslant \varepsilon
$$

and

$$
\varphi^{\prime}(x) \geqslant b / 2 \geqslant 0
$$

for all $x \geqslant R$. By the Fundamental Theorem of Calculus [MH93, p. 209],

$$
\varphi(x)=\varphi(R)+\int_{R}^{x} \varphi^{\prime}(x) d x,
$$

so

$$
2 \varepsilon \geqslant \varphi(x)-\varphi(R) \geqslant \int_{R}^{x} \frac{b}{2} d x=(x-R) b / 2 .
$$

For $x=5 R$, we get

$$
b \leqslant \varepsilon / R \leqslant \varepsilon .
$$

Since $\varepsilon>0$ was arbitrary, we must have $b=0$.

Solution to 1.1.33: Let $0 \leqslant k_{1}<k_{2}<1$, then for all $x \in(0, \pi / 2)$,

$$
\begin{aligned}
-k_{1} \cos ^{2} x &>-k_{2} \cos ^{2} x \\
\sqrt{1-k_{1} \cos ^{2} x} &>\sqrt{1-k_{2} \cos ^{2} x} \\
\frac{1}{\sqrt{1-k_{1} \cos ^{2} x}} &<\frac{1}{\sqrt{1-k_{2} \cos ^{2} x}} \\
\int_{0}^{\pi / 2} \frac{1}{\sqrt{1-k_{1} \cos ^{2} x}} d x &<\int_{0}^{\pi / 2} \frac{1}{\sqrt{1-k_{2} \cos ^{2} x}} d x .
\end{aligned}
$$

Solution to 1.1.34: With the change of variables $y=x \sqrt{t}$, we have

$$
f(t)=\int_{-\infty}^{\infty} e^{-t x^{2}} d x=\int_{-\infty}^{\infty} e^{-y^{2}} \frac{d y}{\sqrt{t}}=\frac{1}{\sqrt{t}} \int_{-\infty}^{\infty} e^{-y^{2}} d y=\sqrt{\frac{\pi}{t}},
$$

so

$$
f^{\prime}(t)=-\frac{\sqrt{\pi}}{2} t^{-3 / 2} .
$$

Solution to 1.1.35: Let

$$
G(u, v, x)=\int_{v}^{u} e^{t^{2}+x t} d t .
$$

Then $F(x)=G(\cos x, \sin x, x)$, so

$$
\begin{aligned}
F^{\prime}(x) &=\frac{\partial G}{\partial u} \frac{\partial u}{\partial x}+\frac{\partial G}{\partial v} \frac{\partial v}{\partial x}+\frac{\partial G}{\partial x} \\
&=e^{u^{2}+x u}(-\sin x)-e^{\left(v^{2}+x v\right)} \cos x+\int_{v}^{u} t e^{t^{2}+x t} d t
\end{aligned}
$$

and

$$
F^{\prime}(0)=-1+\int_{0}^{1} t e^{t^{2}} d t=\frac{1}{2}(e-3) .
$$

Solution to 1.1.36: 1 . Let $f(z) \neq 0$. Then

$$
f(x) f(z)=f\left(\sqrt{x^{2}+z^{2}}\right)=f(-x) f(z),
$$

so $f(x)=f(-x)$ and $f$ is even.

Also, $f(0) f(z)=f(z)$, so $f(0)=1$.

2. We will show now that $f(\sqrt{n} x)=(f(x))^{n}$ for real $x$ and natural $n$, using the Induction Principle [MH93, p. 7]. The result is clear for $n=1$. Assume it holds for $n=k$. We have

$$
\begin{aligned}
f(\sqrt{k+1} x) &=f\left(\sqrt{(\sqrt{k x})^{2}+x^{2}}\right) \\
&=f(\sqrt{k x}) f(x) \\
&=(f(x))^{k} f(x) \\
&=(f(x))^{k+1}
\end{aligned}
$$

If $p, q \in \mathbb{N}$, then

$$
f(p)=f\left(p^{2} \cdot 1\right)=(f(1))^{p^{2}}
$$

and

$$
f(|p|)=f\left(\sqrt{p^{2}}\left|\frac{p}{q}\right|\right)=\left(f\left(\left|\frac{p}{q}\right|\right)\right)^{q^{2}}
$$

from which follows

$$
\left(f\left(\frac{p}{q}\right)\right)^{q^{2}}=(f(1))^{p^{2}} .
$$

- If $f(1)>0$, we have

$$
f\left(\frac{p}{q}\right)=(f(1))^{\frac{p^{2}}{q^{2}}},
$$

so, by continuity on $\mathbb{R}$,

$$
f(x)=(f(1))^{x^{2}} .
$$

- If $f(1)=0$, then $f$ vanishes on a dense set, so it vanishes everywhere, contradicting the hypothesis.

- To see that $f(1)<0$ cannot happen, consider $p$ even and $q$ odd. We get $f(p / q)>0$, so $f$ is positive on a dense set, and $f(1) \geqslant 0$.

Note that we used only the continuity of $f$ and its functional equation.

Differentiating, we easily check that $f$ satisfies the differential equation. The most general function satisfying all the conditions is then

$$
c^{x^{2}}
$$

with $0<c<1$.

Solution 2. 1. Let $x=y=0$. Then $f(0)^{2}=f(0)$, so $f(0)=0$ or 1. If $f(0)=0$, then $0=f\left(\sqrt{x^{2}}\right)$ for any $x$, so, in fact, $f(x)=0$ for all $x>0$. If $f(y) \neq 0$ for any $y$, then $f(x) f(y)=0$ implies $f(x)=0$ for all $x$, so $f(x)=0$ for all $x$ if $f(0)=0$. Since we assume $f$ is nonzero, we must have $f(0)=1$. Then evaluating at $y=0$ gives $f(x)=f\left(\sqrt{x^{2}}\right)=f(-x)$, so $f$ is an even function.

2. Differentiate with respect to $y$ to get

$$
f(x) f^{\prime}(y)=f^{\prime}(r) r_{y}
$$

where $r=\sqrt{x^{2}+y^{2}}$ and $r_{y}$ denotes the partial derivative of $r$ with respect to $y$. Differentiate again to get

$$
f(x) f^{\prime \prime}(y)=f^{\prime \prime}(r) r_{y}^{2}+f^{\prime}(r) r_{y y} .
$$

Since $r_{y}=y / r$ and $r_{y y}=x^{2} / r^{3}$, we get

$$
f^{\prime}(x)=f^{\prime \prime}(0) x f(x)
$$

for $y=0$. The solution of this differential equation is

$$
f(x)=e^{f^{\prime \prime}(0) x^{2} / 2}
$$

and since $f$ vanishes at infinity, we must have $f^{\prime \prime}(0) / 2=-\gamma<0$. Thus, $f(x)=e^{-\gamma x^{2}}$ for some positive constant $\gamma$.

Solution to 1.1.37: Let $C$ be the set of all sequences $\varepsilon=\left(\varepsilon_{n}\right)_{1}^{\infty}$ such that $\varepsilon_{n}$ equals 1 or $-1$; it is an uncountable set. For $\varepsilon$ in $C$ let $f_{\varepsilon}$ be the function on $[0,1]$ such that

- $f_{\varepsilon}(0)=0$

- $f_{\varepsilon}(1 / n)=\varepsilon_{n} / n$ for $n$ a positive integer;

- on each interval $[1 /(n+1), 1 / n], f_{\varepsilon}$ is the linear function whose values at the endpoints agree with those given by $(2)$. Each function $f_{\varepsilon}$ is continuous: continuity at points of $(0,1]$ is obvious, and continuity at 0 follows because $|f(x)| \leqslant x$. Each $f_{\varepsilon}$ takes rational values at rational points: if $x$ is a rational point between $a=1 /(n+1)$ and $b=1 / n$, then $x=(1-t) a+t b$ with $t$ a rational point of $[0,1]$, so $f_{\varepsilon}(x)=(1-t) f_{\varepsilon}(a)+t f_{\varepsilon}(b)$ is rational because $f_{\varepsilon}(a)$ and $f_{\varepsilon}(b)$ are. The functions $f_{\varepsilon}$ thus form an uncountable subset of $S$, showing that $S$ is uncountable.

\section{$1.2$ Limits and Continuity}

Solution to 1.2.1: Fix $x_{0}$. We have

$$
\left|g(x)-g\left(x_{0}\right)\right| \leqslant \int_{-\infty}^{\infty} \frac{\left|f(x, t)-f\left(x_{0}, t\right)\right|}{1+t^{2}} d x=\int_{-\infty}^{-R}+\int_{-R}^{R}+\int_{R}^{\infty},
$$

where $R$ is any positive number. Fix $\varepsilon>0$. Because of the boundedness of $f$ and the convergence of $\int_{-\infty}^{\infty} \frac{1}{1+t^{2}} d t$, we can choose $R$ so that the first and last integrals on the right are each less than $\varepsilon / 3$. Since $f$ is continuous on $\mathbb{R}^{2}$, it is uniformly continuous on the compact set $\left[x_{0}-1, x_{0}+1\right] \times[-R, R]$. Hence, there is a $\delta$ in $(0,1)$ such that $\left|f(x, t)-f\left(x_{0}, t\right)\right|<\varepsilon / 6 R$ for all $t$ in $[-R, R]$ whenever $\left|x-x_{0}\right|<\delta$. Hence, for $\left|x-x_{0}\right|<\delta$, the middle integral is also less than $\varepsilon / 3$, implying that $\left|g(x)-g\left(x_{0}\right)\right|<\varepsilon$. This establishes the continuity of $g$.

Solution to 1.2.2: Consider $f(x)=\sin x$. The Mean Value Theorem [Rud87, p. 108] implies that

$$
f(x)-f(y)=f^{\prime}(\xi)(x-y)=(\cos \xi)(x-y) \text { for some } \xi \in(0,1) \text {, }
$$

and since $|\cos \xi|<1$, this implies

$$
|f(x)-f(y)|<|x-y| \quad \text { whenever } x \neq y .
$$

However, if $M<1$ were such that

$$
|f(x)-f(y)|<M|x-y| \quad \text { for all } x, y \in I \text {, }
$$

then, putting $x=0$ and letting $y \rightarrow 0$, we would get $\left|f^{\prime}(0)\right| \leqslant M<1$, which contradicts the fact that $f^{\prime}(0)=1$.

Solution to 1.2.3: Suppose $f$ is not continuous at $\xi \in[0,1]$. Then, for some $\varepsilon>0$, there is a sequence $\left(x_{n}\right)$ converging to $\xi$ with $\left|f\left(x_{n}\right)-f(\xi)\right|>\varepsilon$ for all $n$. By the first condition, there is a sequence $\left(y_{n}\right)$ such that $y_{n}$ lies between $\xi$ and $x_{n}$ and $\left|f\left(y_{n}\right)-f(\xi)\right|=\varepsilon$. Then

$y_{n} \in f^{-1}(f(\xi)+\varepsilon) \cup f^{-1}(f(\xi)-\varepsilon) \quad \xi \notin f^{-1}(f(\xi)+\varepsilon) \cup f^{-1}(f(\xi)-\varepsilon)$ which contradicts the second condition.

Solution to 1.2.4: There exists $\delta>0$ such that $|x-y| \leqslant \delta$ implies $|f(x)-f(y)| \leqslant 1$. Let $B=1 / \delta$. Take any $x>0$ and let $n_{x}=\lfloor x / \delta\rfloor$, the greatest integer not exceeding $x / \delta$. Then

$$
\begin{aligned}
|f(x)| &=|f(x)-f(0)| \leqslant\left|f(x)-f\left(n_{x} \delta\right)\right|+\sum_{j=1}^{n_{x}}|f(j)-f((j-1) \delta)| \\
& \leqslant 1+n_{x} \leqslant 1+B x
\end{aligned}
$$

The proof for $x<0$ is similar.

Solution to 1.2.5: 1. Let $f_{1}$ be the restriction of $f$ to $[0,2]$. The ranges of $f$ and $f_{1}$ are the same, by periodicity, so $f$ attains its extrema.

2. Let $\delta>0$. $f_{1}$ is uniformly continuous, being a continuous function defined on a compact set, so there is $\varepsilon>0$ such that

$$
\left|f_{1}(a)-f_{1}(b)\right|<\delta \quad \text { for } a, b \in[0,2],|a-b|<\varepsilon
$$

Let $x, y \in \mathbb{R}$ with $|x-y|<\varepsilon$. Then, there are $x_{1}, x_{2}=x_{1}+1, y_{1}, y_{2}=y_{1}+1 \in$ $[0,2]$ with $f\left(x_{1}\right)=f\left(x_{2}\right)=f(x), f\left(y_{1}\right)=f\left(y_{2}\right)=f(y)$, and $\left|x_{i}-y_{j}\right|<\varepsilon$ for some choice of $i, j \in\{1,2\}$, and the conclusion follows.

3. Let $f$ attain its maximum and minimum at $\xi_{1}$ and $\xi_{2}$, respectively. Then

$$
f\left(\xi_{1}+\pi\right)-f\left(\xi_{1}\right) \leqslant 0 \quad \text { and } \quad f\left(\xi_{2}+\pi\right)-f\left(\xi_{2}\right) \geqslant 0
$$

as $f$ is continuous, the conclusion follows from the Intermediate Value Theorem [Rud87, p. 93].

Solution to 1.2.6: Let $\left(x_{n}\right)$ be a sequence of numbers in $[0,1)$ converging to zero. As $h$ is uniformly continuous, given $\delta>0$ we can find $\varepsilon>0$ such that $|h(x)-h(y)|<\delta$ if $|x-y|<\varepsilon$; therefore, we have

$$
\left|h\left(x_{n}\right)-h\left(x_{m}\right)\right|<\delta
$$

for $n$ and $m$ large enough. $\left(f\left(x_{n}\right)\right)$ is a Cauchy sequence then, so it converges, to $\xi$, say. If $\left(y_{n}\right)$ is another sequence with limit zero, a similar argument applied to $f\left(x_{1}\right), f\left(y_{1}\right), \ldots$ shows that $\lim f\left(y_{n}\right)=\xi$. The function $g:[0,1] \rightarrow \mathbb{R}$ given by

$$
g(x)=\left\{\begin{array}{cll}
h(x) & \text { for } & x \in[0,1) \\
\xi & \text { for } & x=0
\end{array}\right.
$$

is clearly the unique extension of $h$ to $[0,1]$.

Solution to 1.2.7: Let $\lim _{x \rightarrow \infty} f(x)=a$. Given $\varepsilon>0$, let $K>0$ satisfy $|f(x)-a|<\varepsilon / 2$ for $x \geqslant K$. If $x, y \geqslant K$ we have then

$$
|f(x)-f(y)| \leqslant|f(x)-a|+|a-f(y)| \leqslant \varepsilon
$$

The interval $[0, K]$ is compact, so $f$ is uniformly continuous there, that is, there exists $\delta>0$ such that, if $x, y \in[0, K]$ and $|x-y|<\delta$, then $|f(x)-f(y)|<\varepsilon / 2$. Finally, if $x \leqslant K, y>K$ verify $|x-y|<\delta$, we have

$$
|f(x)-f(y)| \leqslant|f(x)-f(K)|+|f(K)-f(y)|<\varepsilon,
$$

therefore, if $x, y \geqslant 0$ satisfy $|x-y|<\delta$, we have $|f(x)-f(y)|<\varepsilon$, as desired.

Solution to 1.2.8: Let $E$ be the set of discontinuities of $f$. We have $E=E_{1} \cup$ $E_{2} \cup E_{3} \cup E_{4}$, where

$$
\begin{array}{ll}
E_{1}=\{x \in E \mid f(x-)=f(x+)<f(x)\}, & E_{2}=\{x \in E \mid f(x-)>f(x+)\} \\
E_{3}=\{x \in E \mid f(x-)=f(x+)>f(x)\}, & E_{4}=\{x \in E \mid f(x-)<f(x+)\} .
\end{array}
$$

For $x \in E_{1}$, let $a_{x} \in \mathbb{Q}$ be such that $f(x-)<a_{x}<f(x+)$. Now let's take $b_{x}, c_{x} \in \mathbb{Q}$ in such a way that $b_{x}<x<c_{x}$ and

$$
b_{x}<t<c_{x}, x \neq t \text { implies } f(t)<a_{x} .
$$

This $\operatorname{map} \varphi: E_{1} \rightarrow \mathbb{Q}^{3}$ given by $x \mapsto\left(a_{x}, b_{x}, c_{x}\right)$ is injective since $\left(a_{x}, b_{x}, c_{x}\right)=$ $\left(a_{y}, b_{y}, c_{y}\right)$ implies $f(y)<a_{x}<f(y)$ for $x \neq y$. So $E_{1}$ is, at most, countable.

For $x \in E_{2}$, take $a_{x} \in \mathbb{Q}$ with $f(x-)>a_{x}>f(x+)$ and choose $b_{x}, c_{x} \in \mathbb{Q}$ such that $b_{x}<x<c_{x}$ and

$$
b_{x}<t<x \text { implies } f(t)>a_{x}
$$

and

$$
t<c_{x} \text { implies } f(t)<a_{x} ;
$$

this map is an injection $E_{2} \rightarrow \mathbb{Q}^{3}$, so $E_{2}$ is, at most, countable.

Similar methods lead to analogous results for $E_{3}$ and $E_{4}$. As the union of countable sets is countable, the result follows.

Solution 2. Define the function $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ by

$$
\sigma(x)=\max \{|f(x)-f(x+)|,|f(x)-f(x-)|\} ;
$$

observe that $\sigma(x)>0$ if and only if $x$ is a discontinuity of $f$.

For each $n \in \mathbb{N}$, let the set $D_{n}$ be given by

$$
D_{n}=\left\{x \in \mathbb{R} \mid \sigma(x) \geqslant \frac{1}{n}\right\} .
$$

It is clear that the set of discontinuities of $f$ is $D=\bigcup_{n=1}^{\infty} D_{n}$. We shall prove that each $D_{n}$ has no accumulation points, so, it is countable. If $a \in D_{n}$, using the fact that $f(a+)=\lim _{x \rightarrow a+} f(x)$, we can find $\delta>0$ such that, for all $x$, $a<x<a+\delta$, we have

$$
f(a+)-\frac{1}{4 n}<f(x)<f(a+)+\frac{1}{4 n},
$$

that is, for every point in this interval, $\sigma(x) \leqslant 1 / 2 n$. In the same fashion, we can find an open set $a-\delta<x<a$ such that no point is in $D_{n}$, showing that $D_{n}$ is made up of isolated points so it is countable, and so is $D$.

Solution to 1.2.9: By Problem 1.2.8, it is enough to show that $f$ has lateral limits at all points. We have, for any $x \in \mathbb{R}$,

$$
-\infty<\sup _{y<x}\{f(y)\}=f(x-) \leqslant f(x+)=\inf _{y<x}\{f(y)\}<\infty
$$

since $f$ is an increasing function.

Solution to 1.2.10: Fix $\varepsilon>0$. For each $x \in[0,1]$, let $\delta_{x}$ be as in the hypothesis and $I_{x}=\left(x-\delta_{x}, x+\delta_{x}\right)$. The open intervals $\left\{I_{x}\right\}$ cover $[0,1]$ so, by compactness and the Heine-Borel Theorem [Rud87, p. 30], we can choose a finite subcover

$$
[0,1] \subset I_{x_{1}} \cup I_{x_{2}} \cup \cdots \cup I_{x_{n}} .
$$

Let $M=\max \left\{f\left(x_{i}\right)+\varepsilon\right\}$. If $x \in[0,1]$ then $f(x)<M$ and $f$ is bounded from above.

Let $N$ be the least upper bound of $f$ on $[0,1]$. Then there is a sequence of points $\left(x_{n}\right)$ such that $\left(f\left(x_{n}\right)\right)$ tends to $N$ from below. Since $[0,1]$ is compact, by the Bolzano-Weierstrass Theorem [Rud87, p. 40], [MH93, p. 153], $\left(x_{n}\right)$ has a convergent subsequence, so (by passing to a subsequence) we may assume that $\left(x_{n}\right)$ converges to some $p \in[0,1]$. By the upper semicontinuity of $f$ and the convergence of $\left(f\left(x_{n}\right)\right.$ ), we have, for $n$ sufficiently large, $f\left(x_{n}\right)<f(p)+\varepsilon$ and $N<f\left(x_{n}\right)+\varepsilon$. Combining these, we get $f(p) \leqslant N<f(p)+2 \varepsilon$. Since this holds for all $\varepsilon>0, f(p)=N$.

Solution to 1.2.11: Suppose $f: \mathbb{R} \rightarrow \mathbb{R}$ is continuous, maps open sets to open sets but is not monotonic. Without loss of generality assume there are three real numbers $a<b<c$ such that $f(a)<f(b)>f(c)$. By Weierstrass Theorem [MH93, p. 189], $f$ has a maximum, $M$, in $[a, c]$, which cannot occur at $a$ or $b$. Then $f((a, c))$ cannot be open, since it contains $M$ but does not contain $M+\varepsilon$ for any positive $\varepsilon$. We conclude then that $f$ must be monotonic.

Solution to 1.2.12: The inequality given implies that $f$ is one-to-one, so $f$ is strictly monotone and maps open intervals onto open intervals, so $f(\mathbb{R})$ is open.

Let $z_{n}=f\left(x_{n}\right)$ be a sequence in $f(\mathbb{R})$ converging to $z \in \mathbb{R}$. Then $z_{n}$ is Cauchy, and, by the stated inequality, so is $x_{n}$. Let $x=\lim x_{n}$. By continuity we have $f(x)=f\left(\lim x_{n}\right)=\lim f\left(x_{n}\right)=z$ so $f(\mathbb{R})$ is also closed. Thus, $f(\mathbb{R})=\mathbb{R}$.

Solution to 1.2.13: 1. For $\varepsilon>0$ let

$$
L=\max _{x \in[0,1]}(|f(x)|+1) \text { and } 0<\delta<\min \left\{\frac{\varepsilon}{2 L}, 1\right\} .
$$

We have

$$
\left|\int_{1-\delta}^{1} x^{n} f(x) d x\right| \leqslant \int_{1-\delta}^{1} x^{n}|f(x)| d x \leqslant L \delta \leqslant \frac{\varepsilon}{2}
$$

and

$$
\left|\int_{0}^{1-\delta} x^{n} f(x) d x\right| \leqslant \int_{0}^{1-\delta}(1-\delta)^{n}|f(x)| d x \leqslant L \delta^{n+1}
$$

so

$$
\lim _{n \rightarrow \infty} \int_{0}^{1} x^{n} f(x) d x=0
$$

2. We will show that

$$
\lim _{n \rightarrow \infty} n \int_{0}^{1} x^{n}(f(x)-f(1)) d x=0
$$

For $\varepsilon>0$ let $\delta$ be such that $|f(x)-f(1)|<\varepsilon / 2$ if $x \in[1-\delta, 1]$. We have

$$
\left|n \int_{1-\delta}^{n} x^{n}(f(x)-f(1)) d x\right| \leqslant n \int_{1-\delta}^{1} x^{n}|f(x)-f(1)| d x \leqslant n \int_{1-\delta}^{1} x^{n} \frac{\varepsilon}{2} d x \leqslant \frac{\varepsilon}{2}
$$

and, letting $L=\sup _{x \in[0,1]}|f(x)-f(1)|$,

$$
\left|n \int_{0}^{1-\delta} x^{n}(f(x)-f(1)) d x\right| \leqslant n \int_{0}^{1-\delta} x^{n} L d x=n \frac{(1-\delta)^{n+1}}{n+1}
$$

and the result follows.

Now it suffices to notice that

$$
n \int_{0}^{1} x^{n} f(x) d x=n \int_{0}^{1} x^{n}(f(x)-f(1)) d x+n \int_{0}^{1} f(1) x^{n} d x
$$

Solution to 1.2.14: Suppose that $f$ is not continuous. Then there exist $\varepsilon>0$, $x \in[0,1]$, and a sequence $\left(x_{n}\right)$ tending to $x$ such that $\left|f(x)-f\left(x_{n}\right)\right| \geqslant \varepsilon$ for all $n$. Consider the sequence $\left(\left(x_{n}, f\left(x_{n}\right)\right)\right)$ in $G_{f}$. Since the unit square is compact, by Bolzano-Weierstrass Theorem [Rud87, p. 40], [MH93, p. 153], this sequence has a convergent subsequence; using this subsequence, we may assume that $\left(\left(x_{n}, f\left(x_{n}\right)\right)\right)$ converges to some point $(y, z)$. Then we must have the sequence $\left(x_{n}\right)$ converging to $y$; so, by the uniqueness of limits, $x=y$. Since $G_{f}$ is closed, we must have $z=f(x)$. Hence, $\left(f\left(x_{n}\right)\right)$ converges to $f(x)$, contradicting our assumption.

The converse of the implication is also true, see the Solution to Problem 2.1.2.

Solution to 1.2.15: For each $y \in[0,1]$, consider the function $g_{y}(x)=f(x, y)$. Then $g(x)=\sup _{y}(x)$. The family $\left\{g_{y}\right\}$ is equicontinuous because $f$ is uniformly continuous. It suffices then to show that the pointwise supremum of an equicontinuous family of functions is continuous. Let $\varepsilon>0, x_{0} \in[0,1]$. There is yo such that

$$
g_{y_{0}}\left(x_{0}\right) \leqslant g\left(x_{0}\right)<g_{y_{0}}\left(x_{0}\right)+\varepsilon
$$

Let $\delta$ be positive such that if $|r-s|<\delta$, then $\left|g_{y}(r)-g_{y}(s)\right|<\varepsilon$ for all $y$, and $\left|x_{0}-x_{1}\right|<\delta$. For some $y_{1}$, we have that

$$
g_{y_{3}}\left(x_{1}\right) \leqslant g\left(x_{1}\right)<g_{y_{1}}\left(x_{1}\right)+\varepsilon .
$$

Further, by equicontinuity of $\left\{g_{y}\right\}$, we have the two inequalities $\left|g_{y_{0}}\left(x_{0}\right)-g_{y_{0}}\left(x_{1}\right)\right|<\varepsilon$ and $\left|g_{y_{1}}\left(x_{0}\right)-g_{y_{1}}\left(x_{1}\right)\right|<\varepsilon$. By combining them we get

$$
g_{y_{0}}\left(x_{0}\right)<g_{y_{0}}\left(x_{1}\right)+\varepsilon<g\left(x_{1}\right)+\varepsilon<g_{y_{1}}\left(x_{1}\right)+2 \varepsilon
$$

and

$$
g_{y_{1}}\left(x_{1}\right)<g_{y_{1}}\left(x_{0}\right)+\varepsilon<g\left(x_{0}\right)+\varepsilon<g_{y_{0}}\left(x_{0}\right)+2 \varepsilon .
$$

These two inequalities imply $\left|g_{y_{1}}\left(x_{1}\right)-g_{y_{0}}\left(x_{0}\right)\right|<2 \varepsilon$. This, combined with the first two inequalities, shows that $\left|g\left(x_{0}\right)-g\left(x_{1}\right)\right|<3 \varepsilon$. Since this holds for all $\varepsilon$ and $x_{0}$ and all $x_{1}$ close to $x_{0}, g$ is continuous.

Solution to 1.2.16: We need only show that $f^{-1}(F)$ is closed whenever $F$ is closed. Let $F$ be a closed subset of $\mathbb{R}$. Let $\left(x_{n}\right)_{1}^{\infty}$ be a convergent sequence in $f^{-1}(F)$ with limit $x_{0}$. We need only show that $x_{0}$ is in $f^{-1}(F)$. For $n>0$ let $y_{n}=f\left(x_{n}\right)$. The sequence $\left(y_{n}\right)_{1}^{\infty}$ is in $F$ and it is bounded (since the convergent sequence $\left(x_{n}\right)_{1}^{\infty}$ is). Passing to a subsequence, we can assume the sequence $\left(y_{n}\right)_{1}^{\infty}$ converges, say to $y_{0}$, which lies in $F$ because $F$ is closed. The set $K=\left\{y_{0}, y_{1}, y_{2}, \ldots\right\}$ is then compact, so $f^{-1}(K)$ is closed. Hence $f^{-1}(K)$ contains its limit points. But the sequence $\left(x_{n}\right)_{1}^{\infty}$ lies in $f^{-1}(K)$ and converges to $x_{0}$. Therefore $x_{0}$ is in $f^{-1}(K)$, and so also in $f^{-1}(F)$, as desired.

\subsection{Sequences, Series, and Products}

Solution to 1.3.1: $A_{1}^{n} \leqslant A_{1}^{n}+\cdots+A_{k}^{n} \leqslant k A_{1}^{n}$, so we have

$$
A_{1}=\lim _{n \rightarrow \infty}\left(A_{1}^{n}\right)^{1 / n} \leqslant \lim _{n \rightarrow \infty}\left(A_{1}^{n}+\cdots+A_{k}^{n}\right)^{1 / n} \leqslant \lim _{n \rightarrow \infty}\left(k A_{1}^{n}\right)^{1 / n}=A_{1} .
$$

showing that the limit equals $A_{1}$.

Solution to 1.3.2: Let $p_{1}=1, p_{2}=(2 / 1)^{2}, p_{3}=(3 / 2)^{3}, \ldots, p_{n}=(n /(n-1))^{n}$. Then

$$
\frac{p_{1} p_{2} \cdots p_{n}}{n}=\frac{n^{n}}{n !},
$$

and since $p_{n} \rightarrow e$, we have $\lim \left(n^{n} / n !\right)^{1 / n}=e$ as well (using the fact that $\left.\lim n^{1 / n}=1\right)$.

Solution 2. As the exponential is a continuous function, $L=\exp \left(\lim _{n \rightarrow \infty} L_{n}\right)$ where

$$
L_{n}=\log n-\frac{1}{n}(\log 1+\log 2+\cdots+\log n) .
$$

Since

$$
\log 1+\log 2+\cdots+\log (n-1) \leqslant \int_{1}^{n} \log x d x=n \log n-n+1,
$$

we have

$L_{n} \geqslant(1-1 / n) \log n-\log n+1-1 / n=1-(1+\log n) / n \rightarrow 1$ as $n \rightarrow \infty$

On the other hand,

$$
\log 1+\log 2+\cdots+\log n \geqslant \int_{1}^{n} \log x d x=n \log n-n+1,
$$

so

$$
L_{n} \leqslant \log n-(n \log n-n+1) / n=1-1 / n .
$$

Hence,

$$
1-(1+\log n) / n \leqslant L_{n} \leqslant 1-1 / n,
$$

so $L_{n} \rightarrow 1$ and $L=\exp (1)=e$.

Solution to 1.3.3: Obviously, $x_{n} \geqslant 1$ for all $n$; so, if the limit exists, it is $\geqslant 1$, and we can pass to the limit in the recurrence relation to get

$$
x_{\infty}=\frac{3+2 x_{\infty}}{3+x_{\infty}} \text {; }
$$

in other words, $x_{\infty}^{2}+x_{\infty}-3=0$. So $x_{\infty}$ is the positive solution of this quadratic equation, that is, $x_{\infty}=\frac{1}{2}(-1+\sqrt{13})$.

To prove that the limit exists, we use the recurrence relation to get

$$
\begin{aligned}
x_{n+1}-x_{n} &=\frac{3+2 x_{n}}{3+x_{n}}-\frac{3+2 x_{n-1}}{3+x_{n-1}} \\
&=\frac{3\left(x_{n}-x_{n-1}\right)}{\left(3+x_{n}\right)\left(3+x_{n+1}\right)}
\end{aligned}
$$

Hence, $\left|x_{n+1}-x_{n}\right| \leqslant \frac{1}{3}\left|x_{n}-x_{n-1}\right|$. Iteration gives

$$
\left|x_{n+1}-x_{n}\right| \leqslant 3^{-n}\left|x_{1}-x_{0}\right|=\frac{1}{3^{n} \cdot 4} .
$$

The series $\sum_{n=1}^{\infty}\left(x_{n+1}-x_{n}\right)$, of positive terms, is dominated by the convergent series $\frac{1}{4} \sum_{n=1}^{\infty} 3^{-n}$ and so converges. We have $\sum_{n=1}^{\infty}\left(x_{n+1}-x_{n}\right)=\lim x_{n}-x_{1}$ and we are done.

Solution 2. To prove the existence of the limit it is enough to notice that if $g$ is defined by

$$
g(x)=\frac{3+2 x}{3+x}
$$

we have

$$
\left|g^{\prime}(x)\right| \leqslant \frac{3}{16} \leqslant 1 \text { for } x \geqslant 1
$$

and apply the Fixed Point Theorem [Rud87, p. 220].

Solution to 1.3.4: We prove, by induction, that $0<x_{n}<1 / 2$ for $n \geqslant 1$. First, $0<x_{1}=1 / 3<1 / 2$. Suppose for some $n \geqslant 1$, that $0<x_{n}<1 / 2$. Then $2 / 5<x_{n+1}=1 /\left(2+x_{n}\right)<1 / 2$. This completes the induction.

Let $f(x)=1 /(2+x)$. The equation $f(x)=x$ has a unique solution in the interval $0<x<1 / 2$ given by $p=\sqrt{2}-1$. Moreover, $\left|f^{\prime}(x)\right|=1 /(2+x)^{2}<$ $1 / 4$ for $0<x<1 / 2$. Thus, for $n \geqslant 1$,

$$
\begin{aligned}
\left|x_{n+1}-p\right| &=\left|f\left(x_{n}\right)-f(p)\right| \\
& \leqslant \frac{1}{4}\left|x_{n}-p\right|,
\end{aligned}
$$

by the Mean Value Theorem.

Iterating, we obtain

$$
\begin{aligned}
\left|x_{n+1}-p\right| & \leqslant\left(\frac{1}{4}\right)^{2}\left|x_{n-1}-p\right| \\
& \leqslant \cdots \\
& \leqslant\left(\frac{1}{4}\right)^{n}\left|x_{1}-p\right| .
\end{aligned}
$$

Hence the sequence $x_{n}$ tends to the limit $\sqrt{2}-1$.

Solution 2. Define $f(x)=1 /(2+x)$ for $0 \leqslant x \leqslant 1$. Then $f$ maps the closed interval $[0,1]$ to $[1 / 3,1 / 2] \subset[0,1]$. Also $\left|f^{\prime}(x)\right|=\left|\frac{1}{(2+x)^{2}}\right| \leqslant \frac{1}{4}$ for $x \in$ $[0,1]$. Therefore $f:[0,1] \rightarrow[0,1]$ is a contraction map with Lipschitz constant $1 / 4<1$, so $f$ has a unique fixed point $y$, and the sequence $x_{n+1}=f\left(x_{n}\right)$ defined above converges to $y$. We have $y=1 /(2+y)$ or $y^{2}+2 y=1$, whence $y=\sqrt{2}-1$ (since $0 \leqslant y \leqslant 1)$

Solution to 1.3.5: By the given relation $x_{n+1}-x_{n}=(\alpha-1)\left(x_{n}-x_{n-1}\right)$. Therefore, by the Induction Principle [MH93, p. 7], we have $x_{n}-x_{n-1}=(\alpha-1)^{n-1}\left(x_{1}-x_{0}\right)$, showing that the sequence is Cauchy and then converges. Hence,

$$
x_{n}-x_{0}=\sum_{k=1}^{n}\left(x_{k}-x_{k-1}\right)=\left(x_{1}-x_{0}\right) \sum_{k=1}^{n}(\alpha-1)^{k-1} \text {. }
$$

Taking limits, we get

$$
\lim _{n \rightarrow \infty} x_{n}=\frac{(1-\alpha) x_{0}+x_{1}}{2-\alpha} .
$$

Solution 2. The recurrence relation can be expressed in matrix form as

$$
\left(\begin{array}{c}
x_{n+1} \\
x_{n}
\end{array}\right)=A\left(\begin{array}{c}
x_{n} \\
x_{n-1}
\end{array}\right), \quad \text { where } \quad A=\left(\begin{array}{cc}
\alpha & 1-\alpha \\
1 & 0
\end{array}\right)
$$

Thus,

$$
\left(\begin{array}{c}
x_{n+1} \\
x_{n}
\end{array}\right)=A^{n}\left(\begin{array}{c}
x_{1} \\
x_{0}
\end{array}\right) .
$$

A calculation shows that the eigenvalues of $A$ are 1 and $\alpha-1$, with corresponding eigenvectors $v_{1}=(1,1)^{t}$ and $v_{2}=(\alpha-1,1)^{t}$. A further calculation shows that

$$
\left(\begin{array}{l}
x_{1} \\
x_{0}
\end{array}\right)=\left(\frac{(1-\alpha) x_{0}+x_{1}}{2-\alpha}\right) v_{1}+\left(\frac{x_{0}-x_{1}}{2-\alpha}\right) v_{2} \text {. }
$$

Hence,

$$
\left(\begin{array}{c}
x_{n+1} \\
x_{n}
\end{array}\right)=A^{n}\left(\begin{array}{c}
x_{1} \\
x_{0}
\end{array}\right)=\frac{(1-\alpha) x_{0}+x_{1}}{2-\alpha} v_{1}+(\alpha-1)^{n} \frac{x_{0}-x_{1}}{2-\alpha} v_{2} .
$$

Since $|\alpha-1|<1$ we have $\lim _{n \rightarrow \infty}(\alpha-1)^{n}=0$, and we can conclude that

$$
\lim _{n \rightarrow \infty} x_{n}=\frac{(1-\alpha) x_{0}+x_{1}}{2-\alpha}
$$

Solution to 1.3.6: The given relation can be written in matrix form as $\left(\begin{array}{c}x_{n+1} \\ x_{n}\end{array}\right)=$ $A\left(\begin{array}{c}x_{n-1} \\ x_{n}\end{array}\right)$, where $A=\left(\begin{array}{cc}2 c & -1 \\ 1 & 0\end{array}\right)$. The required periodicity holds if and only if $A^{k}=$ $\left(\begin{array}{ll}1 & 0 \\ 0 & 1\end{array}\right)$. The characteristic polynomial of $A$ is $\lambda^{2}-2 c \lambda+1$, so the eigenvalues of $A$ are $c \pm \sqrt{c^{2}-1}$. A necessary condition for $A^{k}=\left(\begin{array}{ll}1 & 0 \\ 0 & 1\end{array}\right)$ is that the eigenvalues of $A$ be $k^{t h}$ roots of unity, which implies that $c=\cos \left(\frac{2 \pi j}{k}\right), j=0,1, \ldots,\left\lfloor\frac{k}{2}\right\rfloor$. If $c$ has the preceding form and $0<j<\frac{k}{2}$ (i.e., $-1<c<1$ ), then the eigenvalues of $A$ are distinct (i.e., $A$ is diagonalizable), and the equality $A^{k}=\left(\begin{array}{l}1 \\ 0 \\ 0\end{array}\right)$ holds. If $c=1$ or $-1$, then the eigenvalues of $A$ are not distinct, and $A$ has the Jordan Canonical Form [HK61, p. 247]

$\left(\begin{array}{ll}1 & 1 \\ 0 & 1\end{array}\right)$ or $\left(\begin{array}{cc}-1 & 1 \\ 0 & -1\end{array}\right)$, respectively, in which case $A^{k} \neq\left(\begin{array}{ll}1 & 0 \\ 0 & 1\end{array}\right)$. Hence, the desired periodicity holds if and only if $c=\cos \left(\frac{2 \pi j}{k}\right)$, where $j$ is an integer, and $0<j<$ $k / 2$

Solution to 1.3.7: If $\lim x_{n}=x_{\infty} \in \mathbb{R}$, we have $x_{\infty}=a+x_{\infty}^{2} ;$ so

$$
x_{\infty}=\frac{1 \pm \sqrt{1-4 a}}{2}
$$

and we must have $a \leqslant 1 / 4$.

Conversely, assume $0<a \leqslant 1 / 4$. As $x_{n+1}-x_{n}=x_{n}^{2}-x_{n-1}^{2}$, we conclude, by the Induction Principle [MH93, p. 7], that the given sequence is nondecreasing. Also,

$$
x_{n+1}=a+x_{n}^{2}<\frac{1}{4}+\frac{1}{4}=\frac{1}{2}
$$

if $x_{n}<1 / 2$, which shows that the sequence is bounded. It follows that the sequence converges when $0<a \leqslant 1 / 4$.

Solution to 1.3.8: First we show, by induction, that the sequence $\left(x_{n}\right)$ is bounded. For that, choose $M$ large so that $\max \left|x_{1}\right|,\left|2 x_{n+1}-x_{n}\right| \leq M$, for all $n$.

$$
\left|x_{n+1}\right|=\left|\frac{x_{n}+\left(2 x_{n+1}-x_{n}\right)}{2}\right| \leq \frac{1}{2}\left(\left|x_{n}\right|+\left|2 x_{n+1}-x_{n}\right|\right) \leq M
$$

showing that $\left(x_{n}\right)$ is bounded. Now to compute the limit write

$$
x_{n+1}=\frac{x_{n}+\left(2 x_{n+1}-x_{n}\right)}{2}
$$

and taking the lim sup, we have

$$
\limsup x_{n} \leq \frac{\lim \sup x_{n}+x}{2}
$$

showing that lim sup $x_{n} \leq x$. In the same way we obtain liminf $x_{n} \geq x$, showing that $\lim x_{n}=x$.

Solution to 1.3.9: Let $y_{n}=x_{n} / \sqrt{a}$. It will be shown that $y_{n} \rightarrow 1$. The sequence $\left(y_{n}\right)$ satisfies the recurrence relation

$$
y_{n}=\frac{1}{2}\left(y_{n-1}+\frac{1}{y_{n-1}}\right) .
$$

We have $y_{n} \geqslant 1$ for all $n$ because for any positive $b$

$$
\frac{1}{2}\left(b+\frac{1}{b}\right)-1=\frac{1}{2}\left(\sqrt{b}-\frac{1}{\sqrt{b}}\right)^{2} \geqslant 0 .
$$

Hence, for every $n$,

$$
y_{n-1}-y_{n}=\frac{1}{2}\left(y_{n-1}-\frac{1}{y_{n-1}}\right) \geqslant 0,
$$

so the sequence $\left(y_{n}\right)$ is nonincreasing. As it is also bounded below, it converges. Let $y_{n} \rightarrow y$. The recurrence relation gives

$$
y=\frac{1}{2}\left(y+\frac{1}{y}\right),
$$

i.e., $y=1 / y$, from which $y=1$ follows, since $y \geqslant 1$.

Solution to 1.3.10: Clearly, $0 \leqslant x_{n+1}=x_{n}\left(1-x_{n}^{n}\right) \leqslant x_{n} \leqslant \cdots \leqslant x_{1}$ for all $n$. Thus,

$$
x_{n+1}=x_{n}\left(1-x_{n}^{n}\right) \geqslant x_{n}\left(1-x_{1}^{n}\right),
$$

and therefore

$$
x_{n} \geqslant x_{1} \prod_{1}^{n}\left(1-x_{1}^{k}\right)=x_{1} \exp \left(\sum_{k=1}^{n} \log \left(1-x_{1}^{k}\right)\right) \text {. }
$$

Since $\log \left(1-x_{1}^{k}\right)=O\left(x_{1}^{k}\right)$ as $k \rightarrow \infty$, the sum converges to a finite value $L$ as $n \rightarrow \infty$ and we get

$$
\liminf _{n \rightarrow \infty} x_{n} \geqslant x_{1} \exp (L)>0 .
$$

Solution to 1.3.11: 1. We have

$$
f(x)=\frac{1}{2}-\left(x-\frac{1}{2}\right)^{2}
$$

so $x_{n}$ is bounded by $1 / 2$ and, by the Induction Principle [MH93, p. 7], nondecreasing. Let $\lambda$ be its limit. Then

$$
\lambda=\frac{1}{2}-\left(\lambda-\frac{1}{2}\right)^{2}
$$

and, as the sequence takes only positive values,

$$
\lambda=\frac{1}{2} \text {. }
$$

2. It is clear, from the expression for $f$ above, that

$$
f(x) \leqslant x \text { for } x \leqslant-\frac{1}{2}
$$

and

$$
f(x) \leqslant-\frac{1}{2} \text { for } x \geqslant \frac{3}{2}
$$

therefore, the sequence diverges for such initial values.

On the other hand, if $|x-1 / 2|<1$, we get

$$
\left|f(x)-\frac{1}{2}\right|<\left|x-\frac{1}{2}\right|
$$

so, for these initial values, we get

$$
\left|x_{n+1}-\frac{1}{2}\right|<\left|x-\frac{1}{2}\right|^{n}=o(1)
$$

Solution to 1.3.12: Suppose that $\lim f_{n+1} / f_{n}=a<\infty . a \geqslant 1$ since the the sequence $f_{n}$ is increasing. We have

$$
\frac{f_{n+1}}{f_{n}}=1+\frac{f_{n-1}}{f_{n}} .
$$

Taking the limit as $n$ tends to infinity, we get (since $a \neq 0$ )

$$
a=1+\frac{1}{a}
$$

or

$$
a^{2}-a-1=0 .
$$

This quadratic equation has one positive root,

$$
\varphi=\frac{1+\sqrt{5}}{2} \text {. }
$$

We show now that the sequence $\left(f_{n+1} / f_{n}\right)$ is a Cauchy sequence. Applying the definition of the $f_{n}$ 's, we get

$$
\left|\frac{f_{n+1}}{f_{n}}-\frac{f_{n}}{f_{n-1}}\right|=\left|\frac{f_{n-1}^{2}-f_{n} f_{n-2}}{f_{n-1}^{2}+f_{n-1} f_{n-2}}\right|
$$

Since $f_{n}$ is an increasing sequence,

$$
f_{n-1}\left(f_{n-1}-f_{n-2}\right) \geqslant 0
$$

or

$$
f_{n-1}^{2}+f_{n-1} f_{n-2} \geqslant 2 f_{n-1} f_{n-2} .
$$

By substituting this in and simplifying, we get

$$
\left|\frac{f_{n+1}}{f_{n}}-\frac{f_{n}}{f_{n-1}}\right| \leqslant \frac{1}{2}\left|\frac{f_{n}}{f_{n-1}}-\frac{f_{n-1}}{f_{n-2}}\right| .
$$

By the Induction Principle [MH93, p. 7], we get

$$
\left|\frac{f_{n+1}}{f_{n}}-\frac{f_{n}}{f_{n-1}}\right| \leqslant \frac{1}{2^{n-2}}\left|\frac{f_{3}}{f_{2}}-\frac{f_{2}}{f_{1}}\right| .
$$

Therefore, by the Triangle Inequality [MH87, p. 20], for all $m>n$,

$$
\left|\frac{f_{m+1}}{f_{m}}-\frac{f_{n+1}}{f_{n}}\right| \leqslant\left|\frac{f_{3}}{f_{2}}-\frac{f_{2}}{f_{1}}\right| \sum_{k=n}^{m-1} \frac{1}{2^{k-2}} .
$$

Since the series $\sum 2^{-n}$ converges, the right-hand side tends to 0 as $m$ and $n$ tend to infinity. Hence, the sequence $\left(f_{n+1} / f_{n}\right)$ is a Cauchy sequence, and we are done.

Solution to 1.3.13: We have

$$
\frac{1}{n+1}+\cdots+\frac{1}{2 n}=\sum_{k=1}^{n} \frac{1}{1+\frac{k}{n}} \cdot \frac{1}{n}
$$

which is a Riemann sum for $\int_{0}^{1}(1+x)^{-1} d x$ corresponding to the partition of the interval $[0,1]$ in $n$ subintervals of equal length. Therefore, we get

$$
\lim _{n \rightarrow \infty}\left(\frac{1}{n+1}+\cdots+\frac{1}{2 n}\right)=\int_{0}^{1} \frac{1}{1+x} d x=\log 2 .
$$

Solution 2. Using the inequalities

$$
\left(1+\frac{1}{k}\right)^{k}<e<\left(1+\frac{1}{k-1}\right)^{k} \quad(k \geqslant 2)
$$

we get

$$
\begin{aligned}
\log 2 &=\log \left(\prod_{k=n+1}^{2 n} \frac{k}{k-1}\right)=\sum_{k=n+1}^{2 n} \frac{1}{k} \log \left(\frac{k}{k-1}\right)^{k} \\
&>\sum_{k=n+1}^{2 n} \frac{1}{k}>\sum_{k=n+1}^{2 n} \frac{1}{k} \log \left(\frac{k+1}{k}\right)^{k}=\log \left(\prod_{k=n+1}^{2 n} \frac{k+1}{k}\right) \\
&=\log \left(\frac{2 n+1}{n+1}\right)
\end{aligned}
$$

therefore, we have

$$
\log 2 \geqslant \lim _{n \rightarrow \infty} \sum_{k=n+1}^{2 n} \frac{1}{k} \geqslant \log 2
$$

and the result follows.

Solution 3. We have

$$
\begin{aligned}
\frac{1}{n+1}+\cdots+\frac{1}{2 n} &=1+\frac{1}{2}+\cdots+\frac{1}{2 n}-\left(1+\frac{1}{2}+\cdots+\frac{1}{n}\right) \\
&=1+\frac{1}{2}+\cdots+\frac{1}{2 n}-2\left(\frac{1}{2}+\cdots+\frac{1}{2 n}\right) \\
&=1-\frac{1}{2}+\cdots+\frac{1}{2 n-1}-\frac{1}{2 n}
\end{aligned}
$$

and the result now follows from the Maclaurin expansion [PMJ85, p. 127] of $\log (1+x)$

Solution to 1.3.14: We have

$$
\begin{aligned}
H_{n k}-H_{n} &=\frac{1}{n+1}+\frac{1}{n+2}+\ldots+\frac{1}{n k} \\
&=\frac{1}{n}\left(\frac{1}{1+\frac{1}{n}}+\frac{1}{1+\frac{2}{n}}+\ldots+\frac{1}{k}\right) .
\end{aligned}
$$

The right side is the lower Riemann sum of the integral $\int_{1}^{k} \frac{1}{x} d x$ over the partition of $[1, k]$ into $(k-1) n$ intervals each of length $\frac{1}{n}$. Because the lower Riemann sum is less than the integral, whose value is $\log k$, we get $H_{n k}-H_{n}<\log k$.

To get the other inequality note that the change in $\frac{1}{x}$ on the interval $\left[1,1+\frac{1}{n}\right]$ of the partition is $1-\frac{1}{1+\frac{1}{n}}=\frac{1}{n+1}$, and it is less than that on every other interval of the partition. Hence

$$
\int_{1}^{k} \frac{1}{x} d x-\left(H_{n k}-H_{n}\right)<(k-1) n \cdot \frac{1}{n} \cdot \frac{1}{n+1}=\frac{k-1}{n+1}
$$

since each term in the Riemann sum contributes an error of less than $\frac{1}{n} \cdot \frac{1}{n+1}$, and there are $(k-1) n$ terms. So we can take $C=k-1$.

One can deduce a more precise error bound as follows. For each subinterval of the partition, translate the region between the curve $y=\frac{1}{x}$ and the approximating rectangle horizontally so as to put it above the interval $\left[1,1+\frac{1}{n}\right]$. The translated regions are then nonoverlapping. As they lie within a rectangle of width $\frac{1}{n}$ and height 1, the sum of their areas, which bounds the difference $\log k-\left(H_{n k}-H_{n}\right)$, is less than $\frac{1}{n}$. We can thus take $C=1$. In fact, one easily sees that the translated regions fill up less than half of the preceding rectangle, so $C=\frac{1}{2}$ works.

Solution to 1.3.15: Let $n>0$. For $m \geqslant n$, we have

$$
x_{m} \leqslant x_{n}+\sum_{k=n}^{m-1} \frac{1}{k^{2}} \leqslant x_{n}+\xi_{n}
$$

where

$$
\xi_{n}=\sum_{k=n}^{\infty} \frac{1}{k^{2}} .
$$

Taking the lim sup, with respect to $m$, we have

$$
x_{n} \geqslant \limsup _{m \rightarrow \infty} x_{m}-\xi_{n} \text {. }
$$

The series $\sum k^{-2}$ converges, so $\lim _{n \rightarrow \infty} \xi_{n}=0$. Considering the lim inf with respect to $n$, we get

$$
\liminf _{n \rightarrow \infty} x_{n} \geqslant \limsup _{m \rightarrow \infty} x_{m}-\liminf _{n \rightarrow \infty} \xi_{n} \geqslant \limsup _{m \rightarrow \infty} x_{m} .
$$

The reverse inequality also holds, so $\lim x_{n}$ exists.

Solution to 1.3.16: Fix $\delta>0$, and choose $n_{0}$ such that $\varepsilon_{n}<\delta$ for all $n \geqslant n_{0}$. Then

$$
a_{n_{0}+1} \leqslant k a_{n_{0}}+\varepsilon_{n_{0}}<k a_{n_{0}}+\delta
$$



$$
\begin{aligned}
&a_{n_{0}+2}<k^{2} a_{n_{0}}+k \delta+\varepsilon_{n_{0}+1}<k^{2} a_{n_{0}}+(1+k) \delta \\
&a_{n_{0}+3}<k^{3} a_{n_{0}}+\left(k+k^{2}\right) \delta+\varepsilon_{n_{0}+2}<k^{3} a_{n_{0}}+\left(1+k+k^{2}\right) \delta
\end{aligned}
$$

and, by the Induction Principle [MH93, p. 7],

$$
a_{n_{0}+m}<k^{m} a_{n_{0}}+\left(1+k+\cdots+k^{m-1}\right) \delta<k^{m} a_{n_{0}}+\frac{\delta}{1-k} .
$$

Letting $m \rightarrow \infty$, we find that

$$
\limsup _{n \rightarrow \infty} a_{n} \leqslant \frac{\delta}{1-k} .
$$

Since $\delta$ is arbitrary, we have lim $\sup _{n \rightarrow \infty} a_{n} \leqslant 0$, and thus (since $a_{n}>0$ for all $n$ ) $\lim _{n \rightarrow \infty} a_{n}=0$.

Solution to 1.3.17: If $\left(x_{n}\right)$ is unbounded, then, without loss of generality, it has no finite upper bound. Take $x_{n_{1}}=x_{1}$ and, for each $k \in \mathbb{N}, x_{n_{k}}$ such that $x_{n_{k}}>\max \left\{k, x_{n_{k-1}}\right\}$. This is clearly an increasing subsequence of $x_{n}$.

If $x_{n}$ is bounded, it has a convergent subsequence: $\lim y_{n}=\xi$, say. $y_{n}$ contains a subsequence converging to $\xi+$ or one converging to $\xi-$. Suppose $\left(z_{n}\right)$ is a subsequence of $\left(y_{n}\right)$ converging to $\xi+$. Let $z_{n_{1}}=z_{1}$ and, for each $k \geqslant 1$, let $\xi \leqslant z_{n_{k}}<z_{n_{k-1}}$. This is a monotone subsequence of $\left(x_{n}\right)$.

Solution to 1.3.18: Suppose that there are $x>1, \varepsilon>0$ such that $\left|b_{m} / b_{n}-x\right| \geqslant \varepsilon$ for all $1 \leqslant n<m$. Since $\lim \left(b_{n} / b_{n+1}\right)=1$, for all $k$ sufficiently large there exists an integer $n_{k}>k$ such that $b_{m} / b_{k}<x$ if $m<n_{k}$ and $b_{m} / b_{k}>x$ if $m>n_{k}$. In particular, for each $k$,

$$
\frac{b_{n_{k}+1}}{b_{k}}-\frac{b_{n_{k}}}{b_{k}} \geqslant 2 \varepsilon
$$

or

$$
\frac{b_{n_{k}+1}}{b_{n_{k}}}-1 \geqslant 2 \varepsilon \frac{b_{k}}{b_{n_{k}}}>\frac{2 \varepsilon}{x}>0 .
$$

As $n_{k}$ tends to infinity as $k$ does, the left-hand side should tend to 0 as $k$ tends to infinity, a contradiction.

Solution to 1.3.19: 1. Using the Ratio Test [Rud87, p. 66], we have

$$
\begin{aligned}
\frac{\frac{(2 n) !(3 n) !}{n !(4 n) !}}{\frac{(2 n+2) !(3 n+3) !}{(n+1) !(4 n+4) !}} &=\frac{n !(4 n) !(2 n+2)(2 n+1)(2 n) !(3 n+3)(3 n+2)(3 n+1)(3 n) !}{(2 n) !(3 n) !(n+1) n !(4 n+4)(4 n+3)(4 n+2)(4 n+1)(4 n) !} \\
&=\frac{(2 n+2)(2 n+1)(3 n+3)(3 n+2)(3 n+1)}{(n+1)(4 n+4)(4 n+3)(4 n+2)(4 n+1)} \\
& \rightarrow \frac{27}{64}<1
\end{aligned}
$$

so the series converges. 2. Comparing with the series $\sum 1 /(n \log n)$, which can be seen to diverge using the Integral Test [Rud87, p. 139],

$$
\lim \frac{1 / n^{1+1 / n}}{1 / n \log n}=\lim \frac{\log n}{n^{1 / n}}=\infty
$$

we conclude that the given series diverges.

Solution to 1.3.20: 1. Assume that $\sum a_{n}<\infty$. As $\left(\sqrt{a_{n+1}}-\sqrt{a_{n}}\right)^{2}=a_{n+1}+$ $a_{n}-2 \sqrt{a_{n} a_{n+1}}$, we have

$$
\sum_{n=1}^{\infty} \sqrt{a_{n} a_{n+1}} \leqslant \frac{1}{2} \sum_{n=1}^{\infty}\left(a_{n}+a_{n+1}\right)=\frac{1}{2} a_{1}+\sum_{n=2}^{\infty} a_{n}<\infty .
$$

2. Since $\sum\left(a_{n}+a_{n+1}\right)=2 \sum \sqrt{a_{n} a_{n+1}}+\sum\left(\sqrt{a_{n+1}}-\sqrt{a_{n}}\right)^{2}$, we require a sequence $a_{n}=b_{n}^{2}, b_{n}>0$, such that $\sum b_{n} b_{n+1}<\infty$ but $\sum\left(b_{n+1}-b_{n}\right)^{2}=\infty$. One such example is

$$
b_{n}=\left\{\begin{array}{cccc}
\frac{1}{\sqrt{n}} & \text { if } & n & \text { is odd } \\
\frac{1}{n} & \text { if } & n & \text { is even. }
\end{array}\right.
$$

Solution to 1.3.21: As

$$
\lim \left|\frac{a^{n+1}}{(n+1)^{b}(\log n+1)^{c}} \frac{n^{b}(\log n)^{c}}{a^{n}}\right|=|a|
$$

the series converges absolutely for $|a|<1$ and diverges for $|a|>1$.

- $a=1$.

(i) $b>1$. Let $b=1+2 \varepsilon$; we have

$$
\frac{1}{n^{1+2 \varepsilon}(\log n)^{c}}=o\left(\frac{1}{n^{1+\varepsilon}}\right) \quad(n \rightarrow \infty)
$$

and, as the series $\sum n^{-(1+\varepsilon)}$ converges, the given series converges absolutely for $b>1$.

(ii) $b=1$. The series converges (absolutely) only if $c>1$ and diverges if $c \leqslant 1$, by the Integral Test [Rud87, p. 139].

(iii) $b<1$. Comparing with the harmonic series, we conclude that the series diverges.

- $a=-1$. By Leibniz Criterion [Rud87, p. 71], the series converges exactly when

$$
\lim \frac{1}{n^{b}(\log n)^{c}}=0
$$

which is equivalent to $b>0$ or $b=0, c>0$. Solution to 1.3.22: Note that

$$
\frac{\sqrt{n+1}-\sqrt{n}}{n^{x}} \sim \frac{1}{n^{x+1 / 2}} \quad(n \rightarrow \infty)
$$

that is,

$$
\lim _{n \rightarrow \infty} \frac{\sqrt{n+1}-\sqrt{n} / n^{x}}{1 / n^{x+1 / 2}}=1
$$

so the given series and

$$
\sum_{n=1}^{\infty} \frac{1}{n^{x+1 / 2}}
$$

converge or diverge together. They converge when $x>1 / 2$.

Solution to 1.3.23: If $a \leqslant 0$, the general term does not go to zero, so the series diverges. If $a>0$, we have, using the Maclaurin series [PMJ85, p. 127] for $\sin x$,

$$
\frac{1}{n}-\sin \frac{1}{n}=\frac{1}{6 n^{3}}+o\left(n^{-3}\right) \quad(n \rightarrow \infty)
$$

and, therefore,

$$
\left(\frac{1}{n}-\sin \frac{1}{n}\right)^{a}=\frac{1}{6^{a} n^{3 a}}+o\left(n^{-3 a}\right) \quad(n \rightarrow \infty) .
$$

Thus, the series converges if and only if $3 a>1$, that is, $a>1 / 3$.

Solution to 1.3.24: For $n=1,2, \ldots$ the number of terms in $A$ that are less than $10^{n}$ is $9^{n}-1$, so we have

$$
\sum_{a \in A} \frac{1}{a}=\sum_{n \geqslant 1} \sum_{\substack{10^{n-1} \leqslant a<10^{n} \\ a \in A}} \frac{1}{a} \leqslant \sum_{n \geqslant 1} \frac{9^{n}}{10^{n-1}}=10 \sum_{n \geqslant 1}\left(\frac{9}{10}\right)^{n}<\infty .
$$

Solution to 1.3.25: Let $S$ be the sum of the given series. Let $N_{0}=0$. By convergence, for each $k>0$ there exists an $N_{k}>N_{k-1}$ such that

$$
\sum_{N_{k}+1}^{\infty} a_{n} \leqslant \frac{S}{4^{k}} .
$$

For $N_{k}+1 \leqslant n \leqslant N_{k+1}$ let $c_{n}=2^{k}$. We have $\lim c_{n}=\infty$. As the terms are all positive, we may rearrange the sum and get

$$
\sum_{n=1}^{\infty} c_{n} a_{n}=\sum_{k=0}^{\infty} \sum_{N_{k}+1}^{N_{k+1}} c_{n} a_{n}
$$



$$
\begin{aligned}
&\leqslant \sum_{k=0}^{\infty} 2^{k} \sum_{N_{k}+1}^{\infty} a_{n} \\
&=\sum_{k=0}^{\infty} 2^{-k} S \\
&=2 S .
\end{aligned}
$$

Solution 2. The convergence of the given series shows that there is an increasing sequence of positive integers $\left(N_{k}\right)$ with $\sum_{n=N_{k}}^{\infty} a_{n}<1 / k^{-3}$ for each $k$. Let

$$
c_{n}=\left\{\begin{array}{lll}
1 & \text { if } & n<N_{1} \\
k & \text { if } & N_{k} \leqslant n<N_{k+1} .
\end{array}\right.
$$

Then $c_{n} \rightarrow \infty$, and

$$
\begin{aligned}
\sum_{n=1}^{\infty} c_{n} a_{n} & \leqslant \sum_{n=1}^{N_{1}-1} a_{n}+\sum_{k=1}^{\infty} k \sum_{n=N_{k}}^{\infty} a_{k} \\
& \leqslant \sum_{n=1}^{\infty} a_{n}+\sum_{k=1}^{\infty} \frac{k}{k^{3}} \\
& \leqslant \sum_{n=1}^{\infty} a_{n}+\sum_{k=1}^{\infty} \frac{1}{k^{2}} \\
&<\infty .
\end{aligned}
$$

Solution 3. Let $\sum a_{n}$ converge to $S$. For positive $n$ let $R_{n}=\sum_{k=1}^{n} a_{k}$, and $R_{0}=0$, so we have $a_{n}=R_{n}-R_{n-1}$ for all $n$. Consider the sequence $c_{n}$ defined by $c_{n}=1 /\left(\sqrt{R_{n}}+\sqrt{R_{n-1}}\right)$. As $R_{n} \rightarrow S$ we get $c_{n} \rightarrow \infty$.

We have

$$
\begin{aligned}
\sum_{n=1}^{\infty} c_{n} a_{n} &=\sum_{n=1}^{\infty} \frac{R_{n}-R_{n-1}}{\sqrt{R_{n}}+\sqrt{R_{n-1}}} \\
&=\sum_{n=1}^{\infty} \sqrt{R_{n}}-\sqrt{R_{n-1}} \\
&=\sqrt{S} .
\end{aligned}
$$

Solution to 1.3.26: Using the formula $\sin 2 x=2 \sin x \cos x$ and the Induction Principle [MH93, p. 7], starting with $\sin \frac{\pi}{2}=1$, we see that

$$
\cos \frac{\pi}{2^{2}} \cos \frac{\pi}{2^{3}} \cdots \cos \frac{\pi}{2^{n}}=\frac{1}{2^{n-1} \sin \frac{\pi}{2^{n}}}
$$

So we have

$$
\frac{1}{2^{n-1} \sin \frac{\pi}{2^{n}}}=\frac{2}{\pi} \frac{\frac{\pi}{2^{n}}}{\sin \frac{\pi}{2^{n}}} \sim \frac{2}{\pi} \quad(n \rightarrow \infty)
$$

since $\sin x \sim x(x \rightarrow 0)$

\section{$1.4$ Differential Calculus}

Solution to 1.4.1: Lemma 1: If $\left(x_{n}\right)$ is an infinite sequence in the finite interval $[a, b]$, then it has a convergent subsequence.

Consider the sequence $y_{k}=\sup \left\{x_{n} \mid n \geqslant k\right\}$. By the least upper bound property, we know that $y_{k}$ exists and is in $[a, b]$ for all $k$. By the definition of supremum, it is clear that the $y_{k}$ 's form a nonincreasing sequence. Let $y$ be the infimum of this sequence. From the definition of infimum, we know that the $y_{k}$ 's converge to $y$. Again, by the definition of supremum, we know that we can find $x_{n}$ 's arbitrarily close to each $y_{k}$, so we can choose a subsequence of the original sequence which converges to $y$.

Lemma 2: A continuous function $f$ on $[a, b]$ is bounded.

Suppose $f$ is not bounded. Then, for each $n$, there is a point $x_{n} \in[a, b]$ such that $\left|f\left(x_{n}\right)\right|>n$. By passing to a subsequence, we may assume that the $x_{n}$ 's converge to a point $x \in[a, b]$. (This is possible by Lemma 1.) Then, by the continuity of $f$ at $x$, we must have that $\left|f(x)-f\left(x_{n}\right)\right|<1$ for $n$ sufficiently large, or $\left|f\left(x_{n}\right)\right|<|f(x)|+1$, contradicting our choice of the $x_{n}$ 's.

Lemma 3: A continuous function $f$ on $[a, b]$ achieves its extrema.

It will suffice to show that $f$ attains its maximum, the other case is proved in exactly the same way. Let $M=\sup f$ and suppose $f$ never attains this value. Define $g(x)=M-f(x)$. Then $g(x)>0$ on $[a, b]$, so $1 / g$ is continuous. Therefore, by Lemma $2,1 / g$ is bounded by, say, $N$. Hence, $M-f(x)>1 / N$, or $f(x)<M-1 / N$, contradicting the definition of $M$.

Lemma 4: If a differentiable function $f$ on $(a, b)$ has a relative extremum at $a$ point $c \in(a, b)$, then $f^{\prime}(c)=0$.

Define the function $g$ by

$$
g(x)=\left\{\begin{array}{ccc}
\frac{f(x)-f(c)}{x-c} & \text { for } \quad x \neq c \\
0 & \text { for } \quad x=c
\end{array}\right.
$$

and suppose $g(c)>0$. By continuity, we can find an interval $J$ around $c$ such that $g(x)>0$ if $x \in J$. Therefore, $f(x)-f(c)$ and $x-c$ always have the same sign in $J$, so $f(x)<c$ if $x<c$ and $f(x)>f(c)$ if $x>c$. This contradicts the fact that $f$ has a relative extremum at $c$. A similar argument shows that the assumption that $g(c)<0$ yields a contradiction, so we must have that $g(c)=0$.

Lemma 5 (Rolle's Theorem [MH93, p. 200]): Let $f$ be continuous on $[a, b]$ and differentiable on $(a, b)$ with $f(a)=f(b)$. There is a point $c \in(a, b)$ such that $f^{\prime}(c)=0$. Suppose $f^{\prime}(c) \neq 0$ for all $c \in(a, b)$. By Lemma $3, f$ attains its extrema on $[a, b]$, but by Lemma 4 it cannot do so in the interior since otherwise the derivative at that point would be zero. Hence, it attains its maximum and minimum at the endpoints. Since $f(a)=f(b)$, it follows that $f$ is constant, and so $f^{\prime}(c)=0$ for all $c \in(a, b)$, a contradiction.

Lemma 6 (Mean Value Theorem [Rud87, p. 108]): If $f$ is a continuous function on $[a, b]$, differentiable on $(a, b)$, then there is $c \in(a, b)$ such that $f(b)-f(a)=f^{\prime}(c)(b-a)$.

Define the function $h(x)=f(x)(b-a)-x(f(b)-f(a)) . h$ is continuous on $[a, b]$, differentiable on $(a, b)$, and $h(a)=h(b)$. By Lemma 5, there is $c \in(a, b)$ such that $h^{\prime}(c)=0$. Differentiating the expression for $h$ yields the desired result.

There is a point $c$ such that $f(b)-f(a)=f^{\prime}(c)(b-a)$, but, by assumption, the right-hand side is 0 for all $c$. Hence, $f(b)=f(a)$.

Solution to 1.4.2: 1 . We have $\exp (f(x))=(1+1 / x)^{x}$, which is an increasing function. As the exponential is also increasing, so is $f$.

2. We have

$$
\lim _{x \rightarrow 0} f(x)=\lim _{x \rightarrow 0} \frac{\log (x+1)-\log x}{1 / x}=\lim _{x \rightarrow 0} \frac{1 /(x+1)-1 / x}{-1 / x^{2}}=0 .
$$

On the other hand,

$$
\lim _{x \rightarrow \infty}\left(1+\frac{1}{x}\right)^{x}=e
$$

so $\lim _{x \rightarrow \infty} f(x)=1$.

Solution to 1.4.3: Let $0<s<t$. Then by the Mean Value Theorem, there exist $u$ in the interval $(0, s)$ such that $f(s)=f(s)-f(0)=s f^{\prime}(u)$ and $v$ in the interval $(s, t)$ such that $f(t)-f(s)=(t-s) f^{\prime}(v)$. Then

$$
\frac{f(s)}{s}=f^{\prime}(u) \leqslant f^{\prime}(v)=\frac{f(t)-f(s)}{t-s} .
$$

This inequality reduces to $f(s) / s \leqslant f(t) / t$. Hence $g$ is an increasing function on the interval $(0, \infty)$. Now $g(0)=f^{\prime}(0)=\lim _{x \rightarrow 0} f(x) / x=\lim _{x \rightarrow 0} g(x)$. Fix $x>0$. For $0<t<x, g(t) \leqslant g(x)$. Taking limits as $t \rightarrow 0$ we conclude that $g(0) \leqslant g(x)$. Thus $g$ is increasing on $[0, \infty)$.

For $x>0, g(x)$ is the slope of the chord of the graph of $f$ joining the points $(0, f(0))$ and $(x, f(x))$.

Solution 2. Since $f(0)=0, f(x)=\int_{0}^{x} f^{\prime}(t) d t$ and using the fact that $f^{\prime}(x)$ is increasing, we get

$$
f(x)=\int_{0}^{x} f^{\prime}(t) d t<\int_{0}^{x} f^{\prime}(x) d t=x f^{\prime}(x),
$$

so $g^{\prime}(x)=\frac{x f^{\prime}(x)-f(x)}{x^{2}}$ is positive and $g$ an increasing function of $x$. Solution to 1.4.4: 1 . The function $f(x)$ given by

$$
f(x)=x^{2} \sin \frac{1}{x}
$$

has a derivative that is not continuous at zero:

$$
f^{\prime}(x)=\left\{\begin{array}{lll}
2 x \sin \frac{1}{x}-\cos \frac{1}{x} & \text { for } x \neq 0 \\
0 & \text { for } x=0
\end{array}\right.
$$

![](https://cdn.mathpix.com/cropped/2022_10_26_8d6ad07375e689af3a1cg-037.jpg?height=1106&width=1114&top_left_y=648&top_left_x=511)

2. Consider the function $g$ given by $g(x)=f(x)-2 x$. We then have $g^{\prime}(0)<0<$ $g^{\prime}(1)$. Therefore, $g(x)<g(0)$ for $x$ close to 0 , and $g(x)<g(1)$ for $x$ close to 1 . Then the minimum of $g$ in $[0,1]$ occurs at an interior point $c \in(0,1)$, at which we must have $g^{\prime}(c)=0$, which gives $f^{\prime}(c)=2$.

Solution to 1.4.5: Suppose $y$ assumes a positive maximum at $\xi$. Then $y(\xi)>0$, $y^{\prime}(\xi)=0$, and $y^{\prime \prime}(\xi) \leqslant 0$, contradicting the differential equation. Hence, the maximum of $y$ is 0 . Similarly, $y$ cannot assume a negative minimum, so $y$ is identically 0.

Solution to 1.4.6: 1. Suppose $u$ has a local maximum at $x_{0}$ with $u\left(x_{0}\right)>0$. Then $u^{\prime \prime}\left(x_{0}\right) \leqslant 0$, but $u^{\prime \prime}\left(x_{0}\right)=e^{x_{0}} u\left(x_{0}\right)>0$ and we have a contradiction. So $u$ cannot have a positive local maximum. Similarly, if $u$ has a local minimum at $x_{0}$, then $u^{\prime \prime}\left(x_{0}\right) \geqslant 0$, so we must have $u\left(x_{0}\right) \geqslant 0$ and $u$ cannot have a negative local minimum. 2. Suppose $u(0)=u(1)=0$. If $u\left(x_{0}\right) \neq 0$ for some $x_{0} \in(0,1)$, then, as $u$ is continuous, $u$ attains a positive local maximum or a negative local minimum, which contradicts Part 1.

Solution to 1.4.7: From the given inequality we get

$$
0 \geqslant e^{-K t} y^{\prime}(t)-K e^{-K t} y(t)=\frac{d}{d t}\left(e^{-K t} y(t)\right) \quad(t \geqslant 0) .
$$

Integrating from 0 to $t$ we find that, for $t \geqslant 0$,

$$
e^{-K t} y(t)-y(0) \leqslant 0,
$$

from which the desired inequality follows.

Solution 2. Let $z(t)=\log y(t)$. Then $z^{\prime}(t)=y^{\prime}(t) / y(t) \leqslant K$. Fix $t>0$. By the Mean Value Theorem, there exist $u$ in the interval $(0, t)$ such that $z(t)-z(0)=$ $z^{\prime}(u) t \leqslant K t$. Hence $z(t) \leqslant K t+\log y(0)$. Since the exponential function is strictly increasing, we obtain $y(t) \leqslant e^{K t} y(0)$. This inequality also holds for $t=0$.

Solution to 1.4.8: Without loss of generality assume $x_{0}=0$. As $f$ is continuous and $f(0)=0$, we have $f^{\prime}(x)>f(x)>0$ in some interval $[0, \varepsilon)$. Suppose that $f(x)$ is not positive for all positive values of $x$. Let $c=\inf \{x>0 \mid f(x) \leqslant 0\}$. Since $f$ is continuous and positive in a neighborhood of the origin, we have $c>0$ and $f(c)=0$. By Rolle's Theorem, [MH93, p. 200] there is a point $d$ with $0<d<c$ and $f^{\prime}(d)=0$. However, by the definition of $c$, we have $f^{\prime}(d)>f(d)>0$, a contradiction.

Solution 2. Let $g(x)=e^{-x} f(x)$. Then $g^{\prime}(x)=e^{-x}\left(f^{\prime}(x)-f(x)\right)>0$. As $g$ is an increasing function, we have $g(x)=e^{-x} f(x)>g\left(x_{0}\right)=0$ for $x>x_{0}$, and the conclusion follows.

Solution to 1.4.9: Suppose that $f(x) \leqslant 0$ for some positive value of $x$. Then $a=\inf \{x>0 \mid f(x) \leqslant 0\}$ is positive. Since $f$ is continuous, $f(a)=0$. Let $b \in(0, a)$, then $f(b)>0$. By the Mean Value Theorem [Rud87, p. 108], there exists $c \in(b, a)$ such that $f^{\prime}(c)=(f(a)-f(b)) /(a-b)<0$. Applying the same theorem to $f^{\prime}$ on the interval $(0, c)$ we get a number $d \in(0, c)$ such that $f^{\prime \prime}(d)=\left(f^{\prime}(c)-f^{\prime}(0)\right) / c<0$. Since $0<d<a, f(d)>0$, contradicting the assumption that $f^{\prime \prime}(x) \geqslant f(x)$ for all $x \in(0, a)$. Solution to 1.4.10: Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be defined by $f(x)=a e^{x}-1-x-\frac{x^{2}}{2}$.
We have

$$
\lim _{x \rightarrow-\infty} f(x)=-\infty \text { and } \lim _{x \rightarrow \infty} f(x)=\infty
$$

so $f$ has at least one real root, $x_{0}$, say. We have

$$
f^{\prime}(x)=a e^{x}-1-x>a e^{x}-1-x-\frac{x^{2}}{2}=f(x) \quad \text { for all } \quad x \in \mathbb{R} ;
$$

therefore, by Problem 1.4.8, $f$ has no other root. Solution to 1.4.11: Let $g:[0,1] \rightarrow \mathbb{R}$ be defined by $g(x)=e^{-2 x} f(x)$. We have

$$
g^{\prime}(x)=e^{-2 x}\left(f^{\prime}(x)-2 f(x)\right) \leqslant 0
$$

so $g$ is a decreasing function. As $g(0)=0$ and $g$ is nonnegative, we get $g \equiv 0$, so the same is true for $f$.

Solution to 1.4.12: Consider the function $g$ defined on $[0,1]$ by $g(x)=e^{x} f(x)$. We have

$$
g^{\prime \prime}(x)=e^{x}\left(f^{\prime \prime}(x)+2 f^{\prime}(x)+f(x)\right) \geqslant 0
$$

so $g$ is concave upward; that is, the point $(x, g(x))$ must lie below the chord joining $(0, g(0))$ and $(1, g(1))=(1,0)$ for $x \in(0,1)$. Then $g(x) \leqslant 0$ and the conclusion follows.

Solution to 1.4.13: As $\varphi_{1}$ and $\varphi_{2}$ satisfy the given differential equations, we have $\varphi_{1}^{\prime}(t)=v_{1}\left(\varphi_{1}(t)\right)$ and $\varphi_{2}^{\prime}(t)=v_{2}\left(\varphi_{2}(t)\right)$. Since $\varphi_{1}\left(t_{0}\right)=\varphi_{2}\left(t_{0}\right)$, it follows from our hypotheses that $\varphi_{1}^{\prime}\left(t_{0}\right)<\varphi_{2}^{\prime}\left(t_{0}\right)$. Hence, there exists a point $s_{0}>t_{0}$ such that $\varphi_{1}(t) \leqslant \varphi_{2}(t)$ for $t_{0} \leqslant t \leqslant s_{0}$. Suppose there existed a point $s_{0}<t<b$ such that $\varphi_{1}(t)>\varphi_{2}(t)$. Let $t_{1} \geqslant s_{0}$ be the infimum of all such points $t$. By continuity, we must have that $\varphi_{1}\left(t_{1}\right)=\varphi_{2}\left(t_{1}\right)$. Hence, repeating the above argument, we see that there must be a point $s_{1}>t_{1}$ such that $\varphi_{1}(t) \leqslant \varphi_{2}(t)$ if $t_{1}<t<s_{1}$, contradicting our definition of $t_{1}$.

Solution to 1.4.14: Let $f(x)=x^{2} \sin \frac{1}{x}$ and $g(x)=x$. Then we have $\lim _{x \rightarrow 0} f(x)=\lim _{x \rightarrow 0} g(x)=0$ and

$$
\lim _{x \rightarrow 0} \frac{f(x)}{g(x)}=\lim _{x \rightarrow 0} x \sin \frac{1}{x}=0 .
$$

As $g^{\prime}(x)=1$ and $f^{\prime}(x)=2 x \sin \frac{1}{x}-\cos \frac{1}{x}$, we have

$$
\lim _{x \rightarrow 0} \frac{f^{\prime}(x)}{g^{\prime}(x)}=\lim _{x \rightarrow 0} 2 x \sin \frac{1}{x}-\cos \frac{1}{x}
$$

which does not exists.

Solution to 1.4.15: The given conditions imply that $f^{\prime}(0)=0$, so the question is whether $\lim _{x \rightarrow 0} f_{1}^{\prime}(x) / x$ must exist. The answer is no and one counterexample is $f(x)=x^{3} \sin \frac{1}{x}$. A calculation gives

$$
\frac{f^{\prime}(x)}{x}=3 x \sin \frac{1}{x}-\cos \frac{1}{x} .
$$

Solution to 1.4.16: By Rolle's Theorem [MH93, p. 200], $f^{\prime}\left(x_{1}\right)=0$ for some $x_{1} \in(0,1)$. Then, since $f^{\prime}(0)=0, f^{\prime \prime}\left(x_{2}\right)=0$ for some $x_{2} \in\left(0, x_{1}\right)$. Repeated applications of Rolle's Theorem give $f^{(n)}\left(x_{n}\right)=0$ for some $x_{n} \in\left(0, x_{n-1}\right)$, and therefore, $f^{(n+1)}(x)=0$ for some $x \in\left(0, x_{n}\right) \subset(0,1)$.

Solution to 1.4.17: Since $f^{\prime} \leqslant 0$ and $f \geqslant 0$, we deduce that $f$ is monotone and $\lim _{t \rightarrow \infty} f(t)=\xi$ exists. Hence, for every $\delta>0$, we have

$$
\lim _{t \rightarrow \infty} \frac{f(t+\delta)-f(t)}{\delta}=0 .
$$

On the other hand

$$
\frac{f(t+\delta)-f(t)}{\delta}=f^{\prime}(t)+\frac{1}{2} \delta f^{\prime \prime}(\theta)
$$

for some $\theta$. Therefore,

$$
\limsup _{t \rightarrow \infty}\left|f^{\prime}(t)\right| \leqslant \frac{1}{2} \delta \sup _{\theta}\left|f^{\prime \prime}(\theta)\right| .
$$

Letting $\delta \rightarrow 0$ we get $\lim _{t \rightarrow \infty} f^{\prime}(t)=0$.

Solution to 1.4.18: Let $x>0$ and $\delta>0$. Since $f$ is positive and log is continuous,

$$
\begin{aligned}
\log _{\delta \rightarrow 0} \lim _{\delta(x)}\left(\frac{f(x+\delta x)}{f(x)}\right)^{1 / \delta} &=\lim _{\delta \rightarrow 0} \log \left(\frac{f(x+\delta x)}{f(x)}\right)^{1 / \delta} \\
&=\lim _{\delta \rightarrow 0} \frac{\log f(x+\delta x)-\log f(x)}{\delta} \\
&=\lim _{\delta \rightarrow 0} \frac{x(\log f(x+\delta x)-\log f(x))}{\delta x} \\
&=x(\log f(x))^{\prime} \\
&=\frac{x f^{\prime}(x)}{f(x)}
\end{aligned}
$$

and the result follows, by exponentiating both sides.

Solution to 1.4.19: It is enough to show that

$$
\lim _{h \rightarrow 0^{+}} \frac{f(h)-f(0)}{h} \text { and } \lim _{h \rightarrow 0^{-}} \frac{f(h)-f(0)}{h}
$$

both exist and are equal. By L'Hôpital's Rule [Rud87, p. 109]

$$
\lim _{h \rightarrow 0^{+}} \frac{f(h)-f(0)}{h}=\lim _{h \rightarrow 0^{+}} \frac{f^{\prime}(h)}{1}=\lim _{h \rightarrow 0} f^{\prime}(h) .
$$

The other lateral limit can be treated similarly.

Solution to 1.4.20: We have

$$
\begin{aligned}
&p_{t}(x)=\left(1+t^{2}\right) x^{3}-3 t^{3} x+t^{4} \\
&p_{t}^{\prime}(x)=3\left(1+t^{2}\right) x^{2}-3 t^{3} \\
&p_{t}^{\prime \prime}(x)=6\left(1+t^{2}\right) x
\end{aligned}
$$

- $t<0$. In this case, $p_{t}^{\prime}>0$ and $p_{t}(x)<0$ for $x$ sufficiently negative, and $p_{t}(x)>0$ for $x$ sufficiently positive. Hence, by the Intermediate Value Theorem [Rud87, p. 93], $p_{t}$ has exactly one root, of multiplicity 1 , since the derivative is positive.

- $t=0$. Now $p_{t}(x)=x^{3}$, which has a single zero of multiplicity 3 .

- $t>0$. We have

$$
p_{t}^{\prime}\left(\pm \sqrt{\frac{t^{3}}{1+t^{2}}}\right)=0
$$

and $p_{t}^{\prime \prime}(x)<0$ for negative $x ; p_{t}^{\prime \prime}(x)>0$ for positive $x$. So

$$
p_{t}\left(\sqrt{\frac{t^{3}}{1+t^{2}}}\right)
$$

is a local minimum, and

$$
p_{t}\left(-\sqrt{\frac{t^{3}}{1+t^{2}}}\right)
$$

is a local maximum of $p_{t}$.

We will study the values of $p_{t}$ at these critical points. As $p_{t}(0)>0$ and $p_{t}^{\prime}(0)<0$, the relative maximum must be positive.

We have

$$
p_{t}\left(\sqrt{\frac{t^{3}}{1+t^{2}}}\right)=t^{4}\left(1-\sqrt{\frac{t}{1+t^{2}}}\right)=A_{t}
$$

say. We get

(i) $0<t<2-\sqrt{3}$. In this case, we have $A_{t}>0$, so $p_{t}$ has one single root.

(ii) $2-\sqrt{3}<t<2+\sqrt{3}$. Now $A_{t}<0$ and $p_{t}$ has three roots.

(iii) $t>2+\sqrt{3}$. We have $A_{t}>0$ and $p_{t}$ has one root.

\section{Solution to 1.4.21: Let}

$$
h(x)=\frac{f(x)-f(a)}{x-a}-f^{\prime}(a)
$$

so that $\lim _{x \rightarrow a} h(x)=0$ and $f(x)=f(a)+\left(f^{\prime}(a)+h(x)\right)(x-a)$. Then

$$
\frac{f\left(y_{n}\right)-f\left(x_{n}\right)}{y_{n}-x_{n}}=\frac{f^{\prime}(a)\left(y_{n}-x_{n}\right)+h(x)\left(y_{n}-a\right)-h\left(x_{n}\right)\left(x_{n}-a\right)}{y_{n}-x_{n}}
$$

so that

$$
\begin{aligned}
\left|\frac{f\left(y_{n}\right)-f\left(x_{n}\right)}{y_{n}-x_{n}}-f^{\prime}(a)\right| & \leqslant\left|h\left(y_{n}\right)\right|\left(\frac{y_{n}-a}{y_{n}-x_{n}}\right)+\left|h\left(x_{n}\right)\right|\left(\frac{a-x_{n}}{y_{n}-x_{n}}\right) \\
& \leqslant\left|h\left(y_{n}\right)\right|+\left|h\left(x_{n}\right)\right| \\
&=o(1) \quad(n \rightarrow \infty) .
\end{aligned}
$$

Solution to 1.4.22: By changing variables, it is enough to show that $f(1) \geqslant f(0)$. Without loss of generality assume $f(0)=0$. Consider the function $g$ defined by

$$
g(x)=f(x)-f(1) x .
$$

As $g$ is continuous, it attains a maximum at some point $\xi \in[0,1]$. We can assume $\xi<1$, because $g(1)=g(0)=0$. As $g(\xi) \geqslant g(x)$ for $\xi<x<1$, we have

$$
0 \geqslant \limsup _{x \rightarrow \xi+} \frac{g(x)-g(\xi)}{x-\xi}=-f(1)+\frac{f(x)-f(\xi)}{x-\xi}
$$

As the rightmost term is nonnegative, we have $f(1) \geqslant 0$, as desired.

Solution to 1.4.24: We have

$$
1=f(z)\left(\frac{e^{2}-1}{z}\right)=\left(\xi_{0}+\xi_{1} z+\xi_{2} z^{2}+\cdots\right)\left(1+\frac{z}{2 !}+\frac{z^{2}}{3 !}+\cdots\right) .
$$

Multiplying this out, we get $\xi_{0}=1$ and

$$
\sum_{k=0}^{n} \frac{\xi_{n-k}}{(k+1) !}=0 .
$$

From this, it can easily be seen by the Induction Principle [MH93, p. 7] that all the $\xi_{i}$ 's are rational.

Solution to 1.4.25: The function $f$ given by

$$
f(x)= \begin{cases}(-1 / 3) e^{3-1 / x+2 /(2 x-3)} & \text { for } 0<x<3 / 2 \\ 0 & \text { for } x \leqslant 0 \text { or } x \geqslant 3 / 2\end{cases}
$$

is such a function.

![](https://cdn.mathpix.com/cropped/2022_10_26_8d6ad07375e689af3a1cg-042.jpg?height=361&width=761&top_left_y=2042&top_left_x=671)

This is based on the example of a nonconstant function having derivatives of all orders, vanishing for negative $x$ :

$$
\begin{cases}e^{-1 / x} & \text { for } x>0 \\ 0 & \text { for } x \leqslant 0\end{cases}
$$

Solution to 1.4.26: Let $g(x)=\sin ^{n} x$; it follows from the Taylor series of $\sin x$ that the first nonzero derivative of $g$ at 0 is $g^{(n)}(0)=n !$. For any positive number $\lambda$, set $c=\alpha /\left(n ! \lambda^{n}\right)$; then $f(x)=c \sin ^{n}(\lambda x)$ satisfies $f^{(n)}(0)=c \lambda^{n} n !=\alpha$.

There is a real number $M$ such that $\left|g^{(k)}(x)\right| \leqslant M$ for $x \in[0,2 \pi]$ and $k=0,1, \ldots n-1$. Since $g$ and its derivatives are periodic, with period $2 \pi$, $\left|g^{(k)}(x)\right| \leqslant M$ for all real $x$ and $k<n$. Therefore $\left|f^{(k)}(x)\right|=|c| \lambda^{k}\left|g^{(k)}(x)\right| \leqslant$ $|c| \lambda^{k} M=C \lambda^{k-n}$ for all real $x$, where $C=|\alpha| / n !$. Choosing $\lambda \geqslant \max (1, C / \varepsilon)$ ensures that $\left|f^{(k)}(x)\right| \leqslant \varepsilon$ for all $x \in \mathbb{R}$ and $k=0,1, \ldots n-1$.

Solution to 1.4.28: We claim there is an $\varepsilon>0$ such that $f(t) \neq 0$ for all $t \in(0, \varepsilon)$. Suppose, on the contrary, that there is a sequence $x_{n} \rightarrow 0$ such that $f\left(x_{n}\right)=0$. Considering the real function $\mathfrak{N} f(x)$, to each subinterval $\left[x_{n+1}, x_{n}\right]$, we find a sequence $t_{n} \rightarrow 0, t_{n} \in\left[x_{n+1}, x_{n}\right]$, such that $\mathfrak{A} f^{\prime}\left(t_{n}\right)=0$ for all $n$, but since $\lim _{t \rightarrow 0+} f^{\prime}(t)=C$, this would imply $\Re C=0$. In the same fashion, using the imaginary part of $f(x)$, we see that $\Im C=0$, which is a contradiction.

Since $f(t)$ is nonzero on a small interval starting at 0 , the composition with the $C^{\infty}$-function absolute value

$$
\text { I } 1: \mathbb{C} \backslash\{0\} \rightarrow \mathbb{R}_{+}
$$

will give a $C^{1}$-function $g(t)=|f(t)|$, on a small neighborhood of zero.

Solution to 1.4.29: By Taylor's Theorem [Rud87, p. 110], there is a constant $C$ such that

$$
\left|f(x)-f(0)-f^{\prime}(0) x\right| \leqslant C x^{2}
$$

when $|x|<1$. Since $f^{\prime}(0)=0$, we actually have $\left|f(x)-f(0)-f^{\prime}(0)\right| \leqslant C x^{2}$ and, consequently, by the triangle inequality, $f(x) \leqslant f(0)+C x^{2}$ when $|x|<1$. We conclude:

If $(x, y)$ lies on or below the graph of $f$ and $|x|<1$, then $y \leqslant f(0)+C x^{2}$.

Now consider the disc $D$ centered at $(0, f(0)+b)$ with radius $b$, where $0<b<1$ will be chosen at the end. Clearly, $(0, f(0))$ is on the boundary of D. On the other hand, if $(x, y) \in D$, then $|x|<b<1$ and

$$
\begin{gathered}
x^{2}+(y-f(0)-b)^{2}<b^{2} \\
|y-f(0)-b|<\sqrt{b^{2}-x^{2}} \\
y>f(0)+b-\sqrt{b^{2}-x^{2}}=f(0)+b-b \sqrt{1-x^{2} / b^{2}} \geqslant f(0)+b-b\left(1-x^{2} / 2 b^{2}\right)
\end{gathered}
$$

since $\sqrt{1-x} \leqslant 1-x / 2$ when $0 \leqslant x \leqslant 1$. Thus,

$$
y>f(0)+x^{2} / 2 b \text {. }
$$

If $1 / 2 b \geqslant c$, then it follows that $(x, y)$ must be above the graph of $f$. So we are done if we take $b=\min \{1 / 2,1 / 2 c\}$.

\subsection{Integral Calculus}

Solution to 1.5.1: Let $a=x_{0}<x_{1}<\cdots<x_{n}=b$ be any partition of $[a, b]$. Since $f^{\prime}$ is Riemann integrable, on each interval $\left[x_{i-1}, x_{i}\right]$ it has a finite supremum $M_{i}$ and infimum $m_{i}$.

By the Mean Value Theorem [Rud87, p. 108],

$$
\sum_{i=1}^{n} m_{i}\left(x_{i}-x_{i-1}\right) \leqslant \sum_{i=1}^{n}\left(f\left(x_{i}\right)-f\left(x_{i-1}\right) \leqslant \sum_{i=1}^{n} M_{i}\left(x_{i}-x_{i-1}\right) .\right.
$$

The middle sum simplifies to $f(b)-f(a)$, which therefore lies between the lower and upper sums for the partition. As these sums both converge to the integral of $f^{\prime}$ on $[a, b]$, we must have $\int_{a}^{b} f^{\prime}(x) d x=f(b)-f(a)$.

Solution to 1.5.2: Assume $f$ does not vanish identically. Then, for some $\delta>0$, the set $\{x \in(0,1) \mid f(x)>\delta\}$ is nonempty and, as $f$ is continuous, open; therefore it contains a closed interval $I$ of length $L>0$. Let $g$ be the function that equals $\delta$ on $I$ and 0 off $I$. Then $g$ is Riemann integrable with $\int_{0}^{1} g(x) d x=\delta L$. Since $f \geqslant g$ we have

$$
\int_{0}^{1} f(x) d x \geqslant \int_{0}^{1} g(x) d x=\delta L>0,
$$

as desired.

Solution 2. The properties of the Riemann integral we use are

- If $f, g$ are (Riemann) integrable on $[a, b]$ and $f(x) \geqslant g(x)$ for all $x$ then $\int_{a}^{b} f(x) d x \geqslant \int_{a}^{b} g(x) d x$

- If $f$ is integrable on $[a, b]$ and $a<c<b$ then $f$ is integrable on $[a, c]$ and on $[c, b]$, and $\int_{a}^{b} f(x) d x=\int_{a}^{c} f(x) d x+\int_{c}^{b} f(x) d x$.

Suppose that $f(c)>0$ for some $c$. We consider $0<c<1$. Since $f$ is continuous at $c$, there is an interval $[a, b]$ containing $c$ in which $f(x)>f(c) / 2$. Then

$$
\int_{a}^{b} f(x) d x \geqslant f(c)(b-a) / 2>0,
$$

and hence

$$
\int_{0}^{1} f(x) d x=\int_{0}^{a} f(x) d x+\int_{a}^{b} f(x) d x+\int_{b}^{1} f(x) d x>0
$$

a contradiction. A similar argument applies when $c$ is an endpoint of $[0,1]$.

Solution to 1.5.3: Since $f$ is continuous, it attains its minimum and maximum at $x_{0}$ and $y_{0}$, respectively, in $[0,1]$. So we have

$$
f\left(x_{0}\right) \int_{0}^{1} x^{2} d x \leqslant \int_{0}^{1} x^{2} f(x) d x \leqslant f\left(y_{0}\right) \int_{0}^{1} x^{2} d x
$$

or

$$
f\left(x_{0}\right) \leqslant 3 \int_{0}^{1} x^{2} f(x) d x \leqslant f\left(y_{0}\right) .
$$

Therefore, by the Intermediate Value Theorem [Rud87, p. 93], there is a point $\xi \in[0,1]$ with

$$
f(\xi)=3 \int_{0}^{1} x^{2} f(x) d x
$$

Solution to 1.5.4: Since the discontinuities are only of the first type (the limit exists), they do not have any accumulation point (for a detailed proof of this, see the Solution to Problem 1.2.8), and form a finite set. Let $d_{1}<d_{2}<\cdots<d_{n}$ be the set of discontinuities of $f$. Then $f$ is continuous in every interval $[x, y]$ with $d_{n}<x<y<d_{n+1}$; using the Solution to Problem 1.5.5 (on both endpoints of the interval), $f$ is integrable on each interval of the type $\left[d_{n}, d_{n+1}\right]$, so $f$ is integrable on $[a, b]$

Solution to 1.5.5: 1 . Let $|f(x)| \leqslant M$ for $x \in[0,1]$. If $\left(b_{n}\right)$ is a decreasing vanishing sequence, then $\int_{b_{n}}^{1}|f| \leqslant M$ is a bounded, increasing sequence, so it must converge. We conclude that $|f|$ is Riemann integrable over $[0,1]$, and so is $f$

2. The function $f(x)=1 / x$ is integrable over any interval $[b, 1]$ for positive $b$, but is not integrable over $[0,1]$.

Solution to 1.5.6: Using change of variables,

$$
\int_{-\infty}^{\infty}|f(x)| d x=\int_{0}^{\infty}(|f(x)|+|f(-x)|) d x
$$

If, for $x>1$, we had $x(|f(x)|+|f(-x)|)>1$, we would have the conver gen ce of the integral $\int_{1}^{\infty} \frac{1}{x} d x$. So, there is $x_{1}$ such that $x_{1}\left(\left|f\left(x_{1}\right)\right|+\left|f\left(-x_{1}\right)\right|\right) \leqslant 1$. If, for all $x>\max \left(2, x_{1}\right)$, we have $x(|f(x)|+|f(-x)|)>1 / 2$ we similarly conclude the convergence of $\int_{\max \left(2, x_{1}\right)}^{\infty}(1 / x) d x$, thus, there is $x_{2}>x_{1}$ with $x_{2}\left(\left|f\left(x_{2}\right)\right|+\left|f\left(-x_{2}\right)\right|\right) \leqslant 1 / 2$. Recursively we can then define a sequence $\left(x_{n}\right)$ such that $x_{n+1}>\max \left(n+1, x_{n}\right)$ and $x_{n}\left(\left|f\left(x_{n}\right)\right|+\left|f\left(-x_{n}\right)\right|\right) \leqslant 1 / n$. It follows that $x_{n} \rightarrow \infty$ and

$$
\lim _{n \rightarrow \infty} x_{n}\left|f\left(x_{n}\right)\right|=\lim _{n \rightarrow \infty} x_{n}\left|f\left(-x_{n}\right)\right|=0 .
$$

Solution to 1.5.7: 1. Letting $t=x+s$, we get

$$
f(x)=e^{x^{2} / 2} \int_{0}^{\infty} e^{-(x+s)^{2} / 2} d s=\int_{0}^{\infty} e^{-s x-s^{2} / 2} d s .
$$

Since $s>0, e^{-s^{2} / 2}<1$, so $e^{-s x-s^{2} / 2}<e^{-s x}$ for all positive $x$; then

$$
0<f(x)<\int_{0}^{\infty} e^{-s x} d s=\frac{1}{x} .
$$

2. Let $0<x_{1}<x_{2}$. For $s>0, e^{-s x_{1}-s^{2} / 2}>e^{-s x_{2}-s^{2} / 2}$, so

$$
f\left(x_{1}\right)=\int_{0}^{\infty} e^{-s x_{1}-s^{2} / 2} d s>\int_{0}^{\infty} e^{-s x_{2}-s^{2} / 2} d s=f\left(x_{2}\right)
$$

\section{Solution 2.}

1. The function $f$ is clearly positive. Using integration by parts, we get

$$
\int_{x}^{\infty} e^{-t^{2} / 2} d t=\int_{x}^{\infty} \frac{1}{t}\left(-e^{-t^{2} / 2}\right)^{\prime} d t=\frac{e^{-x^{2} / 2}}{x}-\int_{x}^{\infty} \frac{e^{-t^{2} / 2}}{t^{2}} d t
$$

Therefore,

$$
f(x)=\frac{1}{x}-e^{x^{2} / 2} \int_{x}^{\infty} \frac{e^{-t^{2} / 2}}{t^{2}} d t<\frac{1}{x}
$$

2. We have

$$
\begin{aligned}
f^{\prime}(x) &=\left(\frac{1}{x}-e^{x^{2} / 2} \int_{x}^{\infty} \frac{e^{-t^{2} / 2}}{t^{2}} d t\right)^{\prime} \\
&=-\frac{1}{x^{2}}+x e^{-x^{2} / 2} \int_{x}^{\infty} \frac{e^{-t^{2} / 2}}{t^{2}} d t+\frac{1}{x^{2}} \\
&=x e^{-x^{2} / 2} \int_{x}^{\infty} \frac{e^{-t^{2} / 2}}{t^{2}} d t>0,
\end{aligned}
$$

so $f$ is an increasing function.

Solution to 1.5.8: Integrating by parts and noting that $\varphi$ vanishes at 1 and 2 , we get

$$
\int_{1}^{2} e^{i \lambda x} \varphi(x) d x=\left.\frac{e^{i \lambda x}}{i \lambda} \varphi(x)\right|_{1} ^{2}-\frac{1}{i \lambda} \int_{1}^{2} e^{i \lambda x} \varphi^{\prime}(x) d x=-\frac{1}{i \lambda} \int_{1}^{2} e^{i \lambda x} \varphi^{\prime}(x) d x,
$$

applying integration by parts a second time and using the fact that $\varphi^{\prime}$ also vanishes at the endpoints, we get

$$
\int_{1}^{2} e^{i \lambda x} \varphi(x) d x=-\frac{1}{\lambda^{2}} \int_{1}^{2} e^{i \lambda x} \varphi^{\prime \prime}(x) d x .
$$

Taking absolute values gives

$$
\left|\int_{1}^{2} e^{i \lambda x} \varphi(x) d x\right| \leqslant \frac{1}{\lambda^{2}} \int_{1}^{2}\left|\varphi^{\prime \prime}(x)\right| d x .
$$

Since $\varphi \in C^{2}$, the integral on the right-hand side is finite, and we are done.

Solution to 1.5.9: Suppose that $f$ is such a function. Cauchy-Schwarz Inequality [MH93, p. 69] gives

$$
\begin{aligned}
a &=\int_{0}^{1} x f(x) d x \\
& \leqslant\left(\int_{0}^{1} x^{2} f(x) d x \int_{0}^{1} f(x) d x\right)^{1 / 2} \\
& \leqslant a .
\end{aligned}
$$

So we must have a chain of equalities. For equality to hold in the Cauchy-Schwarz Inequality, we must have $x \sqrt{f(x)}=k \sqrt{f(x)}$ for some constant $k$, so $\sqrt{f(x)} \equiv 0$, which contradicts

$$
\int_{0}^{1} f(x) d x=1
$$

Thus, no such function $f$ can exist.

Solution 2 . Multiplying the given identities by $\alpha^{2},-2 \alpha$, and 1 , respectively, we get

$$
\int_{0}^{1} f(x)(\alpha-x)^{2} d x=0
$$

but the integral above is clearly positive for every positive continuous function, so no such function can exist.

Solution to 1.5.10: Dividing the integral in $n$ pieces, we have

$$
\begin{aligned}
\left|\sum_{j=0}^{n-1} \frac{f(j / n)}{n}-\int_{0}^{1} f(x) d x\right| &=\left|\sum_{j=0}^{n-1}\left(\frac{f(j / n)}{n}-\int_{j / n}^{(j+1) / n} f(x) d x\right)\right| \\
& \leqslant \sum_{j=0}^{n-1} \int_{j / n}^{(j+1) / n}|f(j / n)-f(x)| d x .
\end{aligned}
$$

For every $x \in(j / n,(j+1) / n)$, applying the Mean Value Theorem [Rud87, p. $108]$, there is $c \in(j / n, x)$ with

$$
f^{\prime}(c)=\frac{f(x)-f(j / n)}{x-j / n} .
$$

As the derivative of $f$ is uniformly bounded by $M$, this gives us the inequality

$$
|f(x)-f(j / n)| \leqslant M(x-j / n) .
$$

Therefore,

$$
\begin{aligned}
\left|\sum_{j=0}^{n-1} \frac{f(j / n)}{n}-\int_{0}^{1} f(x) d x\right| & \leqslant \sum_{j=0}^{n-1} \int_{j / n}^{(j+1) / n} M(x-j / n) d x \\
&=M \sum_{j=0}^{n-1}\left(\frac{(j+1)^{2}}{2 n^{2}}-\frac{j^{2}}{2 n^{2}}\right)-\frac{j}{n^{2}} \\
&=M \sum_{j=0}^{n-1} \frac{1}{2 n^{2}} \\
&=\frac{M}{2 n}
\end{aligned}
$$

Solution to 1.5.11: Suppose not. Then, for some $\delta>0$, there is a sequence of real numbers, $\left(x_{n}\right)$, such that $x_{n} \rightarrow \infty$ and $\left|f\left(x_{n}\right)\right| \geqslant \delta$. Without loss of generality, we can assume $f\left(x_{n}\right) \geqslant \delta$.

Let $\varepsilon>0$ verify

$$
|f(x)-f(y)|<\frac{\delta}{2} \quad \text { for } \quad|x-y|<\varepsilon,
$$

then

$$
\sum_{n \geqslant 1} \int_{x_{n}-\varepsilon}^{x_{n}+\varepsilon} f(x) d x \geqslant \sum_{n \geqslant 1} 2 \varepsilon \frac{\delta}{2}=\infty
$$

contradicting the convergence of $\int_{0}^{\infty} f(x) d x$.

Solution to 1.5.12: Let

$$
g(x)=f(x)+\int_{0}^{x} f(t) d t .
$$

The result follows from the following claims.

Claim 1: $\liminf _{x \rightarrow \infty} f(x) \leqslant 0$. If not, there are $\varepsilon, x_{0}>0$ such that $f(x)>\varepsilon$ for $x>x_{0}$. Then, we have

$$
\begin{aligned}
g(x) &=f(x)+\int_{0}^{x_{0}} f(t) d t+\int_{x_{0}}^{x} f(t) d t \\
& \geqslant \varepsilon+\int_{0}^{x_{0}} f(t) d t+\varepsilon\left(x-x_{0}\right) .
\end{aligned}
$$

This is a contradiction since the right side tends to $\infty$ with $x$.

Claim 2: $\limsup _{x \rightarrow \infty} f(x) \geqslant 0$.

This follows from Claim 1 applied to the function $-f$.

Claim 3: $\limsup _{x \rightarrow \infty} f(x) \leqslant 0$.

Assume not. Then, for some $\varepsilon>0$ there is a sequence $x_{1}, x_{2}, \ldots$ tending to $\infty$ such that $f\left(x_{n}\right)>\varepsilon$ for all $n$. By Claim 1 , the function $f$ assumes values $\leqslant \varepsilon / 2$ for arbitrarily large values of its argument. Thus, after possibly deleting finitely many of the $x_{n}$ 's, we can find another sequence $y_{1}, y_{2}, \ldots$ tending to $\infty$ such that $y_{n}<x_{n}$ for all $n$ and $f\left(y_{n}\right) \leqslant \varepsilon / 2$ for all $n$. Let $z_{n}$ be the largest number in $\left[y_{n}, x_{n}\right]$ where $f$ takes the value $\varepsilon / 2$ (it exists by the Intermediate Value Theorem [Rud87, p. 93]). Then

$$
\begin{aligned}
g\left(x_{n}\right)-g\left(z_{n}\right) &=f\left(x_{n}\right)-f\left(z_{n}\right)+\int_{z_{n}}^{x_{n}} f(t) d t \\
&>\varepsilon-\frac{\varepsilon}{2}+\int_{z_{n}}^{x_{n}} \frac{\varepsilon}{2} d t \\
& \geqslant \frac{\varepsilon}{2}
\end{aligned}
$$

which contradicts the existence of $\lim _{x \rightarrow \infty} g(x)$.

Claim 4: $\liminf _{x \rightarrow \infty} f(x) \geqslant 0$.

Apply Claim 3 to the function $-f$.

Solution to 1.5.13: Suppose that for some $\varepsilon>0$, there is a sequence $x_{n} \rightarrow \infty$ with $x_{n} f\left(x_{n}\right) \geqslant \varepsilon$. Then, as $f$ is monotone decreasing, we have $f(x) \geqslant \varepsilon / x$ for $x$ large enough, which contradicts the convergence of $\int_{0}^{\infty} f(x) d x$, and the result follows.

Solution to 1.5.14: Let $0<\varepsilon<1$. As

$$
\int_{0}^{\infty} f(x) d x<\infty
$$

there is an $N>0$ such that for $n>N$,

$$
\int_{n}^{\infty} f(x) d x<\varepsilon
$$

Therefore, for $n$ large enough, that is, such that $n \varepsilon>N$, we have

$$
\int_{0}^{n}\left(\frac{x}{n}\right) f(x) d x=\int_{0}^{n \varepsilon}\left(\frac{x}{n}\right) f(x) d x+\int_{n \varepsilon}^{n}\left(\frac{x}{n}\right) f(x) d x
$$



$$
\begin{aligned}
&<\varepsilon \int_{0}^{n \varepsilon} f(x) d x+\int_{n \varepsilon}^{n} f(x) d x \\
&<\varepsilon \int_{0}^{n \varepsilon} f(x) d x+\varepsilon \\
&<\varepsilon\left(\int_{0}^{\infty} f(x) d x+1\right) .
\end{aligned}
$$

Since this inequality holds for all $\varepsilon>0$ and for all $n$ sufficiently large, it follows that

$$
\lim _{n \rightarrow \infty} \frac{1}{n} \int_{0}^{n} x f(x) d x=0 .
$$

Solution to 1.5.15: Using the Maclaurin expansion [PMJ85, p. 127] of $\sin x$, we get

$$
\frac{\sin x}{x}=\sum_{0}^{\infty}(-1)^{n} \frac{x^{2 n}}{(2 n+1) !} .
$$

The series above is alternating for every value of $x$, so we have

$$
\left|\frac{\sin x}{x}-\sum_{0}^{k}(-1)^{n} \frac{x^{2 n+1}}{(2 n+1) !}\right| \leqslant \frac{x^{2 k+2}}{(2 k+3) !} .
$$

Taking $k=2$, we have

$$
\left|I-\int_{0}^{1 / 2}\left(1-\frac{x^{2}}{3 !}\right) d x\right| \leqslant \int_{0}^{1 / 2} \frac{x^{4}}{5 !} d x
$$

which gives an approximate value of $71 / 144$ with an error bounded by $0.00013$.

Solution to 1.5.16: Let

$$
I(t)=\int_{0}^{1} \frac{d x}{\left(x^{4}+t^{4}\right)^{1 / 4}}+\log t .
$$

It suffices to show that for $t>0$, the function $I(t)$ is bounded below and monotonically increasing. For $x, t \geqslant 0$, we have $(x+t)^{4} \geqslant x^{4}+t^{4}$, so

$$
I(t) \geqslant \int_{0}^{1} \frac{d x}{x+t}+\log t=\int_{t}^{1+t} \frac{d u}{u}+\log t=\log (1+t) \geqslant 0 .
$$

We now show that $I^{\prime}(t) \geqslant 0$ for $t>0$. We have

$$
I(t)=\int_{0}^{t} \frac{d x}{t\left((x / t)^{4}+1\right)^{1 / 4}}+\int_{t}^{1} \frac{d x}{t\left((x / t)^{4}+1\right)^{1 / 4}}+\log t,
$$

letting $y=x / t$, we get

$$
I(t)=\int_{0}^{1} \frac{d y}{\left(y^{4}+1\right)^{1 / 4}}+\int_{1}^{1 / t} \frac{d y}{\left(y^{4}+1\right)^{1 / 4}}+\log t,
$$

so

$$
I^{\prime}(t)=\frac{-1}{t^{2}\left(1 / t^{4}+1\right)^{1 / 4}}+\frac{1}{t} \geqslant 0
$$

Solution to 1.5.17: Integrate by parts to get

$$
\int_{0}^{\infty} f(x)^{2} d x=\int_{0}^{\infty}\left(\frac{d}{d x} x\right) f(x)^{2} d x=-\int_{0}^{\infty} x \cdot 2 f(x) f^{\prime}(x) d x
$$

The boundary terms vanish because $x f(x)^{2}=0$ at $x=0$ and $\infty$. By the CauchySchwarz Inequality [MH93, p. 69],

$$
\left|\int_{0}^{\infty} x f(x) f^{\prime}(x) d x\right| \leqslant \sqrt{\int_{0}^{\infty} x^{2} f(x)^{2} d x} \sqrt{\int_{0}^{\infty} f^{\prime}(x)^{2} d x} .
$$

Solution to 1.5.18: Consider the figure

![](https://cdn.mathpix.com/cropped/2022_10_26_8d6ad07375e689af3a1cg-051.jpg?height=819&width=830&top_left_y=1412&top_left_x=645)

The left side of the desired inequality is the sum of the areas of the two shaded regions. Those regions together contain a rectangle of sides $a$, and $b$, from which the inequality follows. The condition for equality is $b=f(a)$, the condition that the two regions fill the rectangle. Solution 2. Without loss of generality, assume $f(a) \leqslant b$. We have

$$
a b=\int_{0}^{a} f(x) d x+\int_{0}^{a}(b-f(x)) d x .
$$

The second integral is

$$
\lim _{n \rightarrow \infty} \frac{a}{n} \sum_{k=0}^{n-1}\left(b-f\left(\frac{(k+1) a}{n}\right)\right)
$$

For $0 \leqslant k \leqslant n-1$,

$$
\frac{a}{n}=\frac{(k+1) a}{n}-\frac{k a}{n}=g \circ f\left(\frac{(k+1) a}{n}\right)-g \circ f\left(\frac{k a}{n}\right) .
$$

Substituting in the limit above,

$$
\lim _{n \rightarrow \infty} \sum_{k=0}^{n-1}\left(b-f\left(\frac{(k+1) a}{n}\right)\right)\left(g \circ f\left(\frac{(k+1) a}{n}\right)-g \circ f\left(\frac{k a}{n}\right)\right) .
$$

Multiplying out each term in the sum and rearranging them, and noting that $f(0)=g(0)=0$, we get

$$
\lim _{n \rightarrow \infty} \sum_{k=0}^{n-1} g \circ f\left(\frac{k a}{n}\right)\left(f\left(\frac{(k+1) a}{n}\right)-f\left(\frac{k a}{n}\right)\right)+a b-a f(a) .
$$

Since $g$ is continuous, this equals

$$
\int_{0}^{f(a)} g(y) d y+a(b-f(a)) .
$$

As $g(y) \geqslant a$ for $y \in(f(a), b)$, we have

$$
a(b-f(a)) \leqslant \int_{f(a)}^{b} g(y) d y .
$$

This gives the desired inequality. Also, we see that equality holds iff $f(a)=b$.

Solution to 1.5.19: Given $\varepsilon>0$, choose $R$ so that $\int_{|x| \geqslant R}|f(x)| d x<\varepsilon / 4$. Then $\int_{|x| \geqslant R}|f(x) \cos x y| d x<\varepsilon / 4$ for all $y$. So

$$
\begin{aligned}
|g(z)-g(y)|=& \int_{|x| \geqslant R} f(x)(\cos x z-\cos x y) d x \\
&+\int_{|x| \geqslant R} f(x)(\cos x z-\cos x y) d x \\
& \leqslant \frac{\varepsilon}{2}+\int_{|x| \geqslant R}|f(x)||\cos x z-\cos x y| d x .
\end{aligned}
$$

The latter integral approaches 0 as $z \rightarrow y$ by uniform convergence of $\cos (x z)$ to $\cos (x y)$ on the compact interval $-R \leqslant x \leqslant R$. Hence, for $|z-y|$ sufficiently small,

$$
|g(z)-g(y)|<\frac{\varepsilon}{2}+\frac{\varepsilon}{2}=\varepsilon
$$

and $g$ is continuous.

Solution to 1.5.20: Let $M$ be an upper bound of $|f|$, and let $K=\int_{-\infty}^{\infty}|g(x)| d x$. Fix $\varepsilon>0$. Since $\lim _{|x| \rightarrow \infty} f(x)=0$, there exists $R_{1}>0$ such that $|f(x)|<\varepsilon$ for $|x|>R_{1}$. As $K$ is finite, there is $R_{2}>0$ such that $\int_{|x|>R_{2}}|g(x)| d x<\varepsilon$. For $|x|>R_{1}+R_{2}$ we have then

$$
\begin{aligned}
|h(x)| & \leqslant \int_{-R_{2}}^{R_{2}}|f(x-y)||g(y)| d y+\int_{|y|>R_{2}}|f(x-y)||g(y)| d y \\
& \leqslant \varepsilon \int_{-R_{2}}^{R_{2}}|g(y)| d y+M \int_{|y|>R_{2}}|g(y)| d y \\
& \leqslant \varepsilon(K+M) .
\end{aligned}
$$

As $\varepsilon$ is arbitrary, the desired conclusion follows.

Solution to 1.5.21: We will do the proof of the sine integral only. For $n \geqslant 0$, let

$$
S_{n}=\int_{\sqrt{n \pi}}^{\sqrt{(n+1) \pi}} \sin x^{2} d x .
$$

We show that the series $\sum S_{n}$ converges and use this to show that the integral converges.

By the choice of the domains of integration, the $S_{n}$ 's alternate in sign. Also, setting $u=x^{2}$, we get

$$
\begin{aligned}
2\left|S_{n}\right| &=\left|\int_{n \pi}^{(n+1) \pi} \frac{\sin u}{\sqrt{u}} d u\right| \\
&>\left|\int_{n \pi}^{(n+1) \pi} \frac{\sin u}{\sqrt{u+\pi}} d u\right| \\
&=\left|\int_{(n+1) \pi}^{(n+2) \pi} \frac{\sin u}{\sqrt{u}} d u\right| \\
&=2\left|S_{n+1}\right| .
\end{aligned}
$$

Finally, the $S_{n}$ 's tend to 0:

$$
2\left|S_{n}\right|=\left|\int_{n \pi}^{(n+1) \pi} \frac{\sin u}{\sqrt{u}} d u\right|<\frac{1}{\sqrt{n \pi}}
$$

and the right-hand side gets arbitrarily small as $n$ tends to infinity. Therefore, by Leibniz Criterion [Rud87, p. 71], the series $\sum S_{n}$ converges. Let $a>0$ and $n$ be such that $\sqrt{n \pi} \leqslant a<\sqrt{(n+1) \pi}$. Then

$$
\int_{0}^{a} \sin x^{2} d x-\sum_{k=0}^{\infty} S_{k}=\int_{a}^{\sqrt{(n+1) \pi}} \sin x^{2} d x-\sum_{k=n+1}^{\infty} S_{k} .
$$

The second term tends to zero as $n$ tends to infinity. By estimates almost identical to those above,

$$
\left|\int_{a}^{\sqrt{(n+1) \pi}} \sin x^{2} d x\right| \leqslant \frac{\left|(n+1) \pi-a^{2}\right|}{2 a} \leqslant \frac{\pi}{2 \sqrt{n \pi}},
$$

so the first term does as well. Therefore, we have

$$
\int_{0}^{\infty} \sin x^{2} d x=\sum_{n=0}^{\infty} S_{n}<\infty
$$

Solution to 1.5.22: Let $p(x)=\sum_{j=0}^{k} a_{j} x^{j}$ be a polynomial. We have

$$
\lim _{n \rightarrow \infty}(n+1) \int_{0}^{1} x^{n} p(x) d x=\lim _{n \rightarrow \infty} \sum_{j=0}^{k} \frac{n+1}{n+j+1} a_{j}=p(1) .
$$

So the result holds for polynomials. Now let $f$ be a continuous function and $\varepsilon>0$. By the Stone-Weierstrass Approximation Theorem [MH93, p. 284], there is a polynomial $p$ with $\|f-p\|_{\infty}<\varepsilon$. So

$$
\begin{aligned}
\left|(n+1) \int_{0}^{1} x^{n} f(x) d x-f(1)\right| \leqslant &\left((n+1) \int_{0}^{1} x^{n}|f(x)-p(x)| d x\right) \\
&+\left|(n+1) \int_{0}^{1} x^{n} p(x) d x-f(1)\right| \\
\leqslant \varepsilon+\left|(n+1) \int_{0}^{1} x^{n} p(x) d x-f(1)\right| \\
& \rightarrow \varepsilon+|p(1)-f(1)| \\
&<2 \varepsilon .
\end{aligned}
$$

Since $\varepsilon$ is arbitrary, the desired limit holds.

Solution to 1.5.23: $\log x$ is integrable near zero, and near infinity it is dominated by $\sqrt{x}$, so the given integral exists finitely. Making the change of variables $x=a / t$, it becomes 

$$
\begin{aligned}
\int_{0}^{\infty} \frac{\log x}{x^{2}+a^{2}} d x &=\frac{\log a}{a} \int_{0}^{\infty} \frac{d t}{1+t^{2}}-\frac{1}{a} \int_{0}^{\infty} \frac{\log t}{1+t^{2}} d t \\
&=\left.\frac{\log a}{a} \arctan t\right|_{0} ^{\infty}-J \\
&=\frac{\pi \log a}{2 a}-J .
\end{aligned}
$$

If we treat $J$ in a similar way, we get $J=-J$, so $J=0$ and the given integral equals

$$
\frac{\pi \log a}{2 a}
$$

Solution 2. We split the integral in two and use the substitution $x=a^{2} / y$.

$$
\begin{aligned}
\int_{0}^{\infty} \frac{\log x}{x^{2}+a^{2}} d x &=\int_{0}^{a} \frac{\log x}{x^{2}+a^{2}} d x+\int_{a}^{\infty} \frac{\log x}{x^{2}+a^{2}} d x \\
&=\int_{0}^{a} \frac{\log x}{x^{2}+a^{2}} d x+\int_{a}^{0} \frac{\log \left(a^{2} / y\right)}{a^{2}+\left(a^{2} / y\right)^{2}}\left(-\frac{a^{2}}{y^{2}}\right) d y \\
&=\int_{0}^{a} \frac{\log x}{x^{2}+a^{2}} d x+\int_{0}^{a} \frac{2 \log a-\log y}{a^{2}+y^{2}} d y \\
&=\int_{0}^{a} \frac{2 \log a}{a^{2}+y^{2}} d y \\
&=\left.2 \frac{\log a}{a} \arctan \frac{y}{a}\right|_{0} ^{a} \\
&=\frac{\pi \log a}{2 a} .
\end{aligned}
$$

Solution 3. Using residues, and arguing as in the solution to Problem 5.11.28, we get, for $f(z)=\frac{\log z}{z^{2}+a^{2}}$,

$$
\int_{-\infty}^{0} f(z) d z+\int_{0}^{\infty} f(z) d z=2 \pi i \operatorname{Res}(f, i a)=\frac{\pi}{a}\left(\log a+\frac{i \pi}{2}\right) .
$$

On the negative real axis, we have

$$
\begin{aligned}
\int_{-\infty}^{0} f(z) d z &=\int_{-\infty}^{0} \frac{\log (-x)+\pi i}{x^{2}+a^{2}} \\
&=\int_{0}^{\infty} \frac{\log x}{x^{2}+a^{2}} d x+\int_{0}^{\infty} \frac{\pi i}{x^{2}+a^{2}} d x \\
&=\int_{0}^{\infty} \frac{\log x}{x^{2}+a^{2}} d x+\frac{\pi^{2} i}{2 a},
\end{aligned}
$$

therefore,

$$
2 \int_{0}^{\infty} \frac{\log x}{x^{2}+a^{2}} d x+\frac{\pi^{2} i}{2 a}=\frac{\pi}{a}\left(\log a+\frac{i \pi}{2}\right)
$$

and

$$
\int_{0}^{\infty} \frac{\log x}{x^{2}+a^{2}} d x=\frac{\pi \log a}{2 a} .
$$

Solution to 1.5.24: As $\sin x \leqslant 1$, to show that $I$ converges, it is enough to show that $I>-\infty$. By the symmetry of $\sin x$ around $\pi / 2$, we have

$$
\begin{aligned}
I &=\int_{0}^{\pi / 2} \log (\sin x) d x+\int_{\pi / 2}^{\pi} \log (\sin x) d x \\
&=2 \int_{0}^{\pi / 2} \log (\sin x) d x \\
& \geqslant 2 \int_{0}^{\pi / 2} \log (2 x / \pi) d x \\
&>-\infty .
\end{aligned}
$$

The first inequality holds since on $[0, \pi / 2], \sin x \geqslant 2 x / \pi$; see Problem 1.1.25. Letting $x=2 u$, we get

$$
\begin{aligned}
I &=2 \int_{0}^{\pi / 2} \log (\sin 2 u) d u \\
&=2\left(\int_{0}^{\pi / 2} \log 2 d u+\int_{0}^{\pi / 2} \log (\sin u) d u+\int_{0}^{\pi / 2} \log (\cos u) d u\right) .
\end{aligned}
$$

The first integral equals $(\pi / 2) \log 2$. As $\cos u=\sin (\pi / 2-u)$, the last integral is

$$
\int_{0}^{\pi / 2} \log (\sin (\pi / 2-u)) d u=\int_{0}^{\pi / 2} \log (\sin u) d u=\int_{\pi / 2}^{\pi} \log (\sin u) d u .
$$

The above equation becomes $I=\pi \log 2+2 I$, so $I=-\pi \log 2$.

Solution to 1.5.25: The integrand is continuous at $x=0$, so that no trouble arises there. For $r>0$ let

$$
I(r)=\int_{0}^{r} \frac{\sin x}{\sqrt{x}} d x .
$$

For $n$ a positive integer,

$$
I(n \pi)=\sum_{k=1}^{n}(-1)^{k-1} a_{k},
$$

where

$$
a_{k}=\int_{(k-1) \pi}^{k \pi} \frac{|\sin x|}{\sqrt{x}} d x .
$$

From the $\pi$-periodicity of $|\cdot \sin x|$ it follows that $a_{k}>a_{k+1}$ for all $k$, while $a_{k}<1 / \sqrt{(k-1) \pi} \rightarrow 0$. Hence $\lim _{n \rightarrow \infty} I(n \pi)$ exists finitely by the alternating series test. For $r>0$ let $n_{r}$ be the smallest integer such that $r<n \pi$. Then

$$
I(r)-I\left(n_{r} \pi\right)=O\left(\frac{1}{\sqrt{r}}\right) \quad(r \rightarrow \infty),
$$

so $\lim _{n \rightarrow \infty} I(r)$ exists finitely, which means $\int_{0}^{\infty} \frac{\sin x}{\sqrt{x}} d x$ converges. On the other hand,

$$
\begin{aligned}
\int_{0}^{\infty} \frac{|\sin x|}{\sqrt{x}} d x & \geqslant \sum_{k=0}^{\infty} \int_{k \pi+\frac{\pi}{6}}^{k \pi+\frac{5 \pi}{6}} \frac{|\sin x|}{\sqrt{x}} d x \\
& \geqslant \sum_{k=0}^{\infty} \frac{1}{2} \frac{1}{\sqrt{(k+1) \pi}} \frac{2 \pi}{3} \\
&=\frac{\sqrt{\pi}}{3} \sum_{k=0}^{\infty} \frac{1}{\sqrt{k+1}}=\infty .
\end{aligned}
$$

An alternative proof of the convergence of the integral uses integration by parts:

$$
\begin{aligned}
\int_{1}^{r} \frac{\sin x}{\sqrt{x}} d x &=-\int_{1}^{r} \frac{1}{\sqrt{x}} d(\cos x) \\
&=-\left.\frac{\cos x}{\sqrt{x}}\right|_{1} ^{r}+\frac{1}{2} \int_{1}^{r} \frac{\cos x}{x^{3 / 2}} d x \\
&=\cos 1-\frac{\cos r}{\sqrt{r}}+\frac{1}{2} \int_{1}^{r} \frac{\cos x}{x^{3 / 2}} d x .
\end{aligned}
$$

The right side converges as $r \rightarrow \infty$ because $\int_{1}^{\infty} x^{-3 / 2} d x<\infty$.

\subsection{Sequences of Functions}

Solution to 1.6.1: Let $B$ be the set of function that are the pointwise limit of continuous functions defined on $[0,1]$. The characteristic functions of intervals, $\chi_{I}$, are in $B$. Notice also that as $f$ is monotone, the inverse image of an interval is an interval, and that linear combinations of elements of $B$ are in $B$. Without loss of generality, assume $f(0)=0$ and $f(1)=1$. For $n \in \mathbb{N}$, let the functions $g_{n}$ be defined by

$$
g_{n}(x)=\sum_{k=0}^{n-2} \frac{k}{n} \chi_{f^{-1}\left(\left[\frac{k}{n}, \frac{k+1}{n}\right)\right)}(x)+\frac{n-1}{n} \chi_{f^{-1}\left(\left[\frac{n-1}{n}, 1\right]\right)}^{(x) .}
$$



![](https://cdn.mathpix.com/cropped/2022_10_26_8d6ad07375e689af3a1cg-058.jpg?height=754&width=824&top_left_y=203&top_left_x=648)

From the construction we can easily see that $\max _{x \in[0,1]}\left|g_{n}(x)-f(x)\right| \leqslant \frac{1}{n}$, consider now the following result:

Lemma: Let $\left\{h_{n}\right\} \subset B$ with $\max _{x \in[0,1]}\left|h_{n}(x)\right| \leqslant A_{n}$ and $\sum_{n=1}^{\infty} A_{n}<\infty$. Then $\sum_{\substack{n=1 \\ \text { we get }}}^{\infty} h_{n} \in B$. As $\left|g_{2^{k+1}}-g_{2^{k}}\right| \leqslant\left|g_{2^{k+1}}-f\right|+\left|g_{2^{k}}-f\right| \leqslant \frac{1}{2^{k-1}}$ and $\sum \frac{1}{2^{k-1}}<\infty$,
we

$$
\sum_{k=1}^{\infty}\left(g_{2^{k+1}}-g_{2^{k}}\right)=f-g_{2} \in B
$$

so $f-g_{2}+g_{2}=f \in B$.

Proof of the Lemma: For each $n$ let $h_{n}$ be the pointwise limit of $\left\{\varphi_{k}^{n}\right\} \subset B$ such that $\left|h_{n}(x)\right| \leqslant A_{n}$ on $[0,1]$. Consider the functions $\Phi_{k}=\sum_{n=1}^{k} \varphi_{k}^{n}$. Given $\varepsilon>0$, take $m$ such that $\sum_{n=m+1}^{\infty} A_{n}<\varepsilon / 3$. Then the sum $\sum_{n=m+1}^{\infty}\left|h_{n}(x)\right|<\varepsilon / 3$ and $\sum_{n=m+1}^{\infty}\left|\varphi_{k}^{n}(x)\right|<\varepsilon / 3$

For $x \in[0,1]$, take $K$ so that

$$
\left|h_{n}(x)-\varphi_{K}^{n}(x)\right|<\frac{\varepsilon}{3 m} \text { for } n=1, \ldots, m .
$$

For $k>K$ we then have

$$
\begin{aligned}
&\left|\sum_{n=1}^{\infty} h_{n}(x)-\Phi_{k}(x)\right| \leqslant \sum_{k=1}^{m}\left|h_{n}(x)-\varphi_{k}^{n}(x)\right|+\sum_{n=m+1}^{\infty}\left|h_{n}(x)\right|+\sum_{n=m+1}^{k}\left|\varphi_{k}^{n}(x)\right|<\varepsilon \\
&\text { so } \sum_{n=1}^{\infty} h_{n} \in B .
\end{aligned}
$$

Solution to 1.6.2: Let $a<b$ be real numbers and $\varepsilon>0$. Take $n$ large enough so

$$
\left|f_{n}(a)-g(a)\right|<\varepsilon \text { and }\left|f_{n}(b)-g(b)\right|<\varepsilon
$$

Then, using the Mean Value Theorem [Rud87, p. 108],

$$
|g(a)-g(b)| \leqslant\left|g(a)-f_{n}(a)\right|+\left|f_{n}(a)-f_{n}(b)\right|+\left|f_{n}(b)-g(b)\right|<2 \varepsilon+\left|f_{n}^{\prime}(\xi)\right||b-a|
$$

where $a<\xi<b$. As the inequality holds for any $\varepsilon>0$,

$$
|g(a)-g(b)| \leqslant|b-a|
$$

and the continuity of $g$ follows.

Solution 2. Let $N>0$. We will show that $g$ is continuous in $[-N, N]$. We have, for any $n \in \mathbb{N}$, by the Mean Value Theorem,

$$
\left|f_{n}(x)-f_{n}(y)\right|=\left|f_{n}^{\prime}(\xi)(x-y)\right| \leqslant 2 N \text { for } x, y \in[-N, N]
$$

So the sequence $\left\{f_{n}\right\}$ is bounded. The relation

$$
\left|f_{n}(x)-f_{n}(y)\right|=\left|f_{n}^{\prime}(\xi)(x-y)\right| \leqslant|x-y|
$$

shows that it is also equicontinuous. By the Arzelà-Ascoli Theorem [MH93, p. 273] we may assume that $\left\{f_{n}\right\}$ converges uniformly on $[-N, N]$. The function $g$, being the uniform limit of continuous functions, is continuous as well. As $N$ is arbitrary, we are done.

Solution to 1.6.3: Let $\varepsilon>0$. By uniform continuity, for some $\delta>0$ we have $|f(x)-f(y)|<\varepsilon$ for $|x-y|<\delta$. Take $N$ satisfying $1 / N<\delta$. For $0 \leqslant k \leqslant N$, let $\xi_{k}=k / N$, and divide $[0,1]$ into the intervals $\left[\xi_{k-1}, \xi_{k}\right], 1 \leqslant k \leqslant N$. Since $f_{n}$ tends to $f$ pointwise, by taking the maximum over the finite set $\left\{\xi_{k}\right\}$ we know that there exists $M>0$ such that if $n \geqslant M$, then $\left|f_{n}\left(\xi_{k}\right)-f\left(\xi_{k}\right)\right|<\varepsilon$ for $0 \leqslant k \leqslant N$. Each of the $f_{n}$ 's is nondecreasing, so we have, for $x \in\left[\xi_{k-1}, \xi_{k}\right]$,

$$
f\left(\xi_{k-1}\right)-\varepsilon<f_{n}(x)<f\left(\xi_{k-1}\right)+2 \varepsilon
$$

or

$$
\left|f_{n}(x)-f\left(\xi_{k-1}\right)\right|<2 \varepsilon .
$$

Therefore,

$$
\left|f_{n}(x)-f(x)\right| \leqslant\left|f_{n}(x)-f\left(\xi_{k-1}\right)\right|+\left|f\left(\xi_{k-1}\right)-f(x)\right|<3 \varepsilon
$$

Since this bound does not depend on $x$, the convergence is uniform.

Solution to 1.6.4: Assume the contrary. Then, for some real $x$ and positive $\varepsilon$, given any $\delta>0$, we can find $y \in(x-\delta, x+\delta)$ such that $|f(x)-f(y)| \geqslant \varepsilon$. For each positive integer $m$ let $y_{m} \in\left(x-\frac{1}{m}, x+\frac{1}{m}\right)$ with $\left|f(x)-f\left(y_{m}\right)\right| \geqslant \varepsilon$. We know that $\lim _{n \rightarrow \infty} f_{n}\left(y_{m}\right)=f\left(y_{m}\right)$, therefore, given $\varepsilon>0$, we can find, for each $m$, an integer $n_{m}$ such that

$$
\left|f_{n_{m}}\left(y_{m}\right)-f\left(y_{m}\right)\right|<\varepsilon / 2,
$$

and choose them so $n_{1}<n_{2}<\cdots$.

Consider the sequence $\left(x_{n}\right)$ defined by $x_{n}=y_{m}$ for $n_{m-1}<n \leqslant n_{m}$. Then $x_{n} \rightarrow x$, but $f_{n}\left(x_{n}\right) \nrightarrow f(x)$ since, for each $m$,

$$
\begin{aligned}
\left|f_{n_{m}}\left(x_{n_{m}}\right)-f(x)\right| &=\left|f_{n_{m}}\left(y_{m}\right)-f(x)\right| \\
& \geqslant\left|f(x)-f\left(y_{m}\right)\right|-\left|f\left(y_{m}\right)-f_{n_{m}}\left(y_{m}\right)\right| \\
&>\varepsilon-\varepsilon / 2=\varepsilon / 2 .
\end{aligned}
$$

Solution to 1.6.6: 1 . For $k \in \mathbb{N}$, consider the continuous functions $g_{k}$ given by

$$
g_{k}(x)= \begin{cases}4 k-16 k^{2}\left|x-\frac{3}{4} k\right| & \text { if } x \in\left[\frac{1}{2 k}, \frac{1}{k}\right] \\ 0 & \text { if } x \notin\left[\frac{1}{2 k}, \frac{1}{k}\right]\end{cases}
$$

Define $f_{0} \equiv 0$, and, for $k>0, f_{k}(x)=\int_{0}^{x} g_{k}(t) d t$. We have $f_{k} \in C^{1}\left(\mathbb{R}_{+}\right)$, $f_{k}(0)=0$, and $f_{k}^{\prime}(x)=g_{k}(x) \rightarrow 0=f_{0}^{\prime}(x)$ for all $x \in \mathbb{R}_{+}$. However,

$$
\lim _{n \rightarrow \infty} f_{k}(x)=\lim _{n \rightarrow \infty} \int_{0}^{x} g_{k}(t) d t=\int_{0}^{\infty} g_{k}(t) d t=1 \neq f_{0}(x) .
$$

2. $f_{k}^{\prime}(x) \rightarrow f_{0}^{\prime}(x)$ uniformly on $\mathbb{R}$.

Solution to 1.6.7: As $f$ is a homeomorphism of $[0,1]$ onto itself, we may assume without loss of generality (by replacing $f$ by $1-f$ ) that $f$ is strictly increasing, with $f(0)=0$ and $f(1)=1$. We first treat the case where $f^{\prime}$ is a continuous function. By the Stone-Weierstrass Approximation Theorem [MH93, p. $284]$, there is a sequence of polynomials $\left\{P_{n}\right\}$ which converge to $f$ uniformly. Since $f^{\prime}>0$, we may assume (by adding a small constant) that each of the $P_{n}$ is positive. Further, since the $P_{n}$ 's converge uniformly,

$$
\int_{0}^{1} P_{n}(t) d t \rightarrow \int_{0}^{1} f^{\prime}(t) d t=f(1)=1 .
$$

Defining $a_{n}$ by

$$
a_{n}^{-1}=\int_{0}^{1} P_{n}(t) d t
$$

we can replace each $P_{n}$ by $a_{n} P_{n}$, so we may assume that

$$
\int_{0}^{1} P_{n}(t) d t=1 .
$$

Now consider the polynomials

$$
Q_{n}(x)=\int_{0}^{x} P_{n}(t) d t
$$

$Q_{n}(0)=0, Q_{n}(1)=1$, and $Q_{n}^{\prime}(x)=P_{n}(x)>0$ for all $x$ and $n$. Hence, each $Q_{n}$ is a homeomorphism of the unit interval onto itself, and by their definition, the $Q_{n}$ 's converge to $f$ uniformly.

It is enough now to show that any increasing homeomorphism of the unit interval onto itself can be uniformly approximated by $C^{1}$ homeomorphisms. Let $r>0$ and

$$
f_{r}(x)=\left\{\begin{array}{llc}
e^{1-x^{-r}} & \text { for } & 0<x \leqslant 1 \\
0 & \text { for } & x=0
\end{array}\right.
$$

A calculation shows that $f_{r}$ is $C^{1}$ on $[0,1], f_{r}(1)=1, f_{r}^{\prime}(0)=0$, and $f_{r}^{\prime}(1)=r$. For $r, s>0$, let

$$
g_{r s}(x)= \begin{cases}f_{r}(x) & \text { for } \quad x \in[0,1] \\ -f_{s}(-x) & \text { for } \quad x \in[-1,0]\end{cases}
$$

Each $g_{r s}$ is a $C^{1}$-function such that $g_{r s}(0)=0, g_{r s}(1)=1, g_{r s}(-1)=-1$, $g_{r s}^{\prime}(-1)=s$, and $g_{r s}^{\prime}(1)=r$. By scaling and translating, we can find a $C^{1}$ function on any interval such that its values and the values of its derivative at both endpoints are any given positive values desired.

We can now approximate any continuous homeomorphism $f$ as follows: Given $\varepsilon>0$, choose $n>0$ such that if $|x-y|<1 / n$, for these values $|f(x)-f(y)|<\varepsilon$. (This is possible since $[0,1]$ is compact, so $f$ is uniformly continuous there.) Partition $[0,1]$ into $2 n$ intervals of equal length. On the intervals $[2 k / 2 n,(2 k+1) / 2 n], 0 \leqslant k \leqslant n-1$, approximate $f$ by the line segment joining $f(2 k / 2 n)$ and $f((2 k+1) / 2 n)$. On the other intervals, join the line segments by suitable functions as defined above to make the approximating function $C^{1}$. Since $f$ is an increasing function, this approximating function will always lie within $\varepsilon$ of it.

Solution to 1.6.8: 1. Let $S_{\varepsilon}=\{x \in[0,1] \mid f(x) \geqslant M-\varepsilon\}$ for positive $\varepsilon . f(x) \geqslant$ $M-\varepsilon$ if and only if for each $n, f_{n}(x) \geqslant M-\varepsilon$, so $S_{\varepsilon}=\bigcap_{n \geqslant 1} f_{n}^{-1}([M-\varepsilon, \infty))$. So each $S_{\varepsilon}$ is closed. By definition of supremum, each set $S_{\varepsilon}$ is nonempty. Also, if $\varepsilon_{i}$ are finitely many positive numbers, $\bigcap_{i} S_{\varepsilon_{i}}=S_{\min \varepsilon_{i}} \neq \emptyset$. As $[0,1]$ is compact, the intersection of all sets $S_{\varepsilon}$ is nonempty. Let $t$ belong to this intersection. Then $M \geqslant f(t) \geqslant M-\varepsilon$ for arbitrary $\varepsilon>0$, so $f(t)=M$.

2. Take $f_{n}(x)=\min \{n x, 1-x\}$.

Solution to 1.6.9: Fix $\varepsilon>0$. For each $n$, let $G_{n}=\left\{x \mid f_{n}(x)<\varepsilon\right\}$. Then

- $G_{n}$ is open, since $f_{n}$ is continuous.

- $G_{n} \subset G_{n+1}$ since $f_{n} \geqslant f_{n+1}$ - $[0,1]=\cup_{n=1}^{\infty} G_{n}$, since $f_{n}(x) \rightarrow 0$, for each $x$.

Since $[0,1]$ is compact, a finite number of the $G_{n}$ cover $[0,1]$, and by the second condition above, there is $N$ such that $G_{n}=[0,1]$ for all $n \geqslant N$. By the definition of $G_{n}$, for all $n \geqslant N$, we have $0 \leqslant f_{n}(x)<\epsilon$ for all $x \in[0,1]$. This proves that the sequence $f_{n}$ converges uniformly to 0 on $[0,1]$.

Solution to 1.6.10: If $f_{n}(x) \rightarrow f(x)$ pointwise on $E$, then $f_{n}$ converges to $f$ uniformly on $E$ if and only if $\sup _{E}\left|f_{n}(x)-f(x)\right|$ tends to 0 as $n$ tends to $\infty$. We note that if $A \subset B$ then $\sup _{A}\left|f_{n}(x)-f(x)\right| \leqslant \sup _{B}\left|f_{n}(x)-f(x)\right|$.

$k=0$. In this case $\left|f_{n}(x)\right| \leqslant 1 / n$ so $\sup _{\mathbb{R}}\left|f_{n}\right| \rightarrow 0$ as $n \rightarrow \infty$. Hence $f_{n}$ converges uniformly on $\mathbb{R}$, and also on every bounded subset of $\mathbb{R}$.

$k=1 . f_{n}(x)=\frac{x}{x^{2}+n} \rightarrow 0$ pointwise, and the derivative is $f_{n}^{\prime}(x)=\frac{n-x^{2}}{\left(x^{2}+n\right)^{2}}$, so $\left|f_{n}\right|$ attains its maximum at $x=\sqrt{n}$, where

$$
\sup \left|f_{n}\right|=f_{n}(\sqrt{n})=\frac{1}{2 \sqrt{n}}
$$

so that sup $\left|f_{n}\right| \rightarrow 0$ as $n \rightarrow \infty$, hence the convergence is uniform on $\mathbb{R}$ and also on bounded sets.

$k=2$. $f_{n}(x)=\frac{x^{2}}{x^{2}+n}$ and sup $\left|f_{n}\right|=1$, thus the convergence is not uniform on $\mathbb{R}$. However, since the function is even and increasing on $\mathbb{R}_{+}$, the supremum on $[-a, a]$ is $\sup \left|f_{n}\right|=f_{n}(a)=\frac{a^{2}}{a^{2}+n}$ so that $\sup \left|f_{n}\right| \rightarrow 0$ as $n \rightarrow \infty$ and the convergence is uniform on bounded subsets.

$k \geqslant 3$. Again $f_{n}(x) \rightarrow 0$ pointwise, but sup $\left|f_{n}\right|=\infty$, thus the convergence is not uniform on $\mathbb{R}$. In the same way as above, $\left|f_{n}\right|$ is even, and increasing on $\mathbb{R}_{+}$, this can be seen directly or by computing the derivative

$$
f_{n}^{\prime}(x)=\frac{x^{k-1}\left((k-2) x^{2}+k n\right)}{\left(x^{2}+n\right)^{2}}
$$

so sup $\left|f_{n}\right|=f_{n}(a) \rightarrow 0$ as $n \rightarrow \infty$. Thus the convergence is uniform on bounded sets.

Hence the convergence is uniform on $\mathbb{R}$ when $k=0$ and 1 only, and uniform on bounded sets for all $k$.

Solution to 1.6.11: Let $\mathcal{P}_{k}$ be the set of polynomials of degree $\leqslant k$, and let $L=$ $d\left(f, \mathcal{P}_{k}\right)=\inf \left\{\|f-P\| \mid P \in \mathcal{P}_{k}\right\}$, where the norm is that of $\mathcal{C}_{[0,1]}$. Then, there exists a sequence $\left\{P_{n}\right\}$ contained in $\mathcal{P}_{k}$ such that $\left\|f-P_{n}\right\| \rightarrow L$. Then, for some $M>0,\left\|P_{n}\right\| \leqslant\left\|P_{n}-f\right\|+\|f\| \leqslant M$, for all $n \in \mathbb{N}$. Let $x_{1}, \ldots, x_{k} \in[0,1]$ be $k$ distinct points. Thus, $\left|P_{n}\left(x_{i}\right)\right| \leqslant\left\|P_{n}\right\| \leqslant M$, for all $i=1, \ldots, k$ and $n \in \mathbb{N}$. The sequence $\left(P_{n}\left(x_{1}\right)\right)$ is bounded, so, passing to a subsequence if necessary, we may assume that it converges. We can repeat this argument and suppose that $\lim _{l \rightarrow \infty} P_{n}\left(x_{i}\right)=y_{i}$ for any $i=1, \ldots, k$, for some constants $y_{i}$. Let $P(x)=$ $\sum_{i=1}^{k} y_{i} \omega_{i}(x)$ where

$$
\omega_{i}(x)=\frac{\left(x-x_{1}\right) \cdots\left(x-x_{i-1}\right)\left(x-x_{i+1}\right) \cdots\left(x-x_{k}\right)}{\left(x_{i}-x_{1}\right) \cdots\left(x_{i}-x_{i-1}\right)\left(x_{i}-x_{i+1}\right) \cdots\left(x_{i}-x_{k}\right)}
$$

i.e., $P$ is Lagrange's interpolation polynomial. Clearly, there are positive constants $L_{i}$ such that $\left|\omega_{i}(x)\right| \leqslant L_{i}$, for all $i=1, \ldots, k$, and all $x \in[0,1]$. Therefore, for all $x \in[0,1]$, we have $\left|P_{n}(x)-P(x)\right| \leqslant \sum_{i=1}^{k}\left|P_{n}\left(x_{i}\right)-y_{i}\right|\left|\omega_{i}(x)\right| \leqslant$ $\sum_{i=1}^{k} L_{i}\left|P_{n}\left(x_{i}\right)-y_{i}\right| \rightarrow 0$. This means that for this polynomial $P$ in $\mathcal{P}_{k}$, we have $P_{n} \rightarrow P$; but $\left\|P_{n}-f\right\| \rightarrow L$, whence $L=\|P-f\|$, i.e., $d\left(f, \mathcal{P}_{k}\right)=$ $\|f-P\|, P \in \mathcal{P}_{k}$, therefore $P$ is a polynomial of degree $\leqslant k$, which minimizes $d\left(f, \mathcal{P}_{k}\right)$

Solution to 1.6.12: Let $a_{0}, \ldots, a_{D}$ be $D+1$ distinct points in $[0,1]$. The polynomials $f_{m}$, defined by

$$
f_{m}(x)=\prod_{\substack{i=0 \\ i \neq m}}^{D} \frac{x-a_{i}}{a_{m}-a_{i}} \text { for } m=0, \ldots, D
$$

satisfy $f_{m}\left(a_{i}\right)=0$ for $i \neq m, f_{m}\left(a_{m}\right)=1$. Any polynomial of degree, at most, $D$ can be written

$$
P(x)=\sum_{m=0}^{D} P\left(a_{m}\right) f_{m}(x)
$$

since the right-hand side is a polynomial of degree, at most, $D$ which agrees with $P$ in $D+1$ points.

Let $M$ be an upper bound of $\left|f_{m}(x)\right|$ for $x \in[0,1], m=0, \ldots, D$. Given $\varepsilon>0$ let $N \in \mathbb{N}$ be such that $\left|P_{n}\left(a_{m}\right)\right| \leqslant \frac{\varepsilon}{(D+1) M}$ for $n \geqslant N$. Then we have

$$
\left|P_{n}(x)\right| \leqslant \sum_{m=0}^{D}\left|P_{n}\left(a_{m}\right)\right|\left|f_{m}(x)\right|<\varepsilon
$$

therefore the convergence is uniform.

Solution to 1.6.14: Suppose that $f_{n_{j}} \rightarrow f$ uniformly. Then $f$ is continuous, and $f(0)=\lim _{j \rightarrow \infty} \cos 0=1$. So there is $\varepsilon>0$ with $f(x)>1 / 2$ for $|x|<\varepsilon$. If $j$ is large enough, we have, by uniform convergence,

$$
\left|f(x)-f_{n_{j}}(x)\right|<\frac{1}{2} \text { for all } x, \quad \frac{\pi}{2 n_{j}}<\varepsilon .
$$

For one such $j$, and $x=\frac{\pi}{2 n_{j}}$, we get

$$
\frac{1}{2}<f(x) \leqslant\left|f(x)-f_{n_{j}}(x)\right|+\left|f_{n_{j}}(x)\right|<\frac{1}{2}+f_{n_{j}}(x)=\frac{1}{2}+\left|\cos \frac{\pi}{2}\right|=\frac{1}{2}
$$

a contradiction.

Solution to 1.6.15: 1 . In an obvious way we see that $d(f, f)=0$ and $d(f, g)=$ $d(g, f)$, so we only need to verify that $d(f, g)>0$ for $f \neq g$ and the Triangle Inequality [MH87, p. 20].

If $f \neq g$ then $|f-g|$ is positive in a small interval and so is the iuntegrand and the integral is nonzero. Now the function $a \mapsto \frac{a}{1+a}$ is increasing in $[0, \infty)$. Hence, for $a=|f-g|, b=|g-h|, c=|f-h|$, we have $c \leqslant a+b$ and

$$
\frac{c}{1+c} \leqslant \frac{a+b}{1+a+b}=\frac{a}{1+a+b}+\frac{b}{1+a+b} \leqslant \frac{a}{1+a}+\frac{b}{1+b} .
$$

which implies the Triangle Inequality.

2. For natural $n$ define $f_{n}$ by

$$
f_{n}(x)= \begin{cases}n^{2} x, & 0 \leqslant x \leqslant 1 / n \\ 1 / x, & 1 / n \leqslant x \leqslant 1\end{cases}
$$

It is easy to verify that the sequence $\left\{f_{n}\right\}$ is Cauchy, since for any $n, m$ we have,

$$
\begin{aligned}
d\left(f_{m}, f_{n}\right) &=\int_{0}^{\max \{1 / m, 1 / n\}} \frac{\left|f_{m}(x)-f_{n}(x)\right|}{1+\left|f_{m}(x)-f_{n}(x)\right|} d x \\
& \leqslant \int_{0}^{\max \{1 / m, 1 / n\}} 1 d x \\
&=\max \{1 / m, 1 / n\} .
\end{aligned}
$$

Suppose that $\left(\mathcal{C}_{[0,1]}, d\right)$ is complete and take $f$ as the limit of the sequence $\left\{f_{n}\right\}$. If $f(a) \neq 1 / a$ for some $a \in(0,1]$, then, by continuity, there exists $\varepsilon>0$ such that $|1 / x-f(x)| \leqslant \varepsilon$ for $x \in(a-\varepsilon, a]$. Therefore,

$$
d\left(f_{n}, f\right) \leqslant \int_{a-\varepsilon}^{a} \frac{\varepsilon}{1+\varepsilon} d x
$$

for sufficiently large $n$. But the right hand side is a positive constant independent of $n$, which contradicts the convergence $f_{n} \rightarrow f$. Thus $f(a)=1 / a$ for all $a \in(0,1]$. This contradicts the continuity of $f$ on $[0,1]$. We conclude then that $\left(\mathcal{C}_{[0,1]}, d\right)$ is not complete.

Solution to 1.6.16: (a). Let $f_{n}:[0,1] \rightarrow \mathbb{R}$ be defined by $f_{n}(x)=x^{n} \cdot[0,1]$ is compact, $\left\|f_{n}\right\|=1$, but the sequence $f_{n}$ is not equicontinuous.

(b). Let $\Omega=[0,1]$, and $g_{n}(x)=n$. This sequence is clearly equicontinuous, $\Omega$ is compact, but no subsequence of $g_{n}$ can converge. 

![](https://cdn.mathpix.com/cropped/2022_10_26_8d6ad07375e689af3a1cg-065.jpg?height=393&width=396&top_left_y=384&top_left_x=447)

(a) $f_{n}$

![](https://cdn.mathpix.com/cropped/2022_10_26_8d6ad07375e689af3a1cg-065.jpg?height=578&width=398&top_left_y=199&top_left_x=858)

(b) $8 n$

![](https://cdn.mathpix.com/cropped/2022_10_26_8d6ad07375e689af3a1cg-065.jpg?height=385&width=379&top_left_y=390&top_left_x=1293)

(c) $h_{n}$

(c). Consider now $h_{n}: \mathbb{R} \rightarrow \mathbb{R}, h_{n}(x)=\chi_{[n-1, n+1]}(x) \cos ((x-n) \pi / 2)$, where $\chi_{[a, b]}$ is the characteristic function for the interval $[a, b]$. $\left\|h_{n}\right\| \leqslant 1$ and the sequence is equicontinuous, however no subsequence can converge.

Solution to 1.6.17: For each $n$, let $g_{n}$ be the function that equals $f_{n}$ at the points $k / n, k=0,1, \ldots, n$, and that on each interval $[(k-1) / n, k / n]$ interpolates its endpoint values linearly. By the assumption on $f_{n}$, the slope of $g_{n}$ on each of the preceding intervals is at most 1 , so $g_{n}$ is a Lipschitz function with Lipschitz constant at most 1 . Hence the sequence $\left\{g_{n}\right\}_{n=1}^{\infty}$ is uniformly equicontinuous, so by the Arzelà-Ascoli Theorem [MH93, p. 273], it has a uniformly convergent subsequence $\left\{g_{n_{j}}\right\}_{j=1}^{\infty}$. Let $g$ be the limit function. Fix a point $x$ in $[0,1]$. For each $j$, let $x_{j}$ be a point in $[0,1]$ of the form $k / n_{j}\left(k=0, \ldots, n_{j}\right)$ such that $1 / n_{j} \leqslant\left|x-x_{j}\right|<2 / n_{j}$. We have

$$
\begin{aligned}
\left|g(x)-f_{n_{j}}(x)\right| & \leqslant\left|g(x)-g_{n_{j}}(x)\right|+\left|g_{n_{j}}(x)-g_{n_{j}}\left(x_{j}\right)\right|+\left|f_{n_{j}}\left(x_{j}\right)-f_{n_{j}}(x)\right| \\
& \leqslant\left|g(x)-g_{n_{j}}(x)\right|+\frac{2}{n_{j}}+\frac{2}{n_{j}} .
\end{aligned}
$$

As $j \rightarrow \infty$, the first summand on the right tends to 0 uniformly, showing that $f_{n_{j}} \rightarrow g$ uniformly.

Solution to 1.6.18: By the Arzelà-Ascoli Theorem [MH93, p. 273], it will suffice to prove that the sequence $\left\{f_{n}\right\}$ is equicontinuous and uniformly bounded.

Equicontinuity. For $0 \leqslant x<y \leqslant 1$ and any $n$,

$$
\left|f_{n}(y)-f_{n}(x)\right|=\left|\int_{x}^{y} f_{n}^{\prime}(t) d t\right| \leqslant \int_{x}^{y} t^{-\frac{1}{2}} d t=2 \sqrt{y}-2 \sqrt{x} .
$$

The function $F(x)=2 \sqrt{x}$ is continuous on $[0,1]$, hence uniformly continuous. Therefore, given $\varepsilon>0$, there is a $\delta>0$ such that $|F(y)-F(x)|<\varepsilon$ whenever $x$ and $y$ are in $[0,1]$ and $|y-x|<\delta$. By the inequality above, we then have $\left|f_{n}(y)-f_{n}(x)\right|<\varepsilon$ for all $n$ when $|y-x|<\delta$, establishing the equicontinuity of the sequence.

Uniform boundedness. Since $\int_{0}^{1} f_{n}(x) d x=0$, the function $f_{n}$ cannot be always positive or always negative, so there is a point $x_{n}$ on $[0,1]$ such that $f_{n}\left(x_{n}\right)=0$. Then, by the estimate found above, for all $x$ :

$$
\left|f_{n}(x)\right| \leqslant 2\left|\sqrt{x}-\sqrt{x_{n}}\right| \leqslant 2 .
$$

Solution to 1.6.19: We claim that a subset $A$ of $M$ is compact iff $A$ is closed, bounded and $\left\{f^{\prime} \mid f \in A\right\}$ is equicontinuous. If $A$ satisfies all conditions and $\left\{f_{n}\right\}$ is a subsequence in $A$ then $\left\{f_{n}\right\}$ and $\left\{f_{n}^{\prime}\right\}$ are bounded and equicontinuous and by the Theorem of Arzelà-Ascoli $[\mathrm{MH} 93, \mathrm{p} .273]$, there is a subsequence $\left\{f_{n_{j}}\right\}$ such that $\left\{f_{n_{j}}\right\}$ and $\left\{f_{n_{j}}^{\prime}\right\}$ are uniformly convergent and therefore, sequences of Cauchy. Since $M$ is complete and $A$ is closed, $\left\{f_{n_{j}}\right\}$ converges to $f \in A$ in $M$, and $A$ is compact.

On the other hand, if $A$ is compact, consider the spaces:

$$
\tilde{M}=\left\{\left(f, f^{\prime}\right) \mid f \in M\right\} \quad \tilde{A}=\left\{\left(f, f^{\prime}\right) \mid f \in A\right\}
$$

$\tilde{A}$ is compact in $\tilde{M}$, and so are each of the projections, and, by Arzelà-Ascoli Theorem, $\left\{f^{\prime} \mid f \in A\right\}$ is equicontinuous.

Solution to 1.6.20: We consider three cases.

- $\left(a_{n}\right)$ has a vanishing subsequence. Then, the corresponding subsequence of $\left\{f_{n}\right\}$ converges to $x+\cos x$, a continuous function.

- $\left(a_{n}\right)$ has a subsequence with limit $a \neq 0$. Then, the corresponding subsequence of $\left\{f_{n}\right\}$ converges to

$$
\frac{1}{a} \sin (a x)+\cos (x+a),
$$

which is continuous.

- $\left|a_{n}\right| \rightarrow \infty$. In this case, $\frac{1}{a_{n}} \sin a_{n} x \rightarrow 0$, and $\cos \left(x+a_{n}\right)$ depends on $a_{n}(\bmod 2 \pi)$. Let $\left(b_{n}\right)$ be the sequence defined by $b_{n} \equiv a_{n} \quad(\bmod 2 \pi)$, $b_{n} \in[0,2 \pi]$. As $[0,2 \pi]$ is compact, $\left(b_{n}\right)$ has a convergent subsequence, to $b$ say. Then the corresponding subsequence of $\left\{f_{n}\right\}$ converges to $\cos (x+b)$, a continuous function.

Solution to 1.6.21: The answer is no. Consider the sequence of functions $f_{n}:[0,1] \rightarrow \mathbb{R}$ whose graphs are given by the straight lines through the points $(0,0),(1 / 2 n, n),(1 / n, 0)$ to $(1,0)$. 

![](https://cdn.mathpix.com/cropped/2022_10_26_8d6ad07375e689af3a1cg-067.jpg?height=1054&width=631&top_left_y=194&top_left_x=758)
The sequence approximates the zero-function pointwise, but $\int_{0}^{1} f_{n}(x) d x=\frac{1}{2}$
for all $n$.

Solution to 1.6.22: We have

$$
g_{n}(x)=g_{n}(0)+g_{n}^{\prime}(0) x+\frac{g_{n}^{\prime \prime}(\xi)}{2} x^{2}=\frac{g_{n}^{\prime \prime}(\xi)}{2} x^{2} \quad \text { for some } \quad \xi \in(0,1)
$$

so

$$
\left|g_{n}(x)\right| \leqslant \frac{1}{2} \quad \text { for } x \in[0,1] .
$$

Also,

$$
\left|g_{n}^{\prime}(x)\right|=\left|g_{n}^{\prime}(x)-g_{n}^{\prime}(0)\right| \leqslant\left|g_{n}^{\prime \prime}(\xi)(x-0)\right| \leqslant 1 \text { for } x \in[0,1] \text {. }
$$

Therefore,

$$
\left|g_{n}(x)-g_{n}(y)\right| \leqslant|x-y| \quad \text { for } x, y \in[0,1] \text {. }
$$

The sequence $\left\{g_{n}\right\}$ is then equicontinuous and uniformly bounded; so, by the Arzelà-Ascoli's Theorem [MH93, p. 273], it has a uniformly convergent subsequence.

Solution to 1.6.23: Fix $\varepsilon>0$. Since $K$ is continuous on a compact set, it is uniformly continuous. So, there is a $\delta>0$ such that $\left|K\left(x_{1}, y_{1}\right)-K\left(x_{2}, y_{2}\right)\right|<\varepsilon$ whenever $\sqrt{\left(x_{1}-x_{2}\right)^{2}+\left(y_{1}-y_{2}\right)^{2}}<\delta$. Let $f$ and $g$ be as above, and suppose $x_{1}$ and $x_{2}$ are in $[0,1]$ and satisfy $\left|x_{1}-x_{2}\right|<\delta$. Then

$$
\begin{aligned}
\left|f\left(x_{1}\right)-f\left(x_{2}\right)\right| &=\left|\int_{0}^{1} g(y)\left(K\left(x_{1}, y\right)-K\left(x_{2}, y\right)\right) d y\right| \\
& \leqslant \int_{0}^{1}|g(y)|\left|K\left(x_{1}, y\right)-K\left(x_{2}, y\right)\right| d y \\
& \leqslant \int_{0}^{1} 1 \cdot \varepsilon d y=\varepsilon .
\end{aligned}
$$

As the estimate holds for all $f$ in $F$, the family $F$ is equicontinuous.

Solution to 1.6.25: 1. By the Cauchy-Schwarz Inequality [MH93, p. 69], we have

$$
\left|g_{n}(x)\right| \leqslant \sqrt{\int_{0}^{1}(x+y) d y} \sqrt{\int_{0}^{1}\left(f_{n}(y)\right)^{2} d y} \leqslant \sqrt{\int_{0}^{1}(1+y) d y} \sqrt{5}=\sqrt{\frac{15}{2}}
$$

2. Since $\sqrt{x+y}$ is a continuous function of $x$ and $y$ on the compact unit square, it is uniformly continuous there. Hence, given any $\varepsilon>0$, there exists $\delta>0$ such that

$$
\left|\sqrt{x_{1}+y_{1}}-\sqrt{x_{2}+y_{2}}\right|<\varepsilon
$$

whenever $\left|x_{1}-x_{2}\right|+\left|y_{1}-y_{2}\right|<\delta$. In particular, $\left|\sqrt{x_{1}+y}-\sqrt{x_{2}+y}\right|<\varepsilon$ whenever $\left|x_{1}-x_{2}\right|<\delta$, therefore,

$$
\begin{aligned}
\left|g_{n}\left(x_{1}\right)-g_{n}\left(x_{2}\right)\right| &=\left|\int_{0}^{1}\left(\sqrt{x_{1}+y}-\sqrt{x_{2}+y}\right) f_{n}(y) d y\right| \\
& \leqslant \varepsilon \int_{0}^{1}\left|f_{n}(y)\right| d y \\
& \leqslant 5 \varepsilon
\end{aligned}
$$

whenever $\left|x_{1}-x_{2}\right|<\delta$. Since the same value of $\delta$ works for all values of $n$ simultaneously, the family $\left\{g_{n}\right\}$ is equicontinuous. Using the uniform bound established above the conclusion follows from the Arzelà-Ascoli's Theorem.

Solution to 1.6.26: We will first show that $\left\{g_{n}\right\}$ is a Cauchy sequence in sup-norm. Using the Cauchy-Schwarz Inequality [MH93, p. 69], we have

$$
\begin{aligned}
\left|g_{n}(x)-g_{m}(x)\right| & \leqslant \int_{0}^{1}|K(x, y)|\left(f_{n}(y)-f_{m}(y)\right) d y \\
& \leqslant \sqrt{\int_{0}^{1}|K(x, y)|^{2} d y} \sqrt{\int_{0}^{1}\left|f_{n}(y)-f_{m}(y)\right|^{2} d y}
\end{aligned}
$$

hence,

$$
\sup _{x \in[0,1]}\left|g_{n}(x)-g_{n}(x)\right| \leqslant \sup _{x \in[0,1]} \sqrt{\int_{0}^{1}|K(x, y)|^{2} d y} \sqrt{\int_{0}^{1}\left|f_{n}(y)-f_{m}(y)\right|^{2} d y}
$$

Since $K$ is continuous, it is integrable, and taking $M=\sup _{x, y \in[0,1]}|K(x, y)|$, we have

$$
\left\|g_{n}(x)-g_{m}(y)\right\| \leqslant M \sqrt{\int_{0}^{1}\left|f_{n}(y)-f_{m}(y)\right|^{2} d y} \rightarrow 0
$$

showing that the sequence $\left\{g_{n}\right\}$ is a Cauchy sequence in the sup-norm; as $C[0,1]$ is complete on this norm, the sequence converges uniformly.

Solution to 1.6.27: We'll use the Stone-Weierstrass Approximation Theorem [MH93, p. 284] in the space $\mathcal{C}_{[0,1]}$ equipped with the sup norm $\|f-g\|=\sup \{|f(x)-g(x)| \mid x \in[0,1]\}$. Let $f \in \mathcal{C}_{[0,1]}$ and let $\left\{p_{n}\right\}$ be a sequence of polynomials converging to $f$ in the sup norm. Using our hypothesis with $k=0$ we conclude that, for some positive $M$, we have $\left|\int_{0}^{1} \varphi_{n} d x\right| \leqslant M$ for all $n$. Given $\varepsilon>0$ there exists an integer $k(\varepsilon)$ such that $\left\|f-p_{k(\varepsilon)}\right\| \leqslant \frac{\varepsilon}{3 M}$ for $k \geqslant k(\varepsilon)$. As $\int_{0}^{1} p_{k(\varepsilon)} \varphi_{n}$ converges, it is a Cauchy sequence, therefore there is an integer $n(\varepsilon)$ such that

$$
\left|\int_{0}^{1} p_{k(\varepsilon)} \varphi_{n}-\int_{0}^{1} p_{k(\varepsilon)} \varphi_{m}\right|<\frac{\varepsilon}{3} \text { for } m, n \geqslant n(\varepsilon),
$$

for $m, n \geqslant n(\varepsilon)$, we have

$$
\begin{aligned}
\left|\int_{0}^{1} f \varphi_{n}-\int_{0}^{1} f \varphi_{m}\right| & \leqslant\left|\int_{0}^{1} f \varphi_{n}-\int_{0}^{1} p_{k(\varepsilon)} \varphi_{m}\right| \\
&+\left|\int_{0}^{1} p_{k(\varepsilon)} \varphi_{n}-\int_{0}^{1} p_{k(\varepsilon)} \varphi_{m}\right| \\
&+\left|\int_{0}^{1} p_{k(\varepsilon)} \varphi_{m}-\int_{0}^{1} f \varphi_{m}\right| \\
& \leqslant \int_{0}^{1}\left|f-p_{k(\varepsilon)} \varphi_{n}\right|+\frac{\varepsilon}{3}+\int_{0}^{1}\left|f-p_{k(\varepsilon)} \varphi_{m}\right| \\
& \leqslant\left\|f-p_{k(\varepsilon)}\right\| \int_{0}^{1} \varphi_{n}+\frac{\varepsilon}{3}+\left\|f-p_{k(\varepsilon)}\right\| \int_{0}^{1} \varphi_{m} \\
& \leqslant \varepsilon
\end{aligned}
$$

therefore the sequence $\int_{0}^{1} f \varphi_{n}$, being Cauchy, converges.

Solution to 1.6.28: As

$$
\left|\frac{e^{i \lambda_{n} x}}{n^{2}}\right| \leqslant \frac{1}{n^{2}} \text { and } \sum_{n=1}^{\infty} \frac{1}{n^{2}}<\infty
$$

by the Weierstrass $M$-test [Rud87, p. 148], the given series converges uniformly on $\mathbb{R}$ to a continuous function $f$. We have, since the convergence is uniform,

$$
\frac{1}{2 T} \int_{-T}^{T} \sum_{n=1}^{\infty} \frac{e^{i \lambda_{n} x}}{n^{2}} d x=\frac{1}{2 T} \sum_{n=1}^{\infty} \int_{-T}^{T} \frac{e^{i \lambda_{n} x}}{n^{2}} d x=\sum_{n=1}^{\infty} \frac{\sin \lambda_{n} T}{n^{2} \lambda_{n} T} .
$$

As

$$
\left|\frac{\sin \lambda_{n} T}{n^{2} \lambda_{n} T}\right| \leqslant \frac{1}{n^{2}}
$$

we have, again by the Weierstrass $M$-test, that

$$
\sum_{n=1}^{\infty} \frac{\sin \lambda_{n} T}{n^{2} \lambda_{n} T}
$$

converges uniformly in $T$. Therefore, we get

$$
\lim _{T \rightarrow \infty} \frac{1}{2 T} \int_{-T}^{T} f(x) d x=\lim _{T \rightarrow \infty} \sum_{n=1}^{\infty} \frac{\sin \lambda_{n} T}{n^{2} \lambda_{n} T}=\sum_{n=1}^{\infty} \lim _{T \rightarrow \infty} \frac{\sin \lambda_{n} T}{n^{2} \lambda_{n} T}=\sum_{\lambda_{n}=0} \frac{1}{n^{2}} .
$$

Solution to 1.6.29: Let $\sigma>1$. It suffices to show that $\zeta(x)$ is defined and has continuous derivatives for $x \geqslant \sigma$. The series $\sum n^{-x}$ converges for such $x$. As $n^{-x} \leqslant n^{-\sigma}$, it follows from the Weierstrass $M$-test [Rud87, p. 148] that the series converges uniformly, so $\zeta$ is a continuous function. To see that it has continuous derivatives of all orders, we formally differentiate the series $k$ times, getting

$$
\sum_{n=2}^{\infty} \frac{(-\log n)^{k}}{n^{x}}
$$

It is enough to show that this series converges uniformly on $k$. Since

$$
\left|\frac{(-\log n)^{k}}{n^{x}}\right| \leqslant \frac{(\log n)^{k}}{n^{\sigma}},
$$

by the Weierstrass $M$-test, it will suffice to show that the series

$$
\sum_{n=2}^{\infty} \frac{(\log n)^{k}}{n^{\sigma}}
$$

converges. But

for any positive $\varepsilon$. As

$$
\frac{(\log n)^{k}}{n^{\sigma}}=o\left(\frac{1}{n^{\sigma-\varepsilon}}\right) \quad(n \rightarrow \infty),
$$

$$
\sum_{n=1}^{\infty}\left(\frac{1}{n^{\sigma-\varepsilon}}\right)
$$

converges for $\sigma-\varepsilon>1$, we are done.

Solution to 1.6.30: Fix an interval $[a, b]$ and $\varepsilon>0$. Since $f$ is continuous, it is uniformly continuous on the interval $[a, b+1]$, then there exists an $N>0$ such that if $n \geqslant N$ and $|x-y|<1 / n$ we have $|f(x)-f(y)|<\varepsilon$. We will show that $f_{n}(x)$ converges uniformly to

$$
\int_{x}^{x+1} f(y) d y
$$

for all $x$ in the given interval. Fix $x$ and $n \geqslant N$. We have

$$
\left|\int_{x}^{x+1} f(y) d y-f_{n}(x)\right|=\left|\sum_{k=0}^{n-1} \int_{x+k / n}^{x+(k+1) / n} f(y) d y-f_{n}(x)\right|
$$

By the Mean Value Theorem for Integrals [MH93, p. 457], for each $k$ there is $a_{k} \in(x+k / n, x+(k+1) / n)$ such that

$$
\int_{x+k / n}^{x+(k+1) / n} f(y) d y=f\left(a_{k}\right) / n .
$$

Substituting this in the above, expanding using the definition of $f_{n}$, and using uniform continuity, we get

$$
\left|\int_{x}^{x+1} f(y) d y-f_{n}(x)\right| \leqslant \frac{1}{n} \sum_{k=0}^{n-1}\left|f\left(a_{k}\right)-f(x+k / n)\right|<\varepsilon
$$

Since this holds for any $x$, we are done.

Solution to 1.6.31: Let $\alpha>0$, then for $|n|>4 \alpha$, the bound on $f$ gives, for $x \in[-\alpha, \alpha]$

$$
|f(x+n)| \leqslant \frac{C}{1+n^{2} / 2}=M_{n}
$$

As the series

$$
\sum_{-\infty}^{\infty} M_{n}
$$

converges, by the Weierstrass $M$-test [Rud87, p. 148], the series

$$
\sum_{|n|>4 \alpha} f(x+n)
$$

converges uniformly. So the series for $F(x)$ converges uniformly on $[-\alpha, \alpha]$ and $F$ is continuous there. As $\alpha$ is arbitrary, $F$ is continuous on $\mathbb{R}$. We have

$$
\begin{aligned}
F(x+1)-F(x) &=\lim _{\alpha \rightarrow \infty} \sum_{-\alpha}^{\alpha}(f(x+1+n)-f(x+n)) \\
&=\lim _{\alpha \rightarrow \infty}(f(x+1+\alpha)-f(x-\alpha)) \\
&=0
\end{aligned}
$$

the last equality holding by our assumption on $f$.

If $G$ is continuous and periodic with period 1 , then, since the series for $F$ converges uniformly,

$$
\int_{0}^{1} F(x) G(x) d x=\sum_{-\infty}^{\infty} \int_{0}^{1} f(x+n) G(x) d x .
$$

In each integral on the right-hand side, let $y=x+n$, and get, since $G(y-n)=G(y)$,

$$
\sum_{-\infty}^{\infty} \int_{n}^{n+1} f(y) G(y) d y=\int_{-\infty}^{\infty} f(y) G(y) d y .
$$

Solution to 1.6.32: Given $f$ and $\varepsilon$, let $h:[0,1] \rightarrow \mathbb{R}$ be defined by $h(x)=f(\sqrt[4]{x})$. By the Stone-Weierstrass Approximation Theorem [MH93, p. 284], there is a polynomial $P$ such that $|P(x)-h(x)|<\varepsilon / 2$ for $x \in[0,1]$, from which it follows that

$$
\left|P\left(x^{4}\right)-f(x)\right|=\left|P\left(x^{4}\right)-h\left(x^{4}\right)\right|<\varepsilon / 2 \text { for } x \in[0,1] \text {. }
$$

If $P=\sum_{k=0}^{n} a_{k} x^{k}$, take $C_{0}, \ldots, C_{n} \in \mathbb{Q}$ such that $\sum_{k=0}^{n}\left|a_{k}-C_{k}\right|<\varepsilon / 2$. Then we have

$$
\left|\sum_{k=0}^{n} C_{k} x^{4 k}-f(x)\right| \leqslant\left|\sum_{k=0}^{n} C_{k} x^{4 k}-\sum_{k=0}^{n} a_{k} x^{4 k}\right|+\left|\sum_{k=0}^{n} a_{k} x^{4 k}-f(x)\right|<\varepsilon .
$$

\section{$1.7$ Fourier Series}

Solution to 1.7.1: 1. We have, for $n \in \mathbb{N}$,

$$
\frac{1}{\pi} \int_{-\pi}^{\pi} f(x) \cos n x d x=0
$$

because the integrand is an odd function. Also

$$
\frac{1}{\pi} \int_{-\pi}^{\pi} f(x) \sin n x d x=\frac{2}{\pi} \int_{0}^{\pi} f(x) \sin n x d x
$$



$$
\begin{aligned}
&=\frac{2}{\pi}\left(-\left.\frac{x \cos n x}{n}\right|_{0} ^{\pi}+\int_{0}^{\pi} \frac{\cos n x}{n}\right) \\
&=\frac{2}{\pi} \frac{(-1)^{n+1}}{n}
\end{aligned}
$$

so the Fourier series of $f$ is

$$
\sum_{n=1}^{\infty} \frac{(-1)^{n+1} 2}{n} \sin n x .
$$

2. If the series converged uniformly the function $f$ would be continuous, which is not the case.

3. As $f$ and $f^{\prime}$ are sectionally continuous, we have

$$
\sum_{n=1}^{\infty} \frac{(-1)^{n+1} 2 \sin n x}{n}=\frac{f(x-)+f(x+)}{2}= \begin{cases}f(x) & \text { if } x \neq(2 n+1) \pi \\ 0 & \text { if } x=(2 n+1) \pi\end{cases}
$$

where $n \in \mathbb{Z}$.

Solution to 1.7.2: 1 . Since $f(x)$ is an odd function, the integrals

$$
\frac{1}{\pi} \int_{-\pi}^{\pi} f(x) \cos n x d x
$$

vanish for $n \in \mathbb{N}$. The Fourier series has only terms in $\sin n x$ given by

$$
b_{n}=\frac{1}{\pi} \int_{-\pi}^{\pi} x^{3} \sin n x d x .
$$

2. As $f$ and $f^{\prime}$ are sectionally continuous, we have

$$
\sum_{n=1}^{\infty} b_{n} \sin n x=\frac{f(x-)+f(x+)}{2}= \begin{cases}f(x) & \text { if } x \neq(2 n+1) \pi \\ 0 & \text { if } x=(2 n+1) \pi\end{cases}
$$

where $n \in \mathbb{Z}$.

3. Using Parseval's Identity [MH93, p. 577]

$$
\frac{1}{2} a_{0}^{2}+\sum_{n=1}^{\infty}\left(a_{n}^{2}+b_{n}^{2}\right)=\frac{1}{\pi} \int_{-\pi}^{\pi} f^{2}(x) d x
$$

and the fact that all $a_{n}=0$,

$$
\sum_{n=1}^{\infty} b^{2}=\frac{1}{\pi} \int_{-\pi}^{\pi} x^{6} d x=\frac{2}{7} \pi^{6} .
$$

Solution to 1.7.3: The answer is no; $f(x)=1$ satisfies the above equation and is not identically zero. Solution to 1.7.4: As $f$ is $\sqrt{2}$-periodic, we have

$$
\hat{f}(n)=\int_{0}^{1} f(x) e^{-2 n \pi i x} d x=\int_{0}^{1} f(x+\sqrt{2}) e^{-2 n \pi i x} d x .
$$

Letting $y=x+\sqrt{2}$, we get

$$
\hat{f}(n)=e^{2 n \pi i \sqrt{2}} \int_{\sqrt{2}}^{1+\sqrt{2}} f(y) e^{-2 n \pi i y} d y .
$$

Since $f$ is also 1-periodic, we have

$$
\hat{f}(n)=e^{2 n \pi i \sqrt{2}} \int_{0}^{1} f(y) e^{-2 n \pi i y} d y=e^{2 n \pi i \sqrt{2}} \hat{f}(n) .
$$

$e^{2 n \pi i \sqrt{2}} \neq 1$ for $n \neq 0$, so $\hat{f}(n)=0$ for $n \neq 0$ and $f$ is constant.

Solution to 1.7.5: Suppose that such an $f$ exists. As the power series for $e^{x}$ converges uniformly, we have, for $n>0$,

$$
\hat{f}(n)=\int_{0}^{1} f(x) e^{-2 \pi i n x} d x=\sum_{k=0}^{\infty} \frac{(-2 \pi n i)^{k}}{k !} \int_{0}^{1} f(x) x^{k} d x=-2 \pi n i .
$$

This equality contradicts the Riemann-Lebesgue Lemma [MH93, p. 628], which says that $\lim _{n \rightarrow \infty} \hat{f}(n)=0$, so no such a function can exist.

Solution 2 . From the assumptions the function $x^{2} f(x)$ is orthogonal to all polynomials, so by the Stone-Weierstrass Approximation Theorem [MH93, p. 284] it is identically zero, and so it is $f(x)$, which is a contradiction with the first integral.

Solution to 1.7.6: The Fourier series of $f$ converges to $f$ because $f^{\prime \prime}$ exists. Let the Fourier series of $f$ be

$$
\frac{\alpha_{0}}{2}+\sum_{n=1}^{\infty}\left(\alpha_{n} \cos n x+\beta_{n} \sin n x\right)
$$

As $f^{\prime \prime}=g-k f$ is continuous, we obtain its Fourier series by termwise differentiating the series for $f$, and get

$$
\begin{gathered}
\frac{k \alpha_{0}}{2}+\sum_{n=1}^{\infty}\left(\left(k-n^{2}\right) \alpha_{n} \cos n x+\left(k-n^{2}\right) \beta_{n} \sin n x\right)= \\
\frac{a_{0}}{2}+\sum_{n=1}^{\infty}\left(a_{n} \cos n x+b_{n} \sin n x\right) .
\end{gathered}
$$

So we have

$$
\alpha_{0}=\frac{a_{0}}{k}, \quad \alpha_{n}=\frac{a_{n}}{k-n^{2}}, \quad \beta_{n}=\frac{b_{n}}{k-n^{2}} \quad \text { for } n \geqslant 1 .
$$

Solution to 1.7.7: Consider the Fourier series of $f$,

$$
f(x)=\frac{a_{0}}{2}+\sum_{n=1}^{\infty}\left(a_{n} \cos n x+b_{n} \sin n x\right) .
$$

We have $a_{0}=0$, and, by Parseval's Identity [MH93, p. 577],

$$
\int_{0}^{2 \pi} f^{2}(x) d x=\pi \sum_{n=1}^{\infty}\left(a_{n}^{2}+b_{n}^{2}\right) \leqslant \pi \sum_{n=1}^{\infty}\left(n^{2} a_{n}^{2}+n^{2} b_{n}^{2}\right)=\int_{0}^{2 \pi}\left(f^{\prime}(x)\right)^{2} d x .
$$

Solution to 1.7.8: The Riemann-Lebesgue Lemma [MH93, p. 628] states that the result is valid for all functions $g(x)$ of the type

$\cos k \pi x$ and $\sin k \pi x$

using linearity of the integral the result extends to all finite trigonometric polynomills

$$
p(x)=\sum_{k=0}^{n} a_{k} \cos k \pi x+b_{n} \sin k \pi x .
$$

We will now use the fact that the set of trigonometric polynomials is dense in the space of continuous functions with the sup norm (Stone-Weierstrass Approximation Theorem [MH93, p. 284]) to extend it to all continuous functions. Given any $\varepsilon>0$, there exists a $p_{\varepsilon}(x)$ as above such that

$$
\left|g(x)-p_{\varepsilon}(x)\right|<\varepsilon
$$

then

$$
\begin{aligned}
& \lim _{n \rightarrow \infty} \int_{0}^{1} f(x) g(n x) d x-\int_{0}^{1} f(x) d x \int_{0}^{1} g(x) d x \\
& -\left(\lim _{n \rightarrow \infty} \int_{0}^{1} f(x) p_{\varepsilon}(n x) d x-\int_{0}^{1} f(x) d x \int_{0}^{1} p_{\varepsilon}(x) d x\right) \mid \leqslant \\
& \leqslant\left|\lim _{n \rightarrow \infty} \int_{0}^{1} f(x)\left(g(n x)-p_{\varepsilon}(n x)\right) d x\right|+\left|\int_{0}^{1} f(x) d x \int_{0}^{1}\left(g(x)-p_{\varepsilon}(x)\right) d x\right| \\
& \leqslant \lim _{n \rightarrow \infty} \int_{0}^{1}|f(x)|\left|g(n x)-p_{\varepsilon}(n x)\right| d x+\int_{0}^{1}|f(x)| d x \int_{0}^{1}\left|g(x)-p_{\varepsilon}(x)\right| d x \\
& \leqslant 2 \varepsilon \int_{0}^{1}|f(x)| d x 
\end{aligned}
$$



\section{$1.8$ Convex Functions}

Solution to 1.8.1: Let $M=\max _{x \in[0,1]}|f(x)|$. Consider a sequence $\left(x_{n}\right)$ such that $x_{0}=1$, and $0<x_{n}<3^{-1} x_{n-1}$ satisfies $|f(x)|<M / 2^{n}$ for $0<x<x_{n}$. Define $g:[0,1] \rightarrow[0,1]$ by $g(0)=0$ and, using the fact that $x_{n} \rightarrow 0$,

$$
g(x)=t \frac{M}{2^{n}}+(1-t) \frac{M}{2^{n-1}}
$$

for $0<x=t x_{n+1}+(1-t) x_{n}, t \in[0,1], n=0,1, \ldots$. We have $g \geqslant f$ and

$$
\frac{g\left(x_{n}\right)-g\left(x_{n+1}\right)}{x_{n}-x_{n+1}}=\frac{M / 2^{n}}{x_{n}-x_{n+1}}>\frac{M / 2^{n-1}}{x_{n}-x_{n+1}}=\frac{g\left(x_{n-1}\right)-g\left(x_{n}\right)}{x_{n-1}-x_{n}}
$$

so $g$ is concave.

Solution to 1.8.2: For $x \neq y$ and $t \in[0,1]$, let

$$
c=(\log f(y)-\log f(x)) /(x-y) .
$$

By hypothesis, we have

$$
e^{c(t x+(1-t) y)} f(t x+(1-t) y) \leqslant t e^{c x} f(x)+(1-t) e^{c y} f(y)
$$

so

$$
\begin{aligned}
f(t x+(1-t) y) & \leqslant t e^{c(x-y)(1-t)} f(x)+(1-t) e^{-c(x-y) t} f(y) \\
&=t e^{(\log f(y)-\log f(x))(1-t)} f(x)+(1-t) e^{(\log f(x)-\log f(y)) t} f(y) \\
&=t\left(\frac{f(x)}{f(y)}\right)^{t-1} f(x)+(1-t)\left(\frac{f(x)}{f(y)}\right)^{t} f(y) \\
&=f(x)^{t} f(y)^{1-t}
\end{aligned}
$$

taking logarithms, we get that $\log f$ is convex.

Solution to 1.8.3: Consider an interval $[a, b]$ and suppose that the maximum of $f$ does not occur at one of its endpoints. Then, by Weierstrass Theorem [MH93, p. 189], there is $c \in(a, b)$ maximizing $f$. By the continuity of $f$, there are intervals $A=\left[a, a_{0}\right]$ and $B=\left[b_{0}, b\right]$ in $[a, b]$ with $f(x)<f(c)$ if $x$ lies in $A$ or $B$. By the Mean Value Inequality for Integrals [MH93, p. 457], we have

$$
\begin{aligned}
f(c) & \leqslant \frac{1}{2 h} \int_{A} f(y) d y+\frac{1}{2 h} \int_{\left[a_{0}, b_{0}\right]} f(y) d y+\frac{1}{2 h} \int_{B} f(y) d y \\
&<\frac{a_{0}-a}{2 h} f(c)+\frac{b_{0}-a_{0}}{2 h} f(c)+\frac{b-b_{0}}{2 h} f(c) \\
&=f(c) .
\end{aligned}
$$

This contradiction shows that $f$ must attain its maximum at $a$ or $b$.

If $L(x)$ is any linear function, a straightforward calculation shows that $L$ is convex and satisfies the mean value inequality above, and that both of these inequalities are, in fact, equalities. Now let $L$ be given by

$$
L(x)=\frac{(x-a) f(b)-(x-b) f(a)}{b-a}
$$

and consider $G(x)=f(x)-L(x)$. By the linearity of the integral, since $f$ and $L$ satisfy the Mean Value Inequality, $G$ does as well. Therefore, $G$ takes its maximum value at $a$ or $b$. A calculation shows that $G(a)=G(b)=0$. Therefore, we must have that $f(x) \leqslant L(x)$ for all $x \in[a, b]$. For any $t \in[0,1]$, $(1-t) a+t b \in[a, b]$. Substituting this into the inequality gives that $f$ is convex. 

\section{2}

\section{Multivariable Calculus}

\section{$2.1$ Limits and Continuity}

Solution to 2.1.1: Let $x \in \mathbb{R}^{n}, \varepsilon>0$, and let $B$ denote the open ball with center $f(x)$ and radius $\varepsilon$. For $n=1,2, \ldots$, let $K_{n}$ be the closed ball with center $x$ and radius $1 / n$. By (ii) we have $\bigcap_{1}^{\infty} f\left(K_{n}\right)=\{f(x)\}$. By (i) the sets $\left(\mathbb{R}^{n}-B\right) \cap f\left(K_{n}\right)$ are compact for $n=1,2, \ldots$. They form a decreasing sequence, and their intersection is empty, by the preceding equality. Hence, there is an $n_{0}$ such that $\left(\mathbb{R}^{n}-B\right) \cap f\left(K_{n_{0}}\right)=\emptyset$, which means that $|f(y)-\dot{f}(x)|<\varepsilon$ whenever $|y-x|<1 / n_{0}$. So $f$ is continuous at $x$.

Solution to 2.1.2: Let $(x, y) \notin G(g)$. Then $y \neq g(x)$, and there exist disjoint neighborhoods $V$ of $y$ and $V^{\prime}$ of $g(x)$ in $\mathbb{R}^{n}$. By hypothesis, we can find a neighborhood $U$ of $x$ such that $g(U) \subset V^{\prime}$. Then $U \times V$ is a neighborhood of $(x, y)$ disjoint from $G(g)$, and this proves that $G(g)$ is closed.

The converse is false. Take $n=1$, and let $g: \mathbb{R} \rightarrow \mathbb{R}$ be defined by $g(x)=1 / x$, for $x \neq 0 ; g(0)=0$. Then the graph of $g$ is closed in $\mathbb{R} \times \mathbb{R}$, and $g$ is discontinuous at 0 .

Solution 2. Let $\rho: \mathbb{R}^{n} \times \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ defined by $\rho(x, y)=y-f(x)$. Then $\rho$ is continuous and $G_{f}=\rho^{-1}(0)$ is closed being the inverse image of a closed set.

Solution to 2.1.3: If $U \neq \mathbb{R}^{n}$ let $x$ be in the boundary of $U$. Therefore there is a sequence $\left(x_{n}\right)$ of elements of $U$ converging to $x$. $\left(h\left(x_{n}\right)\right)$ is a Cauchy sequence, so it is convergent, to $y$ say. Let $y=h(z)$. Then $z \neq x$ since $z \in U$ and $x \notin U$. Then,

$$
h\left(x_{n}\right) \rightarrow h(z) \text { and } x_{n} \nrightarrow z,
$$

which contradicts the fact that $h$ is a homeomorphism.

Solution to 2.1.4: We show that $f$ is continuous at $(0,0)$; for the general case, use a change of variables. By adding a constant, if necessary, we may assume $f(0,0)=0$. Suppose $f$ is not continuous at the origin. Then, for any $\varepsilon>0$, there is a sequence $\left(\left(x_{n}, y_{n}\right)\right)$ tending to the origin with $\left|f\left(x_{n}, y_{n}\right)\right| \geqslant \varepsilon$ for each $n$. Since $f$ is continuous in the first variable, there exists a $\delta>0$ such that if $|x|<\delta$, then $|f(x, 0)|<\varepsilon / 2$. Applying this to our sequence, we see that there is an $N>0$, such that if $n \geqslant N$ then $\left|x_{n}\right|<\delta$, so $\left|f\left(x_{n}, 0\right)\right|<\varepsilon / 2$. However, for each such $n, f\left(x_{n}, y\right)$ is continuous in the second variable, so by the Intermediate Value Theorem [Rud87, p. 93], there exists $y_{n}^{\prime}, 0<y_{n}^{\prime}<y_{n}$, such that $\left|f\left(x_{n}, y_{n}^{\prime}\right)\right|=n \varepsilon /(n+1)$. Since the $y_{n}$ 's tend to 0 as $n$ tends to infinity; the $y_{n}^{\prime}$ 's do so as well. Hence, the set $E=\left\{\left(x_{n}, y_{n}^{\prime}\right) \mid n \geqslant N\right\} \cup\{(0,0)\}$ is compact. Then by our hypothesis, the set $f(E)$ is compact. But $f(E)=\{n \varepsilon /(n+1) \mid n \geqslant N\} \cup\{0\}$, and $\varepsilon$ is a limit point of this set which is not contained in it, a contradiction. Hence, $f$ is continuous at the origin and we are done.

Solution to 2.1.5: Continuity implies $f(0)=0$, so if any $x_{k}$ is 0 , then so are all subsequent ones, and the desired conclusion holds. Assume therefore, that $x_{k} \neq 0$ for all $k$. The sequence $\left(\left\|x_{k}\right\|\right)$ is then a decreasing sequence of positive numbers, so it has a nonnegative limit, say $c$. Suppose $c>0$. The sequence $\left(x_{k}\right)$, being bounded, has a convergent subsequence, say $\left(x_{k_{j}}\right)$, with limit $\alpha$. Then $\|\alpha\|=\lim _{j \rightarrow \infty}\left\|x_{k_{j}}\right\|=c$. Hence, $\|f(\alpha)\|<c$. But, by the continuity of $f$,

$$
f(\alpha)=\lim _{j \rightarrow \infty} f\left(x_{k_{j}}\right)=\lim _{j \rightarrow \infty} x_{k_{j}+1},
$$

and $\left\|x_{k_{j}+1}\right\| \geqslant c$ for all $j$, so we have a contradiction, and the desired conclusion follows.

Solution to 2.1.6: 1. If $\left\{e_{1}, \ldots, e_{n}\right\}$ denotes the standard basis of $\mathbb{R}^{n}$ and $N\left(e_{i}\right)$ the norm of each base vector, then any vector $v$ in the unit sphere can be written as

$$
v=c_{1} e_{1}+\cdots+c_{n} e_{n} \quad \text { where } 0 \leqslant c_{i} \leqslant 1
$$

so $N(v) \leqslant\left|c_{1}\right| N\left(v_{1}\right)+\cdots+\left|c_{n}\right| N\left(e_{n}\right) \leqslant n \max \left\{N\left(e_{1}\right), \ldots N\left(e_{n}\right)\right\}$ showing that $N$ is bounded on the unit sphere.

2. Let $B$ be the supremum of $N$ in the unit sphere, then for any vector $x \neq y$

$$
y=x+\|y-x\| \frac{y-x}{\|y-x\|}
$$

so $N(y) \leqslant N(x)+\|y-x\| N\left(\frac{y-x}{\|y-x\|}\right)$, that is

$$
N(y)-N(x) \leqslant B\|y-x\|
$$

changing the places of $x$ and $y$ we then get $N(x)-N(y) \leqslant B\|x-y\|$, that is

$$
|N(x)-N(y)| \leqslant B\|x-y\|
$$

and $N$ is continuous.

3. Let $A>0$ be the minimum of the continuous function $N$ on the compact unit sphere, where $B>0$ is already the maximum, then

$$
N(v)=\|v\| N\left(\frac{v}{\|v\|}\right)
$$

so taking the maximum and minimum over all such $v$,

$$
A\|v\| \leqslant N(v) \leqslant B\|v\| .
$$

Solution to 2.1.7: 1. Let $A$ be a closed subset of $\mathbb{R}^{m}$, and suppose $b \in \mathbb{R}^{n} \backslash f(A)$. For $r>0$ let $B_{r}$ be the closed ball in $\mathbb{R}^{n}$ with center $b$ and radius $r$. Since $B_{r}$ is compact and $f$ is proper, $f^{-1}\left(B_{r}\right)$ is compact. As $A$ is closed, $A \cap f^{-1}\left(B_{r}\right)$ is compact. Since $\bigcap_{r>0}\left(A \cap f^{-1}\left(B_{r}\right)\right)=A \cap f^{-1}(b)=\emptyset, A \cap f^{-1}\left(B_{s}\right)=\emptyset$ for some $s>0$, so $f(A)$ is disjoint from the neighborhood $B_{s}$ of $b$, so $f(A)$ is closed.

2. Suppose $B$ is a compact subset of $\mathbb{R}^{n}$. Let $A_{R}=\left\{x \in \mathbb{R}^{m} \mid\|x\| \geqslant R\right\}$. Since $f$ is closed, $f\left(A_{R}\right)$ is closed, so $f\left(A_{R}\right) \cap B$ is compact. As $f$ is $1-1$, $\bigcap_{R>0}\left(f\left(A_{R}\right) \cap B\right)=\emptyset$. Therefore $f\left(A_{S}\right) \cap B=\emptyset$ for some $S>0$, so $f^{-1}(B)$ is contained in the compact set $\mathbb{R}^{n} \backslash A s$. Since $f$ is continuous, $f^{-1}(B)$ is closed. Therefore $f^{-1}(B)$ is compact.

\subsection{Differential Calculus}

Solution to 2.2.1: We maximize the function $f(x, y)=\left(x^{2}+y^{2}\right) e^{-x-y}$ in the first quadrant, $x \geqslant 0$ and $y \geqslant 0$. The function attains a maximum there because it is nonnegative and tends to 0 as $(x, y)$ tends to infinity. We have

$$
\frac{\partial f}{\partial x}=\left(2 x-x^{2}-y^{2}\right) e^{-x-y}, \quad \frac{\partial f}{\partial y}=\left(2 y-x^{2}-y^{2}\right) e^{-x-y} .
$$

The critical points of $f$ are thus the points $(x, y)$ that satisfy

$$
2 x-x^{2}-y^{2}=0=2 y-x^{2}-y^{2} .
$$

These equalities imply $x=y$ and $2 x^{2}-2 x=0$. Hence, the only critical point in the open quadrant is $(1,1)$.

On the $x$-axis, we have $f(x, 0)=x^{2} e^{-x}$ and $\frac{d f(0, x)}{d x}=\left(x^{2}-2 x\right) e^{-x}$, so $\frac{d f(0, x)}{d x}=0$ only for $x=0$ and $x=2$. The point $(2,0)$ is thus another candidate for the point at which $f$ attains its maximum. By the same reasoning, the point $(0,2)$ is another such candidate. The points $(2,0)$ and $(0,2)$ are the only candidates on the boundary of the quadrant.

We have

$$
f(1,1)=2 e^{-2}, \quad f(2,0)=4 e^{-2}=f(0,2) .
$$

Hence, the maximum value of $f$ in the quadrant is $4 e^{-2}$, that is,

$$
\begin{aligned}
\left(x^{2}+y^{2}\right) e^{-x-y} & \leqslant 4 e^{-2} \\
\frac{x^{2}+y^{2}}{4} & \leqslant e^{x+y-2}
\end{aligned}
$$

for $x \geqslant 0, y \geqslant 0$.

Solution 2. Letting $u=t+1$ in the inequality $e^{t} \geqslant t+1$, which holds for any real $t$, we get $e^{u-1} \geqslant u$. For $u=x / 2+y / 2$ we obtain

$$
e^{\frac{x+y}{2}-1} \geqslant \frac{x+y}{2} .
$$

$$
\begin{aligned}
\text { As } \frac{x+y}{2} & \geqslant \sqrt{\frac{x^{2}+y^{2}}{4}}, \text { squaring we get } \\
e^{x+y-2} \geqslant\left(\frac{x+y}{2}\right)^{2} \geqslant \frac{x^{2}+y^{2}}{4} .
\end{aligned}
$$

Solution 3. On each level set $x+y=k$ the maximum for the left-hand side occurs at the extremes, where one of the variables is equal to 0 . To see this observe that $x^{2}+y^{2}$ is the square of the distance of $(x, y)$ to the origin, which is maximal at the extremes of the segment $x+y=k$ in the first quadrant. So we only need to prove that

$$
\frac{x^{2}}{4} \leqslant e^{x-2},
$$

but this can be easily seem by the same argument as in the last solution, $u \leqslant$ $e^{u-1}$, substituting $u=x / 2$ and squaring both sides. Alternatively, we can also differentiate the function $f(x)=e^{-x} x^{2}$ to see that the maximum for positive values of $x$ happens at 2 and then obtain the inequality again.

Solution 4. On each quart-of-circle $0 \leqslant \vartheta \pi / 2$ of radius $r$ the maximum of the left-hand side is $r^{2} / 4$ and the value of the right-hand side is

$$
e^{r(\cos \theta+\sin \theta)-2}
$$

which is a one variable function assuming the minimum at the extremes 0 and $\pi / 2$, where the value is $e^{r-2}$. Using the same argument as before we complete the proof. Solution to 2.2.2: Let the arc length of the circle between the points of contact of the $(i-1)$ th and $i$ th sides be $2 \varphi_{i}$. The area of the corresponding part of the hexagon is $\tan \varphi_{i}$, the total area is $A=\sum_{i=1}^{6} \tan \varphi_{i}$. This is the function to be minimized on $(0, \pi / 2)^{6}$ with the restriction $\sum_{i=1}^{6} \varphi_{i}=\pi$. Since $A$ is a convex function on $(0, \pi / 2)^{6} \subset \mathbb{R}^{6}$, any critical point is an absolute minimum.

![](https://cdn.mathpix.com/cropped/2022_10_26_8d6ad07375e689af3a1cg-083.jpg?height=645&width=827&top_left_y=523&top_left_x=646)

We use the method of Lagrange Multipliers [MH93, p. 414]. Lagrange's equations for a critical point are $\sec ^{2} \varphi_{i}=\lambda$, which are satisfied by $\varphi_{i}=\pi / 6$, corresponding to a regular hexagon. The minimal value of $A$ is $2 \sqrt{3}$.

Solution 2. With the same notation as above, the function tan is convex in the interval $(0, \pi / 2)$, so, by Jensen's Inequality [Str81, p. 201], the minimum for the area will happen when all angles are the same, that is, when the hexagon is regular.

Solution to 2.2.3: 1 . $|f(x, y)| \leq 2 \sqrt{x^{2}+y^{2}}$, and when $(x, y)$ approaches the origin the limit is 0 , and so is the limit of $f(x, y)$.

2. Computing the the directional derivative of $f$ in the direction of $(x, y)$ with $y \neq 0$, we have

$$
\lim _{h \rightarrow 0} \frac{f(h x, h y)}{h}=\lim _{h \rightarrow 0}\left(1-\cos \frac{h^{2} x^{2}}{h y}\right) \sqrt{x^{2}+y^{2}}=0
$$

and in the direction of the $\mathrm{x}$-axis the limit is trivialy zero, because the function is identically zero.

3. From the previous calculation, we know that, if $f$ were differentiable, the derivative would be zero, and then the quotient

$$
\frac{f(x, y)}{\sqrt{x^{2}+y^{2}}} \rightarrow 0
$$

as $(x, y) \rightarrow(0,0)$, but is false. To see it, approach the origin following the curve $y=x^{2} / \pi$, the limit of the quotient is then $1-\cos \pi=2$. Solution to 2.2.4: The function $f$ is differentiable at the point $z=\left(x_{0}, y_{0}\right) \in U$ if there is a linear transformation $f^{\prime}(z) \in L\left(\mathbb{R}^{2}, \mathbb{R}^{1}\right)$ such that

$$
\lim _{h \rightarrow 0} \frac{\left|f(z+h)-f(z)-f^{\prime}(z) h\right|}{\|h\|}=0 .
$$

Continuity of the partial derivatives is a sufficient condition for differentiability. A calculation gives

$$
\begin{gathered}
\frac{\partial f}{\partial x}(x, y)= \begin{cases}(4 / 3) x^{1 / 3} \sin y / x-y x^{-2 / 3} \cos y / x & \text { if } x \neq 0 \\
0 & \text { if } x=0\end{cases} \\
\frac{\partial f}{\partial y}(x, y)= \begin{cases}x^{1 / 3} \cos (y / x) & \text { if } x \neq 0 \\
0 & \text { if } x=0\end{cases}
\end{gathered}
$$

which are continuous on $\mathbb{R}^{2} \backslash\{(0, y) \mid y \in \mathbb{R}\}$. Thus, $f$ is differentiable there. At any point $(0, y)$, we have

$$
\frac{f(h, k)-f(0, y)}{\|(h, k)\|}=O\left(|h|^{1 / 3}\right)=o(1) \quad(h \rightarrow 0)
$$

so $f$ is differentiable at these points also.

Solution to 2.2.5: Let $A=\left(a_{j k}\right)$ be the Jacobian matrix of $F$ at the origin. According to the definition of differentiability we have $F(x)=A x+R(x)$ with $\lim _{x \rightarrow 0}\|R(x)\| /\|x\|=0$. Moreover,

$$
\begin{aligned}
\|A x\|^{2} &=\sum_{j=1}^{n}\left(\sum_{k=1}^{n} a_{j k} x_{k}\right)^{2} \\
& \leqslant \sum_{j=1}^{n}\left(\sum_{k=1}^{n} a_{j k}^{2} \sum_{k=1}^{n} x_{k}^{2}\right) \\
&=\left(\sum_{j, k=1}^{n} a_{j k}^{2}\right)\left(\sum_{k=1}^{n} x_{k}^{2}\right) \\
&=c\|x\|^{2},
\end{aligned}
$$

where we used Cauchy-Schwarz's inequality.

Choose $\varepsilon$ positive such that $\|R(x)\| /\|x\|<1-\sqrt{c}$ for $\|x\|<\varepsilon$, for these values of $x$, we have

$$
\|F(x)\| \leqslant\|A x\|+\|R(x)\| \leqslant \sqrt{c}\|x\|+(1-\sqrt{c})\|x\|=\|x\|,
$$

hence $F$ maps the ball centered at 0 with radius $\varepsilon$ into itself. Solution to 2.2.6: The Jacobian determinant of $f$ is given by

$$
J f(x, y)=\operatorname{det}\left(\begin{array}{rr}
p^{\prime}(x+y) & p^{\prime}(x+y) \\
p^{\prime}(x-y) & -p^{\prime}(x-y)
\end{array}\right)=-2 p^{\prime}(x+y) p^{\prime}(x-y)
$$

The derivative $D f(x, y)$ is invertible exactly when $J f(x, y) \neq 0$, thus if and only if $p^{\prime}(x+y) p^{\prime}(x-y) \neq 0$.

Since $p$ has positive degree, its derivative $p^{\prime}$ is not identically zero, so it has only finitely many zeros, say $\lambda_{1}, \ldots, \lambda_{n}$. The set $\{J f=0\}$ is the union of finitely many lines, the lines $x+y=\lambda_{k}, k=1, \ldots, n$, and the lines $x-y=\lambda_{k}$, $k=1, \ldots, n$. This is a closed nowhere-dense set, so $\{J f \neq 0\}$ is an open dense set.

Solution to 2.2.7: We have

$$
\nabla h(x, y)=\left(g^{\prime}(x) g(y), g^{\prime}(y) g(x)\right)
$$

For the desired condition to hold, $\nabla h(x, y)$ must be a scalar multiple of $(x, y)$, at least for $(x, y) \neq(0,0)$. Thus,

$$
\frac{g^{\prime}(x) g(y)}{x}=\frac{g^{\prime}(y) g(x)}{y},
$$

assuming $x y \neq 0$. This can be written as

$$
\frac{g^{\prime}(x)}{x g(x)}=\frac{g^{\prime}(y)}{y g(y)}
$$

which implies that $\frac{g^{\prime}(x)}{x g(x)}=A=$ constant. The preceding differential equation for $g$ has the general solution

$$
g(x)=B e^{A x^{2} / 2}
$$

with $B$ a constant, positive because $g$ is positive by assumption. Every such $g$ obviously has the required property.

Solution to 2.2.8: Since $f$ is continuous and $\mathbb{R}^{n}$ is connected, $f\left(\mathbb{R}^{n}\right)$ is connected. We will prove that $f\left(\mathbb{R}^{n}\right)$ is both open and closed in $\mathbb{R}^{n}$. This will imply that $f\left(\mathbb{R}^{n}\right)=\mathbb{R}^{n}$, because $f\left(\mathbb{R}^{n}\right) \neq \emptyset$.

Let $y=f(x) \in f\left(\mathbb{R}^{n}\right)$. As the rank of $\left(\partial f_{i} / \partial x_{j}\right)$ is $n$, by the Inverse Function Theorem [Rud87, p. 221], there are open neighborhoods $V_{x}$ and $V_{y}$ of $x$ and $y$ such that $\left.f\right|_{V_{x}}: V_{x} \rightarrow V_{y}$ is a diffeomorphism; therefore, $V_{y}$ is an open neighborhood of $y$, and $f\left(\mathbb{R}^{n}\right)$ is open.

Let $\left(y_{n}\right)$ be a sequence in $f\left(\mathbb{R}^{n}\right)$ converging to $y \in \mathbb{R}^{n}, f\left(x_{n}\right)=y_{n}$, say. The set $K=\left\{y_{n} \mid n \in \mathbb{N}\right\} \cup\{y\}$ is compact; therefore, $f^{-1}(K)$ is also compact. But $\left\{x_{n} \mid n \in \mathbb{N}\right\} \subset f^{-1}(K)$; therefore, it contains a convergent subsequence, say $\left(x_{n_{j}}\right)$ with $x_{n_{j}} \rightarrow x \in \mathbb{R}^{n}$. Since $f$ is continuous, $f\left(x_{n_{j}}\right) \rightarrow f(x)$. But $\lim _{j \rightarrow \infty} y_{n_{j}}=y$; therefore, $f(x)=y$, and $f\left(\mathbb{R}^{n}\right)$ is closed. Solution to 2.2.9: We have $\mathbb{R}^{2}=f(S) \cup\left(\mathbb{R}^{2} \backslash f(S)\right)$ where $S$ is the set of singularities of $f$. It suffices to show that $f$ maps $R=\mathbb{R}^{2} \backslash f^{-1}(f(S))$ onto $\mathbb{R}^{2} \backslash f(S) . f(S)$ is finite, so $\mathbb{R}^{2} \backslash f(S)$ is connected. As $f(S)$ is closed, $R$ is open. It suffices to show that $f(R)$ is open and closed in $\mathbb{R}^{2} \backslash f(S)$.

As $R \cap S=\emptyset$, by the Inverse Function Theorem [Rud87, p. 221], $f$ is invertible in a neighborhood of each point of $R$. Hence, locally, $f$ is an open map, and as the union of open sets is open, $f(R)$ is open.

Let $\xi$ be a limit point of $f(R)$ in $\mathbb{R}^{2} \backslash f(S)$, and $\left(\xi_{n}\right)$ be a sequence in $f(R)$ converging to $\xi$. The set $\left(\xi_{n}\right)$ is bounded, therefore, by hypothesis, $\left(f^{-1}\left(\xi_{n}\right)\right)$ is bounded. Let $\left(x_{n}\right)$ be a sequence such that $f\left(x_{n}\right)=\xi_{n}$. This sequence is bounded, so it must have a limit point, $x$, say. As $f$ is continuous, we have $f(x)=\xi$. Since $\xi \notin f(S), x \notin f^{-1}(f(S))$, so $x \in R$. Therefore, $\xi \in f(R)$, so $f(R)$ is closed in $\mathbb{R}^{2} \backslash f(S)$

Solution to 2.2.10: Consider the scalar field $G: \mathbb{R}^{n} \rightarrow \mathbb{R}$ given by $G(y)=\|\nabla f(y)\|^{2}$. We have

$$
D G(y)=\left(\frac{\partial^{2} f}{\partial x_{i} \partial x_{j}}(y)\right) .
$$

By the hypothesis, we have $G(x)=0, G$ is $C^{1}$, and $G^{\prime}(x) \neq 0$. Therefore, by the Inverse Function Theorem [Rud87, p. 221], $G$ is locally a diffeomorphism onto a neighborhood of 0 in $\mathbb{R}$. In particular, it is injective in some neighborhood of $x$, so it has no other zeros there.

Solution to 2.2.11: 1. Since $f$ is $C^{2}$, we can expand $f$ in a Taylor series [MH93, p. 359] around $a$ and obtain

$$
f(a+h)=f(a)+f^{\prime}(a) \cdot h+\frac{1}{2} f^{\prime \prime}(a) \cdot h^{2}+O\left(|h|^{3}\right) \quad(h \rightarrow 0)
$$

where $f^{\prime \prime}(a) \cdot h^{2}=h^{t} H h=\langle h, H h\rangle, H=\left(\frac{\partial^{2} f}{\partial x_{i} \partial x_{j}}\right)$, and the big Oh notation means that for $h$ in some neighborhood of $0, h \in V_{0}$, we have $\left|O\left(|h|^{3}\right)\right|<K|h|^{3}$ for some $K>0$.

The hypothesis that $a$ is a critical point implies that $f^{\prime}(a) \cdot h=0$ for all $h \in \mathbb{R}^{n}$, and the hypothesis that $H$ is positive definite implies that $\langle h, H h\rangle \geqslant 0$ for all $h \in \mathbb{R}^{n}$ and zero only at $h=0$. Therefore, all the eigenvalues of $H$ are positive and there exists some $c>0$ such that $\langle h, H h\rangle \geqslant c|h|^{2}$ (namely $c$ is the minimum of all the eigenvalues, which are all real because $H$ is symmetric, by the Schwarz Theorem [HK61, p. 340]). Let $W_{a}=\left\{a+h\left|h \in V_{0},\right| h \mid<c / k\right\}$. We have

$$
\begin{aligned}
f(a+h)-f(a) &=\langle h, H h\rangle+O\left(|h|^{3}\right) \geqslant c|h|^{2}+O\left(|h|^{3}\right) \\
& \geqslant c|h|^{2}-K|h|^{3}=|h|^{2}(c-K|h|)>0
\end{aligned}
$$

which shows that $f(a+h)>f(a)$ for $h \neq 0$ and $h \in W_{a}$, and, therefore, $a$ is a local minimum. 2. Assume $f$ has two critical points, $p_{1}$ and $p_{2}$. Since the Hessian matrix is positive definite, $p_{1}$ and $p_{2}$ are local minima. Let $t p_{1}+(1-t) p_{2}, t \in \mathbb{R}$, be the line containing $p_{1}$ and $p_{2}$. Consider the real function $g$ given by $g(t)=f\left(t p_{1}+(1-t) p_{2}\right) \cdot g$ has local minima at $t=0$ and at $t=1$. Therefore, $g$ has a local maximum at some $t_{0} \in(0,1)$. We have $g^{\prime \prime}\left(t_{0}\right) \leqslant 0$, but

$$
g^{\prime \prime}\left(t_{0}\right)=f^{\prime \prime}\left(t p_{1}+\left(1-t_{0}\right) p_{2}\right)\left(p_{1}-p_{2}\right)^{2}=\left\langle p_{1}-p_{2}, H\left(p_{1}-p_{2}\right)\right\rangle
$$

and our assumptions on $H$ imply $\left\langle p_{1}-p_{2}, H\left(p_{1}-p_{2}\right)\right\rangle>0$, a contradiction.

Solution to 2.2.12: As the Laplacian of $f$ is positive, the Hessian of $f$ has positive trace everywhere. However, since $f \in C^{3}$, for $f$ to have a relative maximum its Hessian must have negative eigenvalues and so its trace must be negative.

Solution to 2.2.13: With respect to polar coordinates $r, \theta$, we can write $u$ as $u(r, \theta)=r^{d} g(\theta)$. We need the expression for the Laplacian in polar coordinates, which one can derive on the basis of the easily obtained identities

$$
\frac{\partial r}{\partial x}=\frac{x}{r}, \quad \frac{\partial r}{\partial y}=\frac{y}{r}, \quad \frac{\partial \theta}{\partial x}=-\frac{\sin \theta}{r}, \quad \frac{\partial y}{\partial r}=\frac{\cos \theta}{r} .
$$

After some computations one finds that

$$
\Delta=\frac{\partial^{2}}{\partial x^{2}}+\frac{\partial^{2}}{\partial y^{2}}=\frac{\partial^{2}}{\partial r^{2}}+\frac{1}{r} \frac{\partial}{\partial r}+\frac{1}{r^{2}} \frac{\partial^{2}}{\partial \theta^{2}} .
$$

Hence

$$
\begin{aligned}
0 &=\Delta\left(r^{d} g(\theta)\right)=d(d-1) r^{d-2} g(\theta)+d r^{d-2} g(\theta)+r^{d-2} g^{\prime \prime}(\theta) \\
&=r^{d-2}\left(d^{2} g(\theta)+g^{\prime \prime}(\theta)\right) .
\end{aligned}
$$

Thus $g$ must be a solution of the ordinary differential equation $g^{\prime \prime}+d^{2} g=0$. This equation has the general solution

$$
g(\theta)=a \cos d \theta+b \sin d \theta .
$$

Since $g$ must be $2 \pi$-periodic, $d$ is an integer.

Solution to 2.2.15: The derivative of $T$ is given by

$$
D T=\left(\begin{array}{ll}
1 & 2 u \\
1 & 2 v
\end{array}\right)
$$

which is always nonsingular since $\operatorname{det}(D T)=2 v-2 u$ is never 0 . By the Inverse Function Theorem [Rud87, p. 221], this means that $T$ is locally one-to-one.

2. Considering the function $f(u, v)=u+v$ restricted to $u^{2}+v^{2}=y$, we conclude that $-\sqrt{2 y} \leqslant u+v \leqslant \sqrt{2 y}$; therefore, the range of $T$ is

$$
\{(x, y) \mid y>0,-\sqrt{2 y} \leqslant x \leqslant \sqrt{2 y}\} .
$$

Let $(x, y) \in \operatorname{range}(T) \cdot u+v=x$ is the equation of a straight line with slope $-1$ in the $u, v$-plane which intersects the circle $u^{2}+v^{2}=y$ centered at the origin with radius $\sqrt{y}$. These two lines intersect exactly at one point in $U$, so $T$ is globally injective.

![](https://cdn.mathpix.com/cropped/2022_10_26_8d6ad07375e689af3a1cg-088.jpg?height=864&width=919&top_left_y=468&top_left_x=646)

Solution to 2.2.16: Letting $f_{1}$ and $f_{2}$ be the components of $f$, we have

$$
\frac{d}{d t}\|f(t)\|^{2}=2 f_{1} f_{1}^{\prime}(t)+2 f_{2} f_{2}^{\prime}(t) .
$$

Assume $t>0$ and use the Mean Value Theorem [Rud87, p. 108] to rewrite the right side as

$$
2 t\left(f_{1}^{\prime}\left(\xi_{1}\right) f_{1}^{\prime}(t)+f_{2}^{\prime}\left(\xi_{2}\right) f_{2}^{\prime}(t)\right)
$$

where $0<\xi_{1}=\xi_{1}(t)<t$ and $0<\xi_{2}=\xi_{2}(t)<t$. As $t \searrow 0$, the continuity of $f^{\prime}$ gives

$$
f_{1}^{\prime}\left(\xi_{1}\right) f_{1}^{\prime}(t)+f_{2}^{\prime}\left(\xi_{2}\right) f_{2}^{\prime}(t) \longrightarrow\left\|f^{\prime}(0)\right\|^{2}>0 .
$$

Hence, there is an $\varepsilon>0$ such that $\frac{d}{d t}\left\|f^{\prime}(t)\right\|^{2}>0$ for $0<t<\varepsilon$, and the desired conclusion follows.

Solution to 2.2.17: For $X \in \Sigma$, we have

$$
\begin{aligned}
\|A-X\|^{2} &=(1-x)^{2}+y^{2}+z^{2}+(2-t)^{2} \\
&=y^{2}+z^{2}+1-2 x+x^{2}+(2-t)^{2} \\
& \geqslant \pm 2 y z+1-2 x+2|x|(2-t) \\
&=4|x|-2 x+2(\pm y z-|x| t)+1
\end{aligned}
$$

We can choose the sign, so $\pm y z-|x| t=0$ because $\operatorname{det} X=0$. As $4|x|-2 x \geqslant 0$, we have $\|A-X\| \geqslant 1$ with equality when $4|x|-2 x=0,|x|=2-t, y=\pm z$,

![](https://cdn.mathpix.com/cropped/2022_10_26_8d6ad07375e689af3a1cg-089.jpg?height=79&width=846&top_left_y=313&top_left_x=366)

Solution 2. We can use the Method of Lagrange Multipliers [MH93, p. 414] to find the minimum of the function

$$
d(x, y, z, t)=(x-1)^{2}+y^{2}+z^{2}+(t-2)^{2}
$$

over the surface defined by $\Sigma=$ det $^{-1}(0)$. Setting up the equations

$$
\begin{aligned}
x-1 &=\lambda t \\
y &=-\lambda z \\
z &=-\lambda y \\
t-2 &=\lambda x
\end{aligned}
$$

substituting the third equation into the second we see that if $y \neq 0$ then $\lambda=\pm 1$, and substituting back into the first and last equations we get a contradiction, so indeed $y=0$, and then so is $z=0$. Now solving the first and last equations together we find

$$
\begin{aligned}
&x=\frac{2 \lambda+1}{1-\lambda^{2}} \\
&t=\frac{\lambda+2}{1-\lambda^{2}} .
\end{aligned}
$$

To have determinant zero then either $\lambda=-1 / 2$ or $\lambda=-2$, and the two matrices are

$$
\left(\begin{array}{ll}
0 & 0 \\
0 & 2
\end{array}\right) \text { and }\left(\begin{array}{ll}
1 & 0 \\
0 & 0
\end{array}\right)
$$

and we can easily see that $\left(\begin{array}{ll}0 & 0 \\ 0 & 2\end{array}\right)$ is the closest one.

Solution to 2.2.18: Let $d$ denote the Euclidean metric in $\mathbb{R}^{3}$. We first prove that there exist $p \in S$ and $q \in T$ such that $d(p, q)=d(S, T)$, where

$$
d(s, T)=\inf \{d(s, t) \mid t \in T\}
$$

and

$$
\begin{aligned}
d(S, T) &=\inf \{d(s, t) \mid s \in S, t \in T\} \\
&=\inf \{d(s, T) \mid s \in S\} .
\end{aligned}
$$

The function $S \rightarrow \mathbb{R}$ defined by $x \mapsto d(x, T)$ is continuous and attains its infimum on the compact set $S$, i.e., there exists $p \in S$ such that $d(p, T)=$ $d(S, T)$. The function $\varphi: T \rightarrow \mathbb{R}$ given by $x \mapsto d(p, x)$ is continuous, and $\inf _{T} \varphi=\inf _{T^{\prime}} \varphi$ where $T^{\prime}$ is the compact set

$$
T^{\prime}=\{x \in T \mid d(x, p) \leqslant d(S, T)+1\} .
$$

Hence there exists $q \in T^{\prime} \subset T$ such that $d(p, q)=d(p, T)=d(S, T)$.

If $(x, y, z) \in S$ then $z \geqslant 9$, whereas if $(x, y, z) \in T$ then $z \leqslant 1$. Therefore $p \neq q$. Now $S, T$ are level sets of

$$
\begin{aligned}
&g_{1}(x, y, z)=2 x^{2}+(y-1)^{2}+(z-10)^{2} \\
&g_{2}(x, y, z)=z\left(x^{2}+y^{2}+1\right)
\end{aligned}
$$

respectively.

Define, on $\mathbb{R}^{3} \times \mathbb{R}^{3}$,

$$
\begin{aligned}
F(x, y, z, u, v, w) &=(x-u)^{2}+(y-v)^{2}+(z-w)^{2} \\
f_{1}(x, y, z, u, v, w) &=g_{1}(x, y, z) \\
f_{2}(x, y, z, u, v, w) &=g_{2}(u, v, w) .
\end{aligned}
$$

Then $S \times T=\left\{(\xi, \eta) \in \mathbb{R}^{3} \times \mathbb{R}^{3} \mid f_{1}(\xi, \eta)=1, f_{2}(\xi, \eta)=1\right\}$. We showed above that there exists $(p, q) \in S \times T$ which minimizes $F$ on $S \times T$. For $(\xi, \eta) \in S \times T$, the two vectors $\nabla f_{1}(\xi, \eta)=\left(\nabla g_{1}(\xi), 0\right), \nabla f_{2}(\xi, \eta)=\left(0, \nabla g_{2}(\eta)\right)$ are linearly independent. that

By Lagrange's Multiplier Theorem [MH93, p. 414], there exist $\lambda, \mu \in \mathbb{R}$ such

$$
\nabla F(p, q)=\lambda \nabla f_{1}(p, q)+\mu \nabla f_{2}(p, q) .
$$

Now

$$
\nabla F(p, q)=(2(p-q),-2(p-q)),
$$

and

$$
\begin{aligned}
\lambda \nabla f_{1}(p, q)+\mu \nabla f_{2}(p, q) &=\lambda\left(\nabla g_{1}(p), 0\right)+\mu\left(0, \nabla g_{2}(q)\right) \\
&=\left(\lambda \nabla g_{1}(p), \mu \nabla g_{2}(q)\right) .
\end{aligned}
$$

It follows from the equations above that $\frac{1}{2} \lambda \nabla g_{1}(p)=p-q=-\frac{1}{2} \mu \nabla g_{2}(q)$. None of $p-q, \nabla g_{1}(p)$ and $\nabla g_{2}(q)$ are zero. Thus the vector $p-q$ is parallel to both $\nabla g_{1}(p)$ and $\nabla g_{2}(q)$, from which the result follows.

Solution to 2.2.19: Each element of $P_{2}$ has the form $a x^{2}+b x+c$ for $(a, b, c) \in$ $\mathbb{R}^{3}$, so we can identify $P_{2}$ with $\mathbb{R}^{3}$ and $J$ becomes a scalar field on $\mathbb{R}^{3}$ :

$$
J(a, b, c)=\int_{0}^{1}\left(a x^{2}+b x+c\right)^{2} d x=\frac{a^{2}}{5}+\frac{a b}{2}+\frac{2 a c}{3}+\frac{b^{2}}{3}+b c+c^{2} .
$$

To $Q$ corresponds the set $\{(a, b, c) \mid a+b+c=1\}$. If $J$ achieves a minimum value on $Q$, then, by the Method of Lagrange Multipliers [MH93, p. 414], we know that there is a constant $\lambda$ with $\nabla J=\lambda \nabla g$, where $g(a, b, c)=a+b+c-1$. We have

$$
\nabla J=\left(\frac{2 a}{5}+\frac{b}{2}+\frac{2 c}{3}, \frac{a}{2}+\frac{2 b}{3}+c, \frac{2 a}{3}+b+2 c\right)
$$

and $\nabla g=(1,1,1)$. These and the constraint equation $g(a, b, c)=0$ form the system

$$
\left(\begin{array}{cccc}
2 / 5 & 1 / 2 & 2 / 3 & -1 \\
1 / 2 & 2 / 3 & 1 & -1 \\
2 / 3 & 1 & 2 & -1 \\
1 & 1 & 1 & 0
\end{array}\right)\left(\begin{array}{l}
a \\
b \\
c \\
\lambda
\end{array}\right)=\left(\begin{array}{c}
0 \\
0 \\
0 \\
1
\end{array}\right)
$$

which has the unique solution $\lambda=2 / 9,(a, b, c)=(10 / 3,-8 / 3,1 / 3)$. Therefore, if $J$ attains a minimum, it must do so at this point. To see that $J$ does attain a minimum, parameterize the plane $Q$ with the $x y$ coordinates and consider the quadratic surface with a linear $z$ term defined by $z=J(x, y, 1-x-y)$ in $\mathbb{R}^{3}$. The surface is the graph of the map $J: P_{2} \rightarrow \mathbb{R}$. Rotating around the $z$-axis will eliminate the $x y$ cross-terms in the equation, reducing it to the standard equation of either an elliptic paraboloid or a hyperbolic paraboloid. However, $J$ is always nonnegative, so the surface must be an elliptic paraboloid and, as such, has a minimum.

Solution to 2.2.20: First observe that the group of orthogonal matrices is transitive in $\mathbb{R}^{n}$, that is, given any two vectors $u$ and $v$ there is a orthogonal matrix $A$, such that $A u=v$. Now using the Cauchy-Schwarz Inequality [MH93, p. 69] we see that the maximum and minimum of the inner product of any two vectors of the same size is $\pm\|v\|^{2}$ and they are both achieved when the vectors are the same and have opposite directions, respectively. So the maximum of the function $f$ is achieved on the matrices that leave $v_{0}$ invariant and the minimum on the ones that reverse it.

Solution to 2.2.21: Let $e_{1}, \ldots, e_{n}$ be the unit vectors in $\mathbb{R}^{n}$. Let $a, b \in W$ be such that $b=a+h e_{j}$ for some $h>0$ and some $1 \leqslant j \leqslant n$, and that the line segment $[a, b]=\left\{a+t e_{j} \mid 0 \leqslant t \leqslant h\right\}$ lies in $W$. The function $t \mapsto f\left(a+t e_{j}\right)$ has derivative $D_{j} f=0$, by hypothesis. By the mean value theorem, $f(b)=f(a)$.

In order to prove that $f$ is constant, it is therefore sufficient to note that any two points in $W$ can be joined by a sequence of line segments in $W$ each of which is parallel to some axis of $\mathbb{R}^{n}$.

Solution to 2.2.22: Let

$$
g(x, y)=\frac{-y}{x^{2}+y^{2}} \quad \text { and } \quad f(x, y)=\frac{x}{x^{2}+y^{2}} .
$$

Then

Let $\gamma$ be the curve in $\mathcal{A}$ given by

$$
\frac{\partial g}{\partial y}=\frac{y^{2}-x^{2}}{\left(x^{2}+y^{2}\right)^{2}}=\frac{\partial f}{\partial x} .
$$

$$
(x, y)=(2 \cos t, 2 \sin t), \quad 0 \leqslant t \leqslant 2 \pi .
$$

Then

$$
\int_{y} g(x, y) d x+f(x, y) d y=\int_{0}^{2 \pi}\left(\frac{-2 \sin t}{4}(-2 \sin t)+\frac{2 \cos t}{4}(2 \cos t)\right) d t
$$



$$
\begin{aligned}
&=\int_{0}^{2 \pi} d t \\
&=2 \pi .
\end{aligned}
$$

Suppose there is a function $h$ such that $(g, f)=\nabla h=\left(\frac{\partial h}{\partial x}, \frac{\partial h}{\partial y}\right)$. Then, since $\gamma$ is closed, by the fundamental theorem of vector calculus, we obtain

$$
\int_{y} g d x+f d y=0 .
$$

which contradicts the calculation above.

Solution to 2.2.24: Let $(x, t) \in \mathbb{R}^{2}$. By the Mean Value Theorem, [Rud87, p. 108] and the hypothesis, we have, for some $(\xi, \eta)$ in the segment connecting $(x, y)$ to $(x+y, 0)$,

$$
\begin{aligned}
f(x, t)-f(x+t, 0) &=D f(\xi, \eta) \cdot((x, t)-(x+t, 0)) \\
&=\left(\frac{\partial f}{\partial x}(\xi, \eta), \frac{\partial f}{\partial t}(\xi, \eta)\right) \cdot(-t, t) \\
&=t\left(\frac{\partial f}{\partial x}(\xi, \eta)-\frac{\partial f}{\partial t}(\xi, \eta)\right) \\
&=0
\end{aligned}
$$

so $f(x, t)=f(x+t, 0)>0$.

Solution to 2.2.25: Given two points $x$ and $y \in \mathbb{R}^{n}$ one can build a polygonal path from $x$ to $y$ with $n$ segments all parallel to the axis (adjusting one coordinate at a time). Applying the Mean Value Theorem [Rud87, p. 108] to each of the segments of the path, we have

![](https://cdn.mathpix.com/cropped/2022_10_26_8d6ad07375e689af3a1cg-092.jpg?height=808&width=814&top_left_y=1691&top_left_x=642)

$\left|f\left(x_{1}, \ldots, x_{i-1}, x_{i}, y_{i+1}, \ldots, y_{n}\right)-f\left(x_{1}, \ldots, x_{i-1}, y_{i}, y_{i+1}, \ldots, y_{n}\right)\right| \leqslant K\left|x_{i}-y_{i}\right|$ and then

$$
\begin{aligned}
|f(x)-f(y)| \leqslant & \sum_{i=1}^{n} \mid f\left(x_{1}, \ldots, x_{i-1}, x_{i}, y_{i+1}, \ldots, y_{n}\right) \\
&-f\left(x_{1}, \ldots, x_{i-1}, y_{i}, y_{i+1}, \ldots, y_{n}\right) \mid \\
& \leqslant K \sum_{i=1}^{n}\left|x_{i}-y_{i}\right| .
\end{aligned}
$$

Now applying the Cauchy-Schwarz Inequality [MH93, p. 69] to the vectors $(1,1, \ldots, 1)$ and $x-y$, we get

$$
\begin{aligned}
|f(x)-f(y)| & \leqslant K \sqrt{\sum_{i=1}^{n} 1} \sqrt{\sum_{i=1}^{n}\left|x_{i}-y_{i}\right|^{2}} \\
&=\sqrt{n} K\|x-y\| .
\end{aligned}
$$

Solution to 2.2.26: Let $\left(\left(x_{1}, \ldots, x_{n}\right)\right)_{k}$ be a sequence in $\mathbb{R}^{n}$ converging to $0 \in \mathbb{R}^{n}$. This sequence is Cauchy, so there is an $N>0$ such that if $k, l>N$, then for each of the coordinates we have $\left|x_{i k}-x_{i l}\right|<\varepsilon / 2 n M$. Then we draw a polygonal path, as in the Solution to Problem 2.2.25, from $\left(x_{1 k}, \ldots, x_{n k}\right)$ to $\left(x_{1 l}, \ldots, x_{n l}\right)$, parallel to the axes.

If this path does not goes through the origin, then as before

$$
\left|f\left(x_{1 k}, \ldots, x_{n k}\right)-f\left(x_{1 l}, \ldots, x_{n l}\right)\right|<M \sum_{i=1}^{n}\left|x_{i k}-x_{i l}\right|<\varepsilon
$$

and if the origin is in one of the segments of the polygonal path, we can perturb it a bit, by traversing in the same direction but $\varepsilon / 4 M$ away from the origin. On this altered path

$$
\left|f\left(x_{1 k}, \ldots, x_{n k}\right)-f\left(x_{1 l}, \ldots, x_{n l}\right)\right|<M \sum_{i=1}^{n}\left|x_{i k}-x_{i l}\right|+2 M \frac{\varepsilon}{4 M} \leqslant \varepsilon
$$

in both case the sequence $\left(f\left(x_{1}, \ldots, x_{n}\right)\right)_{k}$ is Cauchy and, thus, it converges. Given any other sequence $\left(\left(y_{1}, \ldots, y_{n}\right)\right)_{k}$, an identical argument shows that $\left|f\left(x_{i n}, \ldots, x_{i n}\right)-f\left(y_{i n}, \ldots, y_{i n}\right)\right|$ tends to 0 , so all sequences must converge to the same value, which can be defined as the continuous extension of $f$ to the origin. For $n=1$, consider the function $f(x)=1$ if $x<0$ and $f(x)=0$ if $x>0$. Solution to 2.2.27: 1. The answer is no, and a counterexample is the function

$$
f(x, y)=\frac{x y}{x^{2}+y^{2}}, \quad \text { for }(x, y) \neq(0,0)
$$

![](https://cdn.mathpix.com/cropped/2022_10_26_8d6ad07375e689af3a1cg-094.jpg?height=805&width=1136&top_left_y=503&top_left_x=489)

$f$ is differentiable everywhere, but cannot be extended continuously to the origin, because it is constant equal to $k /\left(1+k^{2}\right)$ on each line $y=k x$ passing through the origin.

2. The answer is again no, with the counterexample a variant of the previous one, the function

$$
g(x, y)=\frac{x y^{2}}{x^{2}+y^{2}} \quad \text { and } \quad g(0,0)=0
$$

$g$ is now continuous everywhere, but not differentiable because the directional derivative does not depend linearly on the vector. Let $(u, v) \neq(0,0)$. We have

$$
\lim _{t \rightarrow 0} \frac{g((0,0)+t(u, v))-g(0,0)}{t}=\lim _{t \rightarrow 0} t \frac{u v^{2}}{u^{2}+v^{2}}=0 .
$$

So the directional derivatives at the origin exist in all directions. If $g$ were differentiable at $(0,0)$, as all the directional derivatives vanish there, we would have $D g(0,0)=0$ (the zero linear map). Then, by definition of differentiability, we would have

$$
g(x, y)=o(\|(x, y)\|) \quad((x, y) \rightarrow(0,0))
$$

which is absurd, since

$$
\lim _{\substack{(x, y) \rightarrow(0,0) \\ x=y}} \frac{g(x, y)}{\|(x, y)\|}=\lim _{x \rightarrow 0} \frac{x^{3}}{2 x^{2} \sqrt{2 x^{2}}} \neq 0 .
$$



![](https://cdn.mathpix.com/cropped/2022_10_26_8d6ad07375e689af3a1cg-095.jpg?height=602&width=1046&top_left_y=211&top_left_x=580)

\section{Both examples are from [Lim82].}

Solution to 2.2.28: A simple counterexample is

$$
f(x, y)=\left\{\begin{array}{lll}
x & \text { if } & y=0 \\
0 & \text { if } & y \neq 0
\end{array}\right.
$$

and not even continuity at the origin and $C^{1}$ on the rest of the plane is enough to guarantee differentiability, as shown in the counterexample of Problem 2.2.27, Part 2.

Solution to 2.2.29: Let $\varepsilon>0$. By the hypothesis, there is $\delta$ such that $\|D f(w)\|<\varepsilon$ if $\|w\|<\delta$. For $\|x\|<\delta$, by the Mean Value Theorem [Rud87, p. 108 ], applied to the line segment joining 0 and $x$, we have

$$
\|f(x)-f(0)\| \leqslant\|D f(\xi x)\|\|x-0\|<\varepsilon\|x\| \text { for some } 0<\xi<1 \text {, }
$$

which implies differentiability at the origin.

Solution to 2.2.31: Define $f: \mathbb{R}^{2} \rightarrow \mathbb{R}$ by

$$
f(x, y)=\int_{(0,0)}^{(x, y)} u d x+v d y,
$$

the line integral taken along the straight line segment from $(0,0)$ to $(x, y)$. Fix $a=\left(a_{1}, a_{2}\right) \in \mathbb{R}^{2}$, and $t \neq 0$. Using Green's Theorem [Rud87, p. 253], on the triangle with vertices $(0,0),\left(a_{1}, a_{2}\right)$ and $\left(a_{1}+t, a_{2}\right)$, the parametrization $x=a_{1}+\tau, y=a_{2}, 0 \leqslant \tau \leqslant t$, and the mean value theorem for integrals, we obtain

$$
\frac{f\left(a_{1}+t, a_{2}\right)-f\left(a_{1}, a_{2}\right)}{t}=\frac{1}{t} \int_{\left(a_{1}, a_{2}\right)}^{\left(a_{1}+t, a_{2}\right)} u d x+v d y
$$



$$
\begin{aligned}
&=\frac{1}{t} \int_{0}^{t} u\left(a_{1}+\tau, a_{2}\right) d \tau \\
&=u\left(a_{1}+\theta t, a_{2}\right) \text { for some } 0<\theta<1 .
\end{aligned}
$$

Taking the limit when $t \rightarrow 0$; we obtain $\frac{\partial f}{\partial x}=u$ and similarly, $\frac{\partial f}{\partial y}=v$.

2. Let $U=\mathbb{R}^{2} \backslash\{0\}$. If such a function $f: U \rightarrow \mathbb{R}$ existed then, for any closed curve $C$ in $U$,

$$
\int_{C} \frac{\partial f}{\partial x} d x+\frac{\partial f}{\partial y} d y=0 .
$$

However, for the unit circle with the parametrization $x=\cos t, y=\sin t$, $0 \leqslant t \leqslant 2 \pi$,

$$
\int_{C} \frac{-y}{x^{2}+y^{2}} d x+\frac{x}{x^{2}+y^{2}} d y=2 \pi .
$$

which is a contradiction.

Solution to 2.2.32: The answer is no; to see it, consider the function $f(x, y, z)=1-x^{2}-y^{2}-z^{2}$.

Solution to 2.2.33: Fix a point $x \in \mathbb{R}^{n}$. By the Chain Rule [Rud87, p. 214],

$$
D(g \circ f)(x)=((D g)(f(x)))((D f)(x))=0 .
$$

The transformation $(D g)(f(x)): \mathbb{R}^{n} \rightarrow \mathbb{R}$ is nonzero because $g$ has no critical points. The preceding equality therefore implies that the transformation $(D f)(x): \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ is noninvertible, so its determinant vanishes. That determinant is the Jacobian determinant of $f$ at $x$.

Solution to 2.2.34: 1. Let $F: \mathbb{R}^{2} \rightarrow \mathbb{R}$ be defined by $F(x, t)=f(x)-\operatorname{tg}(x)$. Then $F$ is a smooth scalar field with $F(0,0)=0$ and

$$
\frac{\partial F}{\partial x}(0,0)=f^{\prime}(0)-0 g^{\prime}(0) \neq 0 .
$$

Therefore, by the Implicit Function Theorem [Rud87, p. 224], there exists a positive $\delta$ such that, for $|t|<\delta, x$ is a smooth function of $t$, with $x(0)=0$.

2. Differentiating both sides of $f(x(t))=\operatorname{tg}(x(t))$ with respect to $t$, we have, for $|t|<\delta$,

$$
x^{\prime}(t)=\frac{g(t)}{f^{\prime}(t)} .
$$

As $x(0)=0$, the desired expansion of $x(t)$ is

$$
\frac{g(0)}{f^{\prime}(0)} t .
$$

Solution to 2.2.35: Subtracting the second and third equations from the first we get $u^{4}-3 u=0$, so that $u=0$ or $u=\sqrt[3]{3}$. If $u=0$, then

$$
\begin{array}{r}
3 x+y-z=0 \\
x-y+2 z=0 \\
2 x+2 y-3 z=0
\end{array}
$$

whose solution is given by $x=-z / 4, y=7 z / 4$.

If $u=\sqrt[3]{3}$, then

$$
\begin{aligned}
3 x+y-z &=-3 \sqrt[3]{3} \\
x-y+2 z &=-\sqrt[3]{3} \\
2 x+2 y-3 z &=-2 \sqrt[3]{3}
\end{aligned}
$$

whose solution is given by $x=-(z / 4)-\sqrt[3]{3}, y=7 z / 4$.

The solution set of the system of equations is a disjoint union of two parallel lines in $\mathbb{R}^{4}$ :

$$
\begin{aligned}
&(x, y, z, u)=z\left(-\frac{1}{4}, \frac{7}{4}, 1,0\right) \\
&(x, y, z, u)=z\left(-\frac{1}{4}, \frac{7}{4}, 1,0\right)+(-\sqrt[3]{3}, 0,0, \sqrt[3]{3}) .
\end{aligned}
$$

Thus, for any $\epsilon>0$, the system can be solved for $(x, y, z)$ as a function of $z \in[-\epsilon, \epsilon]$, with $x(0)=y(0)=u(0)=0$. Namely, let $A, B$ be disjoint sets with $A \cup B=[-\epsilon, \epsilon] \backslash\{0\}$, and define

$$
x(z)=\left\{\begin{array}{ll}
-z / 4 & z \in A \\
-(z / 4)-\sqrt[3]{3} & z \in B \\
0 & z=0
\end{array} \quad y(z)=7 z / 4 \quad u(z)= \begin{cases}0 & z \in A \\
\sqrt[3]{3} & z \in B \\
0 & z=0\end{cases}\right.
$$

The function $y(z)$ is unique, continuous, and differentiable. Neither functions $x(z)$ and $u(z)$ are unique. The function $x(z)$ is continuous, and differentiable, if and only if $B=\varnothing$. The function $u(z)$ is continuous, and differentiable, if and only if $B=\varnothing$.

2. We have seen above that the system has a solution if and only if $u=0$ or $u=\sqrt[3]{3}$. So if $0<u<\sqrt[3]{3}$, the system has no solutions. Thus, the system cannot be solved for $(x, y, z)$ as a function of $u \in[-\delta, \delta]$, for any $\delta>0$.

Solution to 2.2.36: Parametrizing the circle by the angle $\theta$, the function becomes

$$
f_{a, b}(\theta)=a \sin ^{2} \theta+b \cos \theta
$$

and the derivative is

$$
\begin{aligned}
f_{a, b}^{\prime}(\theta) &=2 a \sin \theta \cos \theta-b \sin \theta \\
&=\sin \theta(2 a \cos \theta-b)
\end{aligned}
$$

So $f_{a, b}$ always has at least two critical points at $\theta=\pm \pi / 2$ and maybe two additional ones when $\cos \theta=b / 2 a$. For this to happen $|b / 2 a| \leqslant 1$, that is, the $(a, b)$ region is the region in between the lines $b=\pm 2 a$, containing the $x$-axis, including the lines itself and excluding the origin.

Solution 2. Using the method of Lagrange Multipliers [MH93, p. 414] for $f_{a, b}$ and $g(x, y)=x^{2}+y^{2}$, we end up with the system of equations:

$$
\begin{aligned}
b &=\lambda x \\
2 a y &=\lambda y \\
x^{2}+y^{2} &=1 .
\end{aligned}
$$

From this we can easily see that the pair $(x, y)=(\pm 1,0)$ is always a critical point, and if we suppose $y \neq 0$ then $\lambda=2 a$, which implies that the pair of points

$$
\left(\frac{b}{2 a}, \pm \frac{\sqrt{4 a^{2}-b^{2}}}{2 a}\right)
$$

might be two additional critical points. They will occur only when $a \neq 0$ and $4 a^{2}-b^{2} \geqslant 0$, that is $|b| \leqslant 2|a|$, that is, the region in between the lines $b=\pm 2 a$, containing the $x$-axis, including the lines itself and excluding the origin.

Solution to 2.2.37: Consider the function $G: \mathbb{R}^{2} \rightarrow \mathbb{R}^{2}$ given by

$$
G(x, y)=\left(\begin{array}{l}
u(x, y) \\
v(x, y)
\end{array}\right) .
$$

Since $\nabla u$ and $\nabla v$ are linearly dependent and $\nabla u$ is never $0, G^{\prime}$ has rank 1 everywhere. Therefore, by the Rank Theorem [Bar76, p. 391], given a point $p_{0} \in \mathbb{R}^{2}$, there is a neighborhood $V$ of $p_{0}$, an open set $W \subset \mathbb{R}^{2}$, a diffeomorphism $h: W \rightarrow V$, and a $C^{1}$-function $g=\left(g_{1}, g_{2}\right): \mathbb{R} \rightarrow \mathbb{R}^{2}$ such that $G(h(x, y))=$ $g(x)$ on $W$. So $g_{1}^{\prime}(x)$ is never 0 . Therefore, by the Inverse Function Theorem [Rud87, p. 221], $g_{1}$ is locally invertible. By shrinking the set $W$ (and so the set $V)$, we may assume that it is invertible. Therefore, $g_{1}^{-1}(u(h(x, y)))=x$ or $g_{2} \circ g_{1}^{-1}(u(h(x, y)))=v(h(x, y))$ for all $(x, y) \in W$. Since $h$ is a diffeomorphism of $W$ onto $V$, it follows that $g_{2} \circ g_{1}^{-1}(u(x, y))=v(x, y)$ for all $(x, y) \in V$. $F=g_{2} \circ g_{1}^{-1}$ satisfies the required condition.

Solution to 2.2.38: The conclusion is trivial if $f$ is constant, so we assume $f$ is not a constant. There is $\left(x_{0}, y_{0}\right) \in \mathbb{R}^{2}$ such that $D f\left(x_{0}, y_{0}\right) \neq 0$. After performing a rotation of the coordinates, if necessary, we assume $f_{x}\left(x_{0}, y_{0}\right) \neq 0$. Let $a=f\left(x_{0}, y_{0}\right)$, and consider the function $F: \mathbb{R}^{2} \rightarrow \mathbb{R}^{2}$ given by

$$
F(x, y)=(f(x, y), y) .
$$

The Jacobian of $F$ is nonzero at $\left(x_{0}, y_{0}\right)$, so, by the Inverse Function Theorem [Rud87, p. 221], the function $F$ has a local inverse, $G$, defined in a neighborhood of $\left(a, y_{0}\right)$. Thus, $F\left(G\left(a, y_{0}\right)\right)=(a, y)$ for all $y$ in some closed interval $I$ containing $y_{0}$. Let $\gamma$ be any one-to-one map of $[0,1]$ onto $I$. The function

$$
g(t)=G(a, \gamma(t)) \quad(t \in[0,1])
$$

has the desired properties.

Solution to 2.2.39: Consider $F: \mathbb{R}^{2} \rightarrow \mathbb{R}^{2}$ given by

$$
F(x, y)=(f(x),-y+x f(x)) \text {. }
$$

A calculation gives that the Jacobian of $F$ at $\left(x_{0}, y_{0}\right)$ is $-f^{\prime}\left(x_{0}\right) \neq 0$. So, by the Inverse Function Theorem [Rud87, p. 221], $F$ is invertible in a neighborhood of $\left(x_{0}, y_{0}\right)$. Similarly, $f$ has a local inverse, $g$, close to $x_{0}$. In a sufficiently small neighborhood of $\left(x_{0}, y_{0}\right)$ we can then solve for each component of $F^{-1}$ explicitly and get

$$
g(u)=g(f(x))=x
$$

and

$$
y=-v+x f(x)=-v+g(u) f(g(u))=-v+u g(u) .
$$

Solution to 2.2.40: $f$ is clearly a $C^{\infty}$ function, so

$$
\begin{aligned}
f(X+h Y)-f(X) &=(X+h Y)(X+h Y)-X^{2} \\
&=h X Y+h Y X+h^{2} Y^{2}
\end{aligned}
$$

therefore

$$
f^{\prime}(X) \cdot Y=X Y+Y X
$$

Solution to 2.2.41: 1. Let $F: \mathbb{R}^{4} \rightarrow \mathbb{R}^{4}$ be defined by

$$
F(x, y, z, w)=\left(x^{2}+y z, y(x+w), z(x+w), z y+w^{2}\right) .
$$

This map is associated with the given map because

$$
\left(\begin{array}{cc}
x & y \\
z & w
\end{array}\right)^{2}=\left(\begin{array}{cc}
x^{2}+y z & y(x+w) \\
z(x+w) & z y+w^{2}
\end{array}\right) .
$$

The Jacobian of $F$ at $(1,0,0,1)$ is $2^{4}$; therefore, $F$ is locally invertible near that point.

2. We have $F(1, \varepsilon, \varepsilon,-1)=(1,0,0,1)$ for any $\varepsilon$, so $F$ is not invertible near $(1,0,0,-1)$.

Solution to 2.2.42: Identify the matrix $X=\left(\begin{array}{ll}x & y \\ z & w\end{array}\right)$ with the element of $(x, y, z, w)$ $\in \mathbb{R}^{4}$ in the usual way. Let $F$ be defined by $F(X)=X^{2}+X^{t}$. We have

$$
D F(X)(x, y, z, w)=\left(\begin{array}{cccc}
2 x+1 & z & y & 0 \\
y & x+w & 1 & y \\
z & 1 & x+w & z \\
0 & z & y & 2 w+1
\end{array}\right)
$$

so

$$
D F(X)(0,0,0,0)=\left(\begin{array}{llll}
1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1
\end{array}\right)
$$

is invertible; therefore, by the Inverse Function Theorem [Rud87, p. 221], there is such an $\varepsilon$.

Global unicity fails for $X=\left(\begin{array}{rr}-1 & 0 \\ 0 & 0\end{array}\right)$ since $X^{2}+X^{t}=0=0^{2}+0^{t}$.

Solution to 2.2.43: Since $F(0)=0$ and $F$ is clearly a $C^{\infty}$-function, the Inverse Function Theorem [Rud87, p. 221] will yield the result if we can prove that $D F(0)$ is invertible. We have

$F(X+h Y)-F(X)=X+h Y+(X+h Y)^{2}-X-X^{2}=h Y+h X Y+h Y X+h^{2} Y^{2}$

therefore,

$$
D F(X) Y=Y+X Y+Y X
$$

In particular, $D F(0)$ is the identity operator which is invertible.

Solution to 2.2.44: Define the function $F: M_{n \times n} \rightarrow M_{n \times n}$ by $F(X)=X^{4}$ $F$ is clearly a $C^{\infty}$-function and $F(I)=I$, so the Inverse Function Theorem [Rud87, p. 221] will yield the result once we prove that $D F(I)$ is invertible, but the computation of the derivative, as above, is

$$
D F(X) \cdot Y=X^{3} Y+X^{2} Y X+X Y X^{2}+Y X^{3}
$$

In particular, $D F(I)=4 I$ which is invertible, showing that $F$ is a diffeomorphism in a neighbourhood of the Identity matrix.

Solution to 2.2.45: 1. Using the method of Laplace Expansions [HK61, p. 179] we can see that finding the determinant involves only sums and multiplications of the entries of a matrix, therefore, it is a $C^{\infty}$-function.

2. For $i, j=1, \ldots, n$, let $x_{i j}$ denote the $(i, j)^{t h}$ entry of $X$, and let $X_{i j}$ denote the cofactor of $x_{i j}$, so that

$$
\operatorname{det}(X)=\sum_{j=1}^{n} x_{i j} X_{i j} \quad(i=1, \ldots, n)
$$

Since $\frac{\partial X_{i k}}{\partial x_{i j}}=0$ for each $i, j, k$, it follows from the preceding expression that

$$
\frac{\partial \text { det }}{\partial x_{i j}}=X_{i j} .
$$

Thus, $X$ is a critical point of det if and only if $X_{i j}=0$ for every $i$ and $j$ or, what is equivalent, if and only if the rank of $X$ does not exceed $n-2$.

Solution to 2.2.46: $u(t)$ is differentiable as the composition of differentiable functions, and the derivative can be computed using the chain rule.

$$
\begin{aligned}
u^{\prime}(t) &=\operatorname{tr}\left(F^{2}(t) F^{\prime}(t)+F(t) F^{\prime}(t) F(t)+F^{\prime}(t) F^{2}(t)\right) \\
&=\operatorname{tr}\left(F^{2}(t) F^{\prime}(t)\right)+\operatorname{tr}\left(F(t) F^{\prime}(t) F(t)\right)+\operatorname{tr}\left(F^{\prime}(t) F^{2}(t)\right) \\
&=3 \operatorname{tr}\left(F^{2}(t) F^{\prime}(t)\right) .
\end{aligned}
$$

Solution to 2.2.47: Let $A_{j}$ be the 1-column matrix

$$
A_{j}=\left(\begin{array}{c}
a_{1 j} \\
\vdots \\
a_{n j}
\end{array}\right)
$$

so $A$ is the matrix whose column $j$ is $A_{j}$, that is, $A=\left(A_{1}, \ldots, A_{n}\right)$. Since det is an $n$-linear function of $\left(\mathbb{R}^{n}\right)^{n}$ into $\mathbb{R}$, the derivative is given by

$$
\frac{d}{d t} \operatorname{det}(A)=\sum_{j=1}^{n} \operatorname{det}\left(A_{1}, \ldots, \frac{d}{d t} A_{j}, \ldots, A_{n}\right) .
$$

Let $A(i, j)$ denote the cofactor of $a_{i j}$, that is,

$$
A(i, j)=(-1)^{i+j}\left|\begin{array}{cccccc}
a_{11} & \ldots & a_{1 j-1} & a_{1 j+1} & \ldots & a_{1 n} \\
\vdots & & \vdots & \vdots & & \vdots \\
a_{i-11} & \ldots & a_{i-1 j-1} & a_{i-1 j+1} & \ldots & a_{i-1 n} \\
a_{i+11} & \ldots & a_{i+1 j-1} & a_{i+1 j+1} & \ldots & a_{i+1 n} \\
\vdots & & \vdots & \vdots & & \vdots \\
a_{n 1} & \ldots & a_{n j-1} & a_{n j+1} & \ldots & a_{n n}
\end{array}\right| .
$$

Using Laplace's Expansion Method to evaluate the determinant [HK61, p. 179]

$$
\operatorname{det}\left(A_{1}, \ldots, \frac{d}{d t} A_{j}, \ldots, A_{n}\right)
$$

of each component of the derivative, by developing the $j^{t h}$ column we get

$$
\operatorname{det}\left(A_{1}, \ldots, \frac{d}{d t} A_{j}, \ldots, A_{n}\right)=\sum_{i=1}^{n} \frac{d}{d t} a_{i j} A(i, j)
$$

and

$$
\frac{d}{d t} \operatorname{det}(A)=\sum_{j=1}^{n} \sum_{i=1}^{n} \frac{d}{d t} a_{i j} A(i, j)=\sum_{j=1}^{n} \sum_{i=1}^{n} \frac{d}{d t} a_{i j} \operatorname{det}(A) b_{j i}
$$

where the last equality follows from the fact that the inverse matrix is given by $b_{i j}=\frac{1}{\operatorname{det}(A)} \cdot A(j, i)$. Therefore, we have 

$$
\begin{aligned}
\frac{d}{d t} \log (\operatorname{det}(A)) &=\frac{1}{\operatorname{det}(A)} \cdot \frac{d}{d t} \operatorname{det}(A) \\
&=\frac{1}{\operatorname{det}(A)} \cdot \sum_{j=1}^{n} \sum_{i=1}^{n} \frac{d}{d t} a_{i j} \cdot \operatorname{det}(A) \cdot b_{j i} \\
&=\sum_{j=1}^{n} \sum_{i=1}^{n} \frac{d}{d t} a_{i j} b_{j i} .
\end{aligned}
$$

Solution to 2.2.48: 1 . The measure preserving condition is that the derivative, $f^{\prime}$, has absolute value 1 at every point. Since the function is continuous it must be either $f^{\prime}=-1$ or $f^{\prime}=1$ on the entire domain. Therefore, $f(x)=-x+c$ or $f(x)=x+c$ where $c$ is a constant. Thus, $f$ cannot map $\mathbb{R}$ into $\mathbb{R}_{+}$.

2. Take $f(x, y)=\left(e^{-y} x, e^{y}\right)$.

\subsection{Integral Calculus}

Solution to 2.3.1: Let $f: \mathbb{R}^{3} \rightarrow \mathbb{R}, f(x, y, z)=(a x, b y, c z)$. The volume given is the image, under $f$, of the unit ball of $\mathbb{R}^{3}, \mathcal{B}$. As the Jacobian of $f$ is $a b c$ everywhere, we have

$$
\operatorname{vol}(f(\mathcal{B}))=\iiint_{f(\mathcal{B})} d x d y d z=\iiint_{\mathcal{B}} a b c d x d y d z=\frac{4}{3} \pi a b c .
$$

Solution to 2.3.2: Using polar coordinates, we have

$$
\begin{aligned}
\iint_{\mathcal{A}} e^{-x^{2}-y^{2}} d x d y &=\int_{0}^{2 \pi} \int_{0}^{1} \rho e^{-\rho^{2}} d \rho d \theta \\
&=-\frac{1}{2} \int_{0}^{2 \pi} \int_{0}^{1}-2 \rho e^{-\rho^{2}} d \rho d \theta \\
&=-\frac{1}{2} \int_{0}^{2 \pi}\left(e^{-1}-1\right) \\
&=\pi\left(e^{-1}-1\right) .
\end{aligned}
$$

Solution to 2.3.3: We write

$$
\begin{aligned}
I &=\int_{-\infty}^{\infty} e^{-3 y^{2} / 2}\left(\int_{-\infty}^{\infty} e^{-2 x^{2}+2 x y-y^{2} / 2} d x\right) d y \\
&=\int_{-\infty}^{\infty} e^{-3 y^{2} / 2}\left(\int_{-\infty}^{\infty} e^{-2\left(x-\frac{y}{2}\right)^{2}} d x\right) d y
\end{aligned}
$$

making the substitution $t=\sqrt{2}\left(x-\frac{y}{2}\right), d t=\sqrt{2} d x$ on the inner integral, we get

$$
\begin{aligned}
I &=\frac{1}{\sqrt{2}} \int_{-\infty}^{\infty} e^{-3 y^{2} / 2}\left(\int_{-\infty}^{\infty} e^{-t^{2}} d t\right) d y \\
&=\sqrt{\frac{\pi}{2}} \int_{-\infty}^{\infty} e^{-3 y^{2} / 2} d y
\end{aligned}
$$

now making the substitution: $s=\sqrt{\frac{3}{2}} y, d s=\sqrt{\frac{3}{2}} d y$ we obtain

$$
\begin{aligned}
I &=\sqrt{\frac{\pi}{3}} \int_{-\infty}^{\infty} e^{-s^{2}} d s \\
&=\frac{\pi}{\sqrt{3}} .
\end{aligned}
$$

Solution 2. As the integrand is positive, the iterated integral equals the double integral:

$$
I=\int_{\mathbb{R}^{2}} e^{-\left(x^{2}+(y-x)^{2}+y^{2}\right)} d(x, y) .
$$

Let $u=x+y, v=-x+y$; this map is a differentiable bijection $\mathbb{R}^{2} \rightarrow \mathbb{R}^{2}$. Moreover

$$
x^{2}+(y-x)^{2}+y^{2}=\frac{1}{2}\left(u^{2}+3 v^{2}\right) \text { and } \frac{\partial(u, v)}{\partial(x, y)}=2 .
$$

By the Change of Variable Formula for multiple integrals [MH93, p. 505],

$$
\begin{aligned}
\int_{\mathbb{R}^{2}} e^{-\left(x^{2}+(y-x)^{2}+y^{2}\right)} d(x, y) &=\int_{\mathbb{R}^{2}} e^{-\frac{1}{2}\left(u^{2}+3 v^{2}\right)}\left|\frac{\partial(x, y)}{\partial(u, v)}\right| d(u, v) \\
&=\frac{1}{2} \int_{\mathbb{R}^{2}} e^{-\frac{1}{2} u^{2}} e^{-\frac{3}{2} v^{2}} d(u, v) \\
&=\frac{1}{2} \int_{\mathbb{R}} e^{-\frac{1}{2} u^{2}} d u \int_{\mathbb{R}} e^{-\frac{3}{2} v^{2}} d v \\
&=\frac{1}{2}(\sqrt{2 \pi})(\sqrt{2 \pi / 3}) \\
&=\frac{\pi}{\sqrt{3}} .
\end{aligned}
$$

Solution to 2.3.4: Let $f(x, y)=x^{3}-3 x y^{2}$. Derivating twice (see Problem 5.9.1), we can see that $\Delta f=0$, so $f$ is harmonic. Let $\mathcal{B}_{1}=\left\{(x, y) \mid(x+1)^{2}+y^{2} \leqslant 9\right\}$ and $\mathcal{B}_{2}=\left\{(x, y) \mid(x-1)^{2}+y^{2} \leqslant 1\right\}$. We have

$$
\iint_{\mathcal{R}} f=\iint_{\mathcal{B}_{1}} f-\iint_{\mathcal{B}_{2}} f
$$

and

$$
\iint_{\mathcal{B}_{1}} f(x, y) d x d y=\int_{0}^{3}\left(\int_{0}^{2 \pi} f\left(\xi+3 e^{i \theta}\right) d \theta\right) r d r
$$

where $\xi=(-1,0)$ is the center of $B_{1}$. As $f$ is harmonic, the inner integral is equal to $2 \pi f(\xi)=-2 \pi$ and the integral of $f$ over $\mathcal{B}_{1}$ is $-9 \pi$. An identical calculation shows that the integral over $\mathcal{B}_{2}$ is $\pi$, so the integral of $f$ over $\mathcal{R}$ is $-10 \pi$

Solution 2. The integrand is harmonic, being the real part of the holomorphic function $z^{3}$. Its integral over any disc is therefore the value at the center of the disc times the area of the disc. Hence, if we call $\mathcal{D}_{1}$ and $\mathcal{D}_{2}$ the external and internal discs that make up the boundary of $\mathcal{R}$, respectively, we have

$$
\begin{aligned}
\iint_{\mathcal{R}}\left(x^{3}-3 x y^{2}\right) d x d y &=\iint_{\mathcal{D}_{1}}\left(x^{3}-3 x y^{2}\right) d x d y-\iint_{\mathcal{D}_{2}}\left(x^{3}-3 x y^{2}\right) d x d y \\
&=-9 \pi-\pi \\
&=-10 \pi
\end{aligned}
$$

Solution to 2.3.5: Consider the annular region $\mathcal{A}$ between the circles of radius $r$ and $R$, then by the Green's Theorem

$$
\begin{aligned}
&\int_{R} e^{x}\left(-\varphi_{y} d x+\varphi_{x} d y\right)-\int_{r} e^{y}\left(-\varphi_{y} d x+\varphi_{x} d y\right)= \\
&=\int_{\partial \mathcal{A}} e^{x}\left(-\varphi_{x} d x+\varphi_{x} d y\right) \\
&=\iint_{\mathcal{A}} d\left(e^{x}\left(-\varphi_{y} d x+\varphi_{x} d y\right)\right) \\
&=\iint_{\mathcal{A}}-e^{x} \varphi_{y y} d y \wedge d x+\left(e^{x} \varphi_{x}+e^{x} \varphi_{x x}\right) d x \wedge d y \\
&=\iint_{\mathcal{A}} e^{x}\left(\varphi_{x x}+e^{x} \varphi_{x x}+\varphi_{x}\right) d x \wedge d y=0
\end{aligned}
$$

showing that the integral does not depend on the radius $r$. Now, parametrizing the circle of radius $r$

$$
\begin{aligned}
&\int_{C_{r}} e^{x}\left(-\varphi_{y} d x+\varphi_{x} d y\right)= \\
&=\int_{0}^{2 \pi} e^{r \cos \theta}\left(\varphi_{y}(r \cos \theta, r \sin \theta) \sin \theta+\varphi_{x}(r \cos \theta, r \sin \theta) \cos \theta\right) r d \theta
\end{aligned}
$$

but when $r \rightarrow 0$ the integrand converges uniformly to

$$
\frac{\sin \theta}{2 \pi} \cdot \sin \theta+\frac{\cos \theta}{2 \pi} \cdot \cos \theta=\frac{1}{2 \pi}
$$

so the integral approaches 1 when $r \rightarrow 0$ and that is the value of the integral.

Solution to 2.3.6: Using the change of variables

$$
\begin{cases}x=\sin \varphi \cos \theta & 0<\theta<2 \pi \\ y=\sin \varphi \sin \theta & 0<\varphi<\pi \\ z=\cos \varphi & \end{cases}
$$

we have

$$
d A=\sin \varphi d \theta d \varphi
$$

and

$$
\iint_{\mathcal{S}}\left(x^{2}+y+z\right) d A=\int_{0}^{\pi} \int_{0}^{2 \pi}\left(\sin ^{2} \varphi \cos ^{2} \theta+\sin \varphi \sin \theta+\cos \varphi\right) \sin \varphi d \theta d \varphi .
$$

Breaking the integral in three terms, we get

$$
\begin{aligned}
\int_{0}^{\pi} \int_{0}^{2 \pi} \sin \varphi \cos \varphi d \theta d \varphi=2 \pi \cdot \frac{1}{2} \int_{0}^{\pi} \sin 2 \varphi d \varphi=0 \\
\int_{0}^{\pi} \int_{0}^{2 \pi} \sin ^{2} \varphi \sin \theta d \theta d \varphi=\left(\int_{0}^{\pi} \sin ^{2} \varphi d \varphi\right) \int_{0}^{2 \pi} \sin \theta d \theta=0 \\
\int_{0}^{\pi} \int_{0}^{2 \pi} \sin ^{3} \varphi \cos ^{2} \theta d \theta d \varphi &=\left(\int_{0}^{\pi} \sin ^{3} \varphi d \varphi\right)\left(\int_{0}^{2 \pi} \cos ^{2} \theta d \theta\right) \\
&=\int_{0}^{\pi} \frac{1}{4}\left(3 \sin \varphi-\sin ^{3} \varphi\right) d \varphi \int_{0}^{2 \pi} \cos ^{2} \theta d \theta \\
&=\frac{1}{4}\left(-3 \cos \varphi+\left.\frac{1}{3} \cos ^{3} \varphi\right|_{0} ^{\pi}\right)\left(\int_{0}^{2 \pi} \frac{1+\cos 2 \theta}{2} d \theta\right) \\
&=\frac{1}{4}\left(-3(-2)+\frac{1}{3}(-2)\right) \pi \\
&=\frac{1}{4}\left(6-\frac{2}{3}\right) \pi=\frac{4}{3} \pi .
\end{aligned}
$$

Therefore,

$$
\iint_{\mathcal{S}}\left(x^{2}+y+z\right) d A=\frac{4}{3} \pi
$$

Solution 2. The outward unit normal to $\mathcal{S}$ is $\overrightarrow{\mathbf{n}}=(x, y, z)$. Consider the vector field $\overrightarrow{\mathrm{F}}(x, y, z)=(x, 1,1)$. Then $\overrightarrow{\mathbf{F}} \cdot \overrightarrow{\mathbf{n}}=x^{2}+y+z$, and div $\overrightarrow{\mathbf{F}}=1$. Using the Divergence Theorem,

$$
\iint_{\mathcal{S}}\left(x^{2}+y+z\right) d A=\iint_{\mathcal{S}} \overrightarrow{\mathbf{F}} \cdot \overrightarrow{\mathbf{n}} d A
$$



$$
\begin{aligned}
&=\iiint_{x^{2}+y^{2}+z^{2} \leqslant 1} \operatorname{div} \overrightarrow{\mathrm{F}} d V \\
&=\iiint d V \\
&=\frac{4}{3} \pi .
\end{aligned}
$$

Solution to 2.3.7: 1 . We have

$$
\left|\begin{array}{ccc}
\vec{\imath} & \vec{\jmath} & \vec{k} \\
\partial / \partial x & \partial / \partial y & \partial / \partial z \\
x^{2}+y-4 & 3 x y & 2 x z+z^{2}
\end{array}\right|=-2 z \vec{\jmath}+(3 y-1) \vec{k} \text {. }
$$

2. Let $\mathcal{H}=\left\{(x, y, z) \in \mathbb{R}^{3} \mid x^{2}+y^{2}+z^{2}=16, z \geqslant 0\right\}$, and consider the set $\mathcal{D}$ given by $\mathcal{D}=\left\{(x, y, 0) \in \mathbb{R}^{3} \mid x^{2}+y^{2} \leqslant 16\right\} . \mathcal{H}$ and $\mathcal{D}$ have the same boundary, so, by Stokes' Theorem [Rud87, p. 253]

$$
\begin{aligned}
\int_{\mathcal{H}}(\nabla \times F) \cdot \overrightarrow{d S} &=\int_{\partial \mathcal{H}} F \cdot \vec{d} l=\int_{\partial \mathcal{D}} F \cdot \overrightarrow{d l} \\
&=\int_{\partial \mathcal{D}}(\nabla F \times F) \cdot d S=\iint_{\mathcal{D}}(-2 z \vec{j}+(3 y-1) \vec{k}) \cdot \vec{k} d x d y \\
&=\iint_{\mathcal{D}}(3 y-1) d x d y=-16 \pi .
\end{aligned}
$$

Solution to 2.3.8: Let $C$ be a smooth closed path in $\mathbb{R}^{3}$ which does not contain the origin, and let $L$ be any polygonal line from the origin to infinity that does not intersect $C$. $V=\mathbb{R}^{3} \backslash L$ is simply connected; so to show that

$$
\int_{C} F \cdot d s=0
$$

it suffices to show that $\nabla \times F=0$. Let $r=(x, y, z)$ and $F=(P, Q, R)$. We have

$$
F(r)=(g(\|r\|) x, g(\|r\|) y, g(\|r\|) z) .
$$

By the Chain Rule [Rud87, p. 214],

$$
\begin{aligned}
\frac{\partial R}{\partial y}-\frac{\partial Q}{\partial z} &=g^{\prime}(\|r\|) \frac{\partial\|r\|}{\partial y} z+g^{\prime}(\|r\|) \frac{\partial z}{\partial y}-g^{\prime}(\|r\|) \frac{\partial\|r\|}{\partial z} y-g(\|r\|) \frac{\partial y}{\partial z} \\
&=g^{\prime}(\|r\|) \frac{y z}{\|r\|}-g^{\prime}(\|r\|) \frac{y z}{\|r\|} \\
&=0 .
\end{aligned}
$$

Similarly,

$$
\frac{\partial P}{\partial z}-\frac{\partial R}{\partial z}=\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}=0
$$

and we are done.

Solution to 2.3.9: 1 . From the identity

$$
\nabla \cdot(f \vec{J})=(\nabla f) \cdot \vec{J}+f \nabla \cdot \vec{J}=\nabla f \cdot \vec{J}
$$

and Gauss Theorem [Rud87, p. 272], we obtain

$$
\begin{aligned}
\iiint_{\mathcal{B}}(\nabla f) \cdot \vec{J} d x d y d z &=\iiint_{\mathcal{B}} \nabla \cdot(f \vec{J}) d x d y d z \\
&=\iint_{\partial \mathcal{B}} f \vec{J} \cdot \vec{n} d A \\
&=0
\end{aligned}
$$

\section{Apply Part 1 with $f(x, y, z)=x$.}

Solution to 2.3.10: By Gauss Theorem [Rud87, p. 272],

$$
\iint_{\mathcal{D}} \operatorname{div}(u \operatorname{grad} u) d x d y=\int_{\partial \mathcal{D}}(u \operatorname{grad} u) \cdot \vec{n} d s,
$$

where $\vec{n}$ is the unit outward normal, and $d s$ is the differential of arc length. The right-hand side vanishes because $u=0$ on $\partial \mathcal{D}$. The left side equals the left side of $(*)$ because

$$
\begin{aligned}
\operatorname{div}(u \operatorname{grad} u) &=\frac{\partial}{\partial x}\left(u \frac{\partial u}{\partial x}\right)+\frac{\partial}{\partial y}\left(u \frac{\partial u}{\partial y}\right) \\
&=\left(\frac{\partial u}{\partial x}\right)^{2}+\left(\frac{\partial u}{\partial y}\right)^{2}+u\left(\frac{\partial^{2} u}{\partial x^{2}}+\frac{\partial^{2} u}{\partial y^{2}}\right) \\
&=|\operatorname{grad} u|^{2}+\lambda u^{2}
\end{aligned}
$$

\section{Solution to 2.3.11:}

$$
\iint_{\mathcal{D}} u\left(\frac{\partial^{2} u}{\partial x^{2}}+\frac{\partial^{2} u}{\partial y^{2}}-\lambda u\right)=\iint_{\mathcal{D}} 0=0
$$

substituting

$$
u\left(\frac{\partial^{2} u}{\partial x^{2}}+\frac{\partial^{2} u}{\partial y^{2}}\right)=\nabla \cdot(u \nabla u)-\|\nabla u\|^{2}
$$

the equality becomes:

$$
\iint_{\mathcal{D}} \nabla \cdot(u \nabla u)-\iint_{\mathcal{D}}\|\nabla u\|^{2}-\iint_{\mathcal{D}} \lambda u^{2}=0
$$

Now applying Green's Theorem [Rud87, p. 253], to the first term of the equation, we get:

$$
\int_{\partial \mathcal{D}} u D_{n} u-\iint_{\mathcal{D}}\|\nabla u\|^{2}-\iint_{\mathcal{D}} \lambda u^{2}=0
$$

and since $D_{n} u=a u$ on $\partial \mathcal{D}$, we get

$$
a \int_{\partial \mathcal{D}} u^{2}-\iint_{\mathcal{D}}\|\nabla u\|^{2}-\lambda \iint_{\mathcal{D}} u^{2}=0
$$

since $u$ is not identically zero inside the disk, the last integral is positive. Now if $u$ were constant on $\mathcal{D}$, the second equation $D_{n} u=a u$ on $\partial \mathcal{D}$ would force $u=0$, so $\nabla u$ is not identically zero on $\mathcal{D}$ and the integral $\iint_{\mathcal{D}}\|\nabla u\|^{2}$ is also positive, showing that $a \int_{\partial \mathcal{D}} u^{2} \geq 0$ and consequently that $\lambda<0$.

Solution to 2.3.12: By the Change of Variable Formula for multiple integrals [MH93, p. 505],

$$
\operatorname{vol} f\left(Q_{r}\left(x_{0}\right)\right)=\int_{Q_{r}\left(x_{0}\right)}|J(x)| d x .
$$

Hence, if $M_{r}$ is the maximum and $m_{r}$ the minimum of $|J(x)|$ for $x \in Q_{r}\left(x_{0}\right)$, we have

$$
m_{r} \leqslant r^{-3} \text { vol } f\left(Q_{r}\left(x_{0}\right)\right) \leqslant M_{r} .
$$

By the continuity of $J$, we have $m_{r} \rightarrow\left|J\left(x_{0}\right)\right|$ and $M_{r} \rightarrow\left|J\left(x_{0}\right)\right|$ as $r \rightarrow 0$, from which the desired equality follows.

To establish the inequality, we note that the same reasoning gives

$$
\text { (*) }\left|J\left(x_{0}\right)\right|=\lim _{r \rightarrow 0} \frac{\operatorname{vol} f\left(B_{r}\left(x_{0}\right)\right)}{\frac{4}{3} \pi r^{3}}
$$

where $B_{r}\left(x_{0}\right)$ denotes the ball of radius $r$ and center $x_{0}$. Let

$$
K=\limsup _{x \rightarrow x_{0}} \frac{\left\|f(x)-f\left(x_{0}\right)\right\|}{\left\|x-x_{0}\right\|} .
$$

Then, given $\varepsilon>0$, there is an $r_{\varepsilon}>0$ such that $\left\|f(x)-f\left(x_{0}\right)\right\| \leqslant(K+\varepsilon)\left\|x-x_{0}\right\|$ for $\left\|x-x_{0}\right\| \leqslant r_{\varepsilon}$. The latter means that, for $r \leqslant r_{\varepsilon}$,

$$
f\left(B_{r}\left(x_{0}\right)\right) \subset B_{(K+\varepsilon) r}\left(f\left(x_{0}\right)\right)
$$

so that

$$
\frac{\operatorname{vol} f\left(B_{r}\left(x_{0}\right)\right)}{\frac{4}{3} \pi r^{3}} \leqslant \frac{\frac{4}{3}(K+\varepsilon)^{3} r^{3}}{\frac{4}{3} \pi r^{3}}=(K+\varepsilon)^{3} .
$$

In view of the equation (*), this gives $\left|J\left(x_{0}\right)\right| \leqslant(K+\varepsilon)^{3}$. Since $\varepsilon$ is arbitrary, we get $\left|J\left(x_{0}\right)\right| \leqslant K^{3}$, the desired inequality. 

\section{3}

\section{Differential Equations}

\subsection{First Order Equations}

Solution to 3.1.1: For $\left(x_{0}, y_{0}\right)$ to be the midpoint of $L\left(x_{0}, y_{0}\right)$, the $y$ intercept of $L$ must be $2 y_{0}$ and the $x$ intercept must be $2 x_{0}$. Hence, the slope of the tangent line is $-y_{0} / x_{0}$. Let the curve have the equation $y=f(x)$. We get the differential equation

$$
f^{\prime}(x)=-\frac{f(x)}{x},
$$

or

$$
-\frac{1}{x}=\frac{f^{\prime}(x)}{f(x)}=\frac{1}{y} \frac{d y}{d x} .
$$

By separation of variables, we get

$$
\log y=-\log x+C \text {. }
$$

Hence,

$$
f(x)=y=\frac{D}{x}
$$

for some constant $D$. Solving for the initial condition $f(3)=2$, we get $f(x)=6 / x$.

Solution to 3.1.2: We have $g^{\prime}(x) / g(x)=2$, so $g(x)=K e^{2 x}$ where $K$ is a constant. The initial condition $g(0)=a$ gives $g(x)=a e^{2 x}$. Solution to 3.1.3: Suppose $y$ is such a function. Then

$$
y^{\prime}(x)=y(x)^{n}
$$

or

$$
\begin{aligned}
\frac{d y}{y^{n}} &=d x \\
d\left(\frac{y^{-n+1}}{1-n}\right) &=d x \\
y^{-n+1} &=(1-n) x+c .
\end{aligned}
$$

Moreover, $c=1 / y(0)^{n-1}>0$. We thus have

$$
y(x)=\frac{1}{(c-(n-1) x)^{1 /(n-1)}}
$$

This function solves the initial value problem $y^{\prime}=y^{n}, y(0)=c^{-1 /(n-1)}$ in the interval $\left[0, \frac{c}{n-1}\right.$ ), and, by the Picard's Theorem [San79, p. 8], it is the only solution. Since the function tends to $\infty$ as $x \rightarrow \frac{c}{n-1}$, there is no function meeting the original requirements.

Solution to 3.1.4: 1 . The zero function $x(t) \equiv 0$ is a solution for all $\alpha>0$.

2. If $\alpha>1$, then $f(x)=x^{\alpha}$ is lipschitzian in a neighborhood of zero and by Picard's Theorem this is the only solution.

If $0<\alpha<1$, then, as in the solution to Problem 3.1.3, we get

$$
x(t)^{1-\alpha}=(1-\alpha) t+c
$$

and the initial condition implies $c=0$. Therefore

$$
x(t)=((1-\alpha) t)^{\frac{1}{1-\alpha}}
$$

is a solution defined on $\mathbb{R}$.

Solution to 3.1.6: Separating the variables, we obtain

$$
\begin{aligned}
\frac{d y}{d x} &=x^{2}(y-3) \\
\int \frac{d y}{y-3} &=\int x^{2} d x \\
\log (y-3) &=\frac{1}{3} x^{3}+C \\
y-3 &=C e^{x^{3} / 3} \\
y &=3+C e^{x^{3} / 3}
\end{aligned}
$$

From initial conditions, $C=-2$. The unique maximal solution is given by

$$
y=3-2 e^{x^{3} / 3} .
$$

Solution to 3.1.7: Picard's Theorem [San79, p. 8] applies because the function $x^{2}-x^{6}$ is Lipschitzian on finite subintervals of the $x$-axis. Thus, two distinct solution curves are non intersecting. The constant functions $x(t) \equiv 0$ and $x(t) \equiv 1$ are solutions. Hence, if the solution $x(t)$ satisfies $x(0)>1$, then $x(t)>1$ for all $t$, and if it satisfies $0<x(0)<1$, then $0<x(t)<1$ for all $t$

Since

$$
x^{2}-x^{6}=x^{2}\left(1-x^{4}\right)=(1-x) x^{2}\left(1+x+x^{2}+x^{3}\right),
$$

we have

$$
\text { (*) } \frac{d}{d t}(x-1)=-(x-1) x^{2}\left(1+x+x^{2}+x^{3}\right) \text {. }
$$

We see from this (or directly from the original equation) that if $x(0)>1$, then $x-1$ decreases as $t$ increases, and if $0<x(0)<1$, then $1-x$ decreases as $x$ increases.

Case 1: $x(0)>1$. In this case, $(*)$ implies

$$
\frac{d}{d t}(x-1) \leqslant-(x-1)
$$

(since $x(t)>1$ for all $t$ ), so that

$$
\frac{d}{d t}\left(e^{t}(x-1)\right) \leqslant 0 .
$$

Hence, $e^{t}(x(t)-1) \leqslant x(0)-1$, that is, $x(t)-1 \leqslant e^{-t}(x(0)-1)$, from which the desired conclusion follows.

Case 2: $0<x(0)<1$. In this case, $(*)$ implies

$$
\frac{d}{d t}(1-x) \leqslant-x(0)^{2}(1-x)
$$

(since $x(t) \geqslant x(0)$ for all $t$ ), so that

$$
\frac{d}{d t}\left(e^{x(0)^{2} t}(1-x)\right) \leqslant 0 .
$$

Therefore, $e^{x(0)^{2} t}(1-x(t)) \leqslant 1-x(0)$, that is, $1-x(t) \leqslant e^{-x(0)^{2} t}(1-x(0))$, and the desired conclusion follows.

Solution 2 . The constant solutions are given by the roots of $x^{2}-x^{6}=0$, and are $x(t) \equiv 0,1,-1$. The phase portrait for this equation is so 1 is a stable singularity and the only one in the positive axis, so it is the limit of any orbit starting on the positive real axis.

Solution to 3.1.8: From the equation, we get $x^{\prime}=0$ iff $x^{3}-x=0$, so the constant solutions are $x \equiv-1, x \equiv 0$, and $x \equiv 1$.

2. Considering the sign of $x^{\prime}$, we get the phase portrait

![](https://cdn.mathpix.com/cropped/2022_10_26_41d22736fcd19c2c10fdg-012.jpg?height=87&width=824&top_left_y=772&top_left_x=648)

so 0 is a stable singularity, and 1 a unstable one. There are no other singularities in $[0,1]$, so the limit of the orbit of the solution $x(t)$ that verifies $x(0)=1 / 2$ is 0 .

Solution 2. The function $\frac{x^{3}-x}{1+e^{x}}$ is lipschitzian on the finite subintervals of the $\mathrm{x}$-axis, so by Picard's Theorem the solutions by each point are unique. For any $0<x<1, d x / d t<0$ and since $x(t) \equiv 0$ is one of the solutions of the equation, the solution by $x(0)=1 / 2$ is strictly decreasing and in between 0 and $1 / 2$ for all $t>0$. Now for these values of $x(0<x<1 / 2)$ we can easily estimate

$$
\frac{1}{2}>\frac{1-x^{2}}{1+e^{x}}>\frac{1}{4}
$$

so $\frac{d x}{d t}=-x \frac{1-x^{2}}{1+e^{x}}$ can be estimated as

$$
\frac{d x}{d t}<-\frac{1}{4} x \quad \text { for } \quad 0<x<\frac{1}{2}
$$

and

$$
\frac{d}{d t}\left(e^{t / 4} x\right)=e^{t / 4}\left(\frac{x}{4}+\frac{d x}{d t}\right)<0 .
$$

Hence $e^{t / 4} x<x(0)$, that is, $x<\frac{1}{2} e^{-t / 4}$, from which the conclusion follow.

Solution to 3.1.9: The given equation satisfies the hypotheses of Picard's Theorem [San79, p. 8], so a solution $x(t)$ exists in a neighborhood of the origin. Since $x^{\prime}(0)=231+85 \cos 77 \neq 0$, by the Inverse Function Theorem [Rud87, p. 221], $x$ is locally invertible. Its inverse satisfies the initial value problem:

$$
\frac{d t}{d x}=\frac{1}{3 x+85 \cos x}, \quad t(77)=0 .
$$

So

$$
t(x)=\int_{77}^{x} \frac{1}{3 \xi+85 \cos \xi} d \xi
$$

in some neighborhood of 77. Let $\alpha$ be the first zero of $3 x+85 \cos x$ to the left of 77 , since there are no zeroes to the right of it, $t(x)$ above is defined in the entire interval $(\alpha, \infty)$, our maximal solution. The series expansion around $\alpha$ is

$$
3 x+85 \cos x=(3-85 \sin \alpha)(x-\alpha)+O\left((x-\alpha)^{2}\right) \quad(x \rightarrow \alpha) .
$$

We cannot have $3-85 \sin \alpha=0$, since this, together with $3 \alpha+85 \cos \alpha=0$ imply $9+9 \alpha^{2}=0$. Therefore

$$
\lim _{x \rightarrow \alpha} \frac{3 x+85 \cos x}{x-\alpha} \neq 0 .
$$

It is clear that

$$
\lim _{x \rightarrow \infty} \frac{3 x+85 \cos x}{x-\alpha}=3 .
$$

Thus,

$$
\lim _{x \rightarrow \alpha} t(x)=-\infty \text { and } \lim _{x \rightarrow \infty} t(x)=\infty .
$$

We may take the inverse of $t:(\alpha, \infty) \rightarrow \mathbb{R}$ and get a function $x(t)$ that solves our initial value problem and is defined in all of $\mathbb{R}$.

Solution 2. Because the function $3 x+85 \cos x$ is continuous in $|x-b| \leqslant K$ and $|t-a| \leqslant T$, for any positive $K$ and $T$, the differential equation has a unique solution with $x(a)=b$ defined for

$$
|t-a| \leqslant \min \left(T, \frac{K}{3 K+3|b|+85}\right)
$$

For $T=1 / 4$ and $K=3|b|+85$, the solution is defined for $|t-a| \leqslant 1 / 4$, since $a$ and $b$ are arbitrary, the solution can then be continued in patches of size $1 / 2$ to the entire real line.

Solution to 3.1.11: Suppose $y(t)>0$ for $t \in\left(t_{0}, t_{1}\right), y\left(t_{0}\right)=0$. Integrating the equation

$$
\frac{y^{\prime}}{\sqrt{y}}=1
$$

we get the solution $y(t)=(t+c)^{2} / 4$ where $c$ is a constant. Each such solution can be extended to a differentiable function on $\mathbb{R}$ :

$$
y(t)= \begin{cases}0 & \text { if } t \leqslant t_{0} \\ \left(t-t_{0}\right)^{2} / 4 & \text { if } t \geqslant t_{0}\end{cases}
$$

We must have $t_{0} \geqslant 0$ for $y$ to satisfy the given initial condition. $y \equiv 0$ is also a solution. Solution to 3.1.12: The two functions $y \equiv 0$ and $y \equiv 2$ are obviously solutions to the differential equation, and the first one satisfies the given initial condition. We will show that this is the only one. Suppose there is a solution $y(x)$ that satisfies the initial condition and it is not identically zero, then there is a point $x_{0}$ where $y\left(x_{0}\right) \neq 0$ and let $(a, b)$ denote a maximal interval where $y(x) \neq 0$ containing $x_{0}$. Ate least one of the two extremes of the interval exists, let's call it $a$, the other might be infinite and for that extreme $y(a)=0$.

Inside this interval the separable differential equation can be written as

$$
\frac{d y}{\sqrt{y(y-2)}}=d x
$$

and with the substitution $w=x-1$ the first integral becomes

$$
\int \frac{d w}{\sqrt{w^{2}-1}}=\ln \left(w+\sqrt{w^{2}-1}\right)+C
$$

so integrating both sides we get

$$
\ln (y-1+\sqrt{y(y-2)})=x+C
$$

which is the same as

$$
y-1+\sqrt{y(y-2)}=k e^{x} \quad \text { for some } k>0
$$

but taking the limit when $x \rightarrow a, y$ is zero at the limit and we have a contradiction because the right side is positive. So no such functions exist, except for $y(x) \equiv 0$.

Solution to 3.1.13: The given equation is equivalent to $\frac{d}{d x}(x y)=x$. Integrating both sides we get $x y=\frac{x^{2}}{2}+k$, so $y=\frac{x}{2}+\frac{k}{x}$ and $k$ has to be 0 for the function to be differentiable in the given interval.

Solution 2. The equation is equivalent to

$$
\frac{d y}{d x}=1-\frac{y}{x}
$$

which is homogeneous in the sense of $\frac{d y}{d x}=f(x, y)$ and $f(t x, t y)=f(x, y)$, so the substitution $y=x v(x)$ converts it into a separable equation, in this case

$$
v+x \frac{d v}{d x}=1-v(x)
$$

or, after the separation of variables,

$$
\frac{d v}{1-2 v}=\frac{d x}{x}
$$

integrating both sides, we get,

$$
\begin{gathered}
\frac{\ln |1-2 v|}{-2}=\ln |x|+C \\
\ln |1-2 v|=\ln \frac{1}{k^{2} x^{2}} \\
|1-2 v|=\frac{1}{k^{2} x^{2}}
\end{gathered}
$$

solving both cases and renaming the constant we get $y=\frac{x}{2}+\frac{k}{x} \cdot$ We obtain the only one that is differentiable in the given domain making $k^{x}=0$, that is, $f(x)=\frac{x}{2}$

Solution to 3.1.15: 1. Let $u$ be defined by $u(x)=\exp \left(-3 x^{2} / 2\right) y(x)$. The given equation becomes

$$
\begin{aligned}
\frac{d u}{d x} &=-3 x e^{-\frac{3}{2} x^{2}} y(x)+e^{-\frac{3}{2} x^{2}}\left(3 x y(x)+\frac{y(x)}{1+y^{2}}\right) \\
&=\frac{u(x)}{1+e^{3 x^{2}} u(x)^{2}} \\
&=f(x, u) .
\end{aligned}
$$

$f$ is clearly $C^{1}$, so it satisfies the Lipschitz condition on any compact convex domain. The initial value problem

$$
\frac{d u}{d x}=\frac{u}{1+e^{3 x^{2}} u}, \quad u(0)=\frac{1}{n}
$$

then has a unique solution for any $n \in \mathbb{N}$.

2. $f \equiv 0$ is the unique solution of the initial value problem associated with the condition $u(0)=0$. Therefore, $f_{n}$ cannot have any zero, so $f_{n}(x)>0$ for $x \in[0,1]$. For $u(x)=\exp \left(-3 x^{2} / 2\right) f_{n}(x)$, we have

$$
0 \leqslant \frac{u^{\prime}}{u} \leqslant \frac{1}{1+e^{3 x^{2}} u^{2}} \leqslant 1
$$

so

$$
u(0)=\frac{1}{n} \leqslant u(x) \leqslant \frac{1}{n} e^{x}
$$

therefore,

$$
\frac{1}{n} e^{\frac{3}{2}} \leqslant f_{n}(1) \leqslant \frac{1}{n} e^{\frac{5}{2}}
$$

and $f_{n}(1) \rightarrow 0$ when $n \rightarrow \infty$

Solution to 3.1.16: If $y(t) \leqslant 0$ for some $t$, then $y^{\prime}(t) \geqslant 1$, so $y(t)$ is growing faster than $z(t)=t$ for all $t$ where $y(t) \leqslant 0$. Hence, there is a to with $y\left(t_{0}\right)>0$. For $y>0, y^{\prime}>0$, so for $t \geqslant t_{0}, y$ is positive. Further, for $y>0, e^{-y}>e^{-3 y}$, so $y^{\prime}>e^{-5 y}$. Now consider the equation $z^{\prime}=e^{-5 z}$. Solving this by separation of variables, we get $z(t)=\log (t / 5+C) / 5$, and for some choice of $C$, we have $z\left(t_{0}\right)=y\left(t_{0}\right)$. For all $t \geqslant t_{0}, y^{\prime}>z^{\prime}$, so $z(t) \leqslant y(t)$ for $t>t_{0}$. Since $z(t)$ tends to infinity as $t$ does, so does $y$.

Solution to 3.1.17: Since $\frac{d}{d t}\left(e^{\lambda t}\right)=\lambda e^{\lambda t}$, the function $y(t)=e^{\lambda t}$ is a solution if and only if $\lambda=-e^{\lambda_{0}}$, in other words, if and only if $\lambda$ is a zero of the entire function $f(z)=z+e^{-t_{0} z}$.

Assume $0<t_{0}<\pi / 2$. We must show that $f$ has no zeros in the closed halfplane $\Re z \geqslant 0$. If $\Re z \geqslant 0$ and $|z|>1$, then

$$
|f(z)| \geqslant|z|-\left|e^{-t_{0} z}\right|>1-e^{-t_{0} \theta i_{z}} \geqslant 1-1=0,
$$

so $f(z) \neq 0$. If $\Re z \geqslant 0$ and $|\Re z| \leqslant 1$, then

$$
\Re f(z)=\Re z+\Re e^{-t_{0} z}=\Re z+e^{-t_{0} 9 t z} \cos \left(t_{0} \Re z\right)>0
$$

because $\cos \theta>0$ for $-t_{0} \leqslant \theta \leqslant t_{0}$, so $f(z) \neq 0$. Hence $f$ has no zeros for $\Re z \geq 0$.

Solution 2. Suppose $e^{\lambda t}$ with $\lambda=a+i b$ is a solution. We must prove $a<0$. Assume $a \geqslant 0$. Then, from $f(\lambda)=0$, we get

$$
a+i b=-e^{-(a+i b) t_{0}}=-e^{-a}\left(\cos b t_{0}-i \sin b t_{0}\right) .
$$

Hence

$$
a=-e^{-a} \cos b t, \quad b=e^{-a} \sin b t_{0} .
$$

Since $a \geqslant 0$, the second equality implies that $|b| \leqslant 1$. Then $\cos b t_{0}>0$ since $0<t_{0}<\pi / 2$, whence $a<0$ by the first equality, a contradiction.

Solution to 3.1.18: Multiplying the first equation by the integrating factor $\exp \left(\int_{0}^{x} q(t) d t\right)$, we get

$$
\frac{d}{d x}\left(f(x) \exp \left(\int_{0}^{x} q(t) d t\right)\right)=0 .
$$

The general solution is therefore,

$$
f(x)=C \exp \left(-\int_{0}^{x} q(t) d t\right)
$$

where $C$ is a constant. The hypothesis is that $\lim _{x \rightarrow \infty} \int_{0}^{x} q(t) d t=+\infty$. Even if $|p| \geqslant|q|$, the corresponding property may fail for $p$. For example, for $p \equiv-1$ and $q \equiv 1$, the general solutions are respectively $C \exp (-x)$ and $C \exp (x)$.

Solution to 3.1.19: Consider the equation

$$
0=F(x, y, z)=\left(e^{x} \sin y\right) z^{3}+\left(e^{x} \cos y\right) z+e^{y} \tan x .
$$

$F(0,0,0)=0$, and all the partial derivatives of $F$ are continuous, with

$$
\left.\frac{\partial F}{\partial z}\right|_{(0,0,0)}=\left.\left(3 z^{2}\left(e^{x} \sin y\right)+e^{x} \cos y\right)\right|_{(0,0,0)}=1 .
$$

By the Implicit Function Theorem [Rud87, p. 224], there is a real valued function $f$ with continuous partial derivatives, such that, $F(x, y, f(x, y))=0$ in a neighborhood of $(0,0,0)$. Locally, then, the given differential equation is equivalent to $y^{\prime}=f(x, y)$. Since $f$ satisfies the hypotheses of Picard's Theorem [San79, p. 8], there is a unique solution $y$ in a neighborhood of 0 with $y(0)=0$.

Solution to 3.1.20: Since $f$ and $g$ are positive, the solutions of both problems are monotonically increasing. The first differential equation can be rewritten as $d x / f(x)=d t$, so its solution is given by $x=h^{-1}(t)$, where the function $h$ is defined by

$$
h(x)=\int_{0}^{x} \frac{d \xi}{f(\xi)} .
$$

Because the solution is defined for all $t$, we must have

$$
\int_{0}^{\infty} \frac{d \xi}{f(\xi)}=\infty, \quad \int_{0}^{-\infty} \frac{d \xi}{f(\xi)}=-\infty
$$

Since $g \leqslant f$, it follows that

$$
\int_{0}^{\infty} \frac{d \xi}{g(\xi)}=\infty, \quad \int_{0}^{-\infty} \frac{d \xi}{g(\xi)}=-\infty
$$

Using a similar reasoning we can see that the solution of the second equation is given by $x=H^{-1}(t)$, where

$$
H(x)=\int_{0}^{x} \frac{d \xi}{g(\xi)} .
$$

The conditions $\int_{0}^{\infty} \frac{d \xi}{g(\xi)}=\infty$, and $\int_{0}^{-\infty} \frac{d \xi}{g(\xi)}=-\infty$ guarantee that $H$ maps $\mathbb{R}$ onto $\mathbb{R}$, hence.that $H^{-1}$ is defined on all of $\mathbb{R}$. Thus, the solution of the second equation is defined on $\mathbb{R}$.

Solution to 3.1.21: Let $y$ be a solution of the given differential equation. If $y^{\prime}$ never vanishes, then $y^{\prime}$ has constant sign, so $y$ is monotone.

Suppose that $y^{\prime}\left(x_{1}\right)=0$ for some $x_{1}$. Then the constant function $y_{1}(x)=y\left(x_{1}\right)$ is a solution of $f\left(y_{1}\right)=0$. Consider the function $z, z(x)=y_{1}$ for all $x$. Then the differential equation $y^{\prime}=f(y)$ with initial condition $y\left(x_{1}\right)=y_{1}$ is satisfied by $y$ and by $z . f$ is continuously differentiable and by Picard's Theorem [San79, p. 8], $y=z$, so $y$ is constant. 

\subsection{Second Order Equations}

Solution to 3.2.1: By Picard's Theorem [San79, p. 8] there is, at most, one real valued function $f$ on $[0, \infty)$ such that $f(0)=1, f^{\prime}(0)=0$ and $f^{\prime \prime}(x)=\left(x^{2}-1\right) f(x)$. Since the function $e^{-x^{2} / 2}$ satisfies these conditions, we must have $f(x)=e^{-x^{2} / 2}$. We then have

$$
\lim _{x \rightarrow \infty} f(x)=\lim _{x \rightarrow \infty} e^{-x^{2} / 2}=0 .
$$

Solution to 3.2.2: The characteristic polynomial of the given differential equation is $(r-1)^{2}$ so the general solution is

$$
\alpha e^{t}+\beta t e^{t} \text {. }
$$

The initial conditions give $\alpha=1$, and $\beta=0$, so the solution is $y(t)=e^{t}$.

Solution to 3.2.3: The characteristic polynomial of the associated homogeneous equation is

$$
r^{2}-2 r+1=(r-1)^{2}
$$

so the general solution of the homogeneous equation

$$
\frac{d^{2} x}{d t^{2}}-2 \frac{d x}{d t}+x=0
$$

is

$$
A e^{t}+B t e^{t} \quad(A, B \in \mathbb{R}) .
$$

$(\cos t) / 2$ is easily found to be a particular solution of the original equation, so the general solution is

$$
A e^{t}+B t e^{t}+\frac{\cos t}{2} \text {. }
$$

The initial conditions give $A=-\frac{1}{2}$ and $B=\frac{1}{2}$, so the solution is

$$
\frac{1}{2}\left(e^{t}-t e^{t}+\cos t\right) \text {. }
$$

Solution to 3.2.4: The characteristic polynomial of the given equation is

$$
5 r^{2}+10 r+6
$$

which has roots $-1 \pm i / \sqrt{5}$, so the general solution is given by

$$
x(t)=c_{1} e^{-t} \cos \left(\frac{t}{\sqrt{5}}\right)+c_{2} e^{-t} \sin \left(\frac{t}{\sqrt{5}}\right)
$$

where $c_{1}$ and $c_{2}$ are constants. We can assume $c_{1} \neq 0$ or $c_{2} \neq 0$. Using calculus, we can see that $u^{2}\left(1+u^{4}\right)^{-1} \leqslant 1 / 2$ with equality when $u=\pm 1$. Then $f$ attains a maximum of $1 / 2$ iff $x$ attains one of the values $\pm 1$. We have $\lim _{t \rightarrow \infty} x(t)=0$. Suppose $c_{1} \neq 0$. Then, if $k$ is a large enough integer, we have

$$
|x(-\sqrt{5} k \pi)|=\left|c_{1}\right| e^{\sqrt{5} k \pi}>1
$$

so, by the Intermediate Value Theorem [Rud87, p. 93], $x$ attains one of the values $\pm 1$. If $c_{2} \neq 0$, a similar argument gives the same conclusion.

Solution to 3.2.5: We first solve the homogeneous equation $x^{\prime \prime}+8 x^{\prime}+25=0$. The general solution is $x_{0}(t)=c_{1} e^{i r_{1} t}+c_{2} e^{i r_{2} t}$, where $c_{1}$ and $c_{2}$ are constants and $r_{k}=-4 \pm 3 i, k=1,2$, are the roots of the characteristic equation $r^{2}+8 r+25=0$

All the solutions of the differential equation $x^{\prime \prime}+8 x^{\prime}+25 x=2 \cos t$ are of the form $x(t)=x_{0}(t)+s(t)$, where $s(t)$ is any particular solution. We solve for an $s(t)$ by the Method of Undetermined Coefficients [BD65, p. 115]. Consider $s(t)=A \cos t+B \sin t$. Differentiating this expression twice, we get

$$
2 \cos t=s^{\prime \prime}+8 s^{\prime}+25 s=(24 A+8 B) \cos t+(24 B-8 A) \sin t .
$$

Solving the two linear equations gives $A=3 / 40$ and $B=1 / 40$. Therefore, the desired solution $x(t)$ is given by $x_{0}(t)+s(t)$, where $c_{1}$ and $c_{2}$ are chosen to give the correct initial conditions. $x_{0}(t)$ tends to 0 as $t$ tends to infinity; therefore, to finish the problem, we need to find constants $\alpha$ and $\delta$ with $\alpha \cos (t-\delta)=A \cos t+B \sin t$. We have $\alpha \cos (t-\delta)=\alpha \cos t \cos \delta+\alpha \sin t \sin \delta$, so the problem reduces to solving $\alpha \cos \delta=3 / 40$ and $\alpha \sin \delta=1 / 40$. These equations imply $\tan \delta=1 / 3$ or $\delta=\arctan (1 / 3)$. Hence, by elementary trigonometry, $\cos \delta=3 / \sqrt{10}$, so $\alpha=\sqrt{10} / 40$.

Solution to 3.2.6: 1. The differential equation is equivalent to $y^{\prime}=z$ and $z^{\prime}=-|y|$. We have

$$
\begin{aligned}
\left\|\left(z_{1},\left|y_{1}\right|\right)-\left(z_{2},\left|y_{2}\right|\right)\right\| &=\sqrt{\left(z_{1}-z_{2}\right)^{2}+\left(\left|y_{2}\right|-\left|y_{1}\right|\right)^{2}} \\
& \leqslant \sqrt{\left(z_{1}-z_{2}\right)^{2}+\left(y_{2}-y_{1}\right)^{2}} \\
&=\left\|\left(z_{1}, y_{1}\right)-\left(z_{2}, y_{2}\right)\right\|
\end{aligned}
$$

so the Lipschitz condition is verified and our initial value problem has, by $\mathbf{P i}-$ card's Theorem [San79, p. 8], a unique solution. If $y$ is such a solution, define the function $z$ by $z(x)=y(-x)$. We have $z^{\prime \prime}(x)=y^{\prime \prime}(-x)=-|y(-x)|=-|z(x)|$, $z(0)=y(0)=1$ and $z^{\prime}(0)=-y^{\prime}(0)=0$, so $z=y$ and $y$ is even.

2. We have

$$
y^{\prime}(x)=\int_{0}^{x} y^{\prime \prime}(t) d t=-\int_{0}^{x}|y(t)| d t<0
$$

so $y$ is a decreasing function; therefore, it has, at most, one positive zero. If $y$ is positive on $\mathbb{R}_{+}$, by continuity, $y$ is positive in some interval of the form $(-\varepsilon, \infty)$ for some $\varepsilon>0$. Together with $y(0)=1, y^{\prime}(0)=0$ gives $y(x)=\cos x$, which is absurd. We conclude then that $y$ has exactly one positive zero.

Solution to 3.2.7: Substituting $y(x)=x^{a}$ gives the quadratic equation $a(a-1)+1=0$. The two roots are

$$
\frac{1}{2} \pm \frac{\sqrt{3} i}{2},
$$

so the general solution is

$$
y(x)=A \sqrt{x} \cos \left(\frac{\sqrt{3}}{2} \log x\right)+B \sqrt{x} \cos \left(\frac{\sqrt{3}}{2} \log x\right) .
$$

The boundary condition $y(1)=0$ implies $A=0$ and then the boundary condition $y(L)=0$ can be satisfied for nonzero $B$ only if

$$
\sin \left(\frac{\sqrt{3}}{2} \log L\right)=0 .
$$

\section{Equivalently,}

$$
L=e^{2 n \pi / \sqrt{3}}
$$

where $n$ is any positive integer.

Solution to 3.2.8: The general solution of the corresponding homogeneous equation is $A e^{r x}+B e^{s x}$ where $r, s$ are solutions of the characteristic equation $x^{2}+2 p+x 1=0$, i.e.,

$$
r, s=\frac{-2 p \pm \sqrt{4 p^{2}-4}}{2} .
$$

Since the given equation has a constant solution, $y \equiv 3$, it will admit solutions with infinitely many critical points exactly when $r, s$ have a nonvanishing imaginary part, because in this case the homogeneous equation will admit solutions which are product of exponentials and sines or cosines. This happens when $4 p^{2}-4<0$, i.e., for $|p|<1, p \in \mathbb{R}$.

Solution to 3.2.9: Multiplying the first equation by $a(t) / p(t)$ where $a$ is a differentiable function, we get

$$
a(t) x^{\prime \prime}(t)+\frac{a(t) q(t)}{p(t)} x^{\prime}(t)+\frac{a(t) r(t)}{p(t)} x(t)=0 .
$$

Expanding the second given equation, we get

$$
a(t) x^{\prime \prime}(t)+a^{\prime}(t) x^{\prime}(t)+b(t) x(t)=0 .
$$

For the two equations to be equivalent, we must have $a^{\prime}(t)=a(t) q(t) / p(t)$. Solving this by separation of variables, we get

$$
a(t)=\exp \left(\int_{0}^{t} \frac{q(x)}{p(x)} d x\right) .
$$

Letting $b(t)=a(t) r(t) / p(t)$, we are done.

Solution to 3.2.10: The function $x^{\prime}(1) x(t)-x^{\prime}(0) x(t+1)$ satisfies the differential equation and vanishes along with its first derivative at $t=0$. By Picard's Theorem [San79, p. 8] this function vanishes identically. Assuming $x$ is not the zero function, we have $x^{\prime}(0) \neq 0$ (again, by Picard's Theorem), so $x(t+1)=c x(t)$, where $c=x^{\prime}(1) / x^{\prime}(0)$. It follows that the zero set of $x$ is invariant under translation by one unit, which implies the desired conclusion.

Solution to 3.2.11: The characteristic equation is $l^{2}-2 c l+1=0$, which has the roots $c \pm \sqrt{c^{2}-1}$.

Case 1: $|c|>1$. Let $\omega=\sqrt{c^{2}-1}$. Then the general solution is

$$
x(t)=e^{c t}(A \cosh \omega t+B \sinh \omega t) .
$$

The condition $x(0)=0$ implies $A=0$, and then the condition $x(2 \pi k)=0$ implies $B=0$, that is, $x(t) \equiv 0$. There are no nontrivial solutions in this case.

Case 2: $c=1$. The general solution is

$$
x(t)=A e^{t}+B t e^{t} .
$$

The condition $x(0)=0$ implies $A=0$, and then the condition $x(2 \pi k)=0$ implies $B=0$. There are no nontrivial solutions in this case.

Case 3: $c=-1$. Similar reasoning shows that there are no nontrivial solutions in this case.

Case 4: $-1<c<1$. Let $\omega=\sqrt{1-c^{2}}$. The general solution is then

$$
x(t)=e^{c t}(A \cos \omega t+B \sin \omega t) .
$$

The condition $x(0)=0$ implies $A=0$. If $B \neq 0$, the condition $x(2 \pi k)=0$ then implies $2 \pi k \omega=\pi n(n \in \mathbb{Z})$, that is, $\omega=n / 2 k$, and

$$
c^{2}=1-\omega^{2}=1-\frac{n^{2}}{4 k^{2}} .
$$

The right side is nonnegative and less than 1 only for $0<|n| \leqslant 2 k$. The required values of $c$ are thus

$$
c=\pm \sqrt{1-\frac{n^{2}}{4 k^{2}}}, \quad n=1,2, \ldots, 2 k .
$$

Solution to 3.2.12: By the basic Existence-Uniqueness Theorem, the solution set is a two-dimensional vector space, $V$. Fix a real number $s$, and define the functions $p_{s}$ and $q_{s}$ by $p_{s}(t)=p(t+s), q_{s}(t)=q(t+s)$. Let $f$ be in $V$. By the translation invariance of $V, f$ is a solution of the equation

$$
\frac{d^{2} x}{d t^{2}}+p_{s} \frac{d x}{d t}+q_{s} x=0 .
$$

Hence it is also a solution of the equation

$$
\left(p-p_{s}\right) \frac{d x}{d t}+\left(q-q_{s}\right) x=0 .
$$

If $p-p_{s}$ does not vanish identically, there is an interval on which it is nowhere zero. But on such an interval the solutions of $(*)$ form a one-dimensional vector space (by the Basic Uniqueness Theorem). Hence $p-p_{s} \equiv 0$, from which one sees that also $q-q_{s} \equiv 0$. Therefore $p$ and $q$ are constant.

\subsection{Higher Order Equations}

Solution to 3.3.1: 1 . Since the differential equation is linear homogeneous with constant coefficients we can build its characteristic equation, which is,

$$
\lambda^{3}+\lambda^{2}-2=0 .
$$

By inspection we can easily see that 1 is one of the roots, factoring as

$$
(\lambda-1)\left(\lambda^{2}+2 \lambda+2\right)=0
$$

so the roots are 1 and $-1 \pm i$, and the solution to the differential equation has the form

$$
f(x)=c_{1} e^{x}+c_{2} e^{-x} \cos x+c_{3} e^{-x} \sin x
$$

so the space of solutions has dimension 3.

2. $E_{0}$ is the two-dimensional subspace defined by $c_{1}=0$, that is, the function $g(t)=c_{2} e^{-t} \cos t+c_{3} e^{-t} \sin t$, and after the substitution for the initial condition we see that $c_{2}=0$ and $c_{3}=2$, so

$$
g(t)=2 e^{-t} \sin t .
$$

Solution to 3.3.2: The four complex fourth roots of $-1$ are $\frac{\pm 1 \pm i}{\sqrt{2}}$. Therefore the functions $e^{\pm x / \sqrt{2}} \cos (x / \sqrt{2})$ and $e^{\pm x / \sqrt{2}} \sin (x / \sqrt{2})$ satisfy the differential equation $y^{(4)}=-y$.

For $u(t)=e^{-x / \sqrt{2}} \sin (x / \sqrt{2})$ we have $u(0)=\lim _{x \rightarrow \infty} u(x)=\lim _{x \rightarrow \infty} u^{\prime}(x)=0$.

As $u^{\prime}(0)=1 / \sqrt{2}, y(t)=\sqrt{2} e^{-x / \sqrt{2}} \sin (x / \sqrt{2})$ also satisfies $y^{\prime}(0)=1$. Solution to 3.3.4: 1 . The characteristic polynomial of the equation is

$$
x^{7}+\cdots+x+1=\frac{x^{8}-1}{x-1}
$$

which has roots $-1, \pm i$, and $(\pm 1 \pm i) / \sqrt{2}$. For each such root $z_{k}=u_{k}+i v_{k}$, $k=1, \ldots, 7$, we have the corresponding solution

$$
e^{z_{k} t}=e^{u_{k} t}\left(\cos v_{k} t+i \sin v_{k} t\right)
$$

and these form a basis for the space of complex solutions. To get a basis for the real solutions, we take the real and imaginary parts, getting the basis

$$
\begin{gathered}
x_{1}(t)=e^{-t}, \quad x_{2}(t)=\cos t, \quad x_{3}(t)=\sin t, \quad x_{4}(t)=e^{\frac{1}{\sqrt{2}} t} \cos \frac{1}{\sqrt{2}} t, \\
x_{5}(t)=e^{\frac{1}{\sqrt{2}} t} \sin \frac{1}{\sqrt{2}} t, \quad x_{6}(t)=e^{\frac{-1}{\sqrt{2}} t} \cos \frac{1}{\sqrt{2}} t, \quad x_{7}(t)=e^{\frac{-1}{\sqrt{2}} t} \sin \frac{1}{\sqrt{2}} t .
\end{gathered}
$$

2. A solution tends to 0 at $\infty$ iff it is a linear combination of solutions in the basis with the same property. Hence, the functions $x_{1}, x_{6}$, and $x_{7}$ form a basis for the space of solutions tending to 0 at $\infty$.

Solution to 3.3.5: The set of complex solutions of the equation forms a complex vector space which is invariant under differentiation. Hence, the functions $\cos t$ and $\cos 2 t$ are also solutions, and, therefore, so are $e^{\pm i t}=\cos t \pm i \sin t$ and $e^{\pm 2 i t}=\cos 2 t \pm i \sin 2 t$. It follows that the characteristic polynomial of the equation has at least the four roots $\pm i, \pm 2 i$, so it is divisible by the polynomial $\left(\lambda^{2}+1\right)\left(\lambda^{2}+4\right)$. The differential equation is therefore, at least of order 4 . The smallest possible order is, in fact, 4 , because the given functions are both solutions of the equation

that is,

$$
\left(\frac{d^{2}}{d t^{2}}+1\right)\left(\frac{d^{2}}{d t^{2}}+4\right)=0
$$

$$
\frac{d^{4} x}{d t^{4}}+5 \frac{d^{2} x}{d t^{2}}+4 x=0
$$

The preceding reasoning applies for both real and complex coefficients.

Solution to 3.3.6: Solving the characteristic equation $r^{3}-1=0$, we find that the general solution to $y^{\prime \prime \prime}-y=0$ is given by

$$
y(x)=c_{1} e^{x}+c_{2} e^{-x / 2} \cos \frac{\sqrt{3}}{2} x+c_{3} e^{-x / 2} \sin \frac{\sqrt{3}}{2} x,
$$

with $c_{1}, c_{2}$, and $c_{3} \in \mathbb{R}$. $\lim _{x \rightarrow \infty} y(x)=0$ when $c_{1}=0$. But the solution above with $c_{1}=0$, is the general solution of the differential equation with characteristic polynomial $\left(r^{3}-1\right) /(r-1)=r^{2}+r+1$, that is,

$$
y^{\prime \prime}+y^{\prime}+y=0 \text {. }
$$

So $y^{\prime \prime}(0)+y^{\prime}(0)+y(0)=0$, and we can take $a=b=c=1$ and $d=0$. 

\subsection{Systems of Differential Equations}

Solution to 3.4.2: Rewritting the system as a liner homogeneous system of equations we have

$$
\left(\begin{array}{c}
x^{\prime}(t) \\
y^{\prime}(t)
\end{array}\right)=\left(\begin{array}{cc}
2 & -1 \\
1 & 0
\end{array}\right)\left(\begin{array}{l}
x(t) \\
y(t)
\end{array}\right)
$$

so the solutions are of the form

$$
\left(\begin{array}{l}
x(t) \\
y(t)
\end{array}\right)=e^{A t}\left(\begin{array}{l}
c_{1} \\
c_{2}
\end{array}\right)
$$

By the Cayley-Hamilton Theorem [HK61, p. 194], the computation of the exponential of a matrix reduces to a finite sum of terms, in this case two,

$$
e^{A t}=\alpha_{1} A t+\alpha_{0} I
$$

where the $\alpha_{i}(t)$ are functions of $t$ depending on the matrix $A$. Furthermore, if $r(\lambda)=\alpha_{1} \lambda+\alpha_{0}$, then for each eigenvalue $\lambda_{i}$ of $A, r\left(\lambda_{i}\right)=e^{\lambda_{i}}$ and for each one with multiplicity two

$$
e^{\lambda_{i}}=\left.\frac{d}{d \lambda} r(\lambda)\right|_{\lambda=\lambda_{i}}
$$

which is a way to compute the values of $\alpha_{0}$ and $\alpha_{1}$, and then the exponential $e^{A t}$. More generally, if $e^{A t}$ can be computed with the polynomial

$$
e^{A t}=\alpha_{n-1} A^{n-1} t^{n-1}+\alpha_{n-2} A^{n-2} t^{n-2}+\cdots+\alpha_{1} A t+\alpha_{0} I
$$

then if we consider the polynomial

$$
r(\lambda)=\alpha_{n-1} \lambda^{n-1}+\alpha_{n-2} \lambda^{n-2}+\cdots+\alpha_{1} \lambda+\alpha_{0}
$$

for each eigenvalue $\lambda_{i}$ of $A t$

$$
e^{\lambda_{i}}=r\left(\lambda_{i}\right)
$$

and furthermore, if the multiplicity of $\lambda_{i}$ is $k>1$, the each of the equations is also valid:

$$
e^{\lambda_{i}}=\left.\frac{d^{j}}{d \lambda^{j}} r(\lambda)\right|_{\lambda=\lambda_{i}} \quad \text { for } \quad 1 \leqslant j \leqslant k-1 .
$$

For more details on this and other interesting ways to compute the exponential of a matrix see [MVL78], specially Section 5, and the references cited there.

The characteristic polynomial of $A t$ is

$$
\chi_{A t}(\lambda)=\lambda^{2}-(2 t) \lambda+t^{2}
$$

so the eigenvalues are $\lambda_{1}=\lambda_{2}=t$, and using the above formulas for the computation of $\alpha_{i}$, we get

$$
\begin{aligned}
r(\lambda) &=\alpha_{1} \lambda+\alpha_{0} \\
\frac{d r(\lambda)}{d \lambda} &=\alpha_{1}
\end{aligned}
$$

and we end up with the system

$$
\begin{aligned}
e^{t} &=t \alpha_{1}+\alpha_{0} \\
e^{t} &=\alpha_{1}
\end{aligned}
$$

Therefore the solutions are $\alpha_{1}=e^{t}$ and $\alpha_{0}=e^{t}(1-t)$ so the exponential is

$$
e^{A t}=e^{t}\left(\begin{array}{cc}
1+t & -t \\
t & 1-t
\end{array}\right)
$$

The solutions are then

$$
\left(\begin{array}{l}
x(t) \\
y(t)
\end{array}\right)=e^{t}\left(\begin{array}{cc}
1+t & -t \\
t & 1-t
\end{array}\right)\left(\begin{array}{l}
c_{1} \\
c_{2}
\end{array}\right)
$$

that is,

$$
\begin{aligned}
&x(t)=e^{t}\left(k_{1}+k_{2} t\right) \\
&y(t)=e^{t}\left(k_{1}-k_{2}+k_{2} t\right)
\end{aligned}
$$

Solution 2. Alternatively, we can follow the previous solution closely, but compute the matrix $e^{t A}$ by decomposing $A=S+N$, where $S$ is complex diagonalizable and $N$ nilpotent, for details we refer the reader to [HS74, Chap. 6]. This decomposition changes the computation of the exponential into a finite sum, in our case, since $A$ has only one eigenvalue with multiplicity two

$$
S=\left(\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right) \quad N=A-S=\left(\begin{array}{cc}
1 & -1 \\
1 & -1
\end{array}\right)
$$

obviously $N^{2}=0$, and the exponential can now be computed as

$$
e^{t A}=e^{t S} e^{t N}=e^{t}(I+t N)=e^{t}\left(\begin{array}{cc}
1+t & -t \\
t & 1-t
\end{array}\right)
$$

and the solutions follow as before.

Solution 3. Computing the Jordan Canonical Form [HK61, p. 247] of the system we have

$$
\left(\begin{array}{cc}
0 & 1 \\
1 & -1
\end{array}\right)\left(\begin{array}{cc}
2 & -1 \\
1 & 0
\end{array}\right)\left(\begin{array}{ll}
1 & 1 \\
1 & 0
\end{array}\right)=\left(\begin{array}{ll}
1 & 1 \\
0 & 1
\end{array}\right)
$$

which we will write $T^{-1} A T=J_{A}$, so with the change of coordinates

$$
\left(\begin{array}{l}
x \\
y
\end{array}\right)=T\left(\begin{array}{c}
z \\
w
\end{array}\right)
$$

that is,

$$
\left(\begin{array}{l}
x \\
y
\end{array}\right)=\left(\begin{array}{ll}
1 & 1 \\
1 & 0
\end{array}\right)\left(\begin{array}{c}
z \\
w
\end{array}\right)
$$

the system becomes, according to the form $J_{A}$,

$$
\begin{aligned}
z^{\prime}(t) &=z(t)+w(t) \\
w^{\prime}(t) &=w(t)
\end{aligned}
$$

These equations can be readily solved in the reverse order, and we obtain,

$$
w(t)=c_{1} e^{t} \quad z(t)=c_{1} t e^{t}+c_{2} e^{t}
$$

and the solutions $x(t)$ and $y(t)$ can now be written as:

$$
\begin{aligned}
&x(t)=e^{t}\left(c_{1} t+c_{1}+c_{2}\right) \\
&y(t)=e^{t}\left(c_{1} t+c_{2}\right) .
\end{aligned}
$$

Solution 4. Derivating the first equation and substituting the second we end up with a second order homogeneous differential equation

$$
x^{\prime \prime}(t)-2 x^{\prime}(t)+x(t)=0
$$

whose characteristic equation $\lambda^{2}-2 \lambda+1=0$ has one double root $\lambda=1$, so the general solution will be

$$
x(t)=c_{1} e^{t}+c_{2} t e^{t}
$$

substituting back in the first equation we get

$$
y(t)=\left(c_{1}-c_{2}\right) e^{t}+c_{2} t e^{t} .
$$

Solution to 3.4.3: We have

$$
\begin{aligned}
\frac{d}{d t}\left(x^{2}+y^{2}\right) &=2 x \frac{d x}{d t}+2 y \frac{d y}{d t} \\
&=2 x(-x+y)+2 y(\log (20+x)) \\
&=-2 x^{2}+2 x y-2 y^{2}+2 y \log (20+x)
\end{aligned}
$$

As, for any positive $\varepsilon$

$$
\log (20+x)=o\left(x^{\varepsilon}\right) \quad(x \rightarrow+\infty)
$$

and

$$
\begin{aligned}
-2 x^{2}+2 x y-2 y^{2} & \leqslant-2\left(x^{2}+y^{2}-|x y|\right) \\
& \leqslant-2(x-y)^{2} \\
& \leqslant 0
\end{aligned}
$$

we conclude that

$$
\frac{d}{d t}\left(x^{2}+y^{2}\right) \leqslant 0
$$

for $\|(x, y)\|$ large enough so the distance of $(x, y)$ to the origin is bounded.

Solution to 3.4.4: 1. Using polar coordinates, $x=r \cos \theta$ and $y=r \sin \theta$, we get

solving these, we get

$$
\begin{gathered}
\frac{d r}{d t}=\frac{x}{r} \frac{d x}{d t}+\frac{y}{r} \frac{d y}{d t}=r\left(1-r^{2}\right) \\
\frac{d \theta}{d t}=\frac{x}{r^{2}} \frac{d y}{d t}-\frac{y}{r^{2}} \frac{d x}{d t}=-1
\end{gathered}
$$

$$
r=\frac{c_{1} e^{t}}{\sqrt{1+c_{1}^{2} e^{2 t}}} \text { and } \theta=-t+c_{2}
$$

where $c_{1}, c_{2}$ are constants.

For $\left(x_{0}, y_{0}\right)=(0,0)$, we have $\frac{d x}{d t}=\frac{d y}{d t}=0$; therefore, $x=y \equiv 0$. For $\left(x_{0}, y_{0}\right) \neq(0,0)$ let $x_{0}=r_{0} \cos \theta_{0}$ and $y_{0} \stackrel{d t}{=} r_{0} \sin \theta_{0}$. We have

$$
c_{1}=\frac{r_{0}}{\sqrt{1-r_{0}^{2}}} \text { and } c_{2}=\theta_{0}
$$

so

$$
x(t)=\frac{c_{1} e^{t}}{\sqrt{1+c_{1} e^{2 t}}} \cos \left(\theta_{0}-t\right), \quad y(t)=\frac{c_{1} e^{t}}{\sqrt{1+c_{1} e^{2 t}}} \sin \left(\theta_{0}-t\right) .
$$

2. We have

$$
\lim _{t \rightarrow \infty} r=\lim _{t \rightarrow \infty} \frac{c_{1} e^{t}}{\sqrt{1+c_{1} e^{2 t}}}=1 .
$$

Solution to 3.4.5: We have

$$
\frac{d}{d t}\left(\begin{array}{l}
x \\
y \\
z
\end{array}\right)=\left(\begin{array}{lll}
0 & 1 & 0 \\
2 & 0 & 0 \\
0 & 0 & 3
\end{array}\right)\left(\begin{array}{l}
x \\
y \\
z
\end{array}\right)
$$

Let $A$ denote the $3 \times 3$ matrix above. Its characteristic polynomial is

$$
\chi(x)=-x^{3}+3 x^{2}+2 x-6=(x-3)(-x+\sqrt{2})(x+\sqrt{2})
$$

therefore $-\sqrt{2}$ is a simple eigenvalue of $A$. Using an eigenvector $v \in \mathbb{R}^{3}$ associated with this eigenvalue, we get a solution $e^{-\sqrt{2 t}} v$ which tends to $\infty$ as $t \rightarrow-\infty$ and to the origin as $t \rightarrow+\infty$.

Solution to 3.4.6: We have

$$
\begin{aligned}
\frac{d}{d t}\|x(t)\|^{2} &=\frac{d}{d t}\langle x(t), x(t)\rangle \\
&=\left\langle x^{\prime}(t), x(t)\right\rangle+\left\langle x(t), x(t)^{\prime}\right\rangle \\
&=\langle A x(t), x(t)\rangle+\langle x(t), A x(t)\rangle \\
&=\left\langle\left(A+A^{*}\right) x(t), x(t)\right\rangle
\end{aligned}
$$

so it suffices to prove that $A+A^{*}$ is positive definite. We have

$$
A+A^{*}=\left(\begin{array}{ccc}
2 & 2 & -2 \\
2 & 8 & 2 \\
-2 & 2 & 16
\end{array}\right)
$$

and by the Solution to Problem 7.8.2 it is enough to check that the determinant of the principal minors are positive, which is a simple calculation.

Solution to 3.4.7: We have

$$
\frac{d x}{d t}(0)=1,
$$

so, for small positive $t,(x(t), y(t))$ lies in the right half-plane. For $0<x<1$, we have

$$
\frac{d x}{d t}=1+\frac{1}{2} x^{2} \sin y \geqslant 1-\frac{1}{2} x^{2} \geqslant \frac{1}{2} .
$$

Thus, the function $x(t)$ is increasing with slope at least $1 / 2$. Therefore, by the time $t=2$, the curve $(x(t), y(t))$ will cross the line $x=1$.

Solution to 3.4.8: 1 . Let the function $g$ be defined by $g(t)=f\left(x(t), x^{\prime}(t)\right)$. We have

$$
\text { (*) } g^{\prime}(t)=-2\left(x^{\prime}(t)^{2}+x(t)^{4}\right)
$$

so $g$ is a decreasing function.

2. It is enough to show that $\lim _{t \rightarrow \infty} g(t)=0$.

Since $g$ is a positive decreasing function, the limit exists and satisfies $\lim _{t \rightarrow \infty} g(t)=c \geqslant 0$. If $c>0$, then, for some $\varepsilon>0, T \in \mathbb{R}$ we have $x^{\prime}(t)^{2}+x(t)^{4}>\varepsilon$ for $t \geqslant T$. Then, by $(*)$, we have

$$
\frac{g(T)-c}{2}=\int_{T}^{\infty}\left(x^{\prime}(t)^{2}+x(t)^{4}\right) d t>\int_{T}^{\infty} \varepsilon d t=\infty
$$

which is absurd. We must then have $c=0$, as desired.

Solution to 3.4.9: 1 . We have

$$
\frac{\partial F}{\partial t}=\frac{\partial F}{\partial x} \frac{\partial x}{\partial t}+\frac{\partial F}{\partial y} \frac{\partial y}{\partial t}=y\left(x^{3}+x^{5}\right)-y\left(a y+x^{3}+x^{5}\right)=-a y^{2} .
$$

Thus, $\partial F / \partial t \leqslant 0$, which implies that $F$ decreases along any solution $(x(t), y(t))$ of the differential equation.

2. Let $\varepsilon>0$. Since $F$ is continuous, there exists a $\delta>0$ such that $F(x, y)<\varepsilon^{2} / 2$ if $\|(x, y)\|<\delta$. Further, by letting $(x, y)$ vary over all points such that $\|(x, y)\|=\varepsilon$, elementary calculus shows that $F(x, y) \geqslant \varepsilon^{2} / 2$.

Let the initial conditions $x(0)$ and $y(0)$ be such that $\|(x(0), y(0))\|<\delta$. By Picard's Theorem [San79, p. 8], there exist unique solutions $x(t)$ and $y(t)$ to the differential equation satisfying these initial conditions. Since $F(x, y)$ decreases along solutions, we must have that $F(x(t), y(t))<\varepsilon^{2} / 2$ for all $t>0$ in the domain of the solution. Now suppose that for some $t>0$ in this domain, $\|(x(t), y(t))\|=\varepsilon$. We would have $F(x(t), y(t)) \geqslant \varepsilon^{2} / 2$, a contradiction. Therefore, $\|(x(t), y(t))\|<\varepsilon$ for all $t>0$ in the domain of the solution. But this bound is independent of $t$, so the Extension Theorem [San79, p. 131] shows that this solution exists on all positive $t$.

Solution to 3.4.12: 1 . We have

$$
\begin{aligned}
\frac{d H}{d t}(t) &=\frac{1}{2} \frac{d}{d t}\left\langle x^{\prime}(t), x^{\prime}(t)\right\rangle+\left\langle\operatorname{grad} V(x(t)), x^{\prime}(t)\right\rangle \\
&=\left\langle x^{\prime \prime}(t), x^{\prime}(t)\right\rangle+\left\langle\operatorname{grad} V(x(t)), x^{\prime}(t)\right\rangle \\
&\left.=\sum_{i=1}^{n} f_{i}(x(t)) \cdot x_{i}^{\prime}(t)+\sum_{i=1}^{n} \frac{\partial V}{\partial x_{i}}(x(t)) \cdot x_{i}^{\prime}(t)\right) \\
&=0
\end{aligned}
$$

since $f_{i}=-\partial V / \partial x_{i}$. Therefore $H(t)$ is constant on $(a, b)$.

2. Fix $t_{0} \in(a, b)$. Since $H$ is constant, $H(t)=H\left(t_{0}\right)$, for all $t \in(a, b)$. We have

$$
H\left(t_{0}\right)=\frac{1}{2}\left\|x^{\prime}(t)\right\|^{2}+V(x(t)) \geqslant \frac{1}{2}\left\|x^{\prime}(t)\right\|^{2}+M
$$

and then

$$
0 \leqslant\left\|x^{\prime}(t)\right\|^{2} \leqslant 2\left(H\left(t_{0}\right)-M\right)
$$

showing that $x^{\prime}(t)$ is bounded. Noting that

$$
\left|x\left(t_{1}\right)-x\left(t_{2}\right)\right|=\int_{t_{1}}^{t_{2}} x^{\prime}(t) d t \leqslant \sqrt{2\left(H\left(t_{0}\right)-M\right)}\left|t_{1}-t_{2}\right|
$$

we conclude that $x$ is bounded and has limits at the endpoints of the interval because it is Lipschitz. Using the continuity of $f$ we get

$$
\lim _{t \rightarrow b} x_{i}^{\prime \prime}(t)=\lim _{t \rightarrow b} f_{i}(x(t))=f_{i}\left(\lim _{t \rightarrow b} x(t)\right)
$$

so $\lim _{t \rightarrow b} x^{\prime \prime}(t)$ exists, for $i=1, \ldots, n$. The corresponding facts for $t \rightarrow a$ can be treated similarly. Therefore the closure of $x((a, b))$ is compact and $x_{i}^{\prime \prime}$ is bounded because its image is contained in the image of a compact under the continuous extension of $f_{i}$.

Using the maximum of $\left\|x^{\prime \prime}(t)\right\|$ on $(a, b)$ we see that $x^{\prime}$ is a Lipschitz function, and that the limit $\lim _{t \rightarrow b} x^{\prime}(t)$ exists, completing the proof.

Solution to 3.4.14: Let $f_{1}, \ldots, f_{n}$ be a basis for $V$. Since $V$ is closed under differentiation, we have the system of equations

$$
f_{i}^{\prime}=\sum_{j=1}^{n} a_{i j} f_{j} \quad 1 \leqslant i \leqslant n .
$$

Let $\mathrm{A}=\left(a_{i j}\right)$. This system has solutions of the form $f_{i}(x)=C_{i} e^{\lambda_{i} x}$, where the $C_{i}$ 's are constants and the $\lambda_{i}$ 's are the (complex) eigenvalues of the matrix $A$. By the properties of the exponential function, we immediately have that $f_{i}(x+a)=$ $C f_{i}(x)$ for some constant $C$ depending on $a$, so $f_{i}(x+a) \in V$. Since the $f_{i}$ 's form a basis of $V, V$ is closed under translation.

Solution to 3.4.15: Let $V$ be such a space, and let $D$ denote the operator of differentiation acting on $V$. If $\chi$ is the characteristic polynomial of $D$, the functions in $V$ are solutions of the linear differential equation $\chi(D) f=0$. The set of solutions of that equation is a three dimensional vector space, since $\chi$ has degree 3 , so it equals $V$.

Case 1. $\chi$ has three distinct roots $\lambda_{1}, \lambda_{2}, \lambda_{3}$. The functions $e^{\lambda_{1} t}, e^{\lambda_{2} t}, e^{\lambda_{3} t}$ form a basis for the set of solutions of $\chi(D) f=0$ and so for $V$.

Case 2 . $\chi$ has a root $\lambda_{1}$ of multiplicity 2 and a root $\lambda_{2}$ of multiplicity 1 . The functions $e^{\lambda_{1} t}, t e^{\lambda_{1} t}, e^{\lambda_{2} t}$ form a basis for $V$.

Case 3. $\chi$ has a single root of multiplicity 3. In this case the functions $e^{\lambda t}, t e^{\lambda t}$, $t^{2} e^{\lambda t}$ form a basis for $V$.

Solution to 3.4.16: We will show that for $1 \leqslant k \leqslant n+1, f_{k}(t)=\sum_{j=k}^{n+1} \xi_{j} e^{-j t}$ where the $\xi_{j}$ 's are constants depending on $k$. From this, it follows that each $f_{k}(t)$ approaches 0 as $t$ tends to infinity.

Solving the second equation, we get

$$
f_{n+1}(t)=\xi_{n+1} e^{-(n+1) t}
$$

for some $\xi_{n+1} \in \mathbb{R}$, which has the desired form. Assume that, for some $k$, the formula holds for $f_{k+1}$. Differentiating it and substituting it into the first equation gives

$$
f_{k}^{\prime}=\sum_{j=k+1}^{n+1}(k+1+j) \xi_{j} e^{-j t}-k f_{k} .
$$

This is a first order linear differential equation which we can solve. Letting $\mu_{j}=(k+1+j) \xi_{j}$, we get

$$
f_{k}=\left(\int_{0}^{t} e^{k s}\left(\sum_{j=k+1}^{n+1} \mu_{j} e^{-j s}\right) d s+C\right) e^{-k t}
$$

where $C$ is a constant. Changing the order of summation and evaluating, we get

$$
f_{k}=\left(\left.\sum_{j=k+1}^{n+1} \frac{\mu_{j}}{k-j} e^{(k-j) x}\right|_{x=0} ^{t}+C\right) e^{-k t}=\sum_{j=k}^{n+1} \xi_{j} e^{-j t}
$$

where the $\xi_{j}$ 's are some real constants, and we are done. Solution to 3.4.17: We solve the case $n=1$ in two different ways. First method: Let $B$ be the indefinite integral of $A$ vanishing at 0 . One can then integrate the equation $\frac{d x}{d t}=A x$ with the help of the integrating factor $e^{-B}$, namely

$$
0=e^{-B} \frac{d x}{d t}-e^{-B} A \frac{d x}{d t}=\frac{d}{d t}\left(e^{-B} x\right),
$$

giving $x(t)=e^{B(t)} x(0)$. Since $A(t) \leqslant \beta$, we have $B(t) \leqslant \beta t$ for $t>0$, so

$$
|x(t)|=|x(0)| e^{B(t)} \leqslant|x(0)| e^{-\beta t},
$$

as desired.

Second method ( $n=1)$ : Consider the derivative of $e^{-\beta t} x$ :

$$
\frac{d}{d t}\left(e^{-\beta t} x\right)=e^{-\beta t}\left(\frac{d x}{d t}-\beta x\right)=e^{-\beta t}(A-\beta) x .
$$

By Picard's Theorem [San79, p. 8], $x$ either has a constant sign or is identically 0 . Hence, $e^{-\beta t} x$ is nonincreasing when $x$ is positive and nondecreasing when $x$ is negative, which gives the desired conclusion.

$n>1$. We have for a solution $x(t)$,

$$
\frac{d}{d t}\|x\|^{2}=2\left\langle\frac{d x}{d t}, x\right\rangle=2\langle A x, x\rangle \leqslant 2 \beta\|x\|^{2}
$$

which reduces the case $n>1$ to the case $n=1$.

Solution to 3.4.18: 1 . We have

$$
\begin{aligned}
\frac{d}{d t}\|X(t)\|^{2} &=\frac{d}{d t}(X(t) \cdot X(t))=2 X(t) \cdot \frac{d X(t)}{d t} \\
&=2 X(t) \cdot W X(t)=2 W^{t} X(t) \cdot X(t) \\
&=-2 W X(t) \cdot X(t)=-2 X(t) \cdot W X(t) \\
&=-\frac{d}{d t}\|X(t)\|^{2},
\end{aligned}
$$

from which it follows that $\frac{d}{d t}\|X(t)\|^{2}=0$, hence that $\|X(t)\|$ is constant.

2. We have

$$
\frac{d}{d t}(X(t) \cdot v)=\frac{d X(t)}{d t} \cdot v=W X(t) \cdot v=X(t) \cdot W^{t} v=-X(t) \cdot W v=0 .
$$

3. It will suffice to show that the null space of $W$ is nontrivial. For if $v$ is a nonzero vector in that null space, then

$$
\|X(t)-v\|^{2}=\|X(t)\|^{2}+\|v\|^{2}-2 X(t) \cdot v,
$$

which is constant by Part 1 and Part 2 , implying that $X(t)$ lies on the intersection of two spheres, one with center 0 and one with center $v$. The nontriviality of the null space of $W$ follows from the antisymmetry of $W$ :

$$
\operatorname{det} W=\operatorname{det} W^{t}=\operatorname{det}(-W)=(-1)^{3} \operatorname{det} W=-\operatorname{det} W .
$$

Hence, $\operatorname{det} W=0$, so $W$ is singular.

Solution to 3.4.19: Consider the function $u$ defined by $u(t)=\|x(t)\|^{2}$. We have, using Rayleigh's Theorem [ND88, p. 418],

$$
u^{\prime}(t)=2\left\langle x(t), x^{\prime}(t)\right\rangle=2\langle x(t), P(t) x(t)\rangle \leqslant-2\langle x(t), x(t)\rangle=-2 u(t)
$$

which implies that $u(t) \leqslant u(0) \exp (-2 t)$ for $t>0$, so $\lim _{t \rightarrow \infty} u(t)=0$, and the result follows.

Solution to 3.4.20: Expanding the matrix differential equation, we get the family of differential equations

$$
\frac{d f_{i j}}{d t}=f_{i-1 j-1}, \quad 1 \leqslant i, j \leqslant n,
$$

where $f_{i j} \equiv 0$ if $i$ or $j$ equals 0 . Solving these, we get

$$
X(t)=\left(\begin{array}{cccc}
\xi_{11} & \xi_{12} & \xi_{13} & \xi_{14} \\
\xi_{21} & \xi_{11} t+\xi_{22} & \xi_{12} t+\xi_{23} & \xi_{13} t+\xi_{24} \\
\xi_{31} & \xi_{21} t+\xi_{32} & \frac{1}{2} \xi_{11} t^{2}+\xi_{22} t+\xi_{33} & \frac{1}{2} \xi_{12} t^{2}+\xi_{23} t+\xi_{34} \\
\xi_{41} & \xi_{31} t+\xi_{42} & \frac{1}{2} \xi_{21} t^{2}+\xi_{32} t+\xi_{43} & \frac{1}{6} \xi_{11} t^{3}+\frac{1}{2} \xi_{22} t^{2}+\xi_{33} t+\xi_{44}
\end{array}\right)
$$

where the $\xi_{i j}$ 's are constants.

Solution 2. We will use a power series. Assume $X(t)=\sum_{n=0}^{\infty} t^{n} C_{n}$. The given equation gives

which can be written as

$$
\sum_{n=1}^{\infty} n t^{n-1} C_{n}=\sum_{n=0}^{\infty} A t^{n} C_{n} B
$$

$$
\sum_{n=0}^{\infty} t^{n}\left((n+1) C_{n+1}-A C_{n} B\right)=0
$$

giving the recurrence relation

$$
C_{n+1}=\frac{1}{n+1} A C_{n} B,
$$

so we have

$$
C_{n}=\frac{1}{n !} A^{n} C_{0} B^{n} .
$$

Since $A^{4}=B^{4}=0$, the solution reduces to a polynomial of degree at most 3:

$$
X(t)=C_{0}+t A C_{0} B+\frac{t^{2}}{2} A^{2} C_{0} B^{2}+\frac{t^{3}}{6} A^{3} C_{0} B^{3}
$$

where $C_{0}=X(0)$ is the initial value of $X$. 

\section{4}

\section{Metric Spaces}

\subsection{Topology of $\mathbb{R}^{n}$}

Solution to 4.1.1: Suppose $x$ is a point of $S \backslash C$. Then there is an open interval containing $x$ whose intersection with $S$ is finite or countable and which thus is disjoint from $C$. By the density of $\mathbb{Q}$ in $\mathbb{R}$, there is such an interval with rational endpoints. There are countably many open intervals in $\mathbb{R}$ with rational endpoints. Hence $S \backslash C$ is a finite or countable union of finite or countable sets. Therefore, $S \backslash C$ is finite or countable.

Solution to 4.1.2: 1. For $n \in \mathbb{Z}$ let $A_{n}=A \cap[n, n+1)$. Then $A=\cup_{n \in \mathbb{Z}} A_{n}$. Since $A$ is uncountable, at least one of these sets, $A_{k}$ say, is uncountable. Then $A_{k}$ contains a bounded sequence with distinct terms, which has a convergent subsequence. The limit of this subsequence is an accumulation point of $A$.

2. Let $M$ be the set of accumulation points of $A$. Suppose $M$ is at most countable. As $M$ is closed, its complement $\mathbb{R} \backslash M$ is open. Then we can represent it as a countable union of closed sets, $\mathbb{R} \backslash M=\cup C_{n}$. If $M$ is at most countable, then $A$ must have uncountable many elements in $\mathbb{R} \backslash M$, therefore in some $C_{m}$. By part $1 ., A \cap C_{m}$ has an accumulation point. $C_{m}$ is closed, so this accumulation point is in $C_{m}$. This contradicts the fact that all accumulation points of $A$ are in $M$.

Solution 2. Suppose there are no accumulation points, then for each point $r \in \mathbb{R}$ there is a open interval $I_{r}$ around $r$ with only finitely many points of $A$. Reducing the interval a bit further we may assume it contains at most one point of the set $A$ (the possibility being $r$ itself). This open set form an open covering of the compact set $[n, n+1]$, so it has a finite subcover and as such $A$ only has finite number of elements in the interval $[n, n+1]$, hence at most a countable number of elements, a contradiction, so $A$ must be uncountable.

Solution to 4.1.3: For each $i$, the given condition guarantees that the sequence $(x(i, j))_{j=1}^{\infty}$ is Cauchy, hence convergent. Let $a_{i}$ denote its limit. We have, by the same condition,

$$
\rho(x(i, j), x(k, j)) \leqslant \max \left\{\frac{1}{i}, \frac{1}{k}\right\},
$$

giving $\rho\left(a_{i}, a_{k}\right) \leqslant \max \left\{\frac{1}{i}, \frac{1}{k}\right\}$. Hence the sequence $\left(a_{i}\right)_{i=1}^{\infty}$ is Cauchy, therefore convergent. Let $a$ denote its limit.

Each sequence $(x(i, j))_{i=1}^{\infty}$ is Cauchy, hence convergent, say to $b_{j}$, and the sequence $\left(b_{j}\right)_{j=1}^{\infty}$ is Cauchy, hence convergent to $b$.

Again, by the same condition as before,

$$
\rho(x(i, i), x(i, j)) \leqslant \max \left\{\frac{1}{i}, \frac{1}{j}\right\} .
$$

In the limit as $j \rightarrow \infty$ this gives $\rho\left(x(i, i), a_{i}\right) \leqslant \frac{1}{i} \cdot$ Hence $\lim _{i \rightarrow \infty} x(i, i)=a$. By the same reasoning, $\lim _{i \rightarrow \infty} x(i, i)=b$.

Solution to 4.1.4: The closures $\bar{T}_{n}$ form a nested sequence of compact sets whose diameters tend to 0 . The intersection of the closures therefore consists of a single point. The question is whether that point belongs to every $T_{n}$.

It is asserted that the centroid $x_{0}=\frac{1}{3}(A+B+C)$ of the vertices $A, B, C$ is in every $T_{n}$. It is in $T_{0}$, being a convex combination of the vertices $A, B, C$ with nonzero coefficients, in fact, $T_{0}=\{\alpha A+\beta B+\gamma C \mid \alpha, \beta, \gamma>0, \alpha+\beta+\gamma=1\}$. Moreover the centroid of the vertices of $T_{1}$ is

$$
\frac{1}{3}\left(\frac{A+B}{2}+\frac{B+C}{2}+\frac{C+A}{2}\right)=x_{0} .
$$

The obvious induction argument shows that $x_{0}$ is the centroid of the vertices of $T_{n}$ for every $n$. Hence $\bigcap_{n=0}^{\infty} T_{n}=\left\{x_{0}\right\}$.

Solution to 4.1.5: Suppose there is no such $\varepsilon$. Then there exists a sequence $\left(x_{n}\right)$ in $K$ such that none of the balls $B_{1 / n}\left(x_{n}\right)$ is contained in any of the balls $B_{j}$. Since $K$ is compact, this sequence has a limit point, by the Bolzano-Weierstrass Theorem [Rud87, p. 40], [MH93, p. 153], $x \in K$. Then, since the $B_{j}$ 's are an open cover of $K$, there is a $j$ and an $\varepsilon>0$ such that $B_{\varepsilon}(x) \subset B_{j}$. Let $1 / N<\varepsilon / 2$, and choose $n>N$ such that $\left|x-x_{n}\right|<\varepsilon / 2$. Then $B_{1 / n}\left(x_{n}\right) \subset B_{\varepsilon}(x) \subset B_{j}$, contradicting our choice of $x_{n}$ 's. Hence, the desired $\varepsilon$ must exist.

Solution 2. Suppose the conclusion is false. Then, for each positive integer $n$, there are two points $x_{n}$ and $y_{n}$ in $K$ such that $\left|x_{n}-y_{n}\right|<1 / n$, yet no $B_{j}$ contains both $x_{n}$ and $y_{n}$. Since $K$ is compact, the sequence $\left(x_{n}\right)$ has a convergent subsequence, $\left(x_{n_{k}}\right)$ say, with limit $\rho \in K$. Then, obviously, $y_{n_{k}} \rightarrow \rho$. There is a $B_{j}$ that contains $\rho$. Since $B_{j}$ is open and $\rho=\lim x_{n_{k}}=\lim y_{n_{k}}$, both $x_{n_{k}}$ and $y_{n_{k}}$ must be in $B_{j}$ for $k$ sufficiently large, in contradiction to the way the points $x_{n}$ and $y_{n}$ were chosen. Solution 3. By compactness, we can choose a finite subcover $\left\{B_{j}\right\}_{j=1}^{N}$ of $K$, [Rud87, p. 30]. For $x \in K$, define

$$
f(x)=\max \left\{\operatorname{dist}\left(x, \mathbb{R}^{n} \backslash B_{j}\right) \mid x \in B_{j}\right\} .
$$

Then $f(x)>0$ for each $x \in K$, because each $B_{j}$ is open and there are only finitely many of them. Since $K$ is compact and $f$ is continuous and strictly positive on $K, f$ has a positive minimum $\varepsilon>0$. By definition of $f$, every $\varepsilon$-ball centered at a point of $K$ is contained in some $B_{j}$.

Solution 4. For each positive integer $k$ let $E_{k}=\left\{x \in M \mid B(x, 1 / k) \subset U_{\alpha}\right.$ for some $\alpha \in I\}$. For each $x \in E_{k}$ we clearly have $B(x, 1 / k) \subset E_{k}$ so $E_{k}$ is open. The sets $E_{k}$ form an open cover of $C$, and are nested. Since $C$ is compact, $C \subset E_{K}$ for some $K$, and we are done.

Solution to 4.1.6: Suppose $U_{n}$ is an open set of real numbers for $n \in \mathbb{N}$, such that $\mathbb{Q}=\cap U_{n}$. Then each set $\mathbb{R} \backslash U_{n}$ is nowhere dense, since it is a closed set which contains only irrational numbers. We then have

$$
\mathbb{R}=\bigcup_{n \in \mathbb{N}} U_{n} \bigcup_{q \in \mathbb{Q}}\{q\}
$$

but $\mathbb{R}$ is not a countable union of nowhere dense sets, by Baire's Category Theorem [MH93, p. 175]. So $\mathbb{Q}$ cannot be a countable intersection of open sets.

Solution to 4.1.7: Suppose $x, y \in X$. Without loss of generality, assume $x<y$. Let $z$ be such that $x<z<y$ (for instance, $z$ irrational verifying the double inequality). Then

$$
(-\infty, z) \cap X, \quad(z, \infty) \cap X
$$

is a disconnection of $X$. We conclude then that $X$ can have only one element.

Solution to 4.1.8: The Cantor set [Rud87, p. 41] is an example of a closed set having uncountably many connected components.

Let $A$ be an open set and suppose $C_{\alpha}, \alpha \in \Gamma$ are its connected components. Each $C_{\alpha}$ is an open set, so it contains a rational number. As the components are disjoint, we have an injection of $\Gamma$ in $\mathbb{Q}$, so $\Gamma$ is, at most, countable.

Solution to 4.1.9: We show $S$ is bounded and closed.

To establish boundedness, choose $n_{0}$ such that $\left\|f-f_{n}\right\| \leqslant 1$ on $K$ for $n \geqslant n_{0}$. The function $f$ is continuous, being the uniform limit of continuous functions, so $f(K)$ is bounded, hence $f(K) \cup \bigcup_{n=n_{0}}^{\infty} f_{n}(K)$ is bounded. Each $f_{n}(K)$ is bounded. Therefore so is $\bigcup_{1 \leqslant n<n_{0}} f_{n}(K)$, whence $S$ is bounded.

To prove that $S$ is closed, let $\left\{y_{j}\right\}$ be a convergent sequence in $S$ with limit $y$. It will suffice to show that $y$ is in $S$. If infinitely many terms of the sequence lie in $f(K)$, or in $f_{n}(K)$ for some $n$, then $y$ belongs to the same set, and hence to S. (Reason: $f(K)$ and all the $f_{n}(K)$ are compact.) In the contrary case we may assume, after passing to a subsequence, that $y_{j}$ is in $f_{m_{j}}(K)$, where $m_{1}<m_{2}<$ ..., say $y_{j}=f_{m_{j}}\left(x_{j}\right)$. Since $K$ is compact we may assume, passing to a further subsequence, that the sequence $\left(x_{j}\right)$ converges, say to $x$. Then

$$
\|y-f(x)\| \leqslant\left\|y-f_{m_{j}}\left(x_{j}\right)\right\|+\left\|f_{m_{j}}\left(x_{j}\right)-f\left(x_{j}\right)\right\|+\left\|f\left(x_{j}\right)-f(x)\right\| \text {, }
$$

and all three summands on the right tend to 0 as $j \rightarrow \infty$. Hence $y=f(x)$, so $y$ is in $S$.

Solution to 4.1.10: Suppose we have

$$
[0,1]=\bigcup_{i \in \mathbb{N}}\left[a_{i}, b_{i}\right]
$$

where the $\left[a_{i}, b_{i}\right]$ 's are non empty pairwise disjoint intervals. Let $X$ be the set of the corresponding endpoints:

$$
X=\left\{a_{1}, a_{2}, \ldots\right\} \cup\left\{b_{1}, b_{2}, \ldots\right\} .
$$

We will show that $X$ is a perfect set, so it cannot be countable.

The complement of $X$ in $[0,1]$ is a union of open intervals, so it is open, and $X$ is closed. By the assumption, there must be elements of $X$ in $\left(a_{i}-\varepsilon, a_{i}\right)$ for each $\varepsilon>0$, and each $i \in \mathbb{N}$, and similarly for the $b_{i}$ 's. Each element of $X$ is then an accumulation point, and $X$ is perfect.

Solution to 4.1.11: 1. Let $X=\{x\}$ and $\left(y_{n}\right)$ be a sequence in $Y$ such that $\left|x-y_{n}\right|<d(X, Y)+1 / n$. As $\left(y_{n}\right)$ is bounded, passing to a subsequence, we may assume that it converges, to $y$, say. As $Y$ is closed, $y \in Y$ and, by the continuity of the norm, $|x-y|=d(X, Y)$.

2. Let $\left(x_{n}\right)$ be a sequence in $X$ such that $d\left(\left(x_{n}\right), Y\right)<d(X, Y)+1 / n$. As $X$ is compact, by the Bolzano-Weierstrass Theorem [Rud87, p. 40], [MH93, p. 153], we may assume, passing to a subsequence, that $\left(x_{n}\right)$ converges, to $x$, say. We then have $d(X, Y)=d(\{x\}, Y)$ and the result follows from Part 1 .

3. Take $X=\{(x, 1 / x) \mid x>0\}$ and $Y=\{(x, 0) \mid x>0\}$ in $\mathbb{R}^{2}$.

Solution to 4.1.12: Suppose that $S$ contains no limit points. Then, for each $x \in S$, there is a $\delta_{x}>0$ such that $B_{\delta_{x}} \cap S=\{x\}$. Let $\varepsilon_{x}=\delta_{x} / 2$. The balls $B_{\varepsilon_{x}}(x)$ are disjoint, so we can choose a distinct point from each one with rational coordinates. Since the collection of points in $\mathbb{R}^{n}$ with rational coordinates is countable, the set $S$ must be countable, a contradiction. Hence, $S$ must contain one of its limit points.

Solution to 4.1.13: Let $y$ be a limit point of $Y$ and $\left(y_{n}\right)$ a sequence in $Y$ converging to $y$. Without loss of generality, we may suppose that $\left|y_{n}-y\right|<r$. By the definition of $Y$, there is a sequence $\left(x_{n}\right)$ in $X$ with $\left|x_{n}-y_{n}\right|=r$. Therefore, $\left|x_{n}-y\right| \leqslant\left|x_{n}-y_{n}\right|+\left|y_{n}-y\right|<2 r$, so the sequence $\left(x_{n}\right)$ is bounded. Hence, it has a limit point $x \in X$. By passing to subsequences of $\left(x_{n}\right)$ and $\left(y_{n}\right)$, if necessary, we may assume that $\lim x_{n}=x$. Let $\varepsilon>0$. For $n$ large, we have

$$
|x-y| \leqslant\left|x-x_{n}\right|+\left|x_{n}-y_{n}\right|+\left|y_{n}-y\right| \leqslant r+2 \varepsilon
$$

and

$$
r=\left|x_{n}-y_{n}\right| \leqslant\left|x_{n}-x\right|+|x-y|+\left|y-y_{n}\right| \leqslant|x-y|+2 \varepsilon .
$$

Since $\varepsilon$ is arbitrary, $|x-y|=r$. Hence, $y \in Y$ and $Y$ is closed.

Solution to 4.1.14: Let $A$ be an infinite closed subset of $\mathbb{R}^{n}$. For $k=1, \ldots$, let $B_{k}$ be the family of open balls in $\mathbb{R}^{n}$ whose centers have rational coordinates and whose radii are $1 / k$. Each family $B_{k}$ is countable. For each ball $B \in B_{k}$ such that $B \cap A \neq \emptyset$, choose a point in $B \cap A$, and let $A_{k}$ be the set of chosen points. Each $A_{k}$ is a countable subset of $A$, so the set $A_{\infty}=\cup A_{k}$ is a countable subset of $A$. Since $A$ is closed, the inclusion $\overline{A_{\infty}} \subset A$ is obvious. Suppose $a \in A$ and fix a positive integer $k$. Then $a$ lies in some ball $B \in B_{k}$, and this ball $B$ must then contain a point of $A_{k}$, hence of $A_{\infty}$. Thus, some point of $A_{\infty}$ lies within a distance of $2 / k$ of $a$. Since $k$ is arbitrary, it follows that $a \in \overline{A_{\infty}}$ and, thus, that $A \subset \overline{A_{\infty}}$.

Solution to 4.1.15: For $k=1,2, \ldots$, let $B_{k}$ be the closed ball in $\mathbb{R}^{n}$ with center at 0 and radius $k$. Each compact of $\mathbb{R}^{n}$ is contained in some $B_{k}$. As each $B_{k}$ is compact, it is covered by finitely many $U_{j}$ 's, by the Heine-Borel Theorem, [Rud87, p. 30]. Let $j_{k}$ be the smallest index such that $B_{k}$ is covered by $U_{1}, \ldots, U_{j_{k}}$. Define $V_{j}$ by setting $V_{j}=U_{j} \backslash B_{k}$ for $j_{k}+1 \leqslant j \leqslant j_{k+1}$ (if the indices $j_{k}$ are all equal from some point on, set $V_{j}=\emptyset$ for $j$ larger than their ultimate value.) The sets $V_{1}, V_{2}, \ldots$ have the required property.

Solution to 4.1.16: If $K$ is not bounded, then the function $x \mapsto\|x\|$ is not bounded on $K$. If $K$ is bounded but not compact, then it is not closed, by the Heine-Borel Theorem, [Rud87, p. 40], [MH93, p. 155]; therefore, there exists $\xi \in \bar{K} \backslash K$. In this case, the function $x \mapsto\|x-\xi\|^{-1}$ is not bounded on $K$.

Solution to 4.1.17: 1 . Suppose not. Then there is a positive number $\delta$ and a subsequence of $\left(x_{i}\right),\left(y_{n}\right)$, such that

$$
\left|y_{n}-x\right| \geqslant \delta \text {. }
$$

As $A$ is compact, by the Bolzano-Weierstrass Theorem [Rud87, p. 40], [MH93, p. 153], $\left(y_{n}\right)$ has a convergent subsequence, which, by hypothesis, converges to $x$, contradicting the inequality.

2. Let $A=\mathbb{R}$ and consider

$$
x_{i}=\left\{\begin{array}{clll}
i & \text { if } & i & \text { is odd } \\
1 / i & \text { if } & i & \text { is even. }
\end{array}\right.
$$

All the convergent subsequences converge to zero, but $\left(x_{i}\right)$ diverges.

Solution to 4.1.18: Let $\varepsilon>0$. As $f$ is uniformly continuous on $X$, there is a $\delta>0$ such that

$$
|f(x)-f(y)|<\varepsilon \leqslant \varepsilon+M_{1}|x-y|
$$

for $|x-y|<\delta$ and any $M_{1} \geqslant 0$.

Assume $|x-y| \geqslant \delta$. As the function $f$ is bounded, there is an $M_{2}>0$ with $|f(x)-f(y)| \leqslant M_{2}$ for all $x$ and $y$. Let $M_{1}=M_{2} / \delta$. We have

$$
|f(x)-f(y)| \leqslant \delta M_{1} \leqslant M_{1}|x-y| \leqslant M_{1}|x-y|+\varepsilon
$$

for all $x, y \in X$.

Solution to 4.1.19: Let

$$
S=\bigcup_{\alpha} S_{\alpha}=A \cup B
$$

where $A$ and $B$ are open. The origin belongs to $A$ or to $B$. Without loss of generality, assume $O \in A$. For every $\alpha$, we have

$$
S_{\alpha}=\left(S_{\alpha} \cap A\right) \cup\left(S_{\alpha} \cap B\right)
$$

so, as $S_{\alpha}$ is connected and $O \in S_{\alpha} \cap A$, we get $S_{\alpha} \cap B=\emptyset$. Therefore,

$$
S \cap B=\bigcup_{\alpha}\left(S_{\alpha} \cap B\right)=\emptyset
$$

and $S$ is connected.

Solution to 4.1.20: Proof of $1 \Rightarrow 2$ : Suppose that $G(f)$ is disconnected. Let $A, B$ be disjoint non-empty open subsets of $G(f)$ with $A \cup B=G(f)$. Let $A_{0}=\left\{x \in \mathbb{R}^{n} \mid(x, f(x)) \in A\right\}, B_{0}=\left\{x \in \mathbb{R}^{n} \mid(x, f(x)) \in B\right\}$. If $x \in A_{0}$ then $(x, f(x)) \in A$. Since $A$ is open in $G(f)$ there exist neighborhoods $U$ of $x$, and $V$ of $f(x)$, with $(U \times V) \cap G(f) \subset A$. Since $f$ is continuous at $x$, we can find a neighborhood $U^{\prime}$ of $x$ such that $f\left(U^{\prime}\right) \subset V$. Then $U \cap U^{\prime}$ is a neighborhood of $x$, and for any $z \in U \cap U^{\prime},(z, f(z)) \in(U \times V) \cap G(f) \subset A$, and hence $A_{0}$ contains $U \cap U^{\prime}$. This proves that $A_{0}$ is open in $\mathbb{R}^{n}$. Similarly, $B_{0}$ is an open set. Clearly $A_{0}$ and $B_{0}$ are non-empty, $A_{0} \cap B_{0}=\emptyset$ and $A_{0} \cup B_{0}=\mathbb{R}^{n}$. This is a contradiction since $\mathbb{R}^{n}$ is connected.

Counterexample to $2 \Rightarrow 1$ : Take $n=1$, and let $S_{1}=\{(x, \sin (1 / x)) \mid x<0\}$, $S_{2}=\{(x, \sin (1 / x)) \mid x>0\} . S_{1}$ is the image of the continuous map $(-\infty, 0) \rightarrow$ $\mathbb{R} \times \mathbb{R}, x \mapsto(x, \sin (1 / x))$ and therefore it is a connected subset of $\mathbb{R} \times \mathbb{R}$. Similarly $S_{2}$ is a connected subset of $\mathbb{R} \times \mathbb{R}$. Since $(0,0)$ belongs to the closure in $\mathbb{R} \times \mathbb{R}$ of both $S_{1}$ and $S_{2}$, the sets $S_{1} \cup\{(0,0)\}$ and $S_{2} \cup\{(0,0)\}$ are connected. Since these sets have a point in common, their union $\left(S_{1} \cup\{(0,0)\}\right) \cup\left(S_{2} \cup\{(0,0)\}\right)$ is connected. This union is the graph of the function $f: \mathbb{R} \rightarrow \mathbb{R}$ defined by $f(x)=\sin (1 / x), x \neq 0, f(0)=0$, which is not continuous at $x=0$.

Solution to 4.1.21: Let $S$ be a countable dense subset of $U$, for example, the set of points in $U$ with rational coordinates. Let $f$ be the function that equals 0 off $S$ and, at any point $x$ in $S$, equals the distance from $x$ to $\mathbb{R}^{n} \backslash U$. Then $f$ has the required property. Solution to 4.1.22: The assertion is true. Let $S$ be such a set and $a$ a point in $S$. Define the set $S_{a}=\{b \in S \mid a$ and $b$ are connected by a path in $S\}$. $S_{a}$ is open (because $S$ is locally path connected) as well as its complement in $S$, so these two sets make up a partition of $S$, which is connected, therefore, the partition is trivial and $S_{a}$ is the whole $S$.

Solution to 4.1.24: As $X$ is compact, there is a constant $B>0$ such that $B \geqslant\left|a_{i j}\right|$ for all matrices $A=\left(a_{i j}\right)$ in $X$. This bound, together with the equation $A x=\lambda x$ give us the bound $|\lambda| \leqslant \sum_{i, j}\left|a_{i j}\right| \leqslant n^{2} B$, so $S$ is bounded.

Let $\left(\lambda_{i}\right)$ be a sequence in $S$ converging to $\mu$. For each $\lambda_{i}$ there is an element $M_{i}$ of $X$ such that $\lambda_{i}$ is an eigenvalue of $M_{i}$. The sequence $\left(M_{i}\right)$ has a convergent subsequence, by compacity, so we may assume it converges to $M \in X$. The set of singular matrices in $M_{n \times n}$ is closed, since it is the inverse image of $\{0\}$ under the continuous map det : $M_{n \times n} \rightarrow \mathbb{R}$, therefore $\lim _{i \rightarrow \infty}\left(M_{i}-\lambda_{i} I\right)$ is singular. As this limit equals $M-\lambda I$ we conclude that $\lambda \in S$, so $S$ is also closed, therefore it is compact.

Solution to 4.1.25: 1. $P^{2}$ is the quotient of the sphere $S^{2}$ by the equivalence relation that identifies two antipode points $x$ and $-x$. If $\pi: S^{2} \rightarrow P^{2}$ is the natural projection which associates each point $x \in S^{2}$ to its equivalence class $\pi(x)=\{x,-x\} \in P^{2}$, the natural topology is the quotient topology; that is, $A \subset P^{2}$ is open if and only if $\pi^{-1}(A) \subset S^{2}$ is open. With this topology, the projection $\pi$ is a continuous function and $P^{2}=\pi\left(S^{2}\right)$ is compact, being the image of a compact by a continuous function.

Another topology frequently referred to as the usual topology of $P^{2}$ is the one defined by the metric

$$
d(x, y)=\min \{|x-y|,|x+y|\} .
$$

It is a straightforward verification that the function $d$ above satisfies all axioms of a metric. We will show now that it defines the same topology as the one above, on the space that we will call $\left(P^{2}, d\right)$.

The application $\pi: S^{2} \rightarrow P^{2}$ with the metric as above satisfies the inequality

$$
d(\pi(x), \pi(y)) \leqslant|x-y|
$$

so $d$ is continuous. This defines a function $\bar{\pi}$ on the quotient which is

![](https://cdn.mathpix.com/cropped/2022_10_26_41d22736fcd19c2c10fdg-039.jpg?height=344&width=395&top_left_y=2045&top_left_x=865)

the identity and then continuous. Since $P^{2}$ is compact and $\left(P^{2}, d\right)$ Hausdorff, $\bar{\pi}$ is a homeomorphism and the two topologies in $P^{2}$ are equivalent. Now $S O(3)$ is the group of orthogonal transformations of $\mathbb{R}^{3}$ with determinant 1 , so every matrix in this set of satisfies

$$
X \cdot X^{t}=\left(\begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right)
$$

therefore, $\sum_{k=1}^{3} X_{i k}^{2}=1$, for $i=1,2,3$, implying that $S \mathbb{O}(3)$ is bounded. Consider now the transformation

$$
\begin{aligned}
f: M_{3 \times 3} \approx \mathbb{R}^{9} & \rightarrow M_{3 \times 3} \times \mathbb{R} \\
X & \rightarrow\left(X^{t} X, \operatorname{det} X\right) .
\end{aligned}
$$

$f$ is continuous and $S \mathbb{O}(3)=f^{-1}(I, 1)$, that is, the inverse image of a closed set, then itself a closed set, showing that $S O(3)$ is compact. Another way to see this is to observe that the function

$$
X \rightarrow \sqrt{\operatorname{tr}\left(X^{t} X\right)}
$$

is a norm on the space of matrices $M_{n \times n} \equiv \mathbb{R}^{n^{2}}$ and that for matrices in the orthogonal group $\operatorname{tr}\left(X^{t} X\right)=n$, so $\mathbb{O}(n)$ and, consequentially, $S \mathbb{O}(n)$ are compact. 2. To see the homeomorphism between $P^{2}$ and $Q$, first define the application $\varphi: P^{2} \rightarrow Q$ given by the following construction: For each line $x$ through the origin, take $\varphi_{x}: \mathbb{R}^{3} \rightarrow \mathbb{R}^{3}$ as the rotation of $180^{\circ}$ around the $x$ axis. This is well defined and continuous. To see that it is surjective, notice that every orthogonal matrix in $\operatorname{dim} 3$ is equivalent to one of the form

$$
\left(\begin{array}{ccc}
1 & 0 & 0 \\
0 & \cos \theta & \sin \theta \\
0 & -\sin \theta & \cos \theta
\end{array}\right)
$$

and with the additional condition of symmetry $\theta=\pi$, which is a rotation of $180^{\circ}$ around an axis. For more details see the Solution to Problem 7.4.24. Since $\varphi$ is continuous and injective on a compact, it is an homeomorphism.

Solution to 4.1.27: Convergence in $M_{n \times n}$ is entrywise convergence. In other words, the sequence $\left(A_{k}\right)$ in $M_{n \times n}$ converges to the matrix $A$ if and only if, for each $i$ and $j$, the $(i, j)^{t h}$ entry of $A_{k}$ converges to the $(i, j)^{t h}$ entry of $A$. It follows that the operator of multiplication in $M_{n \times n}$ is continuous; in other words, if $A_{k} \rightarrow A$ and $B_{k} \rightarrow B$, then $A_{k} B_{k} \rightarrow A B$. Now suppose $\left(A_{k}\right)$ is a sequence of nilpotent matrices in $M_{n \times n}$ and assume $A_{k} \rightarrow A$. Then $A_{k}^{n} \rightarrow A^{n}$ by the continuity of multiplication. But $A_{k}^{n}=0$ for each $k$ since $A_{k}$ is nilpotent. Hence, $A^{n}=0$, that is, $A$ is nilpotent. As a subset of a metric space is closed exactly when it contains all its limit points, the conclusion follows.

Solution to 4.1.28: For real $l$ let $T_{l}=\{t \in \mathbb{R} \mid S \cap(-\infty, t)$ is countable $\}$. If ( $\left.t_{n}\right)$ is sequence in $T_{l}$ with limit $t \in \mathbb{R}$, then

$$
S \cap(-\infty, t) \subset \bigcup_{i=1}^{\infty}\left(S \cap\left(-\infty, t_{i}\right)\right) .
$$

As a subset of a countable union of countable sets is countable we conclude that $t \in T_{l}$, so $T_{l}$ is closed. If $S \cap(-\infty, n)$ were countable for every integer $n$, then $S$ would be countable, a contradiction. Therefore $T_{l} \neq \mathbb{R}$.

Similarly, $T_{r}=\{t \in \mathbb{R} \mid S \cap(r, \infty)$ is countable $\}$ is closed and $T_{R} \neq \mathbb{R}$. We have $T_{l} \cap T_{r}=\emptyset$ since if $t \in T_{l} \cap T_{r}$ then $S \subset(S \cap(-\infty, t)) \cup(S \cap(r, \infty)) \cup$ $\{t\}$ would be countable. If $T_{l} \cup T_{r}=\mathbb{R}$, then we would have $\mathbb{R}$ as a union of disjoint proper subsets, contradicting the connectivity of $\mathbb{R}$. Hence there exists $t \in \mathbb{R} \backslash T_{l} \cup T_{r}$. For such a $t S \cap(-\infty, t)$ and $S \cap(r, \infty)$ are uncountable.

\subsection{General Theory}

Solution to 4.2.1: We'll prove that $\rho\left(g(y), g_{n}(y)\right) \rightarrow 0$ uniformly as $n \rightarrow \infty$. Since a uniformly continuous map preserves uniform convergence, and $g$ is uniformly continuous, it will suffice to show that $\sigma\left(y, f\left(g_{n}(y)\right) \rightarrow 0\right.$ uniformly as $n \rightarrow \infty$. But

$$
\sigma\left(y, f\left(g_{n}(y)\right)\right)=\sigma\left(f_{n}\left(g_{n}(y)\right), f\left(g_{n}(y)\right)\right),
$$

so the desired conclusion follows from the hypothesis that $f_{n} \rightarrow f$ uniformly as $n \rightarrow \infty$

Solution to 4.2.2: As the $E_{n}$ 's are non empty we can form a sequence $\left(x_{n}\right)$ satisfying $x_{n} \in E_{n}$ for all positive integers $n$. Given $\varepsilon>0$, there is an integer $n(\varepsilon)$ such that diam $E_{n}<\varepsilon$ for $n \geqslant n(\varepsilon)$. Thus, $d\left(x_{n}, x_{m}\right)<\varepsilon$ for $n, m \geqslant n(\varepsilon)$. This sequence is Cauchy, so it converges, to $x$, say. As $E_{n}$ is closed and contains the sequence $\left(x_{n}\right)$ we must have $x \in E_{n}$ for all $n$, i.e., $x \in \bigcap_{n=1}^{\infty} E_{n}$.

Solution to 4.2.3: Suppose that $p$ is in the closure of $A$. Define the function $f(x)=-d(x, p)$ on $X$. By assumption, the restriction $\left.f\right|_{A}$ attains a maximum on $A$. But since $p$ is in the closure of $A,\left.f\right|_{A}$ is nonpositive while attaining values arbitrarily close to 0 , so the maximum value can only be 0 ; hence $p \in A$. Thus $A$ is a closed subset of a compact space, so $A$ is compact.

Solution to 4.2.4: Let $\mathcal{U}$ be an open cover of $C$. Then there is a set $U_{0}$ in $\mathcal{U}$ that contains $x_{0}$. Since $\lim _{n \rightarrow \infty} x_{n}=x_{0}$, there is an $n_{0}$ such that $x_{n}$ is in $U_{0}$ for all $n>n_{0}$. For each $n \leqslant n_{0}$ there is a set $U_{n}$ in $\mathcal{U}$ that contains $x_{n}$. The subfamily $\left\{U_{0}, U_{1}, \ldots, U_{n_{0}}\right\}$ is then a finite subcover of $C$, proving, by the Heine-Borel Theorem [Rud87, p. 30], that $C$ is compact.

Solution to 4.2.5: Let $X$ be a compact metric space. For each $n \in \mathbb{N}$, consider a cover of $X$ by balls with radius $1 / n, \mathcal{B}(1 / n)=\left\{B_{\alpha}\left(x_{\alpha}, 1 / n\right) \mid x_{\alpha} \in X\right\}$. As $X$ is compact, a finite subcollection of $\mathcal{B}(1 / n), \mathcal{B}^{\prime}(1 / n)$, covers $X$, by the HeineBorel Theorem [Rud87, p. 30]. Let $A$ be the set consisting of the centers of the balls in $\mathcal{B}^{\prime}(1 / n), n \in \mathbb{N}$. $A$ is a countable union of finite sets, so it is countable. It is also clearly dense in $X$. Solution to 4.2.6: Suppose $x \notin f(X)$. As $f(X)$ is closed, there exists a positive number $\varepsilon$ such that $d(x, f(X)) \geqslant \varepsilon$.

As $X$ is compact, using the Bolzano-Weierstrass Theorem [Rud87, p. 40], [MH93, p. 153], the sequence of iterates $\left(f^{n}(x)\right)$ has a convergent subsequence, $\left(f^{n_{i}}(x)\right)$, say. For $i<j$, we have

$$
d\left(f^{n_{i}}(x), f^{n_{j}}(x)\right)=d\left(x, f^{n_{j}-n_{i}}(x)\right) \geqslant \varepsilon
$$

which contradicts the fact that every convergent sequence in $X$ is a Cauchy sequence, and the conclusion follows.

Solution to 4.2.7: For $x \in C$ we clearly have $f(x)=0$. Conversely, if $f(x)=0$, then there is a sequence $\left(y_{n}\right)$ in $C$ with $d\left(x, y_{n}\right) \rightarrow 0$. As $C$ is closed, we have $x \in C$. 20],

Given $x, z \in M$ and $y \in C$, we have, by the Triangle Inequality [MH87, p.

$$
d(x, y) \leqslant d(x, z)+d(z, y) .
$$

Taking the infimum of both sides over $y \in C$, we get

$$
f(x) \leqslant d(x, z)+f(z)
$$

or

$$
f(x)-f(z) \leqslant d(x, z),
$$

and, by symmetry,

$$
f(z)-f(x) \leqslant d(x, z) .
$$

Therefore,

$$
|f(x)-f(z)| \leqslant d(x, z)
$$

and $f$ is continuous.

Solution to 4.2.8: $\|f\| \geqslant 0$ for all $f$ in $C^{1 / 3}$ is clear. If $f \equiv 0$, it is obvious that $\|f\|=0$. Conversely, suppose that $\|f\|=0$. Then, for all $x \neq 0$, we have

$$
0 \leqslant \frac{|f(x)-f(0)|}{|x-0|} \leqslant\|f\|=0 .
$$

Since $f(0)=0$, this implies $f(x)=0$ for all $x$. Let $f, g \in C^{1 / 3}$ and $\varepsilon>0$. There exists $x \neq y$ such that

$$
\begin{aligned}
\|f+g\| & \leqslant \frac{|(f+g)(x)-(f+g)(y)|}{|x-y|^{1 / 3}}+\varepsilon \\
& \leqslant \frac{|f(x)-f(y)|}{|x-y|^{1 / 3}}+\frac{|g(x)-g(y)|}{|x-y|^{1 / 3}}+\varepsilon \\
& \leqslant\|f\|+\|g\|+\varepsilon .
\end{aligned}
$$

Since $\varepsilon$ was arbitrary, the Triangle Inequality [MH87, p. 20$]$ holds. The property $\|c f\|=|c|\|f\|$ for $f \in C^{1 / 3}$ and $c \in \mathbb{R}$ is clear.

Let $\left\{f_{n}\right\}$ be a Cauchy sequence in $C^{1 / 3}$. By the definition of the norm, for all $x \in[0,1]$ and any $\varepsilon>0$ there is an $N>0$ such that if $n, m>N$, we have

$$
\left|\left(f_{n}-f_{m}\right)(x)-\left(f_{n}-f_{m}\right)(0)\right| \leqslant|x-0|^{1 / 3} \varepsilon
$$

or

$$
\left|f_{n}(x)-f_{m}(x)\right| \leqslant \varepsilon .
$$

Hence, the sequence $\left\{f_{n}\right\}$ is uniformly Cauchy. A similar calculation shows that functions in $C^{1 / 3}$ are continuous. Since the space of continuous functions on $[0,1]$ is complete with respect to uniform convergence, there exists a continuous function $f$ such that the $f_{n}$ 's converge to $f$ uniformly. Suppose $f \notin C^{1 / 3}$. Then, for any $M>0$, there exist $x \neq y$ such that

$$
\frac{|f(x)-f(y)|}{|x-y|^{1 / 3}}>M \text {. }
$$

So

$$
\frac{\left|f(x)-f_{n}(x)\right|}{|x-y|^{1 / 3}}+\frac{\left|f_{n}(x)-f_{n}(y)\right|}{|x-y|^{1 / 3}}+\frac{\left|f(y)-f_{n}(y)\right|}{|x-y|^{1 / 3}}>M .
$$

Since the $f_{n}$ 's converge to $f$ uniformly, for fixed $x$ and $y$ we can make the first and third terms as small as desired. Hence, $\left\|f_{n}\right\|>M$ for all $M$ and $n$ sufficiently large, contradicting the fact that $f_{n} \in C^{1 / 3}$ and that, since the $f_{n}$ 's are Cauchy, their norms are uniformly bounded.

Suppose now that the sequence $\left\{f_{n}\right\}$ does not converge to $f$ in $C^{1 / 3}$. Then there is an $\varepsilon>0$ such that $\left\|f_{n}-f\right\|>\varepsilon$ for infinitely many $n$ 's. But then there exist $x \neq y$ with

$$
\frac{\left|f_{n}(x)-f(x)\right|}{|x-y|^{1 / 3}}+\frac{\left|f_{n}(y)-f(y)\right|}{|x-y|^{1 / 3}}>\varepsilon
$$

for those $n$ 's. But, as we have uniform convergence, we can make the left-hand side as small as desired for fixed $x$ and $y$, a contradiction.

Solution to 4.2.9: We first show that $\left\{g_{n}\right\}$ is equicontinuous. Fix $\varepsilon>0$. By the equicontinuity of the sequence $\left\{f_{n}\right\}$, there is a $\delta>0$ such that

$$
\left|f_{n}(x)-f_{n}(y)\right|<\varepsilon
$$

for all $n$, whenever $d(x, y)<\varepsilon$. Fix $n$ and fix $x$ and $y$ with $d(x, y)<\delta$. Let $j, k \leqslant n$ be such that $g_{n}(x)=f_{j}(x)$ and $g_{n}(y)=f_{k}(y)$. Then

$$
\begin{aligned}
g_{n}(x)-g_{n}(y) &=f_{j}(x)-f_{k}(y)=f_{j}(x)-f_{j}(y)+f_{i}(y)-f_{k}(y) \\
& \leqslant f_{j}(x)-f_{j}(y)<\varepsilon .
\end{aligned}
$$

By the same reasoning, $g_{n}(y)-g_{n}(x)<\varepsilon$, from which follows the equicontinuity of the sequence $\left\{g_{n}\right\}$. The sequence $\left\{g_{n}\right\}$ is clearly uniformly bounded, so, by the Arzelà-Ascoli Theorem, it has a uniformly convergent subsequence $\left\{g_{n_{k}}\right\}$, with limit $g$, say. Since the sequence $\left\{g_{n}\right\}$ is nondecreasing, for $n>n_{k}$, we have

$$
g_{n_{k}} \leqslant g_{n} \leqslant g,
$$

implying that $g_{n} \rightarrow g$ uniformly.

Solution to 4.2.11: Since $f(K) \subset f\left(K_{n}\right)$ for all $n$, the inclusion $f(K) \subset$ $\bigcap_{1}^{\infty} f\left(K_{n}\right)$ is clear. Let $y$ be a point in $\bigcap_{1}^{\infty} f\left(K_{n}\right)$. Then, for each $n$, the set $f^{-1}(\{y\}) \cap K_{n}$ is nonempty and compact (the latter because it is a closed subset of the compact set $\left.K_{n}\right)$. Also, $f^{-1}(\{y\}) \cap K_{n+1} \subset f^{-1}(\{y\}) \cap K_{n}$. Hence, by the Nested Set Property [MH93, p. 157], the set

$$
\bigcap_{1}^{\infty}\left(f^{-1}(\{y\}) \cap K_{n}\right)=f^{-1}(\{y\}) \cap K
$$

is nonempty; that is, $y \in f(K)$.

Solution to 4.2.12: 1. The completeness of $X_{1}$ implies the completeness of $X_{2}$. In fact, assume $X_{1}$ is complete, and let $\left(y_{n}\right)$ be a Cauchy sequence in $X_{2}$. The conditions on $f$ imply that it is one-to-one, so each $y_{n}$ can be written uniquely as $f\left(x_{n}\right)$ with $x_{n}$ in $X_{1}$. Then $d_{1}\left(x_{m}, x_{n}\right) \leqslant d_{2}\left(y_{m}, y_{n}\right)$, implying that $\left(x_{n}\right)$ is a Cauchy sequence, hence convergent, say to $x$. Since $f$ is continuous, we then have $\lim y_{n}=f(x)$, proving that $X_{2}$ is complete.

2. The completeness of $X_{2}$ does not imply the completeness of $X_{1}$. For an example, take $X_{1}=\left(-\frac{\pi}{2}, \frac{\pi}{2}\right), X_{2}=\mathbb{R}$, and $f(x)=\tan x$. Since $f^{\prime}(x)=\sec ^{2} x \geqslant 1$ on $X_{1}$, the condition $|x-y| \leqslant|f(x)-f(y)|$ holds.

\subsection{Fixed Point Theorem}

Solution to 4.3.1: The map is the image, by a contraction, of a complete metric space (California!). The result is a consequence of the Fixed Point Theorem [Rud87, p. 220].

Solution to 4.3.2: Let $g(x)=(1+x)^{-1}$. We have

$$
g^{\prime}(x)=\frac{-1}{(1+x)^{2}}
$$

therefore,

$$
\left|g^{\prime}(x)\right| \leqslant \frac{1}{\left(1+x_{0} / 2\right)^{2}}<1 \text { for } x>x_{0} .
$$

Then, by the Fixed Point Theorem [Rud87, p. 220], the sequence given by

$$
x_{0}>0, \quad x_{n+1}=g\left(x_{n}\right)
$$

converges to the unique fixed point of $g$ in $\left[x_{0}, \infty\right)$. Solving $g(x)=x$ in that domain gives us the limit

$$
\frac{-1+\sqrt{5}}{2} \text {. }
$$

Solution to 4.3.3: Let $S=\{x \in[0, \infty) \mid x-f(x) \leqslant 0\}$. $S$ is not empty because $0 \in S$; also, every element of $S$ is less than 100 , so $S$ has a supremum, $x_{0}$, say. For any $\varepsilon>0$, there exists an element of $S, x$, with

$$
x \leqslant x_{0}<x+\varepsilon
$$

so

$$
x_{0}-f\left(x_{0}\right) \leqslant x_{0}-f(x)<x+\varepsilon-f(x) \leqslant \varepsilon
$$

and we conclude, since $\varepsilon$ is arbitrary, that $x_{0} \leqslant f\left(x_{0}\right)$.

Suppose $f\left(x_{0}\right)-x_{0}=\delta>0$. Then, for some $x \in S$, we have $x \leqslant x_{0}<x_{0}+\delta$; therefore,

$$
x \leqslant x_{0}<f\left(x_{0}\right)-x_{0}+x+\delta
$$

and we get

$$
x \leqslant x_{0}<f\left(x_{0}\right)-x_{0}+x
$$

from which follows, since $f$ is an increasing function,

$$
f\left(x_{0}\right)-x_{0}+x \leqslant f\left(x_{0}\right) \leqslant f\left(f\left(x_{0}\right)-x_{0}+x\right)
$$

but then $x_{0}<f\left(x_{0}\right)-x_{0}+x \in S$, which contradicts the definition of $x_{0}$. We must then have $f\left(x_{0}\right)=x_{0}$.

Solution to 4.3.4: Consider $F: K \rightarrow \mathbb{R}$ defined by $F(x)=d(x, \varphi(x)) . \varphi$, being a contraction, is continuous, and so is $F$. Since $K$ is compact and nonempty, $F$ attains its minimum $\varepsilon$ at a point $m \in K, d(m, \varphi(m))=\varepsilon$. From the minimality of $\varepsilon$, it follows that $d(\varphi(m), \varphi(\varphi(m)))=F(\varphi(m)) \geqslant \varepsilon=d(m, \varphi(m))$. The contractiveness assumption implies that $m=\varphi(m)$.

Suppose $n \in K$ also satisfies $n=\varphi(n)$. Then $d(\varphi(n), \varphi(m))=d(n, m)$, which, by the contractiveness assumption, implies $n=m$.

Solution to 4.3.5: As the unit square is compact, $\max |K(x, y)|=M<1$. Consider the map $T: \mathcal{C}_{[0,1]} \rightarrow \mathcal{C}_{[0,1]}$ defined by

$$
T(f)(x)=e^{x^{2}}-\int_{0}^{1} K(x, y) f(y) d y .
$$

We have

$$
|T(f)(x)-T(g)(x)|=\left|\int_{0}^{1} K(x, y)(g(y)-f(y)) d y\right|
$$



$$
\begin{aligned}
&\leqslant \int_{0}^{1} M|g(y)-f(y)| d y \\
&\leqslant M \max _{0 \leqslant y \leqslant 1}|g(y)-f(y)| \\
&=M\|g-f\| .
\end{aligned}
$$

By the Contraction Mapping Principle [MH93, p. 275], $\dot{T}$ has a unique fixed point, $h \in \mathcal{C}_{[0,1]}$. We have

$$
h(x)+\int_{0}^{1} K(x, y) h(y) d y=e^{x^{2}} .
$$

Any such a solution is a fixed point of $T$, so it must equal $h$.

Solution to 4.3.6: Consider the map $T: \mathcal{C}_{[0,1]} \rightarrow \mathcal{C}_{[0,1]}$ defined by

$$
T(f)=g(x)+\int_{0}^{x} f(x-t) e^{-t^{2}} d t .
$$

Given $f, h \in \mathcal{C}_{[0,1]}$, we have

$$
\begin{aligned}
\|T(f)-T(h)\|_{\infty} & \leqslant \sup _{x \in[0,1]} \int_{0}^{x}|f(x-t)-h(x-t)| e^{-t^{2}} d t \\
& \leqslant\|f-h\|_{\infty} \sup _{x \in[0,1]} \int_{0}^{x} e^{-t^{2}} d t \\
&=\|f-h\|_{\infty} \int_{0}^{1} e^{-t^{2}} d t<\|f-h\|_{\infty}
\end{aligned}
$$

so $T$ is a contraction. Since $\mathcal{C}_{[0,1]}$ is a complete metric space, by the Contraction Mapping Principle [MH93, p. 275] there is $f \in \mathcal{C}_{[0,1]}$ such that $T(f)=f$, as desired.

Solution to 4.3.7: Define the operator $T$ on $\mathcal{C}_{[0,1]}$ by

$$
T(f)(x)=\sin x+\int_{0}^{1} \frac{f(y)}{e^{x+y+1}} d y .
$$

Let $f, g \in \mathcal{C}_{[0,1]}$. We have

$$
\begin{aligned}
\|T(f)-T(g)\| & \leqslant \sup _{x}\left\{\int_{0}^{1} \frac{|f(y)-g(y)|}{e^{x+y+1}} d y\right\} \\
& \leqslant \sup _{x}\left\{|f(x)-g(x)| e^{-x}\right\} \int_{0}^{1} \frac{d y}{e^{y+1}} \\
& \leqslant\|f-g\|\left(\frac{1}{e}-\frac{1}{e^{2}}\right) \\
& \leqslant \lambda\|f-g\| .
\end{aligned}
$$

where $0<\lambda<1$ is a constant. Hence, $T$ is a strict contraction. Therefore, by the Contraction Mapping Principle [MH93, p. 275], there is a unique $f \in \mathcal{C}_{[0,1]}$ with $T(f)=f$

Solution to 4.3.8: Since $M$ is a complete metric space and $S^{2}$ is a strict contraction, by the Contraction Mapping Principle [MH93, p. 275] there is a unique point $x \in M$ such that $S^{2}(x)=x$. Let $S(x)=y$. Then $S^{2}(y)=S^{3}(x)=S(x)=y$. Hence, $y$ is a fixed point of $S^{2}$, so $x=y$. Any fixed point of $S$ is a fixed point $S^{2}$, so $S$ has a unique fixed point. 

\section{5}

\section{Complex Analysis}

\subsection{Complex Numbers}

Solution to 5.1.1: We have

$$
1=e^{2 k \pi i} \quad \text { for } \quad k \in \mathbb{Z}
$$

therefore,

$$
\begin{aligned}
1^{\frac{1}{3}+i} &=e^{\left(\frac{1}{3}+i\right) \log 1}=e^{\left(\frac{1}{3}+i\right) 2 k \pi i} \\
&=e^{-2 k \pi+i \frac{2 k \pi}{3}} \\
&=e^{-2 k \pi}\left(\cos \frac{2 k \pi}{3}+i \sin \frac{2 k \pi}{3}\right) \quad(k \in \mathbb{Z})
\end{aligned}
$$

Solution to 5.1.2: We have $i^{i}=e^{i \log i}$ and $\log i=\log |i|+i$ arg $i=$ $i(\pi / 2+2 k \pi), k \in \mathbb{Z}$. So the values of $i^{i}$ are $\left\{e^{-(\pi / 2+2 k \pi)} \mid k \in \mathbb{Z}\right\}$.

Solution to 5.1.3: Let $A, B, C$ be the points $a, b, c$ in the Argand diagram. We suppose that $a, b, c$ are noncollinear.

To prove necessity, suppose $A B C$ is equilateral. To be definite suppose $A B C$ has counterclockwise orientation. Then $c-a=(b-a) e^{i \pi / 3}$ and $a-b=(c-b) e^{i \pi / 3}$. Therefore $(c-a)(c-b)=(a-b)(b-a)$ which is equivalent to the condition $a^{2}+b^{2}+c^{2}=b c+c a+a b$. Conversely, suppose $a^{2}+b^{2}+c^{2}=b c+c a+a b$. Then $(c-a)(c-b)=$ $(a-b)(b-a)$, and

$$
\frac{c-a}{b-a}=\frac{a-b}{c-b}=r e^{i \theta} \text { say. }
$$

Hence

$$
\frac{A C}{A B}=\frac{B A}{B C}=r
$$

and

$$
\angle B A C=\angle C B A=\pm \theta \quad(\bmod 2 \pi) .
$$

From the above equations we get $A C=B C$. Therefore $A B C$ is an equilateral triangle.

Now consider the case that $a, b, c$ are collinear. The condition $a^{2}+b^{2}+c^{2}=$ $b c+c a+a b$ is equivalent to $(c-a)(c-b)=(a-b)(b-a)$. The latter condition is clearly invariant under translation and rotation about the origin. Thus we may assume that $a, b, c$ are real and that $a=0$. The latter condition then reduces to $c^{2}+b^{2}=b c$, which is satisfied only by the real numbers $c=b=0$, as can be seen by using the quadratic formula to solve for $c$. Thus in this case the given condition is equivalent to $a, b, c$ forming a degenerate equilateral triangle.

Solution to 5.1.4: Multiplying by a unimodular constant, if necessary, we can assume $c=1$. Then $\Re a+\mathfrak{\}=0$. So $a=\bar{b}$. Their real part must be negative, since otherwise the real parts of $a, b$, and $c$ would sum to a positive number. Therefore, there is $\theta$ such that $a=\cos \theta+i \sin \theta$ and $b=\cos \theta-i \sin \theta$, $\cos \theta=-1 / 2$. Then $\theta=2 \pi / 3$ and we are done.

Solution 2. Since $b+c=-a$, we get $1=\|b+c\|^{2}=2+2 \Re b \bar{c}$, thus $2 \Re b \bar{c}=-1$, hence $\|b-c\|^{2}=2-2 \Re b \bar{c}=3$. The same calculation gives $\|a-b\|^{2}=3$ and $\|a-c\|^{2}=3$.

Solution to 5.1.5: 1. We have

$$
\begin{aligned}
P_{n-1}(x) &=\frac{x^{n}-1}{x-1} \\
&=x^{n-1}+\cdots+1
\end{aligned}
$$

for $x \neq 1$, so $P_{n-1}(1)=n$.

2. Let

$$
p_{k}=e^{\frac{2 \pi i(k-1)}{n}} \quad \text { for } k=1, \ldots, n
$$

be the $n^{t h}$ roots of 1 . As $p_{1}=1$, we have

$$
\prod_{i=2}^{n}\left(z-p_{k}\right)=P_{n-1}(z) .
$$

Letting $z=1$, and using Part 1, we get the desired result. Solution to 5.1.6: Let $q=e^{i z}$, so $2 \cos z=q+q^{-1}$, and $2 \cos n z=q^{n}+q^{-n}$. The problem reduces to find the polynomial such that $T_{N}\left(q+q^{-1}\right)=q^{n}+q^{-n}$. Now

$$
\begin{aligned}
\left(q+q^{-1}\right)^{n}=\sum_{k=0}^{n}\left(\begin{array}{l}
n \\
k
\end{array}\right) q^{2 k-n}=q^{n}+q^{-n}+\sum_{\substack{0<j<n \\
n-j \text { jeven }}}\left(\begin{array}{c}
n \\
(n-j) / 2
\end{array}\right)\left(q^{j}+q^{-j}\right)+\\
&+\left\{\begin{array}{cl}
\left(\begin{array}{c}
n \\
n / 2
\end{array}\right) & \text { if } \mathrm{n} \text { is even, } \\
0 & \text { otherwise. }
\end{array}\right.
\end{aligned}
$$

Now assuming that we know $T_{j}$ for $j<n$ and proceeding by induction,

$$
T_{n}(x)=x^{n}-\sum_{\substack{0<j<n \\
n-j \text { jeven }}}\left(\begin{array}{c}
n \\
(n-j) / 2
\end{array}\right)\left(T_{j}(x)\right)-\left\{\begin{array}{cl}
\left(\begin{array}{c}
n \\
n / 2
\end{array}\right) & \text { if } \mathrm{n} \text { is even, } \\
0 & \text { otherwise. }
\end{array}\right.
$$

is the sought polynomial.

Solution to 5.1.7: Consider the complex plane divided into four quadrants by the lines $\Re z=\pm \Im z$, and let $\Delta_{i}$ be the set of indices $j$ such that $z_{j}$ lies in the $i^{\text {th }}$ quadrant. The union of the four sets $\Delta_{i}$ is $\{1,2, \ldots, n\}$, so there is an $i$ such that $\Delta=\Delta_{i}$ satisfies

$$
\sum_{j \in \Delta}\left|z_{j}\right| \geqslant \frac{1}{4} \sum_{j=1}^{n}\left|z_{j}\right| .
$$

Since multiplying all of the $z_{j}$ by a unimodular constant will not affect this sum, we may assume that $\Delta$ is the quadrant in the right half-plane, where $\Re z_{j}>0$ and $\left|z_{j}\right| \leqslant \sqrt{2} \Re z_{j}$. So we have

$$
\left|\sum_{j \in \Delta} z_{j}\right| \geqslant \sum_{j \in \Delta} \Re z_{j} \geqslant \frac{1}{\sqrt{2}} \sum_{j \in \Delta}\left|z_{j}\right| .
$$

Combining this with the previous inequality, we get the desired result.

Solution to 5.1.8: The functions $1, e^{2 \pi i x}, \ldots, e^{2 \pi i n x}$ are orthonormal on $[0,1]$. Hence,

$$
\int_{0}^{1}\left|1-\sum_{k=1}^{n} a_{k} e^{2 \pi i k x}\right|^{2} d x=1+\sum_{k=1}^{n}\left|a_{s}\right|^{2} \geqslant 1 \text {. }
$$

Since the integrand is continuous and nonnegative, it must be $\geqslant 1$ at some point. Solution 2. Since $\int_{0}^{1} e^{2 \pi i k x} d x=0$ for $k \neq 0$, we have

$$
1=\int_{0}^{1}\left(1-\sum_{k=1}^{n} a_{k} e^{i k x}\right) d x \leqslant \int_{0}^{1}\left|1-\sum_{k=1}^{n} a_{k} e^{i k x}\right| d x .
$$

Now argue as above.

Solution to 5.1.9: We have

$$
e^{b}-e^{a}=\int_{a}^{b} e^{z} d z
$$

for all complex numbers $a$ and $b$, where the integral is taken over any path connecting them. Suppose that $a$ and $b$ lie in the left half-plane. Then we can take a path also in the same half-plane, and for any $z$ on this line, $\left|e^{z}\right| \leqslant 1$. Therefore, integrating along this line, we get

$$
\left|e^{b}-e^{a}\right| \leqslant \int_{a}^{b}\left|e^{z}\right||d z| \leqslant|b-a| .
$$

Solution to 5.1.10: The boundary of $N(A, r)$ consists of a finite set of circular arcs $C_{i}$, each centered at a point $a_{i}$ in $A$. The sectors $S_{i}$ with base $C_{i}$ and vertex $a_{i}$ are disjoint, and their total area is $L r / 2$, where $L$ is the length of the boundary. Since everything lies in a disc of radius 2 , the total area is at most $4 \pi$, so $L \leqslant$ $8 \pi / r$.

Solution to 5.1.11: Without loss of generality, suppose that

$$
\left|\alpha_{1}\right| \leqslant\left|\alpha_{2}\right| \leqslant \cdots \leqslant\left|\alpha_{l}\right|<\left|\alpha_{l+1}\right|=\cdots=\left|\alpha_{k}\right|
$$

that is, exactly $k-l$ of the $\alpha$ 's with maximum modulus ( $l$ may be zero.)

We will first show that $\left|\alpha_{k}\right|=\sup _{j}\left|\alpha_{j}\right|$ is an upper bound for the expression, and then prove that a subsequence gets arbitrarily close to this value. We have

$$
\left|\sum_{j=1}^{k} \alpha_{j}^{n}\right|^{1 / n} \leqslant\left(\sum_{j=1}^{k}\left|\alpha_{j}\right|^{n}\right)^{1 / n} \leqslant\left(k\left|\alpha_{k}\right|^{n}\right)^{1 / n}=k^{1 / n}\left|\alpha_{k}\right|
$$

the limit on the right exists and is $\left|\alpha_{k}\right|=\sup _{j}\left|\alpha_{j}\right|$, so

$$
\limsup _{n}\left|\sum_{j=1}^{k} \alpha_{j}^{n}\right|^{1 / n} \leqslant \sup _{j}\left|\alpha_{j}\right| .
$$

Now dividing the whole expression by $\alpha_{k}^{n}$ we get

$$
\sum_{j=1}^{k} \alpha_{j}^{n}=\alpha_{k}^{n} \sum_{j=1}^{k}\left(\frac{\alpha_{j}}{\alpha_{k}}\right)^{n}=\alpha_{k}^{n}\left(\sum_{j=1}^{l}\left(\frac{\alpha_{j}}{\alpha_{k}}\right)^{n}+\sum_{j=l+1}^{k} e^{i n \theta_{j}}\right)
$$

since the last $k-l$ terms all have absolute value 1 . It suffices to show that

$$
\text { (*) } \quad \limsup _{n}\left|\sum_{j=1}^{l}\left(\frac{\alpha_{j}}{\alpha_{k}}\right)^{n}+\sum_{j=l+1}^{k} e^{i n \theta_{j}}\right|=k-l
$$

since

$$
\left|\sum_{j=1}^{k} \alpha_{j}^{n}\right|^{1 / n}=\left|\alpha_{k}\right|\left|\sum_{j=1}^{l}\left(\frac{\alpha_{j}}{\alpha_{k}}\right)^{n}+\sum_{j=l+1}^{k} e^{i n \theta_{j}}\right|^{1 / n} .
$$

(*) is a consequence of the the fact that orbits of irrational rotation on the circle are dense in the unit circle. To see that, discard the first term $\sum_{j=1}^{l}\left(\frac{\alpha_{j}}{\alpha_{k}}\right)^{n}$ because its limit exists and equals zero, being a finite sum of terms that converge to zero, and distribute the rest in two sums, one containing all rational angles $\left(p_{i} / q_{i}\right)$, and another one containing all irrational angles $\left(s_{i}\right)$. Without loss of generality we are left to prove that

$$
\limsup _{n}\left|\sum_{j=l+1}^{k^{\prime}} e^{i n \frac{p_{j}}{q_{j}}}+\sum_{j=k^{\prime}+1}^{k} e^{i n s_{j}}\right|=k-l
$$

If the sequence contains only rational angles choose $P=2 \prod_{j} q_{j}$, twice the product of the denominators of the rational angles. Then the sequence $n P$ where $n \in \mathbb{N}$ will land all angles at zero and the summation is equal to $k-l$.

Now if there is at least one irrational angle among them the set of points

$$
\left(e^{n i \frac{p_{l+1}}{q l+1}}, \ldots, e^{n i \frac{p_{k^{\prime}}}{q_{k^{\prime}}}}, e^{n i s_{k^{\prime}+1}}, \ldots, e^{n i s_{k}}\right) \quad n \in \mathbb{N}
$$

in the tori $S^{1} \times S^{1} \times \cdots \times S^{1}$ is infinite (the last coordinates will never repeat) and so has an accumulation point, that is, for any $\varepsilon>0$ there are two iterates $m>n$ such that

$$
\left|\left(e^{m i \frac{p_{l+1}}{q_{l+1}}}, \ldots, e^{m i \frac{p_{k^{\prime}}}{q_{k^{\prime}}}}, \ldots, e^{m i s_{k}}\right)-\left(e^{n i \frac{p_{l+1}}{q l+1}}, \ldots, e^{n i \frac{p_{k^{\prime}}}{q_{k^{\prime}}}}, \ldots, e^{n i s_{k}}\right)\right|<\varepsilon
$$

and then

$$
\left(e^{(m-n) i \frac{p_{l+1}}{q_{l+1}}}, \ldots, e^{(m-n) i \frac{p_{k}^{\prime}}{q_{k^{\prime}}}}, e^{(m-n) i s_{k^{\prime}+1}}, \ldots, e^{(m-n) i s_{k}}\right)
$$

is $\varepsilon$-close to $(1, \ldots, 1)$, and we are done.

\section{$5.2$ Series and Sequences of Functions}

Solution to 5.2.1: Multiplying the first $N+1$ factors we get

$$
\frac{1-z^{10}}{1-z} \frac{1-z^{100}}{1-z^{10}} \frac{1-z^{1000}}{1-z^{100}} \cdots \frac{1-z^{10 N}}{1-z^{10 N-1}}=\frac{1-z^{10 N}}{1-z}
$$

so the product converges to $1 /(1-z)$ as $N \rightarrow \infty$.

Solution to 5.2.2: From the recurrence relation, we see that the coefficients $a_{n}$ grow, at most, at an exponential rate, so the series has a positive radius of convergence. Let $f$ be the function it represents in its disc of convergence, and consider the polynomial $p(z)=3+4 z-z^{2}$. We have

$$
\begin{aligned}
p(z) f(z) &=\left(3+4 z-z^{2}\right) \sum_{n=0}^{\infty} a_{n} z^{n} \\
&=3 a_{0}+\left(3 a_{1}+4 a_{0}\right) z+\sum_{n=0}^{\infty}\left(3 a_{n}+4 a_{n-1}-a_{n-2}\right) z^{n} \\
&=3+z
\end{aligned}
$$

So

$$
f(z)=\frac{3+z}{3+4 z-z^{2}} .
$$

The radius of convergence of the series is the distance from 0 to the closest singularity of $f$, which is the closest root of $p$. The roots of $p$ are $2 \pm \sqrt{7}$. Hence, the radius of convergence is $\sqrt{7}-2$.

Solution to 5.2.3: Let

$$
f(z)=\sum_{n=0}^{\infty} a_{n} z^{n}
$$

be the Maclaurin series of $f$. Substituting the equation we get:

$$
a_{0}=0, \quad a_{1}=1, \quad a_{2}=0, \quad a_{k}=\frac{a_{k-3}}{k(k-1)}(k \geqslant 3) .
$$

and by induction for $k \geqslant 1$, we get,

$$
a_{3 k}=\prod_{j=1}^{k} \frac{1}{3 j(3 j-1)}, \quad a_{3 k+1}=\prod_{j=1}^{k} \frac{1}{3 j(3 j+1)} \quad a_{3 k+2}=0 .
$$

So dividing the sum up in two components and analyzing the convergence of each piece we see that $\lim _{k \rightarrow \infty} \frac{a_{3 k}+3}{a_{3 k}}=0$, so the series $\sum a_{3 k} z^{3 k}$ has infinite radius of convergence. The series relative to the indexes $3 k 1+1$ can be treated in a similar way. We conclude then that the series for $f$ has infinite radius of convergence, which proves the unicity of the solution to the given problem.

Solution to 5.2.4: Let $f(z)=\exp (z /(z-2))$. The series can then be rewritten as $\sum_{n=1}^{\infty} \frac{1}{n^{2}}(f(z))^{n}$, so, by the standard theory of power series, it converges if and only if $|f(z)| \leqslant 1$. The preceding inequality holds when $\mathfrak{R} \frac{z}{z-2} \leqslant 0$, so the problem reduces to that of finding the region sent into the closed left half-plane by the linear fractional map $z \mapsto \frac{z}{z-2}$. The inverse of the preceding map is the map $g$ defined by $g(z)=\frac{2 z}{z-1}$. Since $g(0)=0$ and $g(\infty)=2$, the image of the imaginary axis under $g$ is a circle passing through the points 0 and 2 . As $g$ sends the real axis onto itself, that circle must be orthogonal to the real axis, so it is the circle $|z-1|=1$. Thus, $g$ sends the open left half-plane either to the interior or to the exterior of that circle. Since $g(-1)=1$, the first possibility occurs. We can conclude that $|f(z)| \leqslant 1$ if and only if $|z-1| \leqslant 1$ and $z \neq 2$, which is the region of convergence of the original series.

![](https://cdn.mathpix.com/cropped/2022_10_26_41d22736fcd19c2c10fdg-055.jpg?height=439&width=1176&top_left_y=751&top_left_x=472)

Solution to 5.2.5: The radius of convergence, $R$, of this power series is given by

$$
\frac{1}{R}=\limsup _{n \rightarrow \infty}\left|a_{n}\right|^{1 / n} .
$$

For $|z|<1$, we have

$$
\sum_{n=1}^{\infty} n z^{n-1}=\left(\sum_{n=0}^{\infty} z^{n}\right)^{\prime}=\left(\frac{1}{1-z}\right)^{\prime}=\frac{1}{(1-z)^{2}} .
$$

By the Identity Theorem [MH87, p. 397],

$$
f(z)=\frac{1}{(1-z)^{2}}
$$

where the right-hand side is analytic. Since this happens everywhere except at $z=1$, the power series expansion of $f$ centered at $-2$ will have a radius of convergence equal to the distance between $-2$ and 1 . Hence, $R=3$.

\section{Solution to 5.2.6: As}

$$
1-x^{2}+x^{4}-x^{6}+\cdots=\frac{1}{1+x^{2}}
$$

which has singularities at $\pm i$, the radius of convergence of

$$
\sum_{n=0}^{\infty} a_{n}(x-3)^{n}
$$

is the distance from 3 to $\pm i,|3 \mp i|=\sqrt{10}$. We then have

$$
\limsup _{n \rightarrow \infty}\left(\left|a_{n}\right|^{\frac{1}{n}}\right)=\frac{1}{\sqrt{10}} \cdot
$$

Solution to 5.2.7: As $\lim _{n \rightarrow \infty} \sqrt[n]{n^{2}}=1$, we have

$$
\frac{1}{R}=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|a_{n}\right|}=\limsup _{n \rightarrow \infty} \sqrt[n]{n^{2}\left|a_{n}\right|}
$$

so $\sum a_{n} z^{n}$ and $\sum n^{2} a_{n} z^{n}$ have the same radius of convergence, and the conclusion follows.

Solution to 5.2.8: We'll show that the series converges for $|z|=1, z \neq 1$, which implies that the radius of convergence is at least 1 , therefore the series is convergent for $|z|<1$.

Let $R_{n, p}$ (where $p$ is a positive integer) and $R_{n}$ be defined by

$$
R_{n, p}(z)=\sum_{i=n+1}^{n+p} b_{i} z^{i}, \quad R_{n}(z)=\sum_{i \geqslant n+1} b_{i} z^{i} .
$$

We have,

$$
(z-1) R_{n, p}(z)=-b_{n+1} z^{n+1}+\sum_{i=n+1}^{n+p-1}\left(b_{i}-b_{i+1}\right) z^{i+1}+b_{n+p} z^{n+p+1},
$$

therefore,

$$
|z-1|\left|R_{n, p}(z)\right| \leqslant b_{n+1}|z|^{n+1}+\sum_{i=n+1}^{n+p-1}\left(b_{i}-b_{i+1}\right)|z|^{i+1}+b_{n+p}|z|^{n+p+1} .
$$

Letting $|z|=1, z \neq 1$ in this inequality, we get,

$$
\left|R_{n, p}(z)\right| \leqslant \frac{1}{|z-1|}\left(b_{n+1}+\sum_{i=n+1}^{n+p-1}\left(b_{i}-b_{i+1}\right)+b_{n+p}\right)=\frac{2 b_{n+1}}{|z-1|} .
$$

Since this holds for any positive $p$, we may let $p \rightarrow \infty$ and obtain,

$$
\left|R_{n}(z)\right|=\left|\sum_{i \geq n+1} b_{i} z^{i}\right| \leqslant \frac{2 b_{n+1}}{|z-1|}=o(1) \quad(n \rightarrow \infty),
$$

as we wanted. Nothing can be said about convergence at $z=1$. The series $\sum \frac{z^{l}}{i}$ diverges at $z=1$, while $\sum \frac{z^{p}}{i^{2}}$ converges at the same point.

Solution to 5.2.9: The series $\sum_{n=0}^{\infty} \frac{z^{n}}{n !}$ converges for all $z$.

If $z \neq 0$ then

$$
\left|\frac{n^{2}}{z^{n}}\right|^{1 / n}=\frac{\left(n^{1 / n}\right)^{2}}{|z|} \rightarrow \frac{1}{|z|} .
$$

By Cauchy's Root Test, the series $\sum n^{2} / z^{n}$ converges for $|z|>1$, and diverges for $|z|<1$. Hence the series $\sum\left(z^{n} / n !+n^{2} / z^{n}\right)$ converges for $|z|>1$, and diverges for $|z|<1$; the series is undefined for $z=0$.

Let $|z|=1$. The sequence $z^{n} / n$ ! tends to zero and the sequence $n^{2} / z^{n}$ tends to $\infty$. Therefore the sequence $\left(z^{n} / n !+n^{2} / z^{n}\right)$ tends to $\infty$, which implies that the series $\sum\left(z^{n} / n !+n^{2} / z^{n}\right)$ is divergent for $|z|=1$. Thus the given series converges exactly when $|z|>1$.

Solution to 5.2.10: Fix $z \neq \pm i$. Then

$$
\left|\frac{z}{\left(1+z^{2}\right)^{n}}\right|^{1 / n}=\frac{|z|^{1 / n}}{\left|1+z^{2}\right|} \rightarrow \frac{1}{\left|1+z^{2}\right|} .
$$

By Cauchy's Root Test the given series converges if $\left|1+z^{2}\right|>1$, that is, for points exterior to the lemniscate $\left|1+z^{2}\right|=1$.

Cauchy's test also shows that the given series is divergent for $\left|1+z^{2}\right|<1$. The series clearly converges for $z=0$. If $\left|1+z^{2}\right|=1, z \neq 0$ then $|z| /\left|1+z^{2}\right|^{n}=|z|$ which does not tend to 0 as $n \rightarrow \infty$. Thus the series diverges for points $z \neq 0$ on the lemniscate.

Thus the series is convergent precisely for points exterior to the lemniscate, together with the point $z=0$ on the lemniscate, is undefined at $z=\pm i$, and diverges at all other points.

Solution to 5.2.11: Let $R$ denote the radius of convergence of this power series.

$$
R=\underset{n}{\limsup _{n}\left|n^{\log n}\right|^{1 / n}}=\limsup _{n} e^{(\log n)^{2} / n}=e^{0}=1 .
$$

The series and all term by term derivatives converge absolutely on $|z|<1$ and diverge for $|z|>1$. Let $|z|=1$. For $k \geqslant 0$ the $k^{t h}$ derivative of the power series is

$$
\sum_{n=k}^{\infty} n(n-1) \cdots(n-k+1) \frac{z^{n-k}}{n^{\log n}} .
$$

To see that this converges absolutely, note that

$$
\sum_{n=k}^{\infty} n(n-1) \cdots(n-k+1) \frac{1}{n^{\log n}} \leqslant \sum_{n=k}^{\infty} \frac{1}{n^{\log n-k}} .
$$

Since, for $n$ sufficiently large, $\log n-k>2$, and $\sum 1 / n^{2}$ converges, by the Comparison Test [Rud87, p. 60] it follows that the power series converges absolutely on the circle $|z|=1$.

Solution to 5.2.12: We have

$$
\begin{aligned}
\lim \sup \left|\frac{a_{n}}{n !}\right|^{1 / n} &=\lim \sup \left|a_{n}\right|^{1 / n} \lim \sup \left|\frac{1}{n !}\right|^{1 / n} \\
&=\frac{1}{R} \limsup \left|\frac{1}{n !}\right|^{1 / n} \\
&=\frac{1}{R} \cdot 0 \\
&=0
\end{aligned}
$$

so $h$ is entire.

Let $0<r<R$. Then $1 / R<1 / r$, so there is an $N>0$ such that $\left|a_{n}\right| \leqslant 2 / r^{n}$ for $n>N$. Further, there exists a constant $M>2$ such that $\left|a_{n}\right| \leqslant M / r^{n}$ for $1 \leqslant n \leqslant N$. Therefore, for all $z$,

$$
|h(z)| \leqslant \sum_{n=1}^{\infty}\left|a_{n}\right| \frac{|z|^{n}}{n !} \leqslant M \sum_{n=1}^{\infty} \frac{|z|^{n}}{r^{n} n !}=M e^{|z| / r} .
$$

Solution to 5.2.13: We have

$$
\left|f(z)-s_{k}(z)\right|=\left|\sum_{n=k+1}^{\infty} c_{n} z^{n}\right| \leqslant \sum_{n=k+1}^{\infty}\left|c_{n}\right|\left|z^{n}\right|
$$

Therefore

$$
\begin{aligned}
\sum_{k=0}^{\infty}\left|f(z)-s_{k}(z)\right| & \leqslant \sum_{k=0}^{\infty} \sum_{n=k+1}^{\infty}\left|c_{n}\right|\left|z^{n}\right| \\
&=\sum_{n=1}^{\infty} \sum_{k=0}^{n-1}\left|c_{n}\right|\left|z^{n}\right|=\sum_{n=1}^{\infty} n\left|c_{n}\right|\left|z^{n}\right|
\end{aligned}
$$

From the basic theory of power series, the series $\sum_{n=0}^{\infty} c_{n} z^{n}$ and its formal derivative, the series $\sum_{n=1}^{\infty} n c_{n} z^{n-1}$, have the same radius of convergence. Therefore the series $\sum_{n=1}^{\infty} n c_{n} z^{n}$ also has radius of convergence $R$, so it converges absolutely for $|z|<R$, and the desired conclusion follows.

Solution to 5.2.14: Let the residue of $f$ at 1 be $K$. We have

$$
\sum_{n=0}^{\infty} a_{n} z^{n}=\frac{K}{1-z}+\sum_{n=0}^{\infty} b_{n} z^{n} \quad \text { with } \quad \limsup _{n \rightarrow \infty}\left|b_{n}\right|^{1 / n}>1 .
$$

Therefore,

$$
\sum_{n=0}^{\infty} a_{n} z^{n}=\sum_{n=0}^{\infty}\left(K+b_{n}\right) z^{n}
$$

and $a_{n}=K+b_{n}$. As $\sum b_{n}<\infty$, we have $\lim b_{n}=0$ and $\lim a_{n}=K$.

Solution to 5.2.15: The rational function

$$
f(z)=\frac{1-z^{2}}{1-z^{12}}
$$

has poles at all nonreal twelfth roots of unity (the singularities at $z^{2}=1$ are removable). Thus, the radius of convergence is the distance from 1 to the nearest singularity:

$$
R=|\exp (\pi i / 6)-1|=\sqrt{(\cos (\pi / 6)-1)^{2}+\sin ^{2}(\pi / 6)}=\sqrt{2-\sqrt{3}} .
$$

Solution to 5.2.16: Suppose that $\left\{f_{n}\right\}$ is a sequence of functions analytic in an open set $G$, and that $f_{n}$ converges to the function $f$ uniformly on $G$. Then $f$ is continuous in $G$. Fix $\zeta \in G$. There is $R>0$ with $D(\zeta, R) \subset G$. Let $\gamma$ be a triangular contour in $D(\zeta, R)$. Then $\int_{\gamma} f_{n}(z) d z=0$ for each $n$, by Cauchy's Theorem [MH87, p. 152]. As $f_{n} \rightarrow f$ uniformly on the image of $\gamma$, we deduce that $\int_{\gamma} f_{n}(z) d z \rightarrow \int_{\gamma} f(z) d z$; thus $\int_{\gamma} f(z) d z=0$. By Morera's Theorem, [MH87, p. 173] $f$ is analytic in $D(\zeta, R)$. This proves the result.

The analogous result for real analytic functions is false. Consider the function $f:(-1,1) \rightarrow \mathbb{R}$ defined by $f(x)=|x|$. As $f$ is continuous, it is the uniform limit of polynomials, by Stone-Weierstrass Approximation Theorem [MH93, p. 284]. However, $f$ is not even continuous in all of its domain.

Solution to 5.2.17: By the Hurwitz Theorem [MH87, p. 423], each zero of $g$ is the limit of a sequence of zeros of the $g_{n}$ 's, which are all real, so the limit will be real as well.

Solution to 5.2.18: Let $\varepsilon_{k}=\lim _{n \rightarrow \infty} g_{n}^{(k)}(0)$. Then, clearly, $\left|\varepsilon_{k}\right| \leqslant\left|f^{(k)}(0)\right|$ for all $k$. Since $f$ is an entire function, its Maclaurin series [MH87, p. 234] converges absolutely for all $z$. Therefore, by the Comparison Test [Rud87, p. 60], the series

$$
\sum_{k=0}^{\infty} \varepsilon_{k} z^{k}
$$

converges for all $z$ and defines an entire function $g(z)$. Let $R>0$ and $\varepsilon>0$. For $|z| \leqslant R$, we have

$$
\left|g_{n}(z)-g(z)\right| \leqslant \sum_{k=0}^{N}\left|g_{n}^{(k)}(0)-\varepsilon_{k}\right| R^{k}
$$



$$
\leqslant \sum_{k=0}^{N}\left|g_{n}^{(k)}(0)-\varepsilon_{k}\right| R^{k}+\sum_{k=N+1}^{\infty} 2\left|f^{(k)}(0)\right| R^{k}
$$

taking $N$ sufficiently large, the second term is less than $\varepsilon / 2$ (since the power series for $f$ converges absolutely and uniformly on the disc $|z| \leqslant R$ ). Let $n$ be so large that $\left|g_{n}^{(k)}(0)-\varepsilon_{k}\right|<\varepsilon / 2 M$ for $1 \leqslant k \leqslant N$, where

$$
M=\sum_{k=0}^{N} R^{k} .
$$

Thus, for such $n$, we have $\left|g_{n}(z)-g(z)\right|<\varepsilon$. Since this bound is independent of $z$, the convergence is uniform.

Solution to 5.2.19: We have

$$
\begin{aligned}
\log \left(\frac{z(2-z)}{1-z}\right) &=\log (2-z)+\log \left(\frac{1}{\frac{1}{z}-1}\right) \\
&=\log 2+\log \left(1-\frac{z}{2}\right)+\pi i+\log \left(\frac{1}{1-\frac{1}{z}}\right) .
\end{aligned}
$$

In the unit disc, the principal branch of $\log \left(\frac{1}{1-z}\right)$ is represented by the series $\sum_{n=1}^{\infty} \frac{z^{n}}{n}$, which one can obtain by termwise integration of the geometric series $\frac{1}{1-z}=\sum_{n=0}^{\infty} z^{n}$. Hence,

$$
\begin{aligned}
&\log \left(1-\frac{z}{2}\right)=-\sum_{n=1}^{\infty} \frac{z^{n}}{2^{n} n} \quad(|z|<2), \\
&\log \left(\frac{1}{1-\frac{1}{z}}\right)=\sum_{n=1}^{\infty} \frac{z^{-n}}{n} \quad(|z|>1),
\end{aligned}
$$

and

$$
\log \left(\frac{z(2-z)}{1-z}\right)=-\sum_{n=-\infty}^{-1} \frac{z^{n}}{n}+\log 2+\pi i-\sum_{n=1}^{\infty} \frac{z^{n}}{2^{n} n} \text { for } 1<|z|<2
$$

\section{$5.3$ Conformal Mappings}

Solution to 5.3.1: We will show that the given transformations also map straight lines into circles or straight lines.

$z \mapsto z+b$ and $z \mapsto k z$ clearly map circles and straight lines into circles and straight lines. Let $S=\{z|| z-\alpha \mid=r\}, \alpha=x_{0}+i y_{0}$, and $f(z)=1 / z=w=u+i v$. The equation for $S$ is

$$
(z-\alpha)(\bar{z}-\bar{\alpha})=r^{2}
$$

or

$$
\frac{1}{w \bar{w}}-\frac{\alpha}{\bar{w}}-\frac{\bar{\alpha}}{w}=r^{2}-|\alpha|^{2} .
$$

- If $r=|\alpha|$, that is, when $S$ contains the origin, we get

$$
1-\alpha w-\overline{\alpha w}=0
$$

or

$$
\Re(\alpha w)=\frac{1}{2}
$$

This is equivalent to

$$
u x_{0}-v y_{0}=\frac{1}{2}
$$

which represents a straight line.

- If $r \neq|\alpha|$, we obtain

$$
w \bar{w}-\left(\frac{\bar{\alpha}}{|\alpha|^{2}-r^{2}}\right) \bar{w}-\left(\frac{\alpha}{|\alpha|^{2}-r^{2}}\right) w=\frac{-1}{|\alpha|^{2}-r^{2}} .
$$

\section{Letting}

$$
\beta=\frac{\bar{\alpha}}{|\alpha|^{2}-r^{2}}
$$

we get

$$
w \bar{w}-\beta \bar{w}-\bar{\beta} w+|\beta|^{2}=\frac{r^{2}}{\left(|\alpha|^{2}-r^{2}\right)^{2}}
$$

and

$$
|w-\beta|^{2}=\left(\frac{r}{|\alpha|^{2}-r^{2}}\right)^{2}
$$

which represents the circle centered at $\beta$ with radius $r /\left(|\alpha|^{2}-r^{2}\right)$.

If $S$ is a straight line, then, for some real constants $a, b$, and $c$, we have, for $z=x+i y \in S$,

$$
a x+b y=c .
$$

Letting $\alpha=a-i b$, we get

$$
\Re(\alpha z)=c
$$

or

$$
\alpha z+\overline{\alpha z}=2 c
$$

and it follows, as above, that $f(S)$ is a straight line or a circle. Finally, let

$$
f(z)=\frac{a z+b}{c z+d} .
$$

If $c=0 f$ is linear, so it is the sum of two functions that map circles and lines into circles and lines, so $f$ itself has that mapping property. If $c \neq 0$, we have

$$
\frac{a z+b}{c z+d}=\frac{1}{c}\left(a-\frac{a d-b c}{c z+d}\right)
$$

so $f(z)=f_{3}\left(f_{2}\left(f_{1}(z)\right)\right)$ where

$$
f_{1}(z)=c z+d, \quad f_{2}(z)=\frac{1}{z}, \quad f_{3}(z)=\frac{a}{c}-\frac{a d-b c}{c} z,
$$

each of which has the desired property, and so does $f$.

Solution to 5.3.2: 1. The locus $|z+1|=|z-1|$ is the perpendicular bisector of the line segment joining $-1$ to 1 , that is, the imaginary axis. The set $|z+1|<|z-1|$ is then the set of points $z$ closer to $-1$ than to 1 , that is, the left half-plane $\Re z<0$. Hence, $\Re z<0$ iff $\left|\frac{z+1}{z-1}\right|<1$. The map $f: z \mapsto w=\frac{z+1}{z-1}$ maps $\{z \mid \mathfrak{\Re} z<0\}$ onto $\{w|| w \mid<1\}$, and is conformal as $f^{\prime} \neq 0$. The inverse map is easily seen to be $w \mapsto z=\frac{w+1}{w-1}$.

2. The map $f(z)=i z$ has the specified properties.

3. The map $z \mapsto w=z^{3}$ is conformal (when $z \neq 0$ ), and sends $\left\{z \mid 0<\arg z<\frac{\pi}{2}\right\}$ onto $\left\{w \mid 0<\arg w<\frac{3 \pi}{2}\right\}$. The inverse is the map $w \mapsto z=w^{1 / 3}=$ $\exp \left(\frac{1}{3}(\log |w|+i \arg w)\right)$ where $\arg w$ is chosen in $0<\arg w<\frac{3 \pi}{2}$.

Solution to 5.3.3: Let $A=\{z|| z|<1| z-,1 / 4 \mid>1 / 4\}, B=\{z|r<| z \mid<1\}$. Let $f(z)=(z-\alpha) /(\alpha z-1)$ be a linear fractional transformation mapping $A$ onto $B$, where $-1<\alpha<1$. We have

$$
f(\{z|| z-1 / 4 \mid=1 / 4\})=\{z|| z \mid=r\}
$$

so

$$
\{f(0), f(1 / 2)\}=\{-r, r\}
$$

and

$$
0=r-r=f(0)+f(1 / 2)=\alpha+\frac{1 / 2-\alpha}{\alpha / 2-1}
$$

which implies $\alpha=2-\sqrt{3}$. Therefore, $r=|f(0)|=2-\sqrt{3}$.

Suppose now that $g$ is a linear fractional transformation mapping $C=\{z|s<| z \mid<1\}$ onto $A$. Then $g^{-1}(\mathbb{R})$ is a straight line through the origin, because the real line is orthogonal to the circles $\{z|| z-1 / 4 \mid=1 / 4\}$ and $\{z|| z \mid=1\}$. Multiplying by a unimodular constant, we may assume $g^{-1}(\mathbb{R})=\mathbb{R}$. Then $f \circ g(C)=A$ and $f \circ g(\mathbb{R})=\mathbb{R}$. Replacing, if necessary, $g(z)$ by $g(s / z)$, we may suppose $f \circ g(\{z|| z \mid \leqslant 1\})=\{z|| z \mid \leqslant 1\}$, so

$$
f \circ g(z)=\beta \frac{z-\alpha}{\bar{\alpha} z-1} \quad \text { with } \quad|\alpha|<|\beta|=1 \text {. }
$$

Using the relation $0=f(s)+f(-s)$, we get $\alpha=0$, so $f \circ g(z)=\beta z$ and $s=r=2-\sqrt{3}$.

Solution to 5.3.4: Suppose $f$ is such a function. Let $g: A \rightarrow B$ be defined by $g(z)=f(z)^{2} / z$. Then, as on $C_{1} \cup C_{4}$, the absolute value of $g$ is 1 , then $g$ is a constant, $c$, say. Therefore, $f(z)=\sqrt{c z}$ which is not continuous on $A$. We conclude that no such function can exist.

Solution to 5.3.5: The map $z \mapsto$ iz maps the given region conformally onto $A=\mathbb{D} \cap\{z \mid \mathfrak{z}>0\}$. The map

$$
w \mapsto \frac{1+w}{1-w}
$$

maps $A$ onto the first quadrant, $Q$. The square function takes $Q$ onto $\{z \mid \Im z>0\}$. Finally,

$$
\xi \mapsto \frac{\xi-i}{\xi+i}
$$

takes $\{\xi \mid \Im \xi>0\}$ onto $\mathbb{D}$. Combining these, we get for the requested map:

$$
z \mapsto \frac{(1+i z)^{2}-i(1-i z)^{2}}{(1+i z)^{2}+i(1-i z)^{2}}
$$

Solution to 5.3.6: The map $\varphi_{1}(z)=2 z-1$ maps conformally the semidisc

$$
\left\{z|\mathfrak{I} z>0,| z-\frac{1}{2} \mid<\frac{1}{2}\right\}
$$

onto the upper half of the unit disc. The map

$$
\varphi_{2}(z)=\frac{1+z}{1-z}
$$

maps the unit disc conformally onto the right half-plane. Letting $z=r e^{i \theta}$, it becomes

$$
\frac{1+r e^{i \theta}}{1-r e^{i \theta}}=\frac{1-r^{2}+2 i r \sin \theta}{\left|1+r e^{i \theta}\right|^{2}} .
$$

Since $\sin \theta>0$ for $0<\theta<\pi, \varphi_{2}$ maps the upper half of $\mathbb{D}$ onto the upperright quadrant. The map $\varphi_{3}(z)=z^{2}$ maps the upper-right quadrant conformally onto the upper half-plane. The composition of $\varphi_{1}, \varphi_{2}$, and $\varphi_{3}$ is the desired map, namely the function $z \mapsto \frac{z^{2}}{(1-z)^{2}}$.

Solution to 5.3.7: Let $R$ be the given domain. The map

$$
z \mapsto \frac{1}{z}=\xi
$$

transforms $R$ onto the vertical strip $S=\left\{\xi \in \mathbb{C} \mid \frac{1}{2}<\mathfrak{R} \xi<1\right\}$. Now

$$
\xi \mapsto 2 \pi i\left(\xi-\frac{1}{2}\right)=\eta
$$

maps $S$ onto the horizontal strip $T=\{\eta \in \mathbb{C} \mid 0<\Re \eta<\pi\}$. Finally

$$
\eta \mapsto e^{\eta}=\rho
$$

maps $T$ onto the upper half-plane $U=\{\rho \in \mathbb{C} \mid \mathfrak{\Im} \rho>0\}$. Putting everything together we get the conformal map

$$
z \mapsto e^{2 \pi i\left(\frac{1}{2}-\frac{1}{2}\right)}
$$

Solution 2. Let $C_{1}, C_{2}$ denote the circles $|z-1|=1,\left|z-\frac{1}{2}\right|=\frac{1}{2}$ respectively. Consider the map $z \mapsto \zeta=1 / z$. $C_{1}$ contains the points $0,1+i, 2$ which are sent to $\infty, \frac{1}{2}-\frac{1}{2} i, \frac{1}{2}$ respectively. Thus $C_{1}$ is sent to the line $\Re \zeta=\frac{1}{2}$, and the inside of $C_{1}$ is sent to $\Re \zeta>\frac{1}{2}$ (consider the image of 1). $C_{2}$ contains the points $0, \frac{1}{2}+\frac{1}{2} i, 1$ which are sent to $\infty, 1-i, 1$; thus $C_{2}$ is sent to the line $\Re \zeta=1$ and the outside of $C_{2}$ is sent to $\Re \zeta<1$ (consider the image of 2). Thus the lune is sent to the strip $\frac{1}{2}<\Re \zeta<1$. We follow with the map $\zeta \mapsto w=\exp (-2 \pi i \zeta)$ which sends this strip to the upper half-plane. Composing, we obtain the map

$$
z \mapsto \exp \left(-\frac{2 \pi i}{z}\right)
$$

which is a conformal map from $\{z \in \mathbb{C}|| z-1 \mid \leqslant 1\} \cap\left\{z \in \mathbb{C}|| z-\frac{1}{2} \mid \leqslant \frac{1}{2}\right\}$ onto the upper half-plane.

Solution to 5.3.8: Suppose $f$ is such a map. $f$ is bounded, so the singularity at the origin is removable, $p=\lim _{z \rightarrow 0} f(z)$. Since $f$ is continuous, $p$ is in the closure of $A$.

Suppose that $p$ is on the boundary of $A$. Then $f(G)=A \cup\{p\}$, which is not an open set, contradicting the Open Mapping Theorem, [MH87, p. 436].

Let $p \in A$ and $a \in G$ be such that $f(a)=p$. Take disjoint open neighborhoods $U$ of 0 and $V$ of $a$. By the Open Mapping Theorem, $f(U)$ and $f(V)$ are open sets containing $p$. Then $f(U) \cap f(V)$ is a nonempty open set. Take $x \in f(U) \cap f(V)$, $x \neq p$. Then $x=f(z)$ for some nonzero $z \in U$ and $x=f(w)$ for some $w \in V$. Then $z$ and $w$ are distinct elements of $G$ with $f(z)=f(w)$, contradicting the injectivity of $f$.

Solution to 5.3.9: Let $\omega$ be a primitive $n^{t h}$ root of unity. The transformations $\psi_{j}(z)=\omega^{j} z, j=0, \ldots, n-1$, form a cyclic group of order $n$ and fix the points $z=0$ and $z=\infty$. Let $\chi(z)=\frac{z+1}{z-1}$. Then $\chi(1)=\infty, \chi(-1)=0$, and $\chi^{-1}=\chi$. The transformations $\varphi_{j}=\chi \circ \psi_{j} \circ \chi, j=0, \ldots, n-1$, thus form a cyclic group of order $n$ that fix the points 1 and $-1$. We have explicitly

$$
\varphi_{j}(z)=\frac{\omega^{j}\left(\frac{z+1}{z-1}\right)+1}{\omega^{j}\left(\frac{z+1}{z-1}\right)-1}=\frac{\left(\omega^{j}+1\right) z+\omega^{j}-1}{\left(\omega^{j}-1\right) z+\omega^{j}+1}=\frac{z+\left(\frac{\omega^{j}-1}{\omega^{j}+1}\right)}{\left(\frac{\omega^{j}-1}{\omega^{j}+1}\right) z+1} .
$$



\subsection{Functions on the Unit Disc}

Solution to 5.4.1: The function $f$ is continuous on $S_{\alpha}$, and $\bar{S}_{\alpha}=S_{\alpha} \cup\{0\}$ is compact, so it will be enough to show that $f$ has a continuous extension to $\bar{S}_{\alpha}$. It will be shown that $f_{\alpha}(z) \rightarrow 0$ as $z \rightarrow 0$ from within $S_{\alpha}$. We have

$$
\left|f_{\alpha}\left(r e^{i \theta}\right)\right|=\left|\exp \left(\frac{-e^{-i \theta}}{r}\right)\right|=\left|\exp \left(\frac{-\cos \theta+i \sin \theta}{r}\right)\right|=\exp \left(\frac{-\cos \theta}{r}\right)
$$

Inside $S_{\alpha}$ we have $\cos \theta \geqslant \cos \alpha$, so

$$
|f(z)| \leqslant \exp \left(\frac{-\cos \alpha}{|z|}\right) \rightarrow 0,
$$

when $z \rightarrow 0$, as desired.

Solution to 5.4.2: Let $b / a=r e^{i \beta}$ and consider the function $g$ defined by $g(z)=f(z) a^{-1} e^{-i \beta / 2}$. We have

$$
\begin{aligned}
g\left(e^{i \theta}\right) &=e^{i(\theta-\beta / 2)}+r e^{i(\beta / 2-\theta)} \\
&=(1+r) \cos (\theta-\beta / 2)+i(1-r) \sin (\theta-\beta / 2)
\end{aligned}
$$

so the image of the unit circle under $g$ is the ellipse in standard position with axes $1+r$ and $|1-r|$. As $f(z)=a \exp (i \beta / 2) g(z), f$ maps the unit circle onto the ellipse of axes $|a|(1+r)$ and $|a(1-r)|$, rotated from the standard position $\arg a+\beta / 2$.

Solution to 5.4.3: We have

$$
\begin{aligned}
L &=\int_{0}^{2 \pi}\left|f^{\prime}\left(e^{i \theta}\right)\right|\left|i e^{i \theta}\right| d \theta=\int_{0}^{2 \pi}\left|f^{\prime}\left(e^{i \theta}\right)\right| d \theta \\
& \geqslant\left|\int_{0}^{2 \pi} f^{\prime}\left(e^{i \theta}\right) d \theta\right| \\
&=2 \pi\left|f^{\prime}(0)\right|
\end{aligned}
$$

by the Mean Value Property [MH87, p. 185].

Solution to 5.4.4: As the Jacobian of the transformation is $\left|f^{\prime}(z)\right|^{2}$, we have

$$
A=\int_{\mathbb{D}}\left|f^{\prime}(z)\right|^{2} d x d y .
$$

$f^{\prime}(z)$ can be found by term by term differentiation:

$$
f^{\prime}(z)=\sum_{n=1}^{\infty} n c_{n} z^{n-1}
$$

so

$$
\left|f^{\prime}(z)\right|^{2}=\sum_{j, k=1}^{\infty} j k c_{j} \bar{c}_{k} z^{j-1} \bar{z}^{k-1}
$$

We then have

$$
A=\iint_{\mathbb{D}} \sum_{j, k=1}^{\infty} j k c_{j} \bar{c}_{k} z^{j-1} \bar{z}^{k-1} d x d y
$$

Letting $z=r e^{i \theta}$, we get

$$
A=\sum_{j, k=1}^{\infty} j k c_{j} \bar{c}_{k} \int_{0}^{1} \int_{0}^{2 \pi} r^{j+k-1} e^{i(j-k) \theta} d \theta d r
$$

Since

$$
\int_{0}^{2 \pi} e^{i n \theta} d \theta=0
$$

for $n \neq 0$, we have

$$
A=2 \pi \sum_{n=1}^{\infty} n^{2}\left|c_{n}\right|^{2} \int_{0}^{1} r^{2 n-1} d r=\pi \sum_{n=1}^{\infty} n\left|c_{n}\right|^{2}
$$

Solution to 5.4.5: We have, for $z, w \in \mathbb{D}$,

$$
f(w)=f(z) \text { iff }(w-z)\left(1+\frac{w+z}{2}\right)=0
$$

so $f$ in injective. Then the area of its image is given by

$$
\begin{aligned}
\int_{\mathbb{D}}\left|f^{\prime}(z)\right|^{2} d x d y &=\int_{\mathbb{D}}\left(1+2 \mathfrak{I} z+|z|^{2}\right) d z \\
&=\int_{\mathbb{D}}\left(1+2 x+x^{2}+y^{2}\right) d x d y \\
&=\int_{0}^{2 \pi} \int_{0}^{1}\left(1+2 r \cos \theta+r^{2}\right) d r d \theta \\
&=\frac{3 \pi}{2}
\end{aligned}
$$

Solution to 5.4.6: Fix $z_{0} \in \mathbb{D}$ and let $h$ be defined by $h(z)=f(z)-f\left(z_{0}\right)$. As $h$ has only isolated zeros in $\mathbb{D}$, we can find an increasing sequence $\rho_{i} \rightarrow 1$ with $h(z) \neq 0$ for $|z|=\rho_{i}, i=1, \ldots$ Let $g$ be the function given by $g(z)=a_{1}\left(z-z_{0}\right)$. For $|z|=\rho_{i}$, we have

$$
|g(z)-h(z)|=\left|\sum_{n \geqslant 2} a_{n} z^{n}-\sum_{n \geqslant 2} a_{n} z_{0}^{n}\right|
$$



$$
\begin{aligned}
&\left.\leqslant \max \left|\frac{d}{d z} \sum_{n \geqslant 2} a_{n} z^{n}\right|_{z=w}|| z-z_{0} \mid \quad \text { ( } w \text { in the segment }\left[z, z_{0}\right]\right) \\
&\leqslant \sum_{n \geqslant 2} n a_{n} \rho_{i}^{n-1}\left|z-z_{0}\right| \\
&\leqslant\left|a_{1}\right|\left|z-z_{0}\right| \\
&=|g(z)| .
\end{aligned}
$$

By Rouché's Theorem [MH87, p. 421], $h$ has a unique zero in the disc $\left\{z|| z \mid<\rho_{i}\right\}$, so $f$ assumes the value $f\left(z_{0}\right)$ only once there. Letting $\rho_{i} \rightarrow 1$, we get that $f$ is injective in $\mathbb{D}$.

Solution to 5.4.7: Let $\psi$ be a linear fractional transformation which maps $\mathbb{D}$ onto itself with $\psi(0)=z_{1}$. Then the conjugate $g=\psi^{-1} \circ f \circ \psi$ is analytic in the unit disc $f$, and has two fixed points, namely $0=\psi^{-1}\left(z_{1}\right)$ and $w=\psi^{-1}\left(z_{2}\right)$.

As $g(0)=0$ we can define the analytic function $h$ by $h(z)=g(z) / z$. For each $0<\varepsilon<1$, on the circle $|z|=1-\varepsilon$, we have $|h(z)|=|g(z) /| z \mid \leqslant 1 /(1-\varepsilon)$, so the maximum modulus principle implies $|h(z)| \leqslant 1 /(1-\varepsilon)$ on $|z| \leqslant 1-\varepsilon$. As $\varepsilon$ can be arbitrarily small, we have $|h(z)| \leqslant 1$ for any $z \in \mathbb{D}$.

We know that $h(w)=1$, so $h$ attains its maximum inside $\mathbb{D}$. By the Maximum Modulus Principle $h$ must be constant, that is, $h$ is identically 1 , and in this case $g(z)=f(z)=z$

Solution to 5.4.8: Let $f \in X_{k}, z \in \mathbb{D}$, and $\gamma$ be the circle around $z$ with radius $r=(1-|z|) / 2 . \gamma$ lies inside the unit disc, so, by Cauchy's Integral Formula for derivatives [MH87, p. 169], we have

$$
\left|f^{\prime}(z)\right| \leqslant \frac{1}{2 \pi} \int_{\gamma} \frac{|f(w)|}{|z-w|^{2}}|d w| \leqslant \frac{C}{r(1-|z|)^{k}}=\frac{2 C}{(1-|z|)^{k+1}},
$$

where $C$ is a constant, so $f^{\prime} \in X_{k+1}$.

Let $f^{\prime} \in X_{k+1}$ with $f(0)=0$ (the general case follows easily from this). Letting $z=r e^{i \theta}$, we have

$$
\begin{aligned}
|f(z)| & \leqslant \int_{0}^{z}\left|f^{\prime}(w)\right||d w| \\
&=\int_{0}^{r}\left|f^{\prime}\left(t e^{i \theta}\right)\right| d t \\
& \leqslant \int_{0}^{r} \frac{C}{(1-r)^{k+1}} d t \\
&=\frac{k C}{(1-r)^{k}} .
\end{aligned}
$$

Hence, $f \in X_{k}$.

Solution to 5.4.9: Let $f(z)=\sum_{n \geqslant 0} a_{n} z^{n}$ for $z \in \mathbb{D}=\{z|| z \mid<1\}$. As $f(z)$ is analytic, so is $g: \mathbb{D} \rightarrow \mathbb{C}$ defined by $g(z)=f(z)-f(z)$. We have, for real $z \in \mathbb{D}$,

$$
\begin{aligned}
g(z) &=\sum\left(a_{n}-\bar{a}_{n}\right) z^{m} \\
&=f(z)-\overline{f(\bar{z})} \\
&=f(z)-f(z) \\
&=0
\end{aligned}
$$

therefore, $g(z) \equiv 0$ on $\mathbb{D}$, so the coefficients $a_{n}$ are all real.

Put $z_{0}=e^{i \pi \sqrt{2}}$, and let $a_{k}$ be the nonzero coefficient of smallest index. We have

$$
\frac{f\left(t z_{0}\right)-a_{0}}{a_{k} t^{k}}=z_{0}^{k}+\frac{t}{a_{k}} \sum_{n \geqslant k+1} a_{n} z_{0}^{n} t^{n-k+1} .
$$

$t z_{0} \in \mathbb{D}$ for $t \in[0,1)$ and the left-hand side expression is a real number for all $t$, so

$$
\lim _{t \rightarrow 0} \frac{f\left(t z_{0}\right)-a_{0}}{a_{k} t^{k}}=z_{0}^{k} \in \mathbb{R}
$$

which implies $k=0$, by the irrationality of $\sqrt{2}$, thus $f$ is a constant.

Solution to 5.4.10: Let $\sum_{n=0}^{\infty} c_{n} z^{n}$ be the Maclaurin series for $f$. Then $f^{\prime}(z)=\sum_{n=1}^{\infty} n c_{n} z^{n-1}$. The Cauchy Inequalities [MH87, p. 170] give

$$
\begin{aligned}
\left|c_{n}\right| &=\left|\frac{1}{2 \pi i} \int_{|z|=r} \frac{f^{\prime}(z)}{z^{n}} d z\right| \\
&=\left|\frac{1}{2 \pi r^{n-1}} \int_{0}^{2 \pi} f^{\prime}\left(r e^{i \theta}\right) e^{-i(n-1) \theta} d \theta\right| \\
& \leqslant \frac{M}{r^{n-1}}, \quad 0<r<1 .
\end{aligned}
$$

Letting $r \rightarrow 1$, we get $\left|c_{n}\right| \leqslant \frac{M}{n}(n=1,2, \ldots)$. Hence,

$$
\begin{aligned}
\int_{[0,1)}|f(x)| d x & \leqslant \int_{0}^{1}\left(\sum_{n=0}^{\infty}\left|c_{n}\right| x^{n}\right) d x \\
& \leqslant\left|c_{0}\right|+M \int_{0}^{1}\left(\sum_{n=0}^{\infty} \frac{x^{n}}{n}\right) d x \\
&=\left|c_{0}\right|+M \int_{0}^{1} \log \frac{1}{1-x} d x \\
&=\left|c_{0}\right|+\left.M \lim _{r \rightarrow 1}\left(-(1-x) \log \frac{1}{1-x}+x\right)\right|_{0} ^{r} \\
&=\left|c_{0}\right|+M .
\end{aligned}
$$

Solution to 5.4.11: As $|h(0)|=5$, by the Maximum Modulus Principle [MH87, p. 185], $h$ is constant in the unit disc. Therefore, $h^{\prime}(0)=0$. Solution to 5.4.12: For $r=\log 2$,

$$
T(z)=\frac{z-r}{1-r z}
$$

maps the closed unit disc onto itself with $T(0)=-r$. Then $g(z)=f(T(z))$ is analytic on the closed unit disc and $g(0)=0$. For $z$ on the unit circle we have $|g(z)| \leqslant\left|e^{T(z)}\right|$. Therefore $h(z)=e^{-T(z)} g(z)$ is analytic in the closed unit disc, $h(0)=0$, and $|h(z)| \leqslant 1$ for all $z$ with $|z|=1$. By Schwartz Lemma, [MH87, p. 190], $|h(z)| \leqslant|z|$ for all $z$ with $|z| \leqslant 1$, hence $|f(T(z))| \leqslant|z|\left|e^{T(z)}\right|$ on the closed unit disc. If $w=T(z)$ then $z=-T(-w)$, giving $|f(z)| \leqslant\left|e^{z} T(-z)\right|$. Setting $z=r=\log 2$, we get

$$
|f(\log 2)| \leqslant e^{r} \frac{2 r}{1+r^{2}}=\frac{4 \log 2}{1+(\log 2)^{2}}
$$

This bound is attained by the function $\frac{z+\log 2}{1+z \log 2} e^{z}$.

Solution to 5.4.13:

$$
\varphi(z)=i\left(\frac{1+z}{1-z}\right)
$$

maps the unit disc to the upper half-plane with $\varphi(0)=i$. Thus, $f \circ \varphi$ maps the unit disc into itself fixing 0. By the Schwarz Lemma [MH87, p. 190], $|f \circ \varphi(z)| \leqslant|z|$. Solving $\varphi(z)=2 i$, we get $z=1 / 3$. Hence, $|f(2 i)| \leqslant 1 / 3$. Letting $f=\varphi^{-1}$, we see that this bound is sharp.

Solution to 5.4.14: The function

$$
\varphi(z)=\frac{1+z}{1-z}
$$

maps the unit disc, $\mathbb{D}$, onto the right half-plane, with $\varphi(0)=1$. Therefore, the function $f \circ \varphi$ maps $\mathbb{D}$ into itself, with $f \circ \varphi(0)=0$. By the Schwarz Lemma [MH87, p. 190], we have $\left|(f \circ \varphi)^{\prime}(0)\right| \leqslant 1$, which gives $\left|f^{\prime}(\varphi(0)) \varphi^{\prime}(0)\right| \leqslant 1$ and $\left|f^{\prime}(1)\right| \leqslant 1 / 2$. A calculation shows that equality happens for $f=\varphi^{-1}$.

Solution to 5.4.15: Suppose $f$ has infinitely many zeros in $\mathbb{D}$. If they have a cluster point in $\mathbb{D}$, then $f \equiv 0$ and the result is trivial. Otherwise, since $\{z \in \mathbb{C}|| z \mid \leqslant 1\}$ is compact, there is a sequence of zeros converging to a point in the boundary of $\mathbb{D}$, and the conclusion follows.

Assume now that $f$ has only finitely many zeros in $\mathbb{D}, w_{1}, \ldots, w_{m}$. Then $f$ can be written as

$$
f(z)=\left(z-w_{1}\right)^{\alpha_{1}} \cdots\left(z-w_{m}\right)^{\alpha_{m}} g(z)
$$

where $g$ is analytic and never zero on $\mathbb{D}$. Applying the Maximum Modulus Principle [MH87, p. 185], we get that $1 / g$ attains a maximum in the disc $|z| \leqslant(1-1 / n)$ at a point $z_{n}$ with $\left|z_{n}\right|=1-1 / n(n \geqslant 2)$. Then $\left|g\left(z_{2}\right)\right| \geqslant\left|g\left(z_{3}\right)\right| \geqslant \cdots$. The product $\left(z-w_{1}\right)^{\alpha_{1}} \cdots\left(z-w_{m}\right)^{\alpha_{m}}$ is clearly bounded, and so is $f\left(z_{n}\right)$.

Solution to 5.4.16: 1 . We can assume $f$ has only finitely many zeros. (Otherwise, assuming $f \not \equiv 0$, its zero sequence has the required property, since the zeros of a nonconstant analytic function in an open connected set can cluster only on the boundary of the set.) That done, we can, after replacing $f$ by its quotient with a suitable polynomial, assume $f$ has no zeros. Then $1 / f$ is analytic in the disc. For $n=1,2, \ldots$, let $M_{n}$ be the maximum of $|1 / f(z)|$ for $|z|=1-\frac{1}{n}$. By the Maximum Modulus Principle [MH87, p. 185], $M_{n} \geqslant M_{1}$ for all $n$. Hence, for each $n$, there is a point $a_{n}$ such that $\left|a_{n}\right|=1-\frac{1}{n}$ and $\left|f\left(a_{n}\right)\right|=1 / M_{n} \leqslant 1 / M_{1}=$ $|f(0)|$. Then $\left(f\left(a_{n}\right)\right)$ is a bounded sequence of complex numbers and so has a convergent subsequence, which gives the desired conclusion.

2. Let $\left(z_{n}\right)$ be a sequence with the properties given in Part 1. Subtracting a constant from $f$, if needed, we can assume $\lim f\left(z_{n}\right)=0$. We can suppose also that $\left|z_{n+1}\right|>\left|z_{n}\right|>0$ for all $n$. For each $n$, let $M_{n}$ be the maximum of $|f(z)|$ for $|z|=\left|z_{n}\right|$. The numbers $M_{n}$ are positive (since $f$ is nonconstant) and increase with $n$ (by the Maximum Modulus Principle [MH87, p. 185]). Since $f\left(z_{n}\right) \rightarrow 0$, there is an $n_{0}$ such that $\left|f\left(z_{n}\right)\right|<M_{1}$ for $n \geqslant n_{0}$. For such $n$, the restriction of $|f|$ to the circle $|z|=\left|z_{n}\right|$ is a continuous function that takes values both larger than $M_{1}$ and smaller than $M_{1}$. By the Intermediate Value Theorem [Rud87, p. 93], there is for each $n \geqslant n_{0}$, a point $b_{n}$ such that $\left|b_{n}\right|=\left|z_{n}\right|$ and $\left|f\left(b_{n}\right)\right|=M_{1}$. Then, for the desired sequence $\left(w_{n}\right)$, we can take any subsequence of $\left(b_{n}\right)$ along which $f$ converges. (There will be such a subsequence by the boundedness of the sequence $\left(f\left(b_{n}\right)\right)$.)

Solution to 5.4.17: Suppose $f(a)=a \in \mathbb{D}, f(b)=b \in \mathbb{D}$, and $a \neq b$. Let $\varphi: \mathbb{D} \rightarrow \mathbb{D}$ be the automorphism of the unit disc that maps 0 to $a$, that is, $\varphi(z)=(a-z) /(1-\bar{a} z)$. Then the function $g=\varphi^{-1} \circ f \circ \varphi$ maps $\mathbb{D}$ into itself with $g(0)=0$ and $g\left(\varphi^{-1}(b)\right)=\varphi^{-1}(b)$. Since $\varphi$ is one-to-one and $a \neq$ $b, \varphi^{-1}(b) \neq 0$. Hence, by the Schwarz Lemma [MH87, p. 190], there exists a unimodular constant $\lambda$ such that $g(z)=\lambda z$, and letting $z=\varphi^{-1}(b)$, we see that $\lambda=1$; that is, $g$ is the identity map and so is $f$.

Solution to 5.4.18: Using Cauchy's Integral Formula [MH87, p. 167], for $0<r<1$, we have

$$
\begin{aligned}
\left|\frac{f^{(n)}(0)}{n !}\right| & \leqslant \frac{1}{2 \pi} \int_{|w|=r} \frac{|f(w)|}{|w|^{n+1}}|d w| \\
& \leqslant \frac{1}{2 \pi} \int_{|w|=r} \frac{1}{(1-r) r^{n+1}}|d w|=\frac{1}{(1-r) r^{n}} .
\end{aligned}
$$

Letting $r=n /(n+1)$, we get $\left|f^{(n)}(0) / n !\right| \leqslant(n+1)(1+1 / n)^{n}<(n+1) e$.

Solution to 5.4.19: By the Schwarz Lemma [MH87, p. 190],

$$
|f(z)| \leqslant|z| \text {. }
$$

If $f(z)=a_{1} z+a_{2} z^{2}+\cdots$, let $g$ be defined by

$$
g(z)=\frac{f(z)+f(-z)}{2 z}=a_{2} z+a_{4} z^{3}+a_{6} z^{5}+\cdots
$$

$g$ is analytic in $\mathbb{D}$, and since

$$
|g(z)| \leqslant \frac{|f(z)|+|f(-z)|}{2|z|} \leqslant 1
$$

$g$ maps $\mathbb{D}$ into $\mathbb{D}$. Hence, by the Schwarz Lemma,

$$
|g(z)| \leqslant|z|
$$

or

$$
|f(z)+f(-z)| \leqslant 2|z|^{2} .
$$

Now suppose equality held for $z_{0} \in \mathbb{D}$. We would have $\left|g\left(z_{0}\right)\right|=\left|z_{0}\right|$ so, by the Schwarz Lemma,

$$
g(z)=\lambda z
$$

for some unimodular $\lambda$, or

$$
f(z)+f(-z)=2 \lambda z^{2} .
$$

Plugging this back into the power series for $g(z)$, we get $a_{2}=\lambda$ and $a_{4}=a_{6}=$ $\cdots=0$. Hence,

$$
f(z)=\lambda z^{2}+h(z)
$$

where $h(z)$ is odd. We have

$$
1 \geqslant|f(z)|=\left|\lambda z^{2}+h(z)\right|
$$

and

$$
1 \geqslant|f(-z)|=\left|\lambda z^{2}+h(-z)\right|=\left|\lambda z^{2}-h(z)\right| .
$$

Therefore,

$$
\begin{aligned}
&\left(\lambda z^{2}+h(z)\right)\left(\overline{\lambda z^{2}}+\overline{h(z)}\right) \leqslant 1 \\
&\left(\lambda z^{2}-h(z)\right)\left(\overline{\lambda z^{2}}-\overline{h(z)}\right) \leqslant 1 .
\end{aligned}
$$

Expanding and adding, we get

$$
\begin{aligned}
|z|^{4}+|h(z)|^{2} & \leqslant 1 \\
|h(z)|^{2} & \leqslant 1-|z|^{4}
\end{aligned}
$$

which, by the Maximum Modulus Principle [MH87, p. 185], implies $h(z) \equiv 0$.

Solution to 5.4.20: Schwarz's Lemma [MH87, p. 190] implies that the function $f_{1}(z)=f(z) / z$ satisfies $\left|f_{1}(z)\right| \leqslant 1$. The linear fractional map $z \mapsto \frac{z-r}{1-r z}$ sends the unit disc onto itself. Applying Schwarz's Lemma to the function $f_{2}(z)=f_{1}\left(\frac{z-r}{1-r z}\right)$, we conclude that the function $f_{3}(z)=f_{1}(z) /\left(\frac{z-r}{1-r z}\right)$ satisfies $\left|f_{3}(z)\right| \leqslant 1$. Similarly, the map $z \mapsto \frac{z+r}{1+r z}$ sends the unit disc onto itself, and Schwarz's Lemma applied to the function $f_{4}(z)=f_{3}(z) /\left(\frac{z+r}{1+r z}\right)$ implies that the function $f_{5}(z)=f_{3}\left(\frac{z+r}{1+r z}\right)$ satisfies $\left|f_{5}(z)\right| \leqslant 1$. All together, then,

$$
|f(z)| \leqslant|z|\left|\frac{z-r}{1-r z}\right|\left|\frac{z+r}{1+r z}\right|\left|f_{5}(z)\right| \leqslant|z|\left|\frac{z-r}{1-r z}\right|\left|\frac{z+r}{1+r z}\right|
$$

which is the desired inequality.

Solution to 5.4.21: Let $\varphi_{20}$ be the automorphism of the unit disc given by

$$
\varphi_{z_{0}}(z)=\frac{z-z_{0}}{1-\overline{z_{0}} z},
$$

we have

$$
\varphi_{z 0}^{\prime}(z)=\frac{1-\left|z_{0}\right|^{2}}{\left(1-\overline{z_{0}} z\right)^{2}} .
$$

Now consider the composition

$$
g(z)=\varphi_{f \circ \varphi_{z_{0}}(0)} \circ f \circ \varphi_{z_{0}}(z)=\varphi_{f\left(-z_{0}\right)} \circ f \circ \varphi_{z_{0}}(z)
$$

then $g(0)=0$ and as composition of maps of the unit disc into itself, we can apply the Schwarz Lemma [MH87, p. 190] to obtain $\left|g^{\prime}(0)\right| \leqslant 1$. Computing $g^{\prime}(0)$ using the chain rule, we have

$$
\left|g^{\prime}(0)\right|=\left|\frac{1}{1-\left|f\left(-z_{0}\right)\right|^{2}} \cdot f^{\prime}\left(-z_{0}\right) \cdot\left(1-\left|z_{0}\right|^{2}\right)\right| \leqslant 1
$$

so we can conclude that

$$
\left|f^{\prime}(z)\right| \leqslant \frac{1-|f(z)|^{2}}{1-|z|^{2}} \leqslant \frac{1}{1-|z|^{2}} .
$$

The first inequality is known as Picks' Lemma and is the main ingredient in the proof that an analytic map of the disc that preserves the hyperbolic distance between any two points, preserves all distances, for more detail see [Car60, Vol. 2 , $\S 290]$ or [Kra90, p. 16].

Solution 2. Using the same notation as above,

$$
\left|\left(f \circ \varphi_{z_{0}}\right)^{\prime}(0)\right| \leqslant \frac{1}{2 \pi} \int_{|\omega|=r} \frac{\left|f \circ \varphi_{z_{0}}(w)\right|}{|\omega|^{2}}|d \omega|
$$

that is

$$
\left.\left|f^{\prime}\left(-z_{0}\right)\right||1-| z_{0}\right|^{2} \mid \leqslant \frac{1}{2 \pi} \int_{0}^{2 \pi} \frac{d \theta}{r}=\frac{1}{r}
$$

which holds for any $\left|z_{0}\right|<r=|w|<1$, so the conclusion follows.

Solution to 5.4.22: Fix $z \in \mathbb{D}$ and let $\gamma$ be the circle centered at $z$ parametrized by

$$
w=z+\frac{1-|z|}{2} e^{i \theta}
$$

which is completely contained inside the unit disc. Using Cauchy's Integral Formula for derivatives [MH87, p. 169], we have

$$
f^{\prime}(z)=\frac{1}{2 \pi i} \int_{\gamma} \frac{f(w)}{(w-z)^{2}} d w .
$$

taking absolute values and applying the inequality we get

$$
\left|f^{\prime}(z)\right| \leqslant \frac{1}{2 \pi} \int_{\gamma} \frac{C}{|1-w||w-z|^{2}}|d w| .
$$

From the parametrization we cann see that $|d w|=\frac{|1-| z||}{2} d \theta$, and obtain

$$
\left|f^{\prime}(z)\right| \leqslant \frac{C}{\pi} \int_{0}^{2 \pi} \frac{1-|z|}{(1-|w|)(1-|z|)^{2}} d \theta .
$$

Now $1-|w|$ is minimum at the point of $\gamma$ farthest from the origin, at which we have $1-|w|=(1-|z|) / 2$, therefore,

$$
\left|f^{\prime}(z)\right| \leqslant \frac{C}{\pi} \int_{0}^{2 \pi} \frac{2(1-|z|)}{(1-|z|)(1-|z|)^{2}} d \theta=\frac{4 C}{(1-|z|)^{2}} .
$$

\subsection{Growth Conditions}

Solution to 5.5.1: Consider the function $g(z)=e^{-f(z)}$, it is entire with $|g(z)|=$ $e^{\Re f(z)} \leqslant e^{-2}$. Liouville's Theorem implies that $g$ is constant, and since it is obviously a nonzero contant, $f$ maps the connected set $\mathbb{C}$ into the discrete set of the logarithms, hence $f$ itself is constant.

Solution to 5.5.2: Let $g(z)=f(z)-f(0)$. Then $g(0)=0$, so $g(z) / z$ has a removable singularity at 0 and extends to an entire function. $g(z) / z$ tends to 0 as $|z|$ tends to infinity since $f(z) / z$ does. Let $\varepsilon>0$. There is an $R>0$ such that $|g(z) / z|<\varepsilon$ for $|z| \geqslant R$. By the Maximum Modulus Principle [MH87, p. 185], $|g(z) / z|<\varepsilon$ for all $z$. Since $\varepsilon$ is arbitrary, $g(z) / z$ is identically 0 . Hence, $g(z)=0$ for all $z$ and $f$ is constant.

Solution to 5.5.3: Let $f=p / q$ where $p, q$ are polynomials with $q$ zero free in the upper half plane. For positive $R$ consider the curve $\Gamma_{R}=[-R, R] \cup C_{R}$, where $C_{R}$ is the upper semicircle centered at the origin, with radius $R$. By the Maximum Modulus Principle [MH87, p. 185], we have

$$
\sup \{|f(z)| \mid \Im z \geqslant 0\}=\lim _{R \rightarrow \infty} \sup \left\{|f(z)| \mid z \in \Gamma_{R}\right\} .
$$

Let's call this value $M$. If $\operatorname{deg} q>\operatorname{deg} p$, we have $\lim _{|z| \rightarrow \infty}|f(z)|=0$ so the result is clear. If $\operatorname{deg} q \leqslant p$, then $\lim _{|z| \rightarrow \infty}|f(z)|=M$ and the result follows also.

Solution 2. Case 1.f has a pole at $\infty$. Then $\lim _{z \rightarrow \infty} f(z)=\infty$, so both the suprema equal $\infty$

Case 2. f does not have a pole at $\infty$. Then $f$ has a finite limit at $\infty$, so $f$ extends continuously to the closure of $H=\{z \in \mathbb{C} \mid \Im z>0\}$ in the extended plane. Since that closure is compact, $|f|$ attains a maximum there. By the Maximum Modulus Theorem, $|f|$ cannot attain a local maximum in $H$ unless $f$ is constant. Thus, the maximum of $|f|$ on the extended closure of $H$ is attained at a point of the real axis or at $\infty$. In either case the desired equality holds.

Solution to 5.5.4: The function $g(z)=f(z) / \sin z$ is analytic in the open connected set $G=\mathbb{C} \backslash \pi \mathbb{Z}$. Write $g=u+i v$. By hypothesis, $u^{2}+v^{2}=1$ in $G$. Taking partial derivatives, we have

$$
u u_{x}+v v_{x}=0, \quad u u_{y}+v v_{y}=0 .
$$

Using the Cauchy-Riemann equations we get

$$
u u_{x}-v u_{y}=0, \quad u u_{y}+v u_{x}=0 .
$$

From these we get $u_{x}\left(u^{2}+v^{2}\right)=0$ and $u_{y}\left(u^{2}+v^{2}\right)=0$. Thus $u_{x}=0=u_{y}$ on $G$. It follows that $v_{x}=0=v_{y}$ on $G$. By the Mean Value Theorem, both $u$ and $v$ are constant on every line segment lying in $G$ parallel to the real or imaginary axis. $G$ is step connected: that is, given any two points $z_{0}, z_{1}$ in $G$ there is a path in $G$ from $z_{0}$ to $z_{1}$, consisting of line segments parallel to the axes. Thus $u$ and $v$ are constant in $G$, whence $g=C$ in $G$, for some constant $C$, with $|C|=|g|=1$. That is, $f(z)=C \sin z$ in $G$. By continuity this equality holds in $\mathbb{C}$.

Solution to 5.5.5: If $g \equiv 0$, the result is trivially true. Otherwise, the zeros of $g$ are isolated points. $|f / g|$ is bounded by 1 in $\mathbb{C}$, so all the singularities of $f / g$ are removable, and $f / g$ can be extended to an entire function. Liouville's Theorem [MH87, p. 170] now guarantees that $\mathrm{f} / \mathrm{g}$ must be a constant.

Solution to 5.5.6: Since $p$ and $q$ are of the same degree, the ratio $p / q$ has a nonzero finite limit at $\infty$. The function $f(z)=p(1 / z) / q(1 / z)$ thus has a removable singularity at 0 , as does $1 / f$. Moreover $f$ and $1 / f$ are both analytic in a neighborhood of the disk $|z| \leqslant 1$ (since $p$ and $q$ are without zeros in $|z| \geqslant 1$ ) and $f$ and $1 / f$ have unit modulus on $|z|=1$. By the Maximum Modulus Principle [MH87, p. 185], $|f(z)| \leqslant 1$ and $|1 / f(z)| \leqslant 1$ for $|z| \leqslant 1$. So $|f(z)|=1$ and for $|z| \leqslant 1$, and another application of the Maximum Modulus Principle shows that $f=$ constant, the desired conclusion.

Solution to 5.5.7: Let $g$ be defined on $\mathbb{C}$ by $g(z)=\frac{f(z)-f(0)}{z} \cdot g$ is an entire function such that, on the circle of radius $R$,

$$
|g(z)| \leqslant \frac{|\log z|+|f(0)|}{R} \leqslant \frac{\sqrt{\log ^{2} R+\pi^{2}}+|f(0)|}{R},
$$

where the inequality for $z=-R$ follows by continuity. Using the Maximum Modulus Principle [MH87, p. 185], we get, for $|z| \leqslant R$,

$$
|g(z)| \leqslant \frac{\sqrt{\log ^{2} R+\pi^{2}}+|f(0)|}{R} .
$$

Taking the limit $R \rightarrow \infty$ we get $g \equiv 0$ so $f$ is a constant function. As $|f(1)| \leqslant|\log 1|$ we conclude that $f$ vanishes identically.

Solution 2. Since $f$ is entire, its Maclaurin series converges everywhere, $f(z)=\sum_{n=0}^{\infty} a_{n} z^{n}$. Using Cauchy's Integral Formula for derivatives [MH87, p. 169], we have

$$
a_{n}=\frac{1}{2 \pi i} \int_{C_{R}} \frac{f(\xi)}{\xi^{n+1}} d \xi
$$

where $C_{R}$ is the circle of radius $R$ centered at the origin, positively oriented. Using the given inequality we get, for $n \geqslant 1$,

$$
\left|a_{n}\right| \leqslant \frac{1}{2 \pi} 2 \pi R \frac{\log R}{R^{n+1}}=\frac{\log R}{R}=o(R) \quad(R \rightarrow \infty) .
$$

Therefore $f$ equals a constant, $a_{0}$. As in the previous solution, we must have $a_{0}=0$

Solution to 5.5.8: Let $h(z)=f(z)-k g(z)$. Then $h$ is entire and $\Re h(z) \leqslant 0$. We then have

$$
\left|e^{h(z)}\right| \leqslant 1 \quad \text { for all } z \in \mathbb{C}
$$

therefore, by Liouville's Theorem [MH87, p. 170], $e^{h}$ is constant, and so is $h$.

Solution to 5.5.9: Suppose the maximum of $\varphi$ over the compact set $K$ is attained at a point $z_{0}$ in the interior of $K$. Let $r$ be the distance of $z_{0}$ from the complement of $K$. The circle $\left|z-z_{0}\right|=r$ is then in $K$, so $\varphi(z) \leqslant \varphi\left(z_{0}\right)$ on that circle, giving

$$
\varphi\left(z_{0}\right) \geq \frac{1}{2 \pi} \int_{0}^{2 \pi} \varphi\left(z_{0}+r e^{i \theta}\right) d \theta .
$$

Hence equality holds by the sub-mean-value property. Thus

$$
\int_{0}^{2 \pi}\left(\varphi\left(z_{0}\right)-\varphi\left(z_{0}+r e^{i \theta}\right)\right) d \theta=0 .
$$

The integrand here is nonnegative, so, being continuous, it must vanish identically. Thus $\varphi(z)=\varphi\left(z_{0}\right)$ for $\left|z-z_{0}\right|=r$. Since $r$ is the distance from $z_{0}$ to the complement of $K$, the circle $|z-z 0|=r$ contains at least one point of the boundary of $K$. Hence $\varphi$ attains its maximum over $K$ on the boundary of $K$.

Solution to 5.5.10: 1. Using Cauchy's Integral Formula for derivatives [MH87, p. 169], we get

$$
\begin{aligned}
\left|f^{(k)}(0)\right| & \leqslant \frac{k !}{2 \pi} \int_{|z|=R}\left|\frac{f(z)}{z^{k+1}}\right||d z| \\
& \leqslant \frac{k !}{2 \pi R^{k+1}} \int_{|z|=R}|a \sqrt{|z|}+b||d z| \\
&=\frac{k !(a \sqrt{|z|}+b)}{R^{k}} \\
&=o(1) \quad(R \rightarrow \infty)
\end{aligned}
$$

so $f^{(k)}(0)=0$ for $k \geqslant 1$, and $f$ reduces to a constant, $f(0)$.

2. Using the same method as above for $k \geqslant 3$, we get

$$
\begin{aligned}
\left|f^{(k)}(0)\right| & \leqslant \frac{k !}{2 \pi} \int_{|z|=R}\left|\frac{f(z)}{z^{k+1}}\right||d z| \\
& \leqslant \frac{k !}{2 \pi R^{k+1}} \int_{|z|=R}\left|a \sqrt{|z|^{5}}+b\right||d z| \\
&=\frac{k !\left(a \sqrt{|z|^{5}}+b\right)}{R^{k}}=o(1) \quad(R \rightarrow \infty)
\end{aligned}
$$

so $f^{(k)}(0)=0$ for $k \geqslant 3$ and $f$ reduces to a polynomial of degree, at most, 2 , $f(0)+f^{\prime}(0) z+f^{\prime \prime}(0) z^{2} / 2$.

Solution to 5.5.11: For $r>0$, let $z=r e^{i \theta}$ in Cauchy's Integral Formula for derivatives [MH87, p. 169] to get

$$
f^{(n)}(0)=\frac{n !}{2 \pi i} \int_{0}^{2 \pi} \frac{f\left(r e^{i \theta}\right)}{r^{n} e^{i n \theta}} d \theta .
$$

Combining this with the inequality given yields

$$
\frac{\left|f^{(n)}(0)\right|}{n !} \leqslant \frac{r^{17 / 3-n}}{2 \pi} .
$$

For $n>5$, letting $r$ tend to infinity, we get $f^{(n)}(0)=0$. If $n \leqslant 5$, letting $r$ tend to 0 gives the same result. Hence, the coefficients of the Maclaurin series [MH87, p. 234] of $f$ are all 0 , so $f \equiv 0$.

Solution to 5.5.12: If such a function $f$ exists then $g=1 / f$ is also analytic on $\mathbb{C} \backslash\{0\}$, and satisfies $|g(z)| \leqslant \sqrt{|z|}$. As $g$ is bounded on $\{z|0<| z \mid<1\}, g$ has a removable singularity at 0 , and extends as an analytic function over the complex plane. Fix $z$, choose $R>|z|$, and let $C_{R}$ be the circle with center 0 and radius $R$. Then

$$
g^{\prime}(z)=\frac{1}{2 \pi i} \int_{C_{R}} \frac{g(w)}{(w-z)^{2}} d w
$$

so

$$
\left|g^{\prime}(z)\right| \leqslant \frac{1}{2 \pi} \cdot 2 \pi R \cdot \frac{\sqrt{R}}{(R-|z|)^{2}} \rightarrow 0 \text { as } R \rightarrow \infty .
$$

Thus, $g^{\prime}=0$ everywhere, so $g$ (and, hence, $f$ ) is constant. But this contradicts the hypothesis $|f(z)| \geqslant \frac{1}{\sqrt{|z|}}$ for small $z$, so no such function exists.

Solution to 5.5.13: By Liouville's Theorem [MH87, p. 170], it will be enough to prove that $f$ is bounded. For $|\Re z| \geqslant 1 / 2$, we have $|f(z)| \leqslant \sqrt{2}$. Let $z_{0}$ be a point such that $\left|\Re z_{0}\right|<1 / 2$. Let $S$ be the square with vertices $i \Re z_{0} \pm 1 \pm i$, oriented counterclockwise.

![](https://cdn.mathpix.com/cropped/2022_10_26_41d22736fcd19c2c10fdg-077.jpg?height=716&width=615&top_left_y=1046&top_left_x=755)

Then $z_{0}$ is in the interior of $S$, so Cauchy's Integral Formula [MH87, p. 167] gives

$$
f\left(z_{0}\right)=\frac{1}{2 \pi i} \int_{S} \frac{f(z)}{z-z_{0}} d z .
$$

The absolute value of the integrand is, at most, $2|\mathfrak{\Re z}|^{-1 / 2}$. The contribution to the integral from each vertical edge is thus, at most, 4 in absolute value. The contribution from each horizontal edge is, at most, $2 \int_{-1}^{1}|x|^{-1 / 2} d x=8$ in absolute value. Hence,

$$
\left|f\left(z_{0}\right)\right| \leqslant \frac{1}{2 \pi}(4+4+8+8)=12 \pi
$$

proving that $f$ is bounded.

Solution to 5.5.14: Since $f$ is nonconstant its zero at the origin is isolated. Hence there is a circle $C$ with center at the origin on and within which $f$ does not vanish, except at 0 . Let $\varepsilon$ be the minimum of $|f|$ on $C$. Then the set $\{z|| f(z) \mid<\varepsilon\}$ intersects the interior of $C$ but is disjoint from $C$. Since it is connected it must be contained in the interior of $C$. It follows that the origin is the only zero of $f$, and that $f$ is bounded away from 0 near $\infty$. The latter conclusion implies that $\infty$ is not an essential singularity of $f$ (Casorati-Weierstrass Theorem [MH87, p. 256]), so $\infty$ must be a pole of $f$ (it is not a removable singularity, by Liouville's Theorem [MH87, p. 170]). Therefore $f$ is a polynomial. Since its only zero is at the origin, it is a constant times a positive power of $z$.

\subsection{Analytic and Meromorphic Functions}

Solution to 5.6.1: By the Cauchy-Riemann equations [MH87, p. 72],

$$
u_{x}=v_{y} \quad \text { and } \quad u_{y}=-v_{x} .
$$

Thus, $a u+b v=c$ implies

$$
a u_{x}+b v_{x}=0=a u_{y}+b v_{y}
$$

and, therefore,

$$
a u_{x}-b u_{y}=0=a u_{y}+b u_{x} .
$$

In matrix form, this reads

$$
\left(\begin{array}{cc}
a & -b \\
b & a
\end{array}\right)\left(\begin{array}{l}
u_{x} \\
u_{y}
\end{array}\right)=\left(\begin{array}{l}
0 \\
0
\end{array}\right) .
$$

Since the matrix has nonzero determinant $a^{2}+b^{2}$, the homogeneous system has only the zero solution. Hence, $u_{x}=u_{y}=0$. By the Cauchy-Riemann equations, $v_{x}=v_{y}=0$. Since $D$ is connected, $f$ is constant.

Solution to 5.6.2: $f(z)=\sqrt{z}$ is a counterexample. Define it by making a cut on the negative real axis and choosing an associated branch of the logarithm:

$$
f(z)= \begin{cases}e^{\frac{1}{2} \log z} & z \neq 0 \\ 0 & z=0\end{cases}
$$

$f$ is analytic in the right half-plane and so on the disc $|z-1|<1$. Since $\sqrt{z}$ tends to 0 as $z$ tends to $0, f$ is continuous on the disc $|z-1| \leqslant 1$. However, $f$ cannot be analytic on any open disc of radius larger than 1 . For if it were, $f$ would be analytic at 0 , so

$$
f^{\prime}(0)=\lim _{z \rightarrow 0} \frac{f(z)}{z}
$$

would exist and be finite, which is absurd. Solution to 5.6.3: 1. $f(z)=z^{2}$ is entire and satisfies

$$
f(1 / n)=f(-1 / n)=1 / n^{2} .
$$

2. By the Identity Theorem [MH87, p. 397], in a disc centered at the origin, $g$ would have to be $z^{3}$ and $-z^{3}$, which is not possible; therefore, no such function $g$ can exist.

Solution to 5.6.4: Let $\sum_{n=0}^{\infty} c_{n} z^{n}$ be the power series of such an $f$ at the origin. Then

$$
c_{0}=f(0)=(f(0))^{k}=c_{0}^{k},
$$

so $c_{0}$ is either 0 or a $(k-1)^{t h}$ root of unity. Assume $c_{0} \neq 0$ but that $f$ is not constant. Let $j$ be the smallest positive integer such that $c_{j} \neq 0$. Then $f\left(z^{k}\right)=$ $c_{0}+c_{j} z^{j k}+$ higher order terms, so $(f(z))^{k}=c_{0}+k c_{j} z^{j}+$ higher order terms, which gives a contradiction. Hence, if $f(0) \neq 0$, then $f$ is constant and equal to $\mathrm{a}(k-1)^{t h}$ root of unity.

Assume $f(0)=0$ but $f$ is not identically 0 . Let $j$, as above, be the smallest positive integer such that $c_{j} \neq 0$. We then have $f(z)=z^{j} g(z)$, where $g$ is entire and $g(0)=c_{j}$. The function $z \mapsto z^{j}$ satisfies the condition imposed on $f$, so $g$ also satisfies that condition. By the preceding argument, $g$ is constant, equal to a $(k-1)^{t h}$ root of unity.

Then $f$ is either the zero function or $f(z)=c z^{j}$, where $c$ is a $(k-1)^{t h}$ root of unity and $j$ is a nonnegative integer.

Solution to 5.6.5: We first prove existence. Multiplying the rational function $R$ by a suitably high power of $F$, we can reduce the general case to the case where $R$ is a polynomial. Assuming $R$ is a polynomial, we can use the division algorithm to write $R=a_{0}+R_{1} F$ where $a_{0}$ and $R_{1}$ are polynomials and $\operatorname{deg} a_{0}<d$. Necessarily, $\operatorname{deg} R_{1}<\operatorname{deg} R$. If $\operatorname{deg} R_{1} \geqslant d$ we can use the division algorithm again to write $R_{1}=a_{1}+R_{2} F$ where $\operatorname{deg} a_{1}<d$ and $\operatorname{deg} R_{2}<\operatorname{deg} R_{1}$. The procedure can be iterated and with each iteration the degree of the polynomial multiplying $F$ is reduced. After finitely many iterations we arrive at the desired representation.

To prove uniqueness it will suffice to show that if

$$
\sum_{k=m}^{n} a_{k} F^{k}=0
$$

then every $a_{k}$ is 0 . Assume not. After multiplying the expression by a suitable power of $F$, we can reduce to the case where $m=0$ and $a_{0} \neq 0$. Then the $\sum_{k=1}^{n} a_{k} F^{k} \neq 0$, and $(*)$ implies that $F$ divides $a_{0}$, a contradiction because $\operatorname{deg} a_{0}<d=\operatorname{deg} F$.

Solution to 5.6.6: Let $f$ be such a function. For $n=2,3, \ldots$ the singularity at $1 / n$ is removable because $f$ is bounded in a punctured neighborhood of $1 / n$. Therefore $f$ extends holomorphically to $\mathbb{D} \backslash\{0\}$. Then the origin becomes an isolated singularity of the extended $f$, so, by the same reasoning, $f$ extends holomorphically to $\mathbb{D}$.

Solution to 5.6.7: No. Let $U=\mathbb{C} \backslash\{z \in \mathbb{R} \mid z \leqslant 0\}$, let $f$ be the branch of $\sqrt{z}$ on $U$ with $f(1)=1$, and let $a=-1+i$. Then the Taylor series for $f$ at $a$ has radius of convergence $\sqrt{2}$, since there is a holomorphic branch of $\sqrt{2}$ defined on $D=\{z|| z-a \mid<\sqrt{2}\}$ that agrees with $f$ in a neighborhood of $a=-1+i$.

But $f$ cannot extend to a holomorphic function on $U \cup D$, because $-1 \in D$, and the limit of $f(z)$ as $z$ approaches $-1$ along the unit circle does not exist:

$$
\lim _{t \rightarrow \pi_{-}} f\left(e^{i t}\right)=\lim _{t \rightarrow \pi_{-}} e^{i t / 2}=i \text { and } \lim _{t \rightarrow(-\pi)_{+}} f\left(e^{i t}\right)=\lim _{t \rightarrow(-\pi)_{+}} e^{i t / 2}=-i .
$$

Solution to 5.6.8: 1 . Write $f=u+i v$. Then the Cauchy-Riemann equations hold in $U$. By hypothesis, $v=0$ and hence $u_{x}=u_{y}=0$. The open connected set $U$ is step-path connected in that given $z_{1}, z_{2}$ in $U$ there is a path consisting of line segments parallel to the axes contained in $U$ from $z_{1}$ to $z_{2}$. The Mean Value Theorem applied to $u$ then shows that $u\left(z_{1}\right)=u\left(z_{2}\right)$, and thus $f=u+i v$ is constant.

2. Write $A=\{\Re g(z)+\Re g(z) \mid z \in W\} \subset \mathbb{R}$, and let $h: \mathbb{C} \rightarrow \mathbb{R} ; w \mapsto \Re w+\Re w$. Then $A=h(g(W))$. Fix $x=h(g(z)) \in A$. The hypothesis implies that $g$ is nonconstant. By the Open Mapping Theorem [MH87, p. 436] $g(z)$ is an interior point of $g(W)$ and thus $g(W)$ contains some disc centered at $g(z)$ with radius $r>0$. It is clear that $h(g(W))$ then contains the open interval $(x-r, x+r)$, proving that $A$ is an open set.

Solution to 5.6.9: Suppose $f(\mathbb{C})$ is not dense. Then, for some $w \in \mathbb{C}$ and $\varepsilon>0$, we have $|f(z)-w| \geqslant \varepsilon$ for all $z \in \mathbb{C}$. The function $1 /(f(z)-w)$ is then entire and bounded in modulus by $1 / \varepsilon$, so, by Liouville's Theorem [MH87, p. 170], is a constant, and so is $f$.

Solution to 5.6.10: We may assume that $0 \in L$, otherwise we use $f+c$ instead of $f$, where $c$ is a constant. We may even assume that $L$ is the imaginary axis, otherwise we use $\alpha f$ in the place of $f$, where $\alpha$ is a constant of modulus 1 . Since $f(\mathbb{C})$ is connected, it is contained in one of the halfplanes $\Re z>0, \Re z<0$. We may assume that $f(\mathbb{C})$ is contained in the left halfplane (otherwise we use $-f$ instead of $f$ ). Then $g$ is an entire bounded function, where $g(z)=e^{f(z)}$. By Liouville's Theorem, $g$ is constant, $g(z)=k$, say. Therefore, $f(\mathbb{C})$ is contained in the solution set of $e^{z}=k$, which is discrete. As $f(\mathbb{C})$ is connected we conclude that it must contain only a point, so $f$ is constant as well.

Solution 2. By Picard's Theorem the image of an nonconstant entire function omits at most one complex number, [Ahl79, Sec. 8.3] and [Car63b, Sec 4.4], so the result follows. See the previous problem.

Solution to 5.6.11: 1. If $f$ is entire and $\mathfrak{} f>0$, then $1 /(1+f)$ is bounded and entire, hence constant by Liouville's Theorem [MH87, p. 170]. 2. Assume $F(z)+F(z)^{*}$ is positive definite for each $z$. Since the diagonal entries of a positive definite matrix are positive, it follows that $f_{11}$ and $f_{22}$ have positive real parts. Hence $f_{11}$ and $f_{22}$ are constant by Part 1. Also, the determinant of a positive definite matrix is positive, so

$$
f_{11} f_{22}-\left|f_{12}+\overline{f_{21}}\right|^{2} \geqslant 0 .
$$

The function $f_{12}+\overrightarrow{f_{21}}$ is thus bounded. It follows that the real part of the entire function $f_{12}+f_{21}$ is bounded, hence $f_{12}+f_{21}$ is constant, by Part 1 . Similarly, the real part of the entire function $i\left(f_{12}-f_{21}\right)$ is bounded, so $f_{12}-f_{21}$ is constant, by Part 1 , therefore $f_{12}$ and $f_{21}$ are constant.

Solution to 5.6.12: Let $f(x+i y)=u(x, y)+i v(x, y)$, where $u(x, y)=e^{x} s(y)$ and $v(x, y)=e^{x} t(y)$. From the Cauchy-Riemann equations [MH87, p. 72], we get $e^{x} s(y)=e^{x} t^{\prime}(y)$, so $s(y)=t^{\prime}(y)$. Similarly, $s^{\prime}(y)=-t(y)$. This equation has the unique solution $s(y)=\cos y$ satisfying the initial conditions $s(0)=1$ and $s^{\prime}(0)=-t(0)=0$, which, in turn, implies that $t(y)=-s^{\prime}(y)=\sin y$.

Solution to 5.6.13: $f^{\prime \prime}+f$ is analytic on $\mathbb{D}$ and vanishes on $X=\{1 / n \mid n \geqslant 0\}$, so it vanishes identically. Using the Maclaurin expansion [MH87, p. 234] of $f$, we get

$$
\sum_{k \geqslant 0} \frac{f^{(k)}(0)}{k !} z^{k}=-\sum_{k \geqslant 0} \frac{f^{(k+2)}(0)}{k !} z^{k} .
$$

So we have

$$
f(0)=-f^{\prime \prime}(0)=\cdots=(-1)^{k} f^{(2 k)}(0)=\cdots
$$

and

$$
f^{\prime}(0)=-f^{\prime \prime \prime}(0)=\cdots=(-1)^{k} f^{(2 k+1)}(0)=\cdots
$$

Therefore,

$$
\begin{aligned}
f(z) &=f(0) \sum_{k \geqslant 0} \frac{(-1)^{k}}{(2 k) !} z^{2 k}+f^{\prime}(0) \sum_{k \geqslant 0} \frac{(-1)^{k}}{(2 k+1) !} z^{2 k+1} \\
&=f(0) \cos z+f^{\prime}(0) \sin z .
\end{aligned}
$$

Conversely, any linear combination of $\cos z$ and $\sin z$ satisfies the given equation, so these are all such functions.

Solution to 5.6.14: Let $a$ be the intersection of the lines. Replacing $f(z)$ by $f(z+a)-f(a)$, we may reduce to the case $a=f(a)=0$. Then $f(z)=$ $c z^{n}(1+o(1))$ as $z \rightarrow 0$, for some nonzero $c \in \mathbb{C}$ and $n \geqslant 1$. In particular, $f(z)$ is nonvanishing in some punctured neighborhood of 0 . If the lines are in the directions of $e^{i \alpha}$ and $e^{i \beta}$, then for real $t, f\left(t e^{i \alpha}\right)$ and $f\left(t e^{i \beta}\right)$ are real, and so is

$$
\lim _{t \rightarrow 0} \frac{f\left(t e^{i \alpha}\right)}{f\left(t e^{i \beta}\right)}=\lim _{t \rightarrow 0} \frac{c e^{n i \alpha} t^{n}(1+o(1))}{c e^{n i \beta} t^{n}(1+o(1))}=e^{n i(\alpha-\beta)} .
$$

This forces $n i(\alpha-\beta) \in \pi i \mathbb{Z}$, so $(\alpha-\beta) / \pi$ is rational, as desired.

Solution to 5.6.15: It is enough to show that for any $z \in \Omega$, the derivative in the sense of $\mathbb{R}^{2}$ has an associated matrix

$$
D f(z)=\left(\begin{array}{ll}
a & c \\
b & d
\end{array}\right)
$$

satisfying $a=d$ and $c=-b$. As $D f(z)(1,0) \perp D f(z)(0,1)$, we have $c=-k b$ and $d=k a$ for some $k$. As $f$ preserves orientation, $\operatorname{det} D f(z)>0$, so $k>0$.

If $a^{2}+b^{2}=0$, then $D f(z)=0$ and there is nothing to show.

Assume $a^{2}+b^{2} \neq 0$. As, for $(x, y) \neq 0, D f(z)(x, y) \perp D f(z)(-y, x)$, we have $0=\left(k^{2}-1\right)\left(a^{2}+b^{2}\right) x y$. Therefore, $k=1$ and the result follows.

Solution to 5.6.16: We have $f=f^{3} / f^{2}=g / h$. It is clear that $f^{3}$ and $f^{2}$ have the same zero set. If $z_{0}$ is a common zero, there are analytic functions $g_{1}$ and $h_{1}$ which are not zero at $z_{0}$ such that $f^{3}(z)=\left(z-z_{0}\right)^{k} h_{1}(z)$ and $f^{2}(z)=\left(z-z_{0}\right)^{j} g_{1}(z)$. But $\left(f^{3}\right)^{2}=f^{6}=\left(f^{2}\right)^{3}$, so $\left(z-z_{0}\right)^{2 k} h_{1}(z)^{2}=$ $\left(z-z_{0}\right)^{3 j} g_{1}(z)^{3}$. Rearranging, we get $h_{1}(z)^{2} / g_{1}(z)^{3}=\left(z-z_{0}\right)^{3 j-2 k}$. Neither $h_{1}$ nor $g_{1}$ are zero at $z_{0}$, so the left side is analytic and nonzero at $z_{0}$. Hence, we must have $3 j-2 k=0$, so $k>j$. Therefore, $z_{0}$ has a higher multiplicity as a zero of $f^{3}$ than it does as a zero of $f^{2}$. Thus, the function $f^{3} / f^{2}$ has a removable singularity at $z 0$. Since this holds for every zero, $f=f^{3} / f^{2}$ can be extended to an analytic function on $\mathbb{D}$.

Solution to 5.6.17: Let $g=f^{2}$ and let the common domain of $f$ and $g$ be $G$. We will show that $g(G)$ contains no path with winding number 1 about 0 . Suppose that for some path $\gamma:[0,1] \rightarrow G, g(\gamma)$ had winding number 1 about 0 . Since $g(\gamma)$ is compact, there is a finite cover of it by $n$ open, overlapping balls, none of which contain 0 . In the first ball, define the function $h_{1}(z)=\sqrt{g(z)}$, where the branch of the square root is chosen so that $h_{1}(\gamma(0))=f(\gamma(0))$. In each successive ball, define the function $h_{k}(z)=\sqrt{g(z)}$, with the branch chosen so that $h_{k}$ is an analytic continuation of $h_{k-1}$. This implies that if $\gamma(t)$ is in the domain $h_{k}$, we must have $h_{k}(\gamma(t))=f(\gamma(t))$. However, since these analytic extensions wrap around the origin, $h_{n}(\gamma(1))=h_{n}(\gamma(0)) \neq h_{1}(\gamma(0))$, which contradicts the continuity of $f$.

Therefore, since $g(G)$ contains no path with winding number 1 about the origin, there exists a branch of the square root on it which is analytic and such that $f(z)=\sqrt{g(z)}$ for all $z$ in $G$. Thus, $f$ is an analytic function.

Solution to 5.6.18: 1. Let $w$ be an $n^{t h}$ primitive root of unity. Then the function $w^{k} g, 0 \leqslant k \leqslant n-1$ are all $n^{t h}$ roots of $f$, analytic and distinct.

Let $h$ be any analytic $n^{t h}$ root of $f$. Fix $z_{0} \in G, \varepsilon>0$ such that $f(z) \neq 0$ for $\left|z-z_{0}\right|<\varepsilon . h / g$ is continuous in $\left|z-z_{0}\right|<\varepsilon$ and since $h^{n} / g^{n}=1, h / g$ has its range among the $n$ points $1, w, \ldots, w^{n-1}$. Since it is continuous, it must be constant. Therefore, $h=w^{k} g$ for some $k$ in this little neighborhood, so $h \equiv w^{k} g$. 2. The function $f:[0,1] \rightarrow \mathbb{R}$ defined by $f(x)=(x-1 / 2)^{2}$ has four continuous square roots, $f_{1}, f_{2}, f_{3}$, and $f_{4}$, given by

$$
\begin{gathered}
f_{1}(x)=x-1 / 2 \quad f_{2}(x)=-f_{1}(x) \\
f_{3}(x)=\left\{\begin{array}{ll}
x-1 / 2 & \text { for } 0 \leqslant x \leqslant 1 / 2 \\
1 / 2-x & \text { for } \quad 1 / 2 \leqslant x \leqslant 1
\end{array} \quad f_{4}(x)=-f_{3}(x) .\right.
\end{gathered}
$$

Solution to 5.6.19: 1. Since $f(z) \neq 0$ for all $z \in \mathbb{D}, \frac{f^{\prime}(z)}{f(z)}$ is a holomorphic function on $\mathbb{D}$. For $z \in \mathbb{D}$ set $h(z)=\int_{[0, z]} \frac{f^{\prime}(z)}{f(z)} d z$, where $[0, z]$ stands for the oriented segment from the origin to $z$. Using the independence of path, which is a consequence of Cauchy's Theorem [MH87, p. 152], we get $h^{\prime}(z)=\frac{f^{\prime}(z)}{f(z)}$ for all $z \in \mathbb{D}$, so $e^{-h(z)} f(z)$ has derivative $e^{-h(z)}\left(f^{\prime}(z)-h^{\prime}(z) f(z)\right)=0$, therefore $e^{-h(z)} f(z)=f(0)$ for all $z \in \mathbb{D}$. Since $f(0) \neq 0$ there exists $c \in \mathbb{C}$ such that $e^{c}=f(0)$. Set $g(z)=h(z)+c$; then $e^{g(z)}=f(z)$ for all $z \in \mathbb{D}$.

2. No. Consider $D=\mathbb{C} \backslash\{0\}$ and $f(z)=z$. Suppose $z=e^{g(z)}$ with $g$ holomorphic on $D$. Then $g^{\prime}(z)=1 / z$, so, for any closed curve in $D, \int_{C} \frac{d z}{z}$ would vanish, which is absurd since $\int_{C} \frac{d z}{z}=2 \pi i$ if $C$ is the unit circle.

Solution to 5.6.20: The function $g(z)=\overline{f(\bar{z})}$ is analytic in the same region as $f$, and $f-g=0$ on $(1, \infty)$. Since the zero set of $f-g$ has limit points in the region $|z|>1$, the Identity Theorem [MH87, p. 397] implies that $f-g \equiv 0$. Hence, $f(z) \equiv \overline{f(\bar{z})}$. In particular, for $x$ in $(-\infty,-1), f(x)=\overline{f(x)}$.

Solution 2. Let $\sum_{-\infty}^{\infty} c_{n} z^{n}$ be the Laurent expansion [MH87, p. 246] of $f$ about $\infty$. It will suffice to show that $c_{n}$ is real for all $n$. The series $\sum_{-\infty}^{\infty}\left(\Re c_{n}\right) z^{n}$ converges everywhere the original series does (since its terms are dominated in absolute value by those of the original series); let $g(z)=\sum_{-\infty}^{\infty}\left(\Re c_{n}\right) z^{n}$. For $x$ in $(1, \infty)$

$$
g(x)=\mathfrak{R f} f(x)=f(x) .
$$

As above, the Identity Theorem [MH87, p. 397] implies $g=f$, so each $c_{n}$ is real, as desired.

Solution to 5.6.21: The function $g(z)=\overline{f(\bar{z})}$ is analytic and coincides with $f$ on the real axis; therefore, it equals $f$. The line in question is its own reflection with respect to the real axis. Since it also passes through the origin, it must be one of the axes.

Solution to 5.6.22: The function $\overline{f(\bar{z})}$ is also entire, and it agrees with $f$ on the real axis. By the Identity Theorem [MH87, p. 397], it equals $f$ everywhere. In particular, then, $f(x-\pi i)=f(x+\pi i)$ for all real $x$. On the line $\Im z=-\pi$ the function $f(z+2 \pi i)-f(z)$ vanishes identically, so it is identically 0 , by the Identity Theorem.

Solution to 5.6.23: By the Schwarz Reflection Principle for circles [BN82, p. 85], we have

$$
f(z)=\overline{f(1 / \bar{z})} .
$$

For $x$ real, we get

$$
f(x)=\overline{f(1 / x)}=\overline{f(x)}
$$

so $f(x)$ is real.

Solution to 5.6.24: 1. Let $z_{1}, z_{2}, \ldots, z_{n}$ be the zeros of $p$, ennumerated with multiplicities, so that

$$
p(z)=c\left(z-z_{1}\right)\left(z-z_{2}\right) \cdots\left(z-z_{n}\right)
$$

where $c$ is a constant. Then

$$
\frac{p^{\prime}(z)}{p(z)}=\frac{1}{z-z_{1}}+\frac{1}{z-z_{2}}+\cdots+\frac{1}{z-z_{n}}
$$

and, for $x$ real,

$$
\Im \frac{p^{\prime}(z)}{p(z)}=\frac{\Im z_{1}}{\left|x-z_{1}\right|^{2}}+\frac{\Im z_{2}}{\left|x-z_{2}\right|^{2}}+\cdots+\frac{\Im z_{n}}{\left|x-z_{n}\right|^{2}} .
$$

Part 1 is now obvious.

2. Write $z_{j}=x_{j}+y_{j}$, so that

$$
\begin{aligned}
\int_{-\infty}^{\infty} \frac{\Im_{j}}{\left|x-z_{j}\right|^{2}} &=\int_{-\infty}^{\infty} \frac{y_{j}}{\left(x-x_{j}\right)^{2}+y_{j}^{2}} d x \\
&=\int_{-\infty}^{\infty} \frac{y_{j}}{x^{2}+y_{j}^{2}} d x \\
&=\int_{-\infty}^{\infty} \frac{d}{d x}\left(\arctan \frac{x}{y_{j}}\right) d x \\
&=\pi .
\end{aligned}
$$

Hence,

$$
\int_{-\infty}^{\infty} \Im \frac{p^{\prime}(x)}{p(x)} d x=\pi n=\pi \operatorname{deg} p .
$$

Solution to 5.6.25: Let $D$ be an open disc with $\bar{D} \subset G$. It will suffice to show that there is an $n$ such that $f^{(n)}$ has infinitely many zeros in $D$. For then, the zeros of $f^{(n)}$ will have a limit point in $G$, forcing $f^{(n)}$ to vanish identically in $G$ by the Identity Theorem [MH87, p. 397], and it follows that $f$ is a polynomial of degree, at most, $n-1$. By hypothesis, $D$ is the union of the sets $Z_{n}=\left\{z \in D \mid f^{(n)}(z)=0\right\}$ for $n=1,2, \ldots$. Since $D$ is uncountable, at least one $Z_{n}$ is, in fact, uncountable (because a countable union of finite sets is, at most, countable).

Solution to 5.6.26: Let $u=\Re f$ and $v=\Re f$. For $x$ real, we have

$$
f^{\prime}(x)=\frac{\partial u}{\partial x}(x, 0)=\frac{\partial v}{\partial y}(x, 0),
$$

where the first equality holds because $v=0$ on the real axis and the second one follows from the Cauchy-Riemann equations [MH87, p. 72]. Since $v$ is positive in the upper half-plane, $\frac{\partial v}{\partial y} \geqslant 0$ on the real axis. It remains to show that $f^{\prime}$ does not vanish on the real axis.

It suffices to show that $f^{\prime}(0) \neq 0$. In the contrary case, since $f$ is nonconstant, we have

$$
f(z)=c z^{k}(1+O(z)) \quad(z \rightarrow 0)
$$

where $c \neq 0$ is real and $k \geqslant 2$. For small $z$, the argument of the factor $1+O(z)$ lies between $-\frac{\pi}{4}$ and $\frac{\pi}{4}$, say, whereas on any half-circle in the upper half-plane centered at 0 , the factor $c z^{k}$ assumes all possible arguments. On a sufficiently small such half-circle, therefore, the product will assume arguments between $\pi$ and $2 \pi$, contrary to the assumption that $\Im f(z)>0$ for $\Re z>0$. This proves $f^{\prime}(0) \neq 0$

Solution to 5.6.27: 1. The function $f$ is not constant, because if $f$ took the constant value $c$, then $f^{-1}(\{c\})$ would equal $U$, a noncompact set. Since $f$ is holomorphic and nonconstant, it is an open map, and $f(U)$ is open. Since $V$ is connected, it only remains to show that $f(U)$ is closed relative to $V$. Let $a \in V \cap \overline{f(U)}$. There is a sequence $\left(w_{n}\right)$ in $f(U)$ such that $a=\lim w_{n}$. For each $n$, there is a point $z_{n}$ in $U$ with $w_{n}=f\left(z_{n}\right)$. The set $K=\left\{a, w_{1}, w_{2}, \ldots\right\}$ is a compact subset of $V$, so $f^{-1}(K)$ is also compact. Since the sequence $\left(z_{n}\right)$ lies in a compact subset of $U$, it has a subsequence, $\left(z_{n_{k}}\right)$, converging to a point $b$ of $U$. Then $f(b)=\lim f\left(z_{n_{k}}\right)=\lim w_{n_{k}}=a$, proving that $a$ is in $f(U)$ and hence that $f(U)$ is closed relative to $V$.

2. Take $U=V=\mathbb{C}$ and $f(z)=|z|$.

Solution to 5.6.28: By the Inverse Function Theorem [Rud87, p. 221], it will suffice to prove that $J h(0) \neq 0$, where $J h$ denotes the Jacobian of $h$ :

$$
J h=\operatorname{det}\left(\begin{array}{ll}
\frac{\partial}{\partial x}(u+p) & \frac{\partial}{\partial x}(v-q) \\
\frac{\partial}{\partial y}(u+p) & \frac{\partial}{\partial y}(v-q)
\end{array}\right)
$$

By the Cauchy-Riemann equations [MH87, p. 72],

$$
\frac{\partial u}{\partial x}=\frac{\partial v}{\partial y} \quad \frac{\partial u}{\partial y}=-\frac{\partial v}{\partial x} \quad \frac{\partial p}{\partial x}=\frac{\partial q}{\partial y} \quad \frac{\partial p}{\partial y}=-\frac{\partial q}{\partial x} .
$$

Hence,

$$
\begin{aligned}
J h &=\operatorname{det}\left(\begin{array}{cc}
\frac{\partial u}{\partial x}+\frac{\partial p}{\partial x} & \frac{\partial v}{\partial x}-\frac{\partial q}{\partial x} \\
-\frac{\partial v}{\partial x}-\frac{\partial q}{\partial x} & \frac{\partial u}{\partial x}-\frac{\partial p}{\partial x}
\end{array}\right) \\
&=\left(\frac{\partial u}{\partial x}\right)^{2}-\left(\frac{\partial p}{\partial x}\right)^{2}+\left(\frac{\partial v}{\partial x}\right)^{2}-\left(\frac{\partial q}{\partial x}\right)^{2} \\
&=\left|f^{\prime}\right|^{2}-\left|g^{\prime}\right|^{2} .
\end{aligned}
$$

Since $\left|g^{\prime}(0)\right|<\left|f^{\prime}(0)\right|$, it follows that $(J h)(0) \neq 0$, as desired.

Solution to 5.6.29: $f$ does not have a removable singularity at $\infty$. If $f$ had an essential singularity at infinity, for any $w \in \mathbb{C}$ there would exist a sequence $z_{n} \rightarrow \infty$ with $\lim f\left(z_{n}\right)=w$. Therefore, $f$ has a pole at infinity and is a polynomial.

Solution to 5.6.30: Clearly, entire functions of the form $f(z)=a z+b, a, b \in \mathbb{C}$ $a \neq 0$, are one-to-one maps of $\mathbb{C}$ onto $\mathbb{C}$. We will show that these are all such maps by considering the kind of singularity such a map $f$ has at $\infty$. If it has a removable singularity, then it is a bounded entire function, and, by Liouville's Theorem [MH87, p. 170], a constant.

If it has an essential singularity, then, by the Casorati-Weierstrass Theorem [MH87, p. 256], it gets arbitrarily close to any complex number in any neighborhood of $\infty$. But if we look at, say, $f(0)$, we know that for some $\varepsilon$ and $\delta$, the disc $|z|<\delta$ is mapped onto $|f(0)-z|<\varepsilon$ by $f$. Hence, $f$ is not injective.

Therefore, $f$ has a pole at $\infty$, so is a polynomial. But all polynomials of degree 2 or more have more than one root, so are not injective.

Solution to 5.6.31: 1. If $w$ is a period of $f$, an easy induction argument shows that all integer multiples of $w$ are periods of $f$. It is also clear that any linear combination of periods of $f$, with integer coefficients, is a period of $f$.

2. If $f$ had infinitely many periods in a bounded region, by the Identity Theorem [MH87, p. 397], $f$ would be constant.

Solution to 5.6.32: For $0<r<r_{0}$, by the formula for Laurent coefficients [MH87, p. 246], we have

$$
\begin{aligned}
\left|c_{n}\right| & \leqslant \frac{1}{2 \pi r^{n+1}} \int_{|z|=r}|f(z)||d z| \\
&=\frac{1}{2 \pi r^{n}} \int_{0}^{2 \pi}\left|f\left(r e^{i \theta}\right)\right| d \theta \\
& \leqslant \sqrt{\frac{M}{2 \pi}} r^{-(n+2)} .
\end{aligned}
$$

If $n<-2$, as $r$ gets arbitrarily close to zero, this upper bound gets arbitrarily small. Hence, for $n<-2, c_{n}=0$. Solution to 5.6.33: We think of the rational function $f$ as a continuous map of $\overline{\mathbb{C}}$, the Riemann sphere, into itself. The roots of the denominator of $f$ are $\frac{1 \pm \sqrt{1-4 a}}{2 a}$, whose real parts are positive. Hence $f$ is holomorphic on the closed left halfplane. Let $M=\sup \{|f(z)| \mid \Re z<0\}$. By the Maximum Modulus Principle [MH87, p. 185], there is no point $z$ in the open left half-plane where $|f(z)|=M$. Take a sequence $\left(z_{n}\right)_{n=1}^{\infty}$ in the open left half-plane such that $\left|f\left(z_{n}\right)\right| \rightarrow M$. Passing to a subsequence if necessary, we can assume, by the compactness of $\overline{\mathbb{C}}$, that the sequence converges in $\overline{\mathbb{C}}$, to $z_{0}$, say. Then, as $f$ is continuous, we have $\left|f\left(z_{0}\right)\right|=M$; therefore $z_{0}$ is not in the open left half-plane. Hence, either $z_{0}=\infty$ or $z_{0}$ is on the imaginary axis. We have $f(\infty)=1$. On the imaginary axis

$$
|f(i y)|=\left|\frac{1+i y-a y^{2}}{1-i y-a y^{2}}\right|=1 \text {. }
$$

Since numerator and denominator are complex conjugates of each other. Hence $M=\left|f\left(z_{0}\right)\right|=1$, as desired.

Solution 2. The three facts below are verified by straightforward algebra.

- If $w=u+i v \neq-1$, then

$$
\Re\left(\frac{w-1}{w+1}\right)=\frac{u^{2}+v^{2}-1}{(u+1)^{2}+v^{2}} .
$$

Thus, $\mathfrak{R}\left(\frac{w-1}{w+1}\right)<0 \Leftrightarrow|w|<1$.

- If $w=\frac{1+z+a z^{2}}{1-z+a z^{2}}$, then

$$
\frac{w-1}{w+1}=\frac{z}{1+a z^{2}} .
$$

- If $z=x+i y$, then

$$
\mathfrak{\Re}\left(\frac{z}{1+a z^{2}}\right)=\frac{x\left(1+a x^{2}+a y^{2}\right)}{\left|1+a z^{2}\right|^{2}} .
$$

Thus, when $a>0$, we have $\Re\left(\frac{z}{1+a z^{2}}\right)<0 \Leftrightarrow \Re z<0$.

Now let $\Re z<0$. From the last statement, we have $\Re\left(\frac{z}{1+a z^{2}}\right)<0$ and from the first two it follows that $|f(z)|<1$.

Solution to 5.6.34: 1. The Maclaurin series for $\cos z$ is $\sum_{n=0}^{\infty} \frac{(-1)^{n} z^{2 n}}{(2 n) !}$, and it converges uniformly on compact sets. Hence, for fixed $z$,

$$
f(t) \cos z t=\sum_{n=0}^{\infty} \frac{(-1)^{n} f(t) t^{2 n} z^{2 n}}{(2 n) !}
$$

with the series converging uniformly on $[0,1]$. We can therefore, interchange the order of integration and summation to get

$$
h(z)=\sum_{n=0}^{\infty} \frac{(-1)^{n}}{(2 n) !}\left(\int_{0}^{1} t^{2 n} f(t) d t\right) z^{n}
$$

in other words, $h$ has the power series representation

$$
h(z)=\sum_{n=0}^{\infty} c_{2 n} z^{2 n} \quad \text { with } \quad c_{2 n}=\frac{(-1)^{n}}{(2 n) !} \int_{0}^{1} t^{2 n} f(t) d t .
$$

Since $h$ is given by a convergent power series, it is analytic.

2. Suppose $h$ is the zero function. Then, by Part $1, \int_{0}^{1} t^{2 n} f(t) d t=0$ for $n=0,1,2, \ldots$ Hence, if $p$ is any polynomial, then $\int_{0}^{1} p\left(t^{2}\right) f(t) d t=0$. By the Stone-Weierstrass Approximation Theorem [MH93, p. 284], there is a sequence $\left\{p_{k}\right\}$ of polynomials such that $p_{k}(t) \rightarrow f(\sqrt{t})$ uniformly on $[0,1]$. Then $p_{k}\left(t^{2}\right) \rightarrow f(t)$ uniformly on $[0,1]$, so

$$
\int_{0}^{1} f\left(t^{2}\right) d t=\lim _{k \rightarrow \infty} \int_{0}^{1} p_{k}\left(t^{2}\right) f(t) d t=0
$$

implying that $f \equiv 0$.

Solution to 5.6.35: Let $z \in \mathbb{C}$. We have

$$
g(z)=\int_{0}^{1} \sum_{n=0}^{\infty} f(t) \frac{t^{n} z^{n}}{n !} d t .
$$

Since $f$ is bounded, this series converges uniformly in $t$, so we can change the order of summation and get

$$
g(z)=\sum_{n=0}^{\infty} z^{n}\left(\frac{1}{n !} \int_{0}^{1} f(t) t^{n} d t\right)=\sum_{n=0}^{\infty} \xi_{n} z^{n}
$$

where

$$
\xi_{n}=\frac{1}{n !} \int_{0}^{1} f(t) t^{n} d t .
$$

We have

$$
\left|\xi_{n}\right| \leqslant \frac{1}{n !} \int_{0}^{1}|f(t)| d t
$$

so the radius of convergence of the series of $g$ is $\infty$.

Solution 2. Let $z_{0} \in \mathbb{C}$. We have

$$
\frac{g(z)-g\left(z_{0}\right)}{z-z_{0}}=\int_{0}^{1} f(t) \frac{e^{t z}-e^{t z_{0}}}{z-z_{0}} d t .
$$

From the power series expansion $e^{t\left(z-z_{0}\right)}=\sum_{n=0}^{\infty} \frac{t^{n}\left(z-z_{0}\right)^{n}}{n !}$, one gets

$$
\frac{e^{t z}-e^{t z_{0}}}{z-z_{0}}=t e^{t z_{0}}+O\left(z-z_{0}\right)
$$

uniformly on $0 \leqslant t \leqslant 1$, when $z \rightarrow z_{0}$. Thus, as $z \rightarrow z_{0}$, the integrand in the integral above converges uniformly on $[0,1]$ to $t f(t) e^{t z_{0}}$, and one can pass to the limit under the integral sign to get

$$
\lim _{z \rightarrow z_{0}} \frac{g(z)-g\left(z_{0}\right)}{z-z_{0}}=\int_{0}^{1} t f(t) e^{t z_{0}} d t,
$$

proving that $g$ is differentiable at $z 0$.

Solution 3. The integrand in the integral defining $g$ is a continuous function of the pair of variables $(t, z) \in[0,1] \times \mathbb{C}$, implying that $g$ is continuous. If $R \subset \mathbb{C}$ is a rectangle, then by Fubini's Theorem [MH93, p. 500] and the analyticity of $e^{t z}$ with respect to $z$,

$$
\int_{R} g(z) d z=\int_{R} \int_{0}^{1} f(t) e^{t z} d t d z=\int_{0}^{1} \int_{R} f(t) e^{t z} d z d t=0 .
$$

By Morera's Theorem [MH87, p. 173], $g$ is analytic.

Solution to 5.6.36: 1. Let $f$ and $g$ have the Maclaurin expansions [MH87, p. 234]

$$
f(z)=\sum_{k=0}^{\infty} a_{n} z^{n}, \quad g(z)=\sum_{k=0}^{\infty} b_{n} z^{n} .
$$

By Cauchy's Integral Formula [MH87, p. 167] and the uniform convergence of the series for $g$, we have

$$
\begin{aligned}
\frac{1}{2 \pi i} \int_{C_{r}} \frac{1}{w} f(w) g\left(\frac{z}{w}\right) d w &=\frac{1}{2 \pi i} \int_{C_{r}} \sum_{k=0}^{\infty} b_{n} z^{n} \frac{f(w)}{w^{n+1}} d w \\
&=\sum_{k=0}^{\infty} b_{n} z^{n}\left(\frac{1}{2 \pi i} \int_{C_{r}} \frac{f(w)}{w^{n+1}} d w\right) \\
&=\sum_{k=0}^{\infty} a_{n} b_{n} z^{n} .
\end{aligned}
$$

As

$$
\limsup _{n \rightarrow \infty} \sqrt[n]{\left|a_{n}\right|} \limsup _{n \rightarrow \infty} \sqrt[n]{\left|b_{n}\right|} \geqslant \limsup _{n \rightarrow \infty} \sqrt[n]{\left|a_{n} b_{n}\right|}
$$

the radius of convergence of $h$ is at least 1 . 2. If we take $f(z)=\sin z$ and $g(z)=\cos z$, we have $a_{2 n}=b_{2 n-1}=0$ for $n=1,2, \ldots$, so $h \equiv 0$.

Solution to 5.6.37: 1 . The function $f$ is defined on the open set $\Omega=\mathbb{C} \backslash[0,1]$. Suppose $D(a, R) \subset \Omega$. For every $z \in D(a, R)$ and $t \in[0,1]$,

$$
\left|\frac{z-a}{t-a}\right| \leqslant \frac{|z-a|}{R}<1 \text {. }
$$

Thus,

$$
\sum_{n=0}^{\infty} \frac{F(t)(z-a)^{n}}{(t-a)^{n+1}}=\frac{F(t)}{(t-a)} \frac{1}{\left(1-\frac{z-a}{t-a}\right)}=\frac{F(t)}{t-z} .
$$

The convergence of the series, for a fixed $z \in D(a, R)$, is uniform in $t \in[0,1]$. This follows from Weierstrass's $M$-test and the inequality

$$
\left|\frac{F(t)(z-a)^{n}}{(t-a)^{n+1}}\right| \leqslant \frac{\|F\|}{R}\left(\frac{|z-a|}{R}\right)^{n} .
$$

Thus we have the power series expansion, valid for $z \in D(a, R)$,

$$
f(z)=\int_{0}^{1} \frac{F(t)}{t-z} d t=\int_{0}^{1} \sum_{n=0}^{\infty} \frac{F(t)(z-a)^{n}}{(t-a)^{n+1}} d t=\sum_{n=0}^{\infty} c_{n}(z-a)^{n}
$$

where

$$
c_{n}=\int_{0}^{1} \frac{F(t)}{(t-a)^{n+1}} d t \quad(n=0,1,2, \ldots) .
$$

This proves that $f$ is analytic in $\Omega$.

2. For $|z|>1$ and $0 \leqslant t \leqslant 1$, we have,

$$
\frac{F(t)}{t-z}=-\frac{F(t)}{z} \cdot \frac{1}{\left(1-\frac{t}{z}\right)}=-\frac{F(t)}{z} \sum_{n=0}^{\infty} \frac{t^{n}}{z^{n}}
$$

where the convergence, for a fixed $z$ in $|z|>1$, is uniform for $t \in[0,1]$ since $|t / z| \leqslant 1 /|z|<1$.

We thus have the Laurent expansion, valid in $|z|>1$,

$$
f(z)=-\sum_{n=0}^{\infty} z^{-(n+1)} \int_{0}^{1} t^{n} F(t) d t=\sum_{n=1}^{\infty} b_{n} z^{-n}
$$

where

$$
b_{n}=-\int_{0}^{1} t^{n-1} F(t) d t \quad(n=1,2, \ldots) .
$$

The above is a transform $F \in \mathcal{C}[0,1] \mapsto f \in H(\Omega)$. We have seen above that the Laurent coefficients of $f$ are the moments $\int_{0}^{1} t^{n} F(t) d t(n \geqslant 0)$ of $F$. Now the Laurent coefficients are determined by $f$. To show that $F$ is determined by $f$ it is sufficient to show that the moments of $F$ determine $F$. This in turn is a consequence of the following result.

If $\varphi \in \mathcal{C}[0,1]$, and $\int_{0}^{1} t^{n} \varphi(t) d t=0$ for all $n \geqslant 0$, then $\varphi=0$.

It suffices to consider real-valued $\varphi$. By hypothesis, $\int_{0}^{1} p(t) \varphi(t) d t=0$ for all polynomials $p$. The Weierstrass Approximation Theorem asserts that there is a sequence of polynomials $p_{k}$ which converges to $\varphi$ uniformly on $[0,1]$. Thus $\int_{0}^{1} p_{k}(t) \varphi(t) d t \rightarrow \int_{0}^{1} \varphi^{2}(t) d t$ as $k \rightarrow \infty$, and so $\int_{0}^{1} \varphi^{2}(t) d t=0$. This implies that $\varphi=0$ on $[0,1]$.

Solution to 5.6.38: First we will show that $\int_{\Gamma} p(z) f(z) d z=0$ for every polynomial $p$. To see this consider the polynomial $p(z)$ and $p(z)+1$ apply the condition and subtract them, we find that

$$
\int_{\Gamma}(2 p(z)+1) f(z) d z=0 .
$$

and since every polynomial can be written in the form $2 p(z)+1$, we conclude that

$$
\int_{\Gamma} p(z) f(z) d z=0,
$$

for every polynomial $p$.

Suppose that $f$ has a pole of order $n$ at $a \in \mathbb{C}$. Then $(z-a)^{n-1} f(z)$ has a nonzero residue at $a$, therefore,

$$
\int_{\Gamma}(z-a)^{n-1} f(z) d z \neq 0,
$$

for a sufficiently small loop around $a$. Thus, $f$ cannot have any poles and must be entire.

\section{$5.7$ Cauchy's Theorem}

Solution to 5.7.1: By Cauchy's Integral Formula [MH87, p. 167], we have

$$
e^{0}=\frac{1}{2 \pi i} \int_{|z|=1} \frac{e^{z}}{z} d z=\frac{1}{2 \pi} \int_{0}^{2 \pi} e^{e^{i \theta}} d \theta
$$

therefore,

$$
\int_{0}^{2 \pi} e^{e^{i \theta}} d \theta=2 \pi .
$$

Solution 2. We can also compute it using residues. Parametrize the unit circle by $\gamma(t)=e^{i t}$ for $0 \leqslant t \leqslant 2 \pi$. Then

$$
\int_{\gamma} \frac{e^{z}}{z} d z=\int_{0}^{2 \pi} \frac{e^{\gamma(t)}}{\gamma(t)} \gamma^{\prime}(t) d t=\int_{0}^{2 \pi} \frac{e^{e^{i t}}}{e^{i t}} i e^{i t} d t=i \int_{0}^{2 \pi} e^{e^{i t}} d t
$$

Since $e^{z} / z$ is a quotient of analytic functions, it is itself analytic everywhere except at $z=0$, hence the only singularity lying inside $\gamma$ is the origin, so

$$
\int_{0}^{2 \pi} e^{e^{i t}} d t=\frac{1}{i} \int_{\gamma} \frac{e^{z}}{z} d z=2 \pi \operatorname{Res}\left(\frac{e^{z}}{z}, 0\right)=2 \pi \lim _{z \rightarrow 0} z \cdot \frac{e^{z}}{z}=2 \pi .
$$

Solution to 5.7.2: By Cauchy's Integral Formula for derivatives [MH87, p. 169], we have

therefore,

$$
\left.\frac{d}{d z} e^{z}\right|_{z=0}=\frac{1}{2 \pi i} \int_{|z|=1} \frac{e^{z}}{z^{2}} d z=\frac{1}{2 \pi} \int_{0}^{2 \pi} e^{e^{i \theta}-i \theta} d \theta
$$

$$
\int_{0}^{2 \pi} e^{e^{i \theta}-i \theta} d \theta=2 \pi .
$$

Solution to 5.7.3: We'll apply Cauchy's Integral Formula [MH87, p. 167] to $f(z)=1 /(1-\bar{a} z)$, which is holomorphic on a neighborhood of $|z| \leqslant 1$. We have

$$
\begin{aligned}
\int_{|z|=1} \frac{|d z|}{|z-a|^{2}} &=\int_{-\pi}^{\pi} \frac{1}{\left|e^{i \theta}-a\right|^{2}} d \theta \\
&=\int_{-\pi}^{\pi} \frac{1}{\left(e^{i \theta}-a\right)\left(e^{-i \theta}-\bar{a}\right)} d \theta \\
&=\int_{-\pi}^{\pi} \frac{e^{i \theta}}{\left(e^{i \theta}-a\right)\left(1-\bar{a} e^{i \theta}\right)} d \theta \\
&=\frac{1}{i} \int_{|z|=1} \frac{1}{(z-a)(1-\bar{a} z)} d z \\
&=\frac{2 \pi}{1-|a|^{2}} .
\end{aligned}
$$

Solution 2. The function $f(z)=1 /(\bar{a} z-1)(z-a)$ has a simple pole inside the circle $|z|=1$ at $z=a$, with residue $1 /\left(|a|^{2}-1\right)$. Therefore, we have, by the Residue Theorem [MH87, p. 280],

$$
\begin{aligned}
\int_{|z|=1} \frac{|d z|}{|z-a|^{2}} &=i \int_{|z|=1} \frac{d z}{(\bar{a} z-1)(z-a)} \\
&=2 \pi \operatorname{Res}\left(\frac{1}{(\bar{a} z-1)(z-a)}, a\right) \\
&=\frac{2 \pi}{1-|a|^{2}} .
\end{aligned}
$$

Solution to 5.7.4: The integrand equals

$$
\frac{1}{\left(a e^{i \theta}-b\right)^{2}\left(a e^{-i \theta}-b\right)^{2}}=\frac{e^{2 i \theta}}{\left(a e^{i \theta}-b\right)^{2}\left(a-b e^{i \theta}\right)^{2}}
$$

Thus, I can be written as a complex integral,

$$
I=\frac{1}{2 \pi i} \int_{|z|=1} \frac{z}{(a z-b)^{2}(a-b z)^{2}} d z=\frac{1}{2 \pi i b^{2}} \int_{|z|=1} \frac{z}{(a z-b)^{2}\left(z-\frac{a}{b}\right)^{2}} d z
$$

By Cauchy's Theorem for derivatives, we have

$$
I=\left.\frac{1}{b^{2}} \frac{d}{d z}\left(\frac{z}{(a z-b)^{2}}\right)\right|_{z=\frac{a}{b}}=\frac{a^{2}+b^{2}}{\left(b^{2}-a^{2}\right)^{3}}
$$

Solution to 5.7.5: Let $p(z)=a_{n} z^{n}+\cdots+a_{0}$. If $p$ has no zeros then $1 / p$ is entire. As $\lim _{|z| \rightarrow \infty} p(z)=\infty, 1 / p$ is bounded. By Liouville's Theorem [MH87, p. 170] $1 / p$ is constant, and so is $p$.

Solution 2. Let $p(z)=a_{n} z^{n}+\cdots+a_{0}, n \geqslant 1$. If $p$ has no zeros, then $1 / p$ is entire. As $\lim _{|z| \rightarrow \infty} p(z)=\infty$, the Maximum Modulus Principle [MH87, p. 185] gives

$$
\max _{z \in \mathbb{C}} \frac{1}{|p(z)|}=\lim _{R \rightarrow \infty} \max _{|z| \leqslant R} \frac{1}{|p(z)|}=\lim _{R \rightarrow \infty|z|=R} \max _{|p(z)|} \frac{1}{\mid p(z)}=0
$$

which is a contradiction.

Solution 3. For $p(z)=a_{n} z^{n}+\cdots+a_{0}$ with $n \geqslant 1$ let the functions $f$ and $g$ be given by $f(z)=a_{n} z^{n}, g(z)=p(z)-f(z)$. For $R>1$ consider the circle centered at the origin with radius $R, C_{R}$. For $z \in C_{R}$ we have

$$
|f(z)|=\left|a_{n}\right| R^{n} \quad \text { and } \quad|g(z)| \leqslant\left(\left|a_{0}\right|+\cdots+\left|a_{n-1}\right|\right) R^{n-1} .
$$

Therefore, on $C_{R}$,

$$
|g|<|f| \text { if } R>\frac{\left|a_{0}\right|+\cdots+\left|a_{n-1}\right|}{\left|a_{n}\right|}
$$

so, by Rouché's Theorem [MH87, p. 421], $f+g=p$ has $n$ zeros in $\{z \in \mathbb{C}|| z \mid<R\}$

Solution 4. Let $P(z)$ be a nonconstant polynomial. We may assume $P(z)$ is real for real $z$, otherwise we consider $P(z) \bar{P}(z)$. Suppose that $P$ is never zero. Since $P(z)$ does not either vanish or change sign for real $z$, we have

$$
\text { (*) } \quad \int_{0}^{2 \pi} \frac{d \theta}{P(2 \cos \theta)} \neq 0 \text {. }
$$

But

$$
\begin{aligned}
\int_{0}^{2 \pi} \frac{d \theta}{P(2 \cos \theta)} &=\frac{1}{i} \int_{|z|=1} \frac{d z}{z P\left(z+z^{-1}\right)} \\
&=\frac{1}{i} \int_{|z|=1} \frac{z^{n-1} d z}{Q(z)}
\end{aligned}
$$

where $Q(z)=z P\left(z+z^{-1}\right)$ is a polynomial. For $z \neq 0, Q(z) \neq 0$; in addition, if $a_{n}$ is the leading coefficient of $P$, we have $Q(0)=a_{n} \neq 0$. Since $Q(z)$ is never zero, the last integrand is analytic and, hence, the integral is zero, by Cauchy's Theorem [MH87, p. 152], contradicting (*).

This solution is an adaptation of [Boa64].

Solution 5. Let $p(z)=a_{n} z^{n}+\cdots+a_{0}, n \geqslant 1$. We know that $\lim _{|z| \rightarrow \infty}|p(z)|=$ $\infty$, thus the preimage, by $p$, of any bounded set is bounded. Let $w$ be in the closure of $p(\mathbb{C})$. There exists a sequence $\left\{w_{n}\right\} \subset p(\mathbb{C})$ with $\lim _{n} w_{n}=w$. The set $\left\{w_{n} \mid n \in \mathbb{N}\right\}$ is bounded so, by the previous observation, so is its preimage, $p^{-1}\left(\left\{w_{n} \mid n \in \mathbb{N}\right\}\right)=X$. $X$ contains a convergent sequence, $z_{n} \rightarrow z_{0}$, say. By continuity we have $p\left(z_{0}\right)=w$, so $w \in p(\mathbb{C})$. We proved then that $p(\mathbb{C})$ is closed. As any analytic function is open we have that $p(\mathbb{C})$ is closed and open. As only the empty set and $\mathbb{C}$ itself are closed and open we get that $p(\mathbb{C})=\mathbb{C}$ and $p$ is onto. In fact, all we need here, in order to show that $p$ is open, is that $p: \mathbb{R}^{2} \rightarrow \mathbb{R}^{2}$ has isolated singularities, which guides us into another proof.

Solution 6. By the Solution to Problem 2.2.9 the map $p: \mathbb{R}^{2} \rightarrow \mathbb{R}^{2}$ is onto guaranteeing a point where $p(x, y)=(0,0)$.

Solution 7. Let $p(z)=a_{n} z^{n}+\cdots+a_{0}, n \geqslant 1$. Consider the polynomial $q$ given by $q(z)=\overline{a_{n}} z^{n}+\cdots+\overline{a_{0}}$. Assume $p$ has no zeros. As the conjugate of any root of $q$ is a root of $p, q$ is also zero free. Then the function $1 / p q$ is entire. By Cauchy's Theorem [MH87, p. 152], we have

$$
\int_{\Gamma} \frac{d z}{p(z) q(z)}=0
$$

where $\Gamma$ is the segment from $-R$ to $R$ in the horizontal axis together with the half-circle $C=\{z \in \mathbb{C}|| z \mid=R, \Im(z)>0\}$. But we have

$$
\begin{aligned}
\int_{\Gamma} \frac{d z}{p(z) q(z)} &=\int_{C} \frac{d z}{p(z) q(z)}+\int_{-R}^{R} \frac{d z}{|p(z)|^{2}} \\
&=o(1)+\int_{-R}^{R} \frac{d z}{|p(z)|^{2}}(R \rightarrow \infty)
\end{aligned}
$$

this gives, for large $R$,

$$
\int_{-R}^{R} \frac{d z}{|p(z)|^{2}}=0
$$

which is absurd since the integrand is a continuous positive function.

Solution 8. Let $p(z)=a_{n} z^{n}+\cdots+a_{0}, n \geqslant 1$. For $R$ large enough, as $\lim _{|z| \rightarrow \infty} p(z)=\infty,|p|$ has a minimum in $\{z \in \mathbb{C}|| z \mid<R\}$, at $z_{0}$, say. Suppose $p\left(z_{0}\right) \neq 0$. Expanding $p$ around $z_{0}$ we get

$$
p(z)=p\left(z_{0}\right)+\sum_{j=k}^{n} b_{j}\left(z-z_{0}\right)^{j} \quad b_{k} \neq 0 .
$$

Let $w$ be a k-root of $-p\left(z_{0}\right) / b_{k}$. We get, for $\varepsilon>0$,

$$
\begin{aligned}
p\left(z_{0}+w \varepsilon\right) &=p\left(z_{0}\right)+b_{k} w^{k} \varepsilon^{k}+\sum_{j=k+1}^{n} b_{j} w^{j} \varepsilon^{j} \\
&=p\left(z_{0}\right)\left(1-\varepsilon^{k}\right)+\sum_{j=k+1}^{n} b_{j} w^{j} \varepsilon^{j}
\end{aligned}
$$

therefore, for $\varepsilon$ small enough, we have $\left|z_{0}+w \varepsilon\right|<R$ and

$$
\begin{aligned}
\left|p\left(z_{0}+w \varepsilon\right)\right| & \leqslant\left|p\left(z_{0}\right)\right|\left|1-\varepsilon^{k}\right|-\sum_{j=k+1}^{n}\left|b_{j} w^{j}\right| \varepsilon^{j} \\
&=\left|p\left(z_{0}\right)\right|-\left(\left|p\left(z_{0}\right)\right|-\sum_{j=k+1}^{n}\left|b_{j} w^{j}\right| \varepsilon^{j-k}\right) \varepsilon^{k} \\
&<\left|p\left(z_{0}\right)\right|
\end{aligned}
$$

which contradicts the definition of $z_{0}$. We conclude then that $p\left(z_{0}\right)=0$.

Solution 9. Let $p(z)=a_{n} z^{n}+\cdots+a_{0}, n \geqslant 1$. We have

$$
\operatorname{Res}\left(\frac{p^{\prime}}{p}, \infty\right)=-\lim _{|z| \rightarrow \infty} z \frac{p^{\prime}(z)}{p(z)}=-n .
$$
As the singularities of $\frac{p^{\prime}}{p}$ occur at the zeros of its denominator, the conclusion
follows.

Solution to 5.7.6: By Morera's Theorem [MH87, p. 173], it suffices to show that

$$
\int_{\gamma} f(z) d z=0
$$

for all rectangles $\gamma$ in $\mathbb{C}$. Since $f$ is analytic on $\{z \mid \mathfrak{\nexists} \neq 0\}$, which is simply connected, it is enough to consider rectangles which contain part of the real axis in their interiors.

Let $\gamma$ be such a rectangle and $l$ be the segment of $\mathbb{R}$ in its interior. For $\varepsilon>0$ small enough, draw line segments $l_{1}$ and $l_{2}$ parallel to the real axis at distance $\varepsilon$ above and below it, forming contours $\gamma_{1}$ and $\gamma_{2}$.

Since $f$ is continuous, its integral depends continuously on the path. So, as $\varepsilon$ tends to 0 ,

$$
\text { (*) } \quad \int_{\gamma_{1}} f(z) d z+\int_{\gamma_{2}} f(z) d z=\int_{\gamma_{1}+\gamma_{2}} f(z) d z \longrightarrow \int_{\gamma} f(z) d z \text {, }
$$

since the integrals along $l_{1}$ and $l_{2}$ have opposite orientation, in the limit, they cancel each other. By Cauchy's Theorem [MH87, p. 152], the left side of $(*)$ is always 0 , so

$$
\int_{\gamma} f(z) d z=0 .
$$



![](https://cdn.mathpix.com/cropped/2022_10_26_41d22736fcd19c2c10fdg-096.jpg?height=995&width=827&top_left_y=207&top_left_x=649)

Solution to 5.7.7: Since $p(z)$ has lower degree than $q(z), f(z)=O(1 / z)$ as $z \rightarrow \infty$, so the integrand is $O\left(1 / t^{2}\right)$ as $t \rightarrow \infty$, and the integral converges. For $R>\left|z_{0}\right|$, let $\gamma_{1}$ be the straight-line path from $-R$ to $R$, and let $\gamma_{2}$ be the counterclockwise semicircle $\gamma_{2}(t)=R e^{i t}, t \in[0, \pi]$. Let $\gamma$ be the closed contour consisting of $\gamma_{1}$ followed by $\gamma_{2}$. The function $f(z)$ is holomorphic on and inside $\gamma$. By Cauchy's formula,

$$
f\left(z_{0}\right)=\frac{1}{2 \pi i} \int_{\gamma} \frac{f(z)}{z-z_{0}} d z=\frac{1}{2 \pi i} \int_{-R}^{R} \frac{f(z)}{z-z_{0}} d z+\frac{1}{2 \pi i} \int_{\gamma_{2}} \frac{f(z)}{z-z_{0}} d z .
$$

We will obtain the desired formula by taking the limit of both sides as $R \rightarrow \infty$. The first term on the right tends to

$$
\frac{1}{2 \pi i} \int_{-\infty}^{+\infty} \frac{f(t)}{t-z 0} d t \quad \text { as } \quad R \rightarrow \infty .
$$

The second term on the right tends to 0 as $R \rightarrow \infty$, since the integrand is $O\left(1 / R^{2}\right)$ while the length of $\gamma_{2}$ is $O(R)$.

Solution to 5.7.8: We have, using the fact that the exponential is $2 \pi$-periodic,

$$
\begin{aligned}
\frac{1}{2 \pi i} \int_{|z|=R} z^{n-1}|f(z)|^{2} d z &=\frac{1}{2 \pi i} \int_{|z|=R} z^{n-1}\left|\sum_{i=0}^{n} a_{i} z^{i}\right|^{2} d z \\
&=\frac{1}{2 \pi i} \int_{|z|=R} z^{n-1} \sum_{i=0}^{n} a_{i} z^{i} \sum_{i=0}^{n} \bar{a}_{i} z^{i} d z
\end{aligned}
$$



$$
\begin{aligned}
&=\frac{1}{2 \pi} \int_{0}^{2 \pi} R^{n} e^{i n \theta} \sum_{i, j=0}^{n} a_{i} \overline{a_{j}} R^{i+j} e^{(i-j) \theta} d \theta \\
&=\frac{1}{2 \pi} \int_{0}^{2 \pi} \sum_{i, j=0}^{n} a_{i} \overline{a_{j}} R^{n+i+j} e^{(n+i-j) \theta} d \theta \\
&=\frac{1}{2 \pi} \int_{0}^{2 \pi} a_{0} \overline{a_{n}} R^{2 n} d \theta \\
&=a_{0} \overline{a_{n}} R^{2 n} .
\end{aligned}
$$

Solution to 5.7.9: Let $\sum_{n=0}^{\infty} c_{n} z^{n}$ be the power series for $f$. Since the series converges uniformly on each circle $|z|=r$, we have

$$
\begin{aligned}
\int_{0}^{2 \pi}\left|f\left(r e^{i \theta}\right)\right|^{2} d \theta &=\sum_{n=0}^{\infty} \int_{0}^{2 \pi} c_{n} r^{n} e^{i n \theta} \overline{f\left(r e^{i \theta}\right)} d \theta \\
&=\sum_{n=0}^{\infty} \sum_{m=0}^{\infty} \int_{0}^{2 \pi} c_{n} \bar{c}_{m} r^{n+m} e^{i(n-m) \theta} d \theta \\
&=2 \pi \sum_{n=0}^{\infty}\left|c_{n}\right|^{2} r^{2 n}
\end{aligned}
$$

Thus, for each $n$ we have $2 \pi\left|c_{n}\right|^{2} \leqslant A r^{2 k-2 n}$. If $n>k$ we let $r \rightarrow \infty$ to get $c_{n}=0$. If $n<k$ we let $r \rightarrow 0$ to get $c_{n}=0$. Hence $c_{n}=0$ whenever $n \neq k$, and the desired conclusion follows.

Solution 2. Assume $f$ is not the zero function, and let $c_{n}$ be its lowest order nonzero power series coefficient at the origin, that is,

$$
f(z)=c_{n} z^{n}+c_{n+1} z^{n+1}+\cdots
$$

then $z^{-n} f(z) \rightarrow c_{n}$ as $z \rightarrow 0$, so

$$
\lim _{r \rightarrow 0} \frac{1}{r^{2 n}} \int_{0}^{2 \pi}\left|f\left(r e^{i \theta}\right)\right|^{2} d \theta=2 \pi\left|c_{n}\right|^{2} .
$$

On the other hand,

$$
\limsup _{r \rightarrow 0} \frac{1}{r^{2 k}} \int_{0}^{2 \pi}\left|f\left(r e^{i \theta}\right)\right|^{2} d \theta \leqslant A,
$$

so $r^{2 n} / r^{2 k}$ stays bounded as $r \rightarrow 0$, implying that $n \geqslant k$. Hence the function $g(z)=f(z) / z^{k}$ is entire.

Fix $z_{0}$ in $\mathbb{C}$. For $r>\left|z_{0}\right|$, Cauchy's formula gives

$$
g\left(z_{0}\right)^{2}=\frac{1}{2 \pi i} \int_{|z|=r} \frac{g(z)^{2}}{z-z_{0}} d z,
$$

so

$$
\begin{aligned}
\left|g\left(z_{0}\right)\right|^{2} & \leqslant \frac{r}{2 \pi\left(r-\left|z_{0}\right|\right)} \int_{0}^{2 \pi}\left|g\left(r e^{i \theta}\right)\right|^{2} d \theta \\
&=\frac{r^{1-2 k}}{2 \pi\left(r-\left|z_{0}\right|\right)} \int_{0}^{2 \pi}\left|f\left(r e^{i \theta}\right)\right|^{2} d \theta \\
& \leqslant \frac{r A}{2 \pi\left(r-\left|z_{0}\right|\right)} \rightarrow \frac{A}{2 \pi}
\end{aligned}
$$

It follows that $g$ is bounded, hence constant, as desired.

Solution to 5.7.10: By Cauchy's Theorem [MH87, p. 152],

$$
\frac{1}{2 \pi i} \int_{|z|=r} f(z)^{2} \frac{d z}{z}=f(0)^{2}
$$

for $r>1$. Parameterizing the domain of integration by $z=r e^{i \theta}$, we find

$$
\frac{1}{2 \pi i} \int_{0}^{2 \pi} f\left(r e^{i \theta}\right)^{2} r i e^{i \theta} \frac{d \theta}{r e^{i \theta}}=f(0)^{2}
$$

Simplifying and taking real parts gives

$$
\int_{0}^{2 \pi}\left(u\left(r e^{i \theta}\right)^{2}-v\left(r e^{i \theta}\right)^{2}\right) d \theta=2 \pi\left(u(0)^{2}-v(0)^{2}\right)=0
$$

Solution to 5.7.11: We have

$$
\left|\frac{d^{m} f}{d z^{m}}\right| \leqslant M\left(1+|z|^{k}\right)
$$

for all z. Dividing both sides by $|z|^{k}$ and taking the limit as $|z|$ tends to infinity, we see that $d^{m} f / d z^{m}$ has a pole at infinity of order at most, $k$ so $d^{m} f / d z^{m}$ is a polynomial of degree, at most, $k$. Letting $n=m+k+1$, we must have that $d^{n} f / d z^{n}=0$ and that $n$ is the best possible such bound.

Solution to 5.7.12: 1. We have

$$
f(z)=\left(z-z_{1}\right)^{n_{1}} \cdots\left(z-z_{k}\right)^{n_{k}} g(z)
$$

where $g$ is an analytic function with no zeros in $\Omega$. So

$$
\frac{f^{\prime}(z)}{f(z)}=\frac{n_{1}}{z-z_{1}}+\cdots+\frac{n_{k}}{z-z_{k}}+\frac{g^{\prime}(z)}{g(z)} .
$$

Since $g$ is never 0 in $\Omega, g^{\prime} / g$ is analytic there, and, by Cauchy's Theorem [MH87, p. 152], its integral around $\gamma$ is 0 . Therefore,

$$
\frac{1}{2 \pi i} \int_{\gamma} \frac{f^{\prime}(z)}{f(z)} d z=\sum_{j=1}^{k} \frac{1}{2 \pi i} \int_{\gamma} \frac{n_{j}}{z-z_{j}} d z
$$



$$
=\sum_{j=1}^{k} n_{j}
$$

2. We have

so

$$
\frac{z f^{\prime}(z)}{f(z)}=\frac{z}{z-z_{1}}+\frac{g^{\prime}(z)}{g(z)}
$$

$$
\frac{1}{2 \pi i} \int_{\gamma} \frac{z f^{\prime}(z)}{f(z)} d z=z_{1} \text {. }
$$

Solution to 5.7.13: Suppose $f\left(z_{1}\right)=f\left(z_{2}\right)$ and let $\gamma$ be the segment connecting these two points. We have $0=\int_{y} f^{\prime}(z) d z$. Hence,

$$
\int_{\gamma}\left(f^{\prime}(z)-f^{\prime}\left(z_{0}\right)\right) d z=-f^{\prime}\left(z_{0}\right)\left(z_{2}-z_{1}\right) .
$$

Taking absolute values, we get

$$
\left|f^{\prime}\left(z_{0}\right)\right|\left|z_{2}-z_{1}\right| \leqslant \int_{\gamma}\left|f^{\prime}(z)-f^{\prime}\left(z_{0}\right)\right||d z|<\int_{\gamma}\left|f^{\prime}\left(z_{0}\right)\right||d z|=\left|f^{\prime}\left(z_{0}\right)\right|\left|z_{2}-z_{1}\right|,
$$

an absurd. We conclude, then, that $f$ is injective.

Solution to 5.7.14: It suffices to show that there exists an integer $n$ such that the image of $\Omega$ under $h(z)=f(z) / z^{n}$ contains no curves with positive winding number about 0 ; because it implies the existence of an analytic branch of the logarithm in $h(\Omega)$. Each closed curve in $h(\Omega)$ is the image of a closed curve in $\Omega$, so it is enough to show that the images of simple closed curves in $\Omega$ have winding number 0 about the origin. Consider two classes of simple closed curves in $\Omega$ :

- $\Gamma_{1}$, the curves with 0 in their interiors, and

- $\Gamma_{2}$, the curves with 0 in their exteriors.

Since $f$ has no zeros in $\Omega$, it is clear that if $\gamma \in \Gamma_{2}$, then $\operatorname{Ind}_{f(\gamma)}(0)=0$. From the shape of $\Omega$, it follows that all the curves in $\Gamma_{1}$ are homotopic. Let $n$ be the winding number about 0 of $f(\gamma)$ for $\gamma \in \Gamma_{1}$. Since $h$ has no zeros in $\Omega$, we must have $\operatorname{Ind}_{h(\gamma)}(0)=0$ for $\gamma \in \Gamma_{2}$. Fix $\gamma \in \Gamma_{1}$; then

$$
\operatorname{Ind}_{h(\gamma)}(0)=\frac{1}{2 \pi i} \int_{\gamma} \frac{h^{\prime}(z)}{h(z)} d z=\frac{1}{2 \pi i} \int_{\gamma}\left(\frac{f^{\prime}(z)}{f(z)}-\frac{n}{z}\right) d z=\operatorname{Ind}_{f(\gamma)(0)}-n=0
$$

and we are done.

Solution to 5.7.15: The analyticity of $f$ can be proved with the aid of Morera's Theorem [MH87, p. 173]. If $\gamma$ is a rectangle contained with its interior in $\mathbb{C} \backslash[0,1]$, then

$$
\int_{\gamma} f(z) d z=\int_{0}^{1}\left(\int_{\gamma} \frac{\sqrt{t}}{t-z} d z\right) d t=0
$$

(by Cauchy's Theorem [MH87, p. 152]). Morera's Theorem thus implies that $f$ is analytic. Alternatively, one can argue directly: for $z_{0}$ in $\mathbb{C} \backslash[0,1]$

$$
\lim _{z \rightarrow z_{0}} \frac{f(z)-f\left(z_{0}\right)}{z-z_{0}}=\lim _{z \rightarrow z_{0}} \int_{0}^{1} \frac{\sqrt{t}}{\left(t-z_{0}\right)(t-z)} d t=\int_{0}^{1} \frac{\sqrt{t}}{\left(t-z_{0}\right)^{2}} d t,
$$

where the passage to the limit in the integral is justified by the uniform convergence of the integrands.

To find the Laurent expansion [MH87, p. 246] about $\infty$ we assume $|z|>1$ and write

$$
\begin{aligned}
f(z) &=-\frac{1}{z} \int_{0}^{1} \frac{\sqrt{t}}{1-\frac{t}{z}} d t \\
&=-\frac{1}{z} \int_{0}^{\infty} \sum_{n=0}^{\infty} \frac{t^{n+\frac{1}{2}}}{z^{n}} d t
\end{aligned}
$$

The series converges uniformly on $[0,1]$, so we can integrate term by term to get

$$
f(z)=-\sum_{n=0}^{\infty}\left(\int_{0}^{1} t^{n+\frac{1}{2}} d t\right) z^{-n-1}=-\sum_{n=0}^{\infty} \frac{z^{-n-1}}{n+\frac{3}{2}} .
$$

Solution to 5.7.16: Let $c>0$. It suffices to show that there is a constant $M$ such that

$$
\left|f\left(z_{1}\right)-f\left(z_{2}\right)\right| \leqslant M\left|z_{1}-z_{2}\right| \text { for all } z_{1}, z_{2} \in\left\{z|\Re z>c,| z_{1}-z_{2} \mid<c\right\} \text {. }
$$

Fix two such points and let $\gamma$ be the circle of radius $c$ whose center is the midpoint of the segment joining them. $\gamma$ lies in the right half-plane, so, by Cauchy's Integral Formula [MH87, p. 167], we have

$$
\begin{aligned}
\left|f\left(z_{1}\right)-f\left(z_{2}\right)\right| & \leqslant \frac{1}{2 \pi} \int_{\gamma}\left|\frac{f(\zeta)}{z_{1}-\zeta}-\frac{f(\zeta)}{z_{2}-\zeta}\right||d \zeta| \\
& \leqslant \frac{N\left|z_{1}-z_{2}\right|}{2 \pi} \int_{\gamma} \frac{|d \zeta|}{\left|z_{1}-\zeta \| z_{2}-\zeta\right|}
\end{aligned}
$$
where $N$ is the supremum of $|f|$ in the right half-plane. On $\gamma,\left|z_{i}-\zeta\right| \geqslant \frac{c}{2}$ for
$i=1,2$, so

$$
\left|f\left(z_{1}\right)-f\left(z_{2}\right)\right| \leqslant \frac{4 N}{c}\left|z_{1}-z_{2}\right| .
$$

\section{$5.8$ Zeros and Singularities}

Solution to 5.8.1: $F$ is a map from $\mathbb{C}^{3}$ to the space of monic polynomials of degree 3 , that takes the roots of a monic cubic polynomial to its coefficients,

because if $\alpha, \beta$, and $\gamma$ are the zeros of $z^{3}-A z^{2}+B z-C$, we have

$$
A=\alpha+\beta+\gamma, \quad \alpha \beta+\alpha \gamma+\beta \gamma=B, \quad \alpha \beta \gamma=C .
$$

Thus, by the Fundamental Theorem of Algebra (for several different proofs see the Solution to Problem 5.7.5), it is clear that $F$ is onto. $F(1,1,0)=F(1,0,1)$, so $F$ is not injective, in fact, $F(u, v, w)=F(v, w, u)=F(w, u, v)$.

Solution to 5.8.2: Using Rouché's Theorem [MH87, p. 421], it is easy to conclude that $p(z)$ has two zeros inside the circle $|z|=3 / 4$.

Solution 2. The constant term of $p$ is 1 , so the product of its roots is 1 , in absolute value. They either all have absolute value 1 , or at least one lies inside $|z|<1$. The former is not possible, since the degree of $p$ is odd, it has at least one real root, and a calculation shows that neither 1 nor $-1$ is a root. So $p$ has a root in the unit disc.

Solution to 5.8.3: For $|z|=1$, we have

$$
\left|-z^{3}\right|=1>|f(z)|
$$

so, by Rouché's Theorem [MH87, p. 421], $f(z)-z^{3}$ and $z^{3}$ have the same number of zeros in the unit disc.

Solution to 5.8.4: Let $f_{1}$ and $f_{2}$ be defined by $f_{1}(z)=3 z^{100}$ and $f_{2}(z)=-e^{z}$. On the unit circle, we have

$$
\left|f_{1}(z)\right|=3>1=\left|f_{2}(z)\right| .
$$

By Rouché's Theorem [MH87, p. 421], we know that $f$ and $f_{1}$ have the same number of zeros in the unit disc, namely 100.

Let $\xi$ be a zero of $f$. Then

$$
f^{\prime}(\xi)=300 \xi^{99}-e^{\xi}=300 \xi^{99}-3 \xi^{100}=3 \xi^{99}(100-3 \xi) \neq 0
$$

so all the zeros of $f$ are simple.

Solution to 5.8.5: 1. Let $f(z)=4 z^{2}$ and $g(z)=2 z^{5}+1$. For $|z|=1$, we have

$$
|f(z)|=4>3 \geqslant|g(z)| \text {. }
$$

By Rouchê's Theorem [MH87, p. 421], $f$ and $p=f+g$ have the same number of roots in $|z|<1$. Since $f$ has two roots in $|z|<1$, so does $p$.

2. There is at least one real root, since $p$ has odd degree. The derivative is $p^{\prime}(z)=10 z^{4}+8 z$, so $p^{\prime}$ has two real zeros, namely at 0 and $-\sqrt[3]{4 / 5}$. Moreover, on the real axis, $p^{\prime}$ is positive on $(-\infty,-\sqrt[3]{4 / 5})$ and $(0, \infty)$, and negative on $(-\sqrt[3]{4 / 5}, 0)$. Thus, $p$ is increasing on the first two intervals and decreasing on the last one. Since $p(0)=1>0$, also $p(-\sqrt[3]{4 / 5})>0$, so $p$ has no root in $[-\sqrt[3]{4 / 5}, \infty)$ and exactly one in $(-\infty,-\sqrt[3]{4 / 5})$. (The real root is actually in $(-2,-1)$, since $p(-1)>0$ and $p(-2)<0$.)

Solution to 5.8.6: Let $p(z)=3 z^{9}+8 z^{6}+z^{5}+2 z^{3}+1$. For $|z|=2$, we have

$$
\begin{aligned}
\left|p(z)-3 z^{9}\right| &=\left|8 z^{6}+z^{5}+2 z^{3}+1\right| \\
& \leqslant 8|z|^{6}+|z|^{5}+2|z|^{3}+1 \\
&=561<1536=\left|3 z^{9}\right|
\end{aligned}
$$

so, by Rouche's Theorem [MH87, p. 421], $p$ has nine roots in $|z|<9$. For $|z|=1$, we have

$$
\begin{aligned}
\left|p(z)-8 z^{6}\right| &=\left|3 z^{9}+z^{5}+2 z^{3}+1\right| \\
& \leqslant 3|z|^{9}+|z|^{5}+2|z|^{3}+1 \\
&=7<8=\left|8 z^{6}\right|
\end{aligned}
$$

and we conclude that $p$ has six roots in $|z|<1$. Combining these results, we get that $p$ has three roots in $1<|z|<2$.

Solution to 5.8.7: For $z$ in the unit circle, we have

$$
\left|5 z^{2}\right|=5>4 \geqslant\left|z^{5}+z^{3}+2\right|
$$

so, by Rouche's Theorem [MH87, p. 421], $p(z)$ has two zeros in the unit disc. For $|z|=2$,

$$
\left|z^{5}\right|=32>30 \geqslant\left|z^{3}+5 z^{2}+2\right|
$$

so $p(z)$ has five zeros in $\{z|| z \mid<2\}$. We conclude then that $p(z)$ has three zeros in $1<|z|<2$.

Solution to 5.8.8: Let $p(z)=z^{7}-4 z^{3}-11$. For $z$ in the unit circle, we have

$$
|p(z)-11|=\left|z^{7}-4 z^{3}\right| \leqslant 5<11
$$

so, by Rouché's Theorem [MH87, p. 421], the given polynomial has no zeros in the unit disc. For $|z|=2$,

$$
\left|p(z)-z^{7}\right|=\left|4 z^{3}+11\right| \leqslant 43<128=\left|z^{7}\right|
$$

so there are seven zeros inside the disc $\{z|| z \mid<2\}$ and they are all between the two given circles.

Solution to 5.8.9: Since $p(0)=-1$ and $p(x) \rightarrow+\infty$ as $x \rightarrow \infty$ on the positive real axis, $p$ has at least one root on $(0, \infty)$. On the negative real axis $p$ is negative, in particular nonzero.

We have $p^{\prime}(z)=3 \varepsilon z^{2}-2 z$, which has roots at 0 and $2 / 3 \varepsilon$. The roots of $p^{\prime}$ lie in the convex hull of the roots of $p$, by the Gauss-Lucas Theorem [LR70, p. 94] On the imaginary axis, $p$ has a nonzero imaginary part, except at the origin. Therefore $p$ has no roots on the imaginary axis. Consequently $p$ must have at least one root in $\Re z<0$. Such a root is nonreal (since $p \neq 0$ on $(-\infty, 0)$ ), and nonreal roots of $p$ occur in conjugate pairs. Therefore $p$ has two roots in $\Re z<0$.

Solution 2. From the expression above for $p^{\prime}$ one sees that $p$ is decreasing on $(0,2 / 3 \varepsilon)$ and increasing on $(2 / 3 \varepsilon, \infty)$. Therefore $p$ has exactly one root on $(0, \infty)$. Let the root be $\lambda$. Since $p$ is nonzero on $(-\infty, 0]$, the other two roots form a conjugate pair, say $\mu$ and $\bar{\mu}$. Since the linear term in $p$ vanishes, we have

$$
\lambda \mu+\lambda \bar{\mu}+\mu \bar{\mu}=0,
$$

which can be written as

implying that $\mathfrak{\mu}<0$.

$$
-\lambda=\frac{2 \Re \mu}{|\mu|^{2}},
$$

Solution to 5.8.10: Rescale by setting $z=\varepsilon^{-1 / 5} w$. Then we need to show that exactly five roots of the rescaled polynomial

$$
p_{\varepsilon}(w)=w^{7}+w^{2}+\delta,
$$

with $\delta=\varepsilon^{2 / 5} \rightarrow 0$ as $\varepsilon \rightarrow 0$, converge to the unit circle as $\varepsilon \rightarrow 0$. We have $p_{0}(w)=w^{2}\left(w^{7}+1\right)$. Since two roots of $p_{0}$ are at $w=0$ and the other five are on the unit circle, the result follows from the continuity of the roots of a polynomial as functions of the coefficients, see Problem 5.8.30.

Solution 2. Let $q(z)=z^{2}+1$, so

$$
|p(z)-q(z)|=\varepsilon|z|^{7}=r^{7} \varepsilon^{-2 / 5}
$$

on the circle $|z|=r \varepsilon^{-1 / 5}$. Also,

$$
|q(z)|=\left|z^{2}+1\right|>r^{2} \varepsilon^{-2 / 5}-1
$$

on $|z|=r \varepsilon^{-1 / 5}$. Since $r<1, r^{7}<r^{2}$, and $r^{7} \varepsilon^{-2 / 5}<r^{2} \varepsilon^{-2 / 5}-1$ for $\varepsilon$ sufficiently small. Then $|p(z)-q(z)|<|q(z)|$ on $|z|=r \varepsilon^{-1 / 5}$, and by Rouché's Theorem [MH87, p. 421], $p$ and $q$ have the same number of zeros inside $|z|=r \varepsilon^{-1 / 5}$, namely two. By the Fundamental Theorem of Algebra (for several different proofs see the Solution to Problem 5.7.5), the other five roots must lie in $|z|>r \varepsilon^{-1 / 5}$.

Now take $q(z)=\varepsilon z^{7}$, so

$$
|p(z)-q(z)|=\left|z^{2}+1\right| \leqslant R^{2} \varepsilon^{-2 / 5}+1
$$

on $|z|=R \varepsilon^{-1 / 5}$, where

$$
|q(z)|=R^{7} \varepsilon^{-2 / 5} \text {. }
$$

Since $R>1$, we have $R^{7}>R^{2}$ and

$$
R^{2} \varepsilon^{-2 / 5}+1<R^{7} \varepsilon^{-2 / 5}
$$

for $\varepsilon$ sufficiently small. Thus, $|p(z)-q(z)|<|q(z)|$ on $|z|=R \varepsilon^{-1 / 5}$, so $p$ and $q$ have the same number of zeros inside $|z|=R \varepsilon^{-1 / 5}$, namely seven. This leaves precisely five roots between the two circles.

Solution to 5.8.11: The determinant of $A(z)$ is $8 z^{4}+6 z^{2}+1$. For $z$ in the unit circle, we have

$$
\left|8 z^{4}\right|=8>7 \geqslant\left|6 z^{2}+1\right|
$$

so, by Rouché's Theorem [MH87, p. 421], det $A(z)$ has four zeros in the unit disc. Also,

$$
\frac{d}{d z}(\operatorname{det} A(z))=z\left(32 z^{3}+12\right)
$$

with roots

$$
0, \pm i \sqrt{\frac{3}{8}}
$$

which are not zeros of $\operatorname{det} A(z)$. Thus, all the four zeros are simple, so they are distinct.

Solution to 5.8.12: Let $\gamma:[a, b] \rightarrow \mathbb{C}$ be a continuous function, with $0 \notin \gamma^{*}$, where we denote the image of $\gamma$ by $\gamma^{*}$. There is a continuous branch of $\arg \gamma$ on $[a, b]$. If $\theta, \phi$ are branches of $\arg \gamma$ on $[a, b]$, then $[\theta]_{a}^{b}=\theta(b)-\theta(a)=[\phi]_{a}^{b}$. We may therefore define $\omega(\gamma, 0)=(\theta(b)-\theta(a)) / 2 \pi$, where $\theta:[a, b] \rightarrow \mathbb{R}$ is any branch of $\arg \gamma$. Note we are not assuming that $\gamma$ is closed. If $\gamma$ lies in some open half-plane, then $|\omega(\gamma, 0)|<\frac{1}{2}$. If $\gamma$ is closed then $\omega(\gamma, 0)$ is an integer. If $\gamma(t)=\gamma_{1}(t) \gamma_{2}(t)$ where $\gamma_{1}, \gamma_{2}$ are curves not containing 0 , then $\omega(\gamma, 0)=$ $\omega\left(\gamma_{1}, 0\right)+\omega\left(\gamma_{2}, 0\right)$; in particular, if $\gamma^{n}(t)=\gamma(t)^{n}$ then $\omega\left(\gamma^{n}, 0\right)=n \omega(\gamma, 0)$ for $n \geqslant 1$. If $\gamma=\gamma_{1}+\cdots+\gamma_{k}$ is a join of curves not containing 0 , then $\omega(\gamma, 0)=$ $\sum_{j=0}^{k} \omega\left(\gamma_{j}, 0\right)$

Choose $R$ sufficiently large, so that the zeros of $f(z)=z^{4}+3 z^{2}+z+1$ lie inside $|z|=R$. Let $\gamma=\gamma_{1}+\gamma_{2}+\gamma_{3}$, where $\gamma_{1}=[i R, 0]$ (line segment from $i R$ to 0 on imaginary axis), $\gamma_{2}=[0, R]$ and $\gamma_{3}(t)=R e^{i t}, 0 \leqslant t \leqslant \frac{\pi}{2}$. Denote $\Gamma_{i}=f \circ \gamma_{i}, \Gamma=f \circ \gamma ;$ so $\Gamma=\Gamma_{1}+\Gamma_{2}+\Gamma_{3}$.

For $z=i y \in \gamma_{1}^{*}, f(z)=f(i y)=y^{4}-3 y^{2}+1+i y \neq 0$. As $0 \leqslant \Re f(z)=$ $y<R, \Gamma_{1}$ lies in the upper half-plane and therefore $\left|\omega\left(\Gamma_{1}, 0\right)\right| \leqslant \frac{1}{2}$.

For $z=x \in \gamma_{2}^{*}, f(z)=x^{4}+3 x^{2}+x+1 \neq 0$, this being a strictly increasing function in $x \geqslant 0$. $\Gamma_{2}$ is a subset of the positive real axis. Thus $\omega\left(\Gamma_{2}, 0\right)=0$.

We have $f(z)=z^{4}(1+g(z))$, where $g(z)=\left(3 z^{2}+z+1\right) / z^{4}$. Now $g(z) \rightarrow 0$ as $z \rightarrow \infty$. For large $R,\left|g\left(R e^{i t}\right)\right|<1$, and $1+g\left(R e^{i t}\right)$ lies in the disc centered at 1 radius 1 ; that is $1+g\left(R e^{i t}\right)$ lies in the right half-plane, and thus $|\omega(\sigma, 0)|<\frac{1}{2}$, where $\sigma(t)=1+g\left(\gamma_{3}(t)\right)$. Thus $\omega\left(\Gamma_{3}, 0\right)=\omega\left(\gamma_{3}^{4}, 0\right)+\omega(\sigma, 0)=4 \omega\left(\gamma_{3}, 0\right)+$ $\omega(\sigma, 0)=1+\omega(\sigma, 0)$

Thus $|\omega(\Gamma, 0)-1|=\left|\omega\left(\Gamma_{1}, 0\right)+\omega\left(\Gamma_{2}, 0\right)+\omega(\sigma, 0)\right|<\frac{1}{2}+\frac{1}{2}=1$. As $\Gamma$ is closed, $\omega(\Gamma, 0)$ is an integer, and so $\omega(\Gamma, 0)=1$. The Argument principle asserts that $\omega(\Gamma, 0)$ equals the number of zeros of $f$ inside $\gamma$. As $R$ is arbitrarily large, we deduce that $f$ has one zero in the first quadrant. The conjugate of this root is the only other zero in the right half-plane. Thus $f$ has two zeros in the right half-plane.

Solution to 5.8.13: Let $z_{1}, \ldots, z_{n}$ be the zeros of $p$, and $z$ a zero of $p^{\prime}, z \neq z_{i}$, $i=1, \ldots, n$. We have

$$
0=\frac{p^{\prime}}{p}(z)=\sum_{i=1}^{n} \frac{1}{z-z_{i}}
$$

Using the fact that $1 / \alpha=\bar{\alpha} /|\alpha|^{2}$ and conjugating we get

$$
\sum_{i=1}^{n} \frac{z-z_{i}}{\left|z-z_{i}\right|^{2}}=0
$$

which is clearly impossible if $\mathfrak{R} z \leqslant 0$.

This result can be generalized to give the Gauss-Lucas Theorem [LR70, p. 94]: The zeros of $p^{\prime}$ lie in the convex hull of the zeros of $p$. If $z_{1}, \ldots, z_{n}$ are the zeros of $p$, and $z$ is a zero of $p^{\prime}, z \neq z_{i}, i=1, \ldots, n$. We have, similar to the above,

$$
\sum_{i=1}^{n} \frac{z-z_{i}}{\left|z-z_{i}\right|^{2}}=0
$$

which is impossible if $z$ is not in the convex hull of $z_{1}, \ldots, z_{n}$.

Solution to 5.8.14: Let $a$ be a real zero of $f$. Then at $a$ the function $f$ restricted to $\mathbb{R}$ has a minimum, therefore $f^{\prime}(a)=0$. Hence the order of the zero of $f$ at $a$ is at least 2 . The function $g(z)=f(z) /(z-a)^{2}$ thus satisfies the same hypotheses as $f$. If $g(a)=0$ we can repeat the preceding argument. After finitely many repetitions we reach the desired conclusion.

Solution to 5.8.15: We may assume $r \neq 0$. Let $n=\operatorname{deg} p$ and $x_{1}<x_{2}<\cdots<$ $x_{k}$ be the roots of $p$, with multiplicities $m_{1}, m_{2}, \ldots, m_{k}$, respectively. If any $m_{j}$ exceeds 1 , then $p-r p^{\prime}$ has a root at $x_{j}$ of multiplicity $m_{j}-1$ (giving a total of $n-k$ roots all together). We have

$$
p(x)=c\left(x-x_{1}\right)^{m_{1}} \ldots\left(x-x_{k}\right)^{m_{k}}
$$

The logarithmic derivative $p^{\prime} / p$ is given by

$$
\frac{p^{\prime}(x)}{p(x)}=\sum_{j=1}^{k} \frac{m_{j}}{x-x_{j}}
$$

Its range on the interval $\left(x_{j}, x_{j+1}\right)(j=1, \ldots, k-1)$ is all of $\mathbb{R}$, since it is continuous there and

$$
\lim _{x \rightarrow x_{j}+} \frac{p^{\prime}(x)}{p(x)}=+\infty, \quad \lim _{x \rightarrow x_{j}-} \frac{p^{\prime}(x)}{p(x)}=-\infty
$$

Hence, there is a point $x \in\left(x_{j}, x_{j+1}\right)$ where $p^{\prime}(x) / p(x)=1 / r$; in other words, where $p-r p^{\prime}$ has a root. Thus, $p-r p^{\prime}$ has at least $k-1$ real roots other than the $n-k$ that are roots of $p$. Hence, $p-r p^{\prime}$ has at least $n-1$ real roots all together, and the nonreal ones come in conjugate pairs. Hence, it has only real roots.

Solution to 5.8.16: Let $R>\lambda+1$ and consider the contour $C_{R}=\Gamma_{R} \cup[-R i, R i]$, where $\Gamma_{R}=\{z|| z \mid=R, \Re z \leqslant 0\}$ and $[-R i, R i]=\{z \mid \mathfrak{R} z=0,-R \leqslant \Im z \leqslant R\}$.

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-006.jpg?height=818&width=811&top_left_y=632&top_left_x=646)

Let the functions $f$ and $g$ be defined by $f(z)=z+\lambda, g(z)=-e^{z}$. On $\Gamma_{R}$, we have

$$
|f(z)| \geqslant|z|-\lambda>1 \geqslant|g(z)|
$$

and on $[-R i, R i]$,

$$
|f(z)| \geqslant \lambda>1=|g(z)| \text {. }
$$

By Rouché's Theorem [MH87, p. 421], $f_{\lambda}$ and $f$ have the same number of zeros inside the contour $C_{R}$, so $f_{\lambda}$ has exactly one zero there. As this conclusion is valid for every $R>\lambda+1$, we conclude that $f_{\lambda}$ has one zero in the left half-plane. As $f$ is real on the real axis and $f(x) f(0)<0$ for $x$ small enough, we get that the zero of $f_{\lambda}$ is real.

Solution 2. We find the number of zeros of $f_{\lambda}$ in the left half-plane by considering a Nyquist diagram [Boa87, p. 106] relative to the rectangle with corners $i y$, $-x+i y,-x-i y$, and $-i y, x, y>\lambda$. This will give the change in $(1 / 2 \pi) \arg f_{\lambda}(z)$. Then we let $x, y \rightarrow \infty$.

On the right side of the rectangle, as $t$ ranges from $-y$ to $y$, then $f_{\lambda}(i t)=i t+\lambda-\cos t-i \sin t$ has a positive real part, and its imaginary part changes sign from negative to positive. On the top of the rectangle, as $s$ ranges from 0 to $-x, f_{\lambda}(s+i y)=s+i y+\lambda-e^{s} \cos y-i e^{s} \sin y$ has positive imaginary part, and its real part changes sign from positive to negative.

Similar reasoning shows that on the left side of the rectangle, $\Re f_{\lambda}<0$ and $\Re f_{\lambda}$ changes sign from positive to negative. On the bottom of the rectangle, $\mathfrak{f} f_{\lambda}<0$ and $\mathfrak{R} f_{\lambda}$ changes sign from negative to positive. Hence, $f_{\lambda}$ is never 0 on this rectangle and the image of the rectangle winds around the origin exactly once. By the Argument Principle [MH87, p. 419], $f_{\lambda}$ has exactly one zero in the interior of this rectangle. Letting $x$ and $y$ tend to infinity, we see that $f_{\lambda}$ has exactly one zero in the left half-plane. As $f_{\lambda}$ is real on the real axis and $f_{\lambda}(x) f_{\lambda}(0)<0$ for $x$ small enough, we get that the zero of $f_{\lambda}$ is real.

Solution to 5.8.17: For $|z|=1$, we have

$$
\left|z e^{\lambda-z}\right|=e^{\Re(\lambda-z)}>e^{0}=1=1-1 \mid
$$

so, by Rouché's Theorem [MH87, p. 421], the given equation has one solution in the unit disc. Let $f(z)=z e^{\lambda-z}$. As, for $z$ real, $f$ increases from $f(0)=0$ to $f(1)=e^{\lambda-1}>1$, by the Intermediate Value Theorem [Rud87, p. 93], $f(\xi)=1$ for some $\xi \in(0,1)$.

Solution to 5.8.18: By the Gauss-Lucas Theorem [LR70, p. 94] (see Solution to 5.8.13), if $p(z)$ is a polynomial, then all of the roots of $p^{\prime}(z)$ lie in the convex hull of the roots of $p(z)$. Let $z=1 / w$. The given equation becomes, after multiplying by $w^{n}, w^{n}+w^{n-1}+a=0$. The derivative of the polynomial on the left-hand side is $n w^{n-1}+(n-1) w^{n-2}$, which has roots 0 and $-(n-1) / n \geqslant 1 / 2$. For these two roots to lie in the convex hull of the roots of $w^{n}+w^{n-1}+a$, the latter must have at least one root in $|w| \geqslant 1 / 2$, which implies that $a z^{n}+z+1$ has at least one root in $|z| \leqslant 2$.

Solution 2. The product of the roots of $p(z)=a z^{n}+z+1$ is its constant term, namely 1, so all $p$ 's roots are unimodular or at least one is in $|z|<1$.

Solution to 5.8.19: Let $\Gamma_{R}$ be the closed curve in the first quadrant consisting of the interval $[0, R]$, the interval $[0, i R]$, and the circular arc centered at 0 joining the points $R$ and $i R$. It suffices to show that, for $R$ arbitrarily large, the function $g(z)=z^{4}+z^{3}+1$ has exactly one zero in the interior of $\Gamma_{R}$. We'll compare $g$ with $f(z)=z^{4}+1$. The function $f$ has one simple zero in the interior of $\Gamma_{R}$, at $z=e^{\pi i / 4}$. By Rouché's Theorem [MH87, p. 421], it remains to verify that $|f(z)-g(z)|<|f(z)|$ for $z$ in $\Gamma_{R}$. We have

- $|f(x)-g(x)|=|x|^{3}<x^{4}+1=|f(x)|$ for $x$ in $\mathbb{R}$ (since $|x|^{3} \leqslant 1$ for $x$ in $[-1,1]$ and $|x|^{3}<x^{4}$ for $x$ in $\left.\mathbb{R} \backslash[-1,1]\right)$

- $|f(i y)-g(i y)|=|y|^{3}<y^{4}+1=|f(i y)|$ for $y$ in $\mathbb{R}$

- $|f(z)-g(z)|=|z|^{3}=R^{3}<R^{4}-1 \leqslant|f(z)|$ for $|z|=R$ (with $R \geqslant 2$, so $\left.R^{4}-1>2 R^{3}-R^{3}=R^{3}\right)$. Solution to 5.8.20: Suppose that $f$ is zero free in the disc $|z|<1$, and hence also in the disc $|z| \leqslant 1$. Then $f$ has no zero in an open set containing the latter disc, so the function $g=1 / f$ is holomorphic there. By the Maximum Modulus Principle [MH87, p. 185] and our hypotheses, we have

$$
\frac{1}{m}<|g(0)| \leqslant \max \{|g(z)||| z \mid=1\}=\frac{1}{m},
$$

a contradiction.

Solution 2. For $|z| \leqslant 1$ Let $g(z)=-f(0)$. Then on $|z|=1$,

$$
|g(z)|=|f(0)|<m<|f(z)| .
$$

By Rouché's theorem [MH87, p. 421], $f$ and $f+g$ have the same number of zeros in the unit disc. Since $f+g=f-f(0)$ has at least one zero, the result follows.

Solution to 5.8.21: Let $p(z)$ denote the polynomial and suppose $p\left(z_{0}\right)=0$ for some $z_{0} \in \mathbb{D}$. Then $z_{0}$ is also a root of $(z-1) p(z)$. We then have

$$
0=a_{0} z_{0}^{n+1}+\left(a_{1}-a_{0}\right) z_{0}^{n}+\cdots+\left(a_{n}-a_{n-1}\right) z_{0}-a_{n} .
$$

Since all the $a_{i}$ 's are positive and $\left|z_{0}\right|<1$, we have, by the Triangle Inequality [MH87, p. 20],

$$
\begin{aligned}
a_{n} &=\left|a_{0} z_{0}^{n+1}+\left(a_{1}-a_{0}\right) z_{0}^{n}+\cdots+\left(a_{n}-a_{n-1}\right) z_{0}\right| \\
&<a_{0}+\left(a_{1}-a_{0}\right)+\cdots+\left(a_{n}-a_{n-1}\right) \\
&=a_{n},
\end{aligned}
$$

a contradiction.

Solution to 5.8.22: By Descartes' Rule of Signs [Caj69, p. 7], [Coh95, vol. 1, pag. 172], the polynomial $p(z)$ has zero or two positive real roots. As $p(0)=3$ and $p(1)=-2$, by the Intermediate Value Theorem [Rud87, p. 93], $p(z)$ has one and so, two, positive real roots. Replacing $z$ by $-z$, and again applying Descartes' Rule of Signs, we see that $p(-z)$ has one positive real root, so $p(z)$ has one negative real root. Applying Rouché's Theorem [MH87, p. 421] to the functions $f=p$ and $g=6 z$ on the unit circle, we see that $p$ has exactly one zero in the unit disc, which is positive as seen above. Hence, the real roots are distinct. (The same conclusion would follow from noticing that $p$ and $p^{\prime}$ have no common roots.) The imaginary roots are conjugate, so they are distinct as well.

Solution 2. Graphing the polynomial $y=x^{5}-6 x+3$ (for real $x$ ), we can see the result easily. First, $y^{\prime}=5 x^{4}-6$ and the only two real roots are $x=\pm \sqrt[4]{6 / 5}$ and none of them are multiple. Now looking at the limits when $x \rightarrow-\infty$ and $x \rightarrow \infty$, we can conclude that the graph looks like

So there are three distinct real roots. There cannot be a forth, otherwise $y^{\prime}$ would have a third root. The other two roots are then complex and not real; since they are conjugate, they are distinct, making for five distinct roots, three of them real.

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-009.jpg?height=786&width=1195&top_left_y=404&top_left_x=495)

Solution to 5.8.23: Let $z$ be a zero of the given polynomial with $|z|=r$. If $r \leqslant 1$, then $z$ lies in the given disc. For $r>1$, we have

$$
r^{2 n}=\left|-z^{n}\right|^{2}=\left|\sum_{i=0}^{n-1} c_{i} z^{i}\right|^{2} .
$$

By the Cauchy-Schwarz Inequality [MH93, p. 69], we get

$$
r^{2 n} \leqslant \sum_{i=0}^{n-1}\left|c_{i}\right|^{2} \sum_{i=0}^{n-1}\left|z^{i}\right|^{2} .
$$
The second sum is a finite geometric series which sums to $\frac{r^{2 n}-1}{r^{2}-1}$. Combining
these, we have

$$
r^{2 n}<\left(\sum_{i=0}^{n-1}\left|c_{i}\right|^{2}\right)\left(\frac{r^{2 n}}{r^{2}-1}\right) .
$$

Multiplying both sides by $\frac{r^{2}-1}{r^{2 n}}$, we get the result wanted.

Solution to 5.8.24: The product of all zeros of $P(x)$ (with multiplicities) equals $\pm 1$, so, if there are no roots inside the unit circle, then there are no roots outside the unit circle either. Hence, all roots are on the unit circle. From $P(0)=-1<0$ and $\lim _{x \rightarrow \infty} P(x)=+\infty$, it follows that $P(x)$ has a real zero in the interval $(0, \infty)$. Since it lies on the unit circle, it must be 1 , so $P(1)=0$. Solution to 5.8.25: Let $R>0$. Consider the semicircle with diameter $[-R i, R i]$ containing $R$ and its diameter. We will apply the Argument Principle [MH87, p. $419]$ to the given function on this curve.

Suppose $n$ is even. Then

$$
(i y)^{2 n}+\alpha^{2}(i y)^{2 n-1}+\beta^{2}=y^{2 n}+\beta^{2}-i \alpha^{2} y^{2 n-1}
$$

is always in the first quadrant for $y<0$, so the change in the argument when we move from 0 to $-R i$ is close to zero, for $R$ large. On the semicircle,

$$
z^{2 n}+\alpha^{2} z^{2 n-1}+\beta^{2}=z^{2 n}\left(1+\frac{\alpha^{2}}{z}+\frac{\beta^{2}}{z^{2 n}}\right)
$$

which is close to $z^{2 n}$ for $R$ large. So the argument changes by $2 \pi n$ when we go from $-R i$ to $R i$. From $R i$ to 0 ,

$$
y^{2 n}+\beta^{2}-i \alpha^{2} y^{2 n-1}
$$

is always in the fourth quadrant, so the change in the argument is close to zero, for $R$ large. The total change is then $2 \pi n$, so there are $n$ roots with positive real part.

Now, suppose that $n$ is odd. We have

$$
(i y)^{2 n}+\alpha^{2}(i y)^{2 n-1}+\beta^{2}=-y^{2 n}+\beta^{2}+i \alpha^{2} y^{2 n-1}
$$

so when we go from the origin to $-R i$, for $R$ large, the argument change is close to $-\pi$. The variation on the semicircle is again about $2 \pi n$. The change when $y$ goes from $R$ to 0 in the argument of

$$
-y^{2 n}+\beta^{2}+i \alpha^{2} y^{2 n-1}
$$

is about $-\pi$. Therefore, the number of zeros with, positive real part is now $(-\pi+2 \pi-\pi) / 2 \pi=n-1$.

Solution to 5.8.26: Let $\rho>0$ and consider the functions

$$
g_{n}(z)=f_{n}(1 / z)=1+z+\frac{z^{2}}{2 !}+\cdots+\frac{z^{n}}{n !} .
$$

Since $g_{n}(0) \neq 0$ for all $n, g_{n}(z)$ has a zero in $|z| \leqslant \rho$ if and only if $f_{n}(z)$ has a zero in $|z| \geqslant \rho . g_{n}(z)$ is a partial sum of the power series for $e^{z}$. Since this series converges locally uniformly and $\{z|| z \mid \leqslant \rho\}$ is compact, for any $\varepsilon>0$ there is $N>0$ such that if $n \geqslant N$, then $\left|g_{n}(z)-e^{z}\right|<\varepsilon$ for all $z$ in this disc. $e^{z}$ attains its minimum $m>0$ in this disc. Taking $N_{0}$ such that if $n \geqslant N_{0},\left|g_{n}(z)-e^{z}\right|<m / 2$ for all $z$ in the disc, we get that $g_{n}(z)$ is never zero for $|z| \leqslant \rho$. Therefore, $f_{n}(z)$ has no zeros outside this disc.

Solution to 5.8.27: If $z=x+i y$ then $e^{z}=e^{x} e^{i y}$, so $w=\exp z$ maps the horizontal line $\Re z=\lambda$ onto the ray $w=r e^{i \lambda}, r>0$. Thus if $0<\Re z<\pi / 2$ then $e^{z}$ lies in the first quadrant. As $\Re(z)$ is also positive, we have $e^{z}+z \neq 0$. If $-\pi / 2<\Re z<0$ then $e^{z}$ lies in the fourth quadrant; whereas $z$ lies in the lower half-plane. Again $e^{z}+z \neq 0$. Consider $z=x$ real. It is easy to see that $e^{x}=-x$ has one root. Thus $f(z)=e^{z}+z$ has precisely one zero in the strip $-\pi / 2<\Im z<\pi / 2$

We now consider the zeros of $g(z)$ in the same strip. As $\arg (-1 / z)=\pi-\arg z$ we deduce that, (writing $z=x+i y$ ), if $x>0,0<y<\pi / 2$, then $-1 / z$ lies in the second quadrant, whereas $e^{z}$ lies in the first quadrant; and if $x>0$, $-\pi / 2<y<0$ then $-1 / z$ lies in the third quadrant, whereas $e^{z}$ lies in the fourth quadrant; that is, $e^{z} \neq-1 / z$. When $z=x>0$ then $z e^{z}+1>0$. Therefore $z e^{z}+1 \neq 0$ for $x>0,-\pi / 2<y<\pi / 2$.

We'll use the notation of the solution to Problem 5.8.12. Consider the rectangular contour $\gamma$, which is the join of the line segments $\gamma_{1}=[-i \pi / 2,0]$, $\gamma_{2}=[0, i \pi / 2], \gamma_{3}=[i \pi / 2,-R+i \pi / 2], \gamma_{4}=[-R+i \pi / 2,-R-i \pi / 2]$, $\gamma_{5}=\left[-R-i \frac{\pi}{2},-i \frac{\pi}{2}\right]$. Write $h(z)=z e^{z}, \Gamma_{i}=h \circ \gamma_{i}$. We will study each piece separately.

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-011.jpg?height=897&width=1114&top_left_y=1102&top_left_x=495)

$\Gamma_{1}$ : We see that $h(-i \pi / 2)=-\pi / 2, h(0)=0$. If $0<y<\pi / 2$ then $h(-i y)=$ $-i y e^{-i y}=y e^{-i\left(\frac{\pi}{2}+y\right)}$ and $-\pi<\arg h(i y)=-\left(\frac{\pi}{2}+y\right)<-\pi / 2$. Thus $\Gamma_{1}$ is a curve from $-\frac{\pi}{2}$ to 0 which, apart from the end points, lies in the third quadrant.

$\Gamma_{2}$ : We find that $h(i \pi / 2)=-\frac{\pi}{2}, h(i y)=i y e^{i y}=y e^{i\left(\frac{\pi}{2}+y\right)}$. For $0<y<\pi / 2$, we have $\frac{\pi}{2}<\arg h(i y)=\frac{\pi}{2}+y<\pi$. Thus $\Gamma_{2}$ is a curve from 0 to $-\frac{\pi}{2}$ which, apart from the end points, lies in the second quadrant. $\Gamma_{3}$ : If $z=-x+i \frac{\pi}{2}, x>0$ then $h(z)=-\frac{\pi}{2} e^{-x}-i x e^{-x}$; so $h(z)$ lies in the third quadrant. Thus $\Gamma_{3}$ is a curve from $-\frac{\pi}{2}$ to $\zeta_{1}=h\left(-R+i \frac{\pi}{2}\right)=-e^{-R}\left(\frac{\pi}{2}+i R\right)$ ), which apart from the initial point, lies in the third quadrant.

$\Gamma_{4}$ : We see that $\zeta_{2}=h\left(-R-i \frac{\pi}{2}\right)=e^{-R}\left(-\frac{\pi}{2}+i R\right)=\overline{\zeta_{1}}$. If $z=-R+i y$, $-\frac{\pi}{2}<y<\frac{\pi}{2}$ then $|h(z)|=\left|-R+i y \| e^{-R+i y}\right| \leqslant\left(R+\frac{\pi}{2}\right) e^{-R}$; thus $h(z)$ lies in the unit disc, for large $R$. For such an $R, \Gamma_{4}$ is a curve in the unit disc from $\zeta_{1}$ to $\zeta_{2}$.

$\Gamma_{5}$ : If $z=-x-i \frac{\pi}{2}, x>0$ then $h(z)=-\frac{\pi}{2} e^{-x}+i x e^{-x}$ which is in the second quadrant. Thus $\Gamma_{5}$ is a curve from $\zeta_{2}$ to $-\frac{\pi}{2}$ which lies, apart from the final point, in the second quadrant.

The curve $1+\Gamma_{1}(t)$ lies in the lower half-plane; by choosing a branch of $\arg \left(1+\Gamma_{1}(t)\right)$, we see that $\omega\left(\Gamma_{1},-1\right)=1 / 2$. Likewise $\omega\left(\Gamma_{2},-1\right)=1 / 2$. If $\alpha=\arg \left(\zeta_{2}+1\right)$ with $0<\alpha<\pi / 2$, then $\omega\left(\Gamma_{5},-1\right)=(\pi-\alpha) / 2 \pi=\omega\left(\Gamma_{3},-1\right)$. The curve $1+\Gamma_{4}(t)$ lies in the disc $D(1 ; 1)$, and thus in the right half-plane. Hence $\omega\left(\Gamma_{4},-1\right)=\alpha / \pi$. Adding, we have $\omega(\Gamma,-1)=2$. By the argument principle, $h(z)=z e^{z}$ takes the value $-1$ twice inside $\gamma$. As $R$ is arbitrarily large, we conclude that $g(z)=z e^{2}+1$ has two zeros in the specified strip. So $f$ has one zero, whereas $g$ has two zeros in the strip.

Solution to 5.8.28: The function $\sin z$ satisfies the identity $\sin (z+\pi)=-\sin z$, and vanishes at the points $n \pi, n \in \mathbb{Z}$, and only at those points. For $m$ a positive integer, let $R_{m}$ denote the closed rectangle with vertices $\left(m-\frac{1}{2}\right) \pi+i \varepsilon$ and $\left(m+\frac{1}{2}\right) \pi \pm i \varepsilon$. The function $\sin z$ has no zeros on the boundary of $R_{m}$, so its absolute value has a positive lower bound, say $\delta$, there. (The number $\delta$ is independent of $m$ because of the identity $\sin (z+\pi)=-\sin z$.) Suppose $\left(m-\frac{1}{2}\right) \pi-|a|>\frac{1}{\delta}$. Then, for $z$ in $R_{m}$, we have

$$
\frac{1}{|z-a|} \leqslant \frac{1}{|z|-a} \leqslant \frac{1}{\left(m-\frac{1}{2}\right) \pi-|a|}<\delta
$$

implying that $\frac{1}{|z-a|}<|\sin z|$ on the boundary of $R_{m}$. By Rouché's Theorem [MH87, p. 421] then, the functions $\sin z$ and $f(z)=\sin z+\frac{1}{z-a}$ have the same number of zeros in the interior of $R_{m}$. Since $\sin z$ has one zero there, so does $f(z)$. As the condition on $m$ holds for all sufficiently large $m$, the desired conclusion follows.

Solution to 5.8.30: We will prove that the simple zeros of a polynomial depend continuously on the coefficients of the polynomial, around a simple root.

Consider

$$
p(z)=\hat{a}_{0}+\hat{a}_{1} z+\cdots+\hat{a}_{n} z^{n}=\hat{a}_{n} \prod_{j=1}^{s}\left(z-z_{j}\right)^{m_{j}} \quad\left(\hat{a}_{n} \neq 0\right) .
$$

For $\left(\xi_{0}, \ldots, \xi_{n-1}\right) \in \mathbb{C}^{n}$, let $F$ be the polynomial given by

$$
F(z)=\hat{a}_{0} \xi_{0}+\left(\hat{a}_{1}+\xi_{1}\right) z+\cdots+\left(\hat{a}_{n-1}+\xi_{n-1}\right) z^{n-1}+\hat{a}_{n} z^{n}
$$

and, for each $1 \leqslant k \leqslant s$, let $0<r_{k}<\min _{k \neq j}\left|z_{k}-z_{j}\right|$.

We will show that for some $\varepsilon>0,\left|\xi_{i}\right|<\varepsilon$ for $i=0, \ldots, n-1$ implies that $F$ has $m_{j}$ zeros inside the circle $C_{k}$ centered at $z_{k}$ with radius $r_{k}$.

Let $\zeta$ be the polynomial given by

$$
\zeta(z)=\xi_{0}+\xi_{1} z+\cdots+\xi_{n-1} z^{n-1} .
$$

On $C_{k}$, we have

$$
|\zeta(z)| \leqslant \varepsilon M_{k}, \quad M_{k}=\sum_{j=1}^{n-1}\left(r_{k}+\left|z_{k}\right|\right)^{j}
$$

and

$$
|p(z)| \geqslant\left|\hat{a}_{n}\right| r_{k}^{m_{k}} \prod_{\substack{j=1 \\ j \neq k}}^{k}\left(\left|z_{j}-z_{k}\right|-r_{k}\right)^{m_{j}}=\delta_{k}>0 .
$$

Taking $\varepsilon<\delta_{k} / M_{k}$, we get $|\zeta(z)|<|p(z)|$ on $C_{k}$; therefore, by Rouché's Theorem [MH87, p. 421], $F$ has the same number of zeros in $C_{k}$ as $p$. As in this domain $p$ has a single zero with multiplicity $m_{j}$, we are done.

Solution to 5.8.31: From

$$
f^{(k)}(z)=\sum_{n \geqslant k} n(n-1) \cdots(n-k+1) a_{n} z^{n-k},
$$

we conclude that

$$
\left|f^{(k)}\left(r e^{i \theta}\right)\right| \leqslant\left|f^{(k)}(r)\right|
$$

for $0<r<1$, and $0<\theta \leqslant 2 \pi$. Suppose $f$ can be analytically continued in a neighborhood of $z=1$; then its power series expansion around $z=1 / 2$,

$$
\sum_{n=0}^{\infty} \beta_{n}(z-1 / 2)^{n}
$$

has a radius of convergence $R>1 / 2$. Let $\left(\gamma_{n}\right)$ be the Taylor coefficients [MH87, p. 233] of the power series expansion of $f$ around the point $(1 / 2) e^{i \theta}$. By the above inequality, $\left|\gamma_{n}\right| \leqslant\left|\beta_{n}\right|$. Therefore, the power series around the point $(1 / 2) e^{i \theta}$ has a radius of convergence of at least $R$. So $f$ can be analytically continued in a neighborhood of every point of the unit circle. However, the Maclaurin series [MH87, p. 234] of $f$ has radius of convergence 1, which implies that at least one point on the unit circle is a singularity of $f$, a contradiction.

Solution to 5.8.32: Without loss of generality, assume $r=1$.

Suppose $f$ is analytic at $z=1$. Then $f$ has a power expansion centered at 1 with positive radius of convergence. Therefore, $f$ has a power series expansion centered at $z=1 / 2$ with radius $1 / 2+\varepsilon$ for some positive $\varepsilon$. 

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-014.jpg?height=691&width=816&top_left_y=213&top_left_x=641)

As

$$
f^{(n)}\left(\frac{1}{2}\right)=\sum_{k \geqslant n} \frac{k !}{(k-n) !} a_{k}\left(\frac{1}{2}\right)^{k-n}
$$

we have, for $1<x<1+\varepsilon$,

$$
\begin{aligned}
f(x) &=\sum_{n \geqslant 0} \frac{f^{(n)}(1 / 2)}{n !}\left(x-\frac{1}{2}\right)^{n} \\
&=\sum_{n \geqslant 0} \frac{1}{n !}\left(x-\frac{1}{2}\right)^{n} \sum_{k \geqslant n} \frac{k !}{(k-n) !} a_{k}\left(\frac{1}{2}\right)^{k-n} \\
&=\sum_{k \geqslant 0} a_{k} \sum_{n=0}^{k} \frac{k !}{n !(k-n) !}\left(x-\frac{1}{2}\right)^{n}\left(\frac{1}{2}\right)^{k-n} \\
&=\sum_{k \geqslant 0} a_{k} x^{k}
\end{aligned}
$$

which is absurd because we assumed the radius of convergence of $\sum a_{n} z^{n}$ to be 1 . This contradiction shows that $f$ cannot be analytic at $z=1$.

Solution to 5.8.33: As

$$
(\tan z)^{-2}-z^{-2}=\frac{z^{2}-(\tan z)^{2}}{z^{2}(\tan z)^{2}}
$$

the Maclaurin expansion [MH87, p. 234] of the numerator has no terms of degree up to 3 , whereas the expansion of the denominator starts with $z^{4}$, therefore, the limit is finite.

As

$$
\tan z=z+\frac{1}{3} z^{3}+o\left(z^{4}\right) \quad(z \rightarrow 0)
$$

we have

$$
(\tan z)^{-2}-z^{-2}=\frac{z^{2}-z^{2}-\frac{2}{3} z^{4}+o\left(z^{4}\right)}{z^{4}+o\left(z^{4}\right)} \quad(z \rightarrow 0)
$$

so the limit at 0 is $-2 / 3$.

Solution to 5.8.34: We use the fact that an entire function is a polynomial exactly when it has limit $\infty$ at $\infty$. (If the function tends to $\infty$ at $\infty$ then it has a pole at $\infty$. In this case, its Laurent series about $\infty$, which is its power series, has only finitely many nonzero terms with positive exponent. The other direction is clear.)

Suppose $g$ is not a polynomial. Then $\left(g\left(z_{n}\right)\right)$ is bounded for some sequence $\left(z_{n}\right)$ tending to $\infty$. Therefore $\left(f\left(z_{n}\right)\right)$ is bounded, against the hypothesis. Hence $g$ is a polynomial.

Suppose $f$ is not a polynomial. Then $\left(f\left(w_{n}\right)\right)$ is bounded for some sequence ( $w_{n}$ tending to $\infty$. As $g$ is onto, there is, for each $n \in \mathbb{N}, w_{n}^{\prime} \in \mathbb{C}$ such that $w_{n}=g\left(w_{n}^{\prime}\right)$. As $w_{n} \rightarrow \infty$ we have $w_{n}^{\prime} \rightarrow \infty$, which is contrary to the hypothesis since $f\left(g\left(w_{n}^{\prime}\right)\right)$ stays bounded. Hence $f$ is a polynomial.

Solution to 5.8.35: When we write $f$ as a single fraction, with denominator $\prod_{i=1}^{n}\left(z-z_{i}\right)$ we see that the number of zeros of $f$ in $\mathbb{C}$ is given by the degree of the numerator of $f$, since the $z_{j}$ 's are no zeros of $f$. The order of the zero at infinity of $f(1 / t)$ gives the difference between the degree of the denominator and the degree of the numerator. We have

$$
\begin{aligned}
f\left(\frac{1}{t}\right) &=\sum_{i=1}^{n} \frac{a_{i} t}{1-z_{i} t}=\sum_{i=1}^{n} a_{i} t\left(1+z_{i} t+z_{i}^{2} t^{2}+\cdots\right) \\
&=\sum_{p=0}^{\infty} S_{p} t^{p+1}=\sum_{p=m}^{\infty} S_{p} t^{p+1}
\end{aligned}
$$

Thus $f(1 / t)$ has a zero of order $m+1$ at $t=0$. Since the denominator of $f(z)$ has degree $n$, the numerator has degree $n-m-1$, so $f$ has $n-m-1$ zeros in $\mathbb{C}$.

We cannot have $m \geqslant n$ because the Vandermonde determinant, see the Solution to Problem 7.2.11 or [HK61, p. 125]

$$
\left|\begin{array}{ccccc}
1 & z_{1} & z_{1}^{2} & \cdots & z_{1}^{n-1} \\
1 & z_{2} & z_{2}^{2} & \cdots & z_{2}^{n-1} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & z_{n} & z_{n}^{2} & \cdots & z_{n}^{n-1}
\end{array}\right|
$$

is not zero, so the linear system $\sum_{i=1}^{k} a_{i} z_{i}^{k}=0$ has only the trivial solution for
$k=0, \ldots, n-1$ 

\section{$5.9$ Harmonic Functions}

Solution to 5.9.1: Derivating twice, we can see that

$$
\frac{\partial^{2} u}{\partial x^{2}}=6 x=-\frac{\partial^{2} u}{\partial y^{2}},
$$

so $\Delta u=0$. The function $f$ is then given by (see [Car63b, pp. 126-127])

$$
f(z)=2 u\left(\frac{z}{2}, \frac{z}{2 i}\right)=2\left(\frac{z^{3}}{8}-3 \frac{z}{2} \frac{z^{2}}{(-4)}\right)=z^{3} .
$$

so, up to a constant, $v(x, y)=3 x^{2} y-y^{3}+k$.

Solution 2. Using the Cauchy-Riemann equations [MH87, p. 72], we see that

$$
\frac{\partial v}{\partial y}=3 x^{2}-3 y^{2}
$$

and integrating with respect to $y$ we obtain

$$
v(x, y)=3 x^{2} y-y^{3}+\psi(x)
$$
and using the Cauchy-Riemann equations again, this time $\frac{\partial v}{\partial x}=-\frac{\partial u}{\partial y}$ we see
that $\psi^{\prime}(x)=0$, so $v(x, y)=3 x^{2} y-y^{3}+k$.

Solution to 5.9.2: Since $u$ is the real part of an analytic function, it is harmonic in the unit disc $\mathbb{D}$. By Green's Theorem [Rud87, p. 253],

$$
\int_{\partial \mathbb{D}} \frac{\partial u}{\partial y} d x-\frac{\partial u}{\partial x} d y=-\int_{\mathbb{D}}\left(\frac{\partial^{2} u}{\partial x^{2}}+\frac{\partial^{2} u}{\partial y^{2}}\right) d x d y=0 .
$$

Solution to 5.9.3: 1. Let $f=u+i v$. Then $v$ is identically 0 on the unit circle. By the Maximum Modulus Principle [MH87, p. 185] for harmonic functions, $v$ is identically zero on $\mathbb{D}$. By the Cauchy-Riemann equations [MH87, p. 72], the partial derivatives of $u$ vanish; hence, $u$ is constant also and so is $f$.

2. Consider

$$
f(z)=i \frac{z+1}{z-1} \text {. }
$$

$f$ is analytic everywhere in $\mathbb{C}$ except at 1 . We have

$$
f\left(e^{i \theta}\right)=i \frac{e^{i \theta}+1}{e^{i \theta}-1}=i \frac{e^{i \theta / 2}+e^{-i \theta / 2}}{e^{i \theta / 2}-e^{-i \theta / 2}}=\cot \left(\frac{\theta}{2}\right) \in \mathbb{R} .
$$

Solution to 5.9.4: We have $u=\Re z^{s}, z^{s} \equiv e^{s \log z}$, and $\log z=$ principal branch of $\log$ with $-\pi<\arg z<\pi . z^{s}$ is analytic in the slit plane $\mathbb{C} \backslash(-\infty, 0]$ and $\frac{d}{d z} e^{s \log z}=s z^{s-1}$. Hence, $u=\mathfrak{P} z^{s}$ is harmonic in the same domain. Solution to 5.9.5: Let $v$ be the harmonic conjugate of $u$. Then, $f=u+i v$ is an entire function. Consider $h=e^{-f}$. Since $u \geqslant 0,|h| \leqslant 1, h$ is a bounded entire function and, by Liouville's Theorem [MH87, p. 170], a constant. Therefore, $u$ is constant as well.

Solution to 5.9.6: Let $v$ be a harmonic conjugate of $u$, and let $f=e^{u+i v}$. Then $f$ is an entire function and, for $|z|>1$, we have

$$
|f(z)|=e^{u(z)} \leqslant e^{a \log |z|+b}=e^{b}|z|^{a} .
$$

Let $n$ be a positive integer such that $n \geqslant a$. Then the function $z^{-n} f(z)$ has an isolated singularity at $\infty$ and, by the preceding inequality, is bounded in a neighborhood of $\infty$. Hence, $\infty$ is a removable singularity of $z^{-n} f(z)$ and, thus, is, at worst, a pole of $f$. That means $f$ is an entire function with, at worst, a pole at $\infty$, and so $f$ is a polynomial. Since nonconstant polynomials are surjective and $f$ omits the value $0, f$ must be constant, and so is $u$, as desired.

Solution to 5.9.7: The linear fractional transformation $w=(1+z) /(1-z)$ maps the unit disc to the halfplane $\Re w>0$, with the upper and lower boundary semicircles mapped to the halffines $i \mathbb{R}_{+}$and $i \mathbb{R}_{-}$, respectively. A branch of $\log w$, defined on $\mathbb{C} \backslash \mathbb{R}_{-}$, has

$$
\oiint \log w= \begin{cases}\pi / 2, & w \in i \mathbb{R}_{+} \\ -\pi / 2, & w \in \mathbb{R}_{-},\end{cases}
$$

so a solution is given by $f$ defined by

$$
f(z)=\frac{2}{\pi} \Re \log \frac{1+z}{1-z} .
$$

\section{$5.10$ Residue Theory}

Solution to 5.10.1: Since the $r_{i}$ 's are distinct, $f$ has a simple pole at each of these points. If $A_{1}, A_{2}, \ldots, A_{n}$ are the residues of $f$ at each of these points, then

$$
g(z)=f(z)-\frac{A_{1}}{z-r_{1}}-\frac{A_{2}}{z-r_{2}}-\cdots-\frac{A_{n}}{z-r_{n}}
$$

is entire. Clearly, $g$ tends to zero as $z$ tends to infinity, so, by the Maximum Modulus Principle [MH87, p. 185], $g$ must be identically zero and we are done.

Solution to 5.10.2: We have

$$
a_{-1}=\operatorname{Res}(\cot \pi z,-1)+\operatorname{Res}(\cot \pi z, 0)+\operatorname{Res}(\cot \pi z, 1)=\frac{3}{\pi} .
$$

For $n<-1$, the coefficients are given by

$$
\begin{aligned}
a_{n} &=\frac{1}{2 \pi} \int_{|z|=3 / 2} \frac{\cot \pi z}{z^{n+1}} d z \\
&=\operatorname{Res}\left(\frac{\cot \pi z}{z^{n+1}},-1\right)+\operatorname{Res}\left(\frac{\cot \pi z}{z^{n+1}}, 1\right) \\
&=\lim _{z \rightarrow-1}(z+1) \frac{\cot \pi z}{z^{n+1}}+\lim _{z \rightarrow 1}(z-1) \frac{\cot \pi z}{z^{n+1}} \\
&=\left((-1)^{-n-1}+1\right) \frac{1}{\pi} .
\end{aligned}
$$

Solution to 5.10.3: The function $f(z)=z /(z-1)(z-2)(z-3)$ has simple poles at $z \in A=\{1,2,3\}$, with residues $\operatorname{Res}(f, 1)=1 / 2, \operatorname{Res}(f, 2)=-2$, and $\operatorname{Res}(f, 3)=3 / 2$. For $f$ to have a primitive in $U$, it is necessary and sufficient that $\int_{\gamma} f(z) d z=0$ for all (piecewise $C^{1}$ ) closed paths $\gamma$ in $U$.

Let $\gamma$ be a closed path in $U$, with image $\gamma^{*}$. Then $U^{c}=\{|z| \leqslant 4\}$ is connected and lies in one connected component of $\mathbb{C} \backslash \gamma^{*}$. Therefore the winding numbers are the same, $\omega(\gamma, 1)=\omega(\gamma, 2)=\omega(\gamma, 3)$.

By the Residue Theorem, [MH87, p. 280],

$$
\begin{aligned}
\int_{\gamma} f(z) d z &=2 \pi i \sum_{z \in A} \operatorname{Res}(f, z) \omega(\gamma, z) \\
&=2 \pi i \omega(\gamma, 1) \sum_{z \in A} \operatorname{Res}(f, z) \\
&=0 .
\end{aligned}
$$

Therefore $f$ has a primitive in $U$. On the other hand, if

$$
g(z)=\frac{z^{2}}{(z-1)(z-2)(z-3)}
$$

then $\operatorname{Res}(g, 1)=1 / 2, \operatorname{Res}(g, 2)=-4$ and $\operatorname{Res}(g, 3)=9 / 2$. If $\gamma$ is the circle $|z|=5$, then

$$
\int_{\gamma} g(z) d z=2 \pi i \sum_{z \in A} \operatorname{Res}(g, z) \omega(\gamma, z) \neq 0,
$$

and $g$ has no primitive in $U$.

Solution to 5.10.4: If the roots of $f$ are not distinct, then some $x_{0}$ satisfies $f\left(x_{0}\right)=f^{\prime}\left(x_{0}\right)=0$. But $f^{\prime}(x)=1+x+\cdots+\frac{x^{m-1}}{(m-1) !}$, so

$$
0=f\left(x_{0}\right)=f^{\prime}\left(x_{0}\right)=\frac{x_{0}^{m}}{m !}
$$

and $x_{0}=0$. However, 0 is clearly not a root of $f$. Hence, the roots of $f$ are distinct and nonzero.

For $0 \leqslant k \leqslant n-2$, consider the integral

$$
I_{k}=\int_{C_{r}} \frac{z^{k}}{f(z)} d z
$$

where $C_{r}$ is a circle of radius $r$ centered at the origin such that all the roots of $f$ lie inside it. By Cauchy's Theorem [MH87, p. 152], $I_{k}$ is independent of $r$, and as $r \rightarrow \infty$, the integral tends to 0 . Hence, $I_{k}=0$. By the Residue Theorem [MH87, p. 280],

$$
0=\sum \operatorname{Res} \frac{z^{k}}{f(z)}=\sum_{i=1}^{N} \frac{z_{i}^{k}}{f^{\prime}\left(z_{i}\right)}=\sum_{i=1}^{n} \frac{z_{i}^{k}}{f\left(z_{i}\right)-\frac{z_{i}^{n}}{n !}}=n ! \sum_{i=1}^{n} z_{i}^{k-n} .
$$

Since $2 \leqslant m-k \leqslant n$, we get the desired result.

Solution to 5.10.5: Let the disc centered at the origin with radius $r$ contain all the zeros of $Q$. Let $C_{R}$ be a circle centered at the origin with radius $R>r$. Then, by the Deformation Theorem [MH87, p. 148],

$$
\int_{C} \frac{P(z)}{Q(z)} d z=\int_{C_{R}} \frac{P(z)}{Q(z)} d z
$$

where $C$ is any closed curve outside $|z|=r$. As

$$
\frac{P(z)}{Q(z)}=O\left(|z|^{-2}\right) \quad(|z| \rightarrow \infty)
$$

we have

$$
\int_{C_{R}} \frac{P(z)}{Q(z)} d z=O\left(|z|^{-2}\right) 2 \pi R=o(1) \quad(R \rightarrow \infty)
$$

and the result follows.

Solution to 5.10.6: Letting $z=i \theta$, we have

$$
\cos \theta=\frac{1}{2}\left(z+z^{-1}\right)
$$

and $d \theta=d z / i z$, so that

$$
\frac{1}{2 \pi} \int_{0}^{2 \pi} e^{2 \zeta \cos \theta} d \theta=\frac{1}{2 \pi} \int_{\gamma} e^{\zeta\left(z+z^{-1}\right)} \frac{d z}{z}
$$

where $\gamma$ is the unit circle. Next,

$$
\begin{aligned}
\frac{1}{2 \pi i} \int_{\gamma} e^{\zeta\left(z+z^{-1}\right) \frac{d z}{z}} &=\frac{1}{2 \pi i} \int_{\gamma} \sum_{n=0}^{\infty} \frac{1}{n !}\left(\frac{\zeta}{z}\right)^{n} e^{\zeta z} \frac{d z}{z} \\
&=\sum_{n=0}^{\infty} \frac{1}{2 \pi i} \int_{\gamma} \frac{\zeta^{n}}{n !} \frac{e^{\zeta z}}{z^{n}} \frac{d z}{z}
\end{aligned}
$$

Now,

$$
\begin{aligned}
\frac{e^{\zeta z}}{z^{n+1}} &=\frac{1}{z^{n+1}}\left(1+\zeta z+\frac{1}{2}(\zeta z)^{2}+\cdots+\frac{1}{k !}(\zeta z)^{k}+\cdots\right) \\
&=\frac{1}{z^{n+1}}+\frac{\zeta}{z^{n}}+\cdots+\frac{1}{n !} \frac{\zeta^{n}}{z}+\frac{\zeta^{n+1}}{(n+1) !}+\cdots
\end{aligned}
$$

Thus, the residue at zero is $\frac{\zeta^{n}}{n !}$ and

$$
\frac{1}{2 \pi i} \int_{\gamma} \frac{e^{\zeta z}}{z^{n+1}} d z=\frac{\zeta^{n}}{n !}
$$

hence

$$
\left(\frac{\zeta^{n}}{n !}\right)^{2}=\frac{1}{2 \pi i} \int_{\gamma} \frac{\zeta^{n} e^{\zeta z}}{n ! z^{n}} \frac{d z}{t}
$$

and the result follows.

Solution to 5.10.7: As the singularities of the integrand, call it $f$, all lie inside the unit disc, we have [MH87, p. 286],

$$
I=-\operatorname{Res}(f(z), \infty)=\operatorname{Res}\left(\frac{1}{z^{2}} f\left(\frac{1}{z}\right), 0\right) .
$$

We have $\frac{1}{z^{2}} f\left(\frac{1}{z}\right)=\frac{(1+2 z)^{2}}{z(2-z)}$, which has residue $\frac{1}{2}$ at 0 . Therefore $I=\frac{1}{2}$.

Solution 2. Let $f(z)=\frac{(z+2)^{2}}{z^{2}(2 z-1)}$. Then $f$ is meromorphic with a double pole at $z=0$ and a simple pole at $z=1 / 2$. Both singularities are inside the contour of integration. By the residue theorem we have

$$
I=\operatorname{Res}(f(z), 0)+\operatorname{Res}\left(f(z), \frac{1}{2}\right) .
$$

We have, near the origin,

$$
f(z)=-\frac{1}{z^{2}}\left(\frac{(z+2)^{2}}{1-2 z}\right)=-\frac{1}{z^{2}}\left(4+4 z+z^{2}\right)\left(1+2 z+4 z^{2}+\cdots\right)=-\frac{4}{z^{2}}-\frac{12}{z}+\cdots
$$

so the residue at the origin is $-12$.

To find the residue at $\frac{1}{2}$ we write $f$ as

$$
f(z)=\frac{(z+2)^{2}}{2 z^{2}\left(z-\frac{1}{2}\right)},
$$

and get

$$
\operatorname{Res}\left(f(z), \frac{1}{2}\right)=\left.\frac{(z+2)^{2}}{2 z^{2}}\right|_{z=\frac{1}{2}}=\frac{25}{2} \text {. }
$$

Therefore, $I=-12+12.5=0.5$.

Solution 3. Since $f$ has no singularities outside the unit disc, we have, by Cauchy's Theorem [MH87, p. 152],

$$
I=\frac{1}{2 \pi i} \int_{|z|=r} f(z) d z
$$

for any $r>1$. Thus

$$
I=\frac{1}{2 \pi} \int_{0}^{2 \pi} \frac{\left(r e^{i \theta}+2\right)^{2} r e^{i \theta}}{r^{2} e^{2 i \theta}\left(2 r e^{i \theta}-1\right)} d \theta \quad(r>1)
$$

It is easy to see that the integrand approaches $\frac{1}{2}$ uniformly when $r \rightarrow \infty$, so $I=\frac{1}{2}$

Solution to 5.10.8: We have

$$
\int_{C_{a}} \frac{z^{2}+e^{z}}{z^{2}(z-2)} d z=\int_{C_{a}} \frac{1}{(z-2)} d z+\int_{C_{a}} \frac{e^{z}}{z^{2}} \frac{1}{(z-2)} d z
$$

We will use the Residue Theorem [MH87, p. 280], to compute the integral, and for the first integral:

$$
\operatorname{Res}\left(\frac{1}{z-2}, 0\right)=0 \quad \text { and } \quad \operatorname{Res}\left(\frac{1}{z-2}, 2\right)=1
$$

Let $f(z)=\frac{e^{z}}{z^{2}(z-2)} \cdot$ The following expansions hold:

$$
\begin{aligned}
\frac{1}{z-2} &=-\frac{1}{2}\left(1+\frac{z}{2}+\frac{z^{2}}{4}+\frac{z^{3}}{8}+\cdots\right) \\
\frac{1}{z^{2}(z-2)} &=-\frac{1}{2 z^{2}}-\frac{1}{4 z}-\frac{1}{8}-\cdots \\
\frac{e^{z}}{z^{2}(z-2)} &=\left(1+z+\frac{z^{2}}{2}+\frac{z^{3}}{3 !}+\cdots\right)\left(-\frac{1}{2 z^{2}}-\frac{1}{4 z}-\frac{1}{8}-\cdots\right) \\
&=-\frac{1}{2 z^{2}}-\frac{3}{4 z}-\frac{5}{8}-\cdots
\end{aligned}
$$

thus

$$
\operatorname{Res}(f(z), 0)=-\frac{3}{4}
$$

Also,

$$
\operatorname{Res}(f(z), 2)=\lim _{z \rightarrow 2} \frac{e^{z}}{z^{2}}=\frac{e^{2}}{4} \text {. }
$$

Therefore, we have, for $a>2$,

$$
\begin{aligned}
\int_{C_{a}} \frac{z^{2}+e^{z}}{z^{2}(z-2)} d z &=2 \pi i\left(\operatorname{Res}\left(\frac{1}{z-2}, 2\right)+\operatorname{Res}(f, 0)+\operatorname{Res}(f, 2)\right) \\
&=\frac{\pi i}{2}\left(e^{2}+1\right)
\end{aligned}
$$

and for $a<2$,

$$
\int_{C_{a}} \frac{z^{2}+e^{z}}{z^{2}(z-2)} d z=2 \pi i \operatorname{Res}(f(z), 0)=-\frac{3 \pi i}{2} .
$$

Solution to 5.10.9: For $|z|=r$ we have $\bar{z}=\frac{r^{2}}{z}$, so we can rewrite $I(a, r)$ as

$$
\begin{aligned}
I(a, r) &=\int_{|z|=r} \frac{1}{a-\frac{r^{2}}{z}} d z=\int_{|z|=r} \frac{z}{a z-r^{2}} d z \\
&=\frac{1}{a} \int_{|z|=r} \frac{z}{z-\frac{r^{2}}{a}} d z \quad(a \neq 0) .
\end{aligned}
$$

If $|a|<r$ the integrand is analytic in and on the circle $|z|=r$, so $I(a, r)=0$ by Cauchy's Theorem [MH87, p. 152]. (Use the second-last expression for the case $a=0$.)

If $|a|>r$ then Cauchy's Integral Formula [MH87, p. 167] gives

$$
I(a, r)=\frac{2 \pi i}{a}\left(\frac{r^{2}}{a}\right)=\frac{2 \pi i r^{2}}{a^{2}} .
$$
Solution to 5.10.10: Let $f(z)=\sqrt{z^{2}-1}=\sum_{k=0}^{\infty}\left(\begin{array}{c}1 / 2 \\ k\end{array}\right) z^{2 k} \cdot f$ is analytic for
$|z|>1$; therefore,

$$
\int_{C} \sqrt{z^{2}-1} d z=-2 \pi i \operatorname{Res}(f(z), \infty) \text {. }
$$

We have

$$
\begin{aligned}
\frac{1}{z^{2}} f\left(\frac{1}{z}\right) &=\frac{1}{z^{3}} \sqrt{1-z^{2}} \\
&=\frac{1}{z^{3}} \sum_{k \geqslant 0}\left(\begin{array}{c}
1 / 2 \\
k
\end{array}\right)(-1)^{k} z^{2 k} \\
&=\sum_{k \geqslant 0}\left(\begin{array}{c}
1 / 2 \\
k
\end{array}\right)(-1)^{k} z^{2 k-3}
\end{aligned}
$$

so

$$
\operatorname{Res}(f(z), \infty)=\operatorname{Res}\left(\frac{1}{z^{2}} f\left(\frac{1}{z}\right), 0\right)=\frac{1}{2}
$$

We then obtain

$$
\int_{C} \sqrt{z^{2}-1} d z=-\pi i
$$

Solution to 5.10.11: The function $\sin z=\left(e^{i z}-e^{-i z}\right) /(2 i)$ vanishes if and only if $e^{2 i z}=1$, which happens if and only if $z$ is an integer multiple of $\pi$. Since $2<\pi<4$, the zeros of $\sin 4 z$ within the unit circle are 0 , $\pm \pi / 4$.

The residue of $1 / \sin 4 z$ at $z=0$ is $1 / 4$, because the numerator is nonvanishing, while the denominator $\sin 4 z$ has a simple zero and its derivative at $z=0$ is 4 . Since $\sin (4(z+\pi / 4))=-\sin 4 z$, the residue of $1 / \sin 4 z$ at $z=\pm \pi / 4$ is $-1 / 4$. By the Residue Theorem [MH87, p. 280], $I=1 / 4+(-1 / 4)+(-1 / 4)=-1 / 4$.

Solution to 5.10.12: The integrand has poles at the integer multiples of $\pi$, all simple poles except for the one at the origin. By the Residue Theorem [MH87, p. $280]$

$$
I_{n}=2 \pi i \sum_{k=-n}^{n} \operatorname{Res}\left(\frac{1}{z^{3} \sin z}, k \pi\right)
$$

Since the integrand is an even function, its Laurent series at the origin contains only even powers of $z$, implying that the residue at the origin is 0 . Hence $I_{0}=0$.

For $k \neq 0$,

$$
\operatorname{Res}\left(\frac{1 / z^{3}}{\sin z}, k \pi\right)=\left.\frac{1 / z^{3}}{\frac{d}{d z}(\sin z)}\right|_{z=k \pi}=\frac{1 / k^{3} \pi^{3}}{\cos k \pi}=\frac{(-1)^{k}}{k^{3} \pi^{3}}
$$

So for $n>0, I_{n}=\frac{4 i}{\pi^{2}} \sum_{k=1}^{n} \frac{(-1)^{k}}{k^{3}}$.

Solution to 5.10.13: Using the change of variables $z=\frac{1}{w}$, we obtain

$$
\frac{1}{2 \pi i} \int_{C} \frac{d z}{\sin \frac{1}{z}}=\frac{1}{2 \pi i} \int_{C^{\prime}} \frac{-d w}{w^{2} \sin w}=\frac{1}{2 \pi i} \int_{C^{\prime \prime}} \frac{d w}{w^{2} \sin w}
$$

where the circles $C^{\prime}$ and $C^{\prime \prime}$ have radius 5 and are oriented negatively and positively, respectively. By the Residue Theorem [MH87, p. 280], we have

$$
\begin{aligned}
\frac{1}{2 \pi i} \int_{C} \frac{-d w}{w^{2} \sin w}=& \operatorname{Res}\left(\frac{1}{w^{2} \sin w}, \pi\right)+\operatorname{Res}\left(\frac{1}{w^{2} \sin w},-\pi\right) \\
&+\operatorname{Res}\left(\frac{1}{w^{2} \sin w}, 0\right) .
\end{aligned}
$$

We have

$$
\operatorname{Res}\left(\frac{1}{w^{2} \sin w}, \pm \pi\right)=\lim _{w \rightarrow \pm \pi} \frac{w \mp \pi}{w^{2} \sin w}=-\frac{1}{\pi^{2}}
$$

and

$$
\begin{aligned}
\frac{1}{w^{2} \sin w}=& \frac{1}{w^{2}\left(w-w^{3} / 3 !+w^{5} / 5 !+\cdots\right)} \\
=& \frac{1}{w^{3}} \frac{1}{1-\left(w^{2} / 3 !-w^{4} / 5 !+\cdots\right)} \\
=& \frac{1}{w^{3}}\left(1+\left(w^{2} / 3 !-w^{4} / 5 !+\cdots\right)\right.\\
&\left.+\left(w^{2} / 3 !-w^{4} / 5 !+\cdots\right)^{2}+\cdots\right) \\
=& \frac{1}{w^{3}}+\frac{1}{3 ! w}-\frac{w}{5 !}+\cdots
\end{aligned}
$$

Then,

$$
\frac{1}{2 \pi i} \int_{C} \frac{d w}{w^{2} \sin w}=\frac{1}{6}-\frac{2}{\pi^{2}} .
$$

Solution to 5.10.15: We assume $C$ has counterclockwise orientation. First, $I_{0}=J_{0}=\int_{C} \frac{d z}{z}=2 \pi i$. Now consider $k \geqslant 1$. We have, for some constants $A_{0}, \ldots, A_{k}$,

$$
f(z)=\frac{1}{z(z-1) \cdots(z-k)}=\frac{A_{0}}{z}+\frac{A_{1}}{z-1}+\cdots+\frac{A_{k}}{z-k} .
$$

Then

$$
1=\sum_{m=0}^{k} A_{m} \prod_{\substack{n=0 \\ n \neq m}}^{k}(z-n) .
$$

Equating coefficients gives $0=\sum_{m=0}^{k} A_{m}$. For $n=0,1, \ldots, k$

$$
\int_{C} \frac{d z}{z-n}=2 \pi i \omega(C, n)=2 \pi i \text {. }
$$

Hence

$$
I_{k}=\int_{C} f(z) d z=\sum_{n=0}^{k} A_{n} \int_{C} \frac{d z}{z-n}=2 \pi i \sum_{n=0}^{k} A_{n}=0 .
$$

On the other hand,

$$
\begin{aligned}
J_{k} &=2 \pi i \operatorname{Res}\left(\frac{(z-1) \ldots(z-k)}{z}, 0\right) \\
&=(-1)^{k} k ! 2 \pi i .
\end{aligned}
$$

Solution to 5.10.16: $\left(e^{2 \pi z}+1\right)^{-2}$ has a double pole at $\pm i / 2$. By the Residue Theorem [MH87, p. 280], the value of this integral is $2 \pi i$ times the sum of the residues of $\left(e^{2 \pi z}+1\right)^{-2}$ at these two points. We have

$$
-e^{2 \pi z}=e^{-\pi i} e^{2 \pi z}=e^{2 \pi(z-i / 2)}=1+2 \pi(z-i / 2)+2 \pi^{2}(z-i / 2)^{2}+\cdots
$$

hence,

$$
e^{2 \pi z}+1=-2 \pi(z-i / 2)-2 \pi^{2}(z-i / 2)^{2}-\cdots
$$

and so

$$
\left(e^{2 \pi z}+1\right)^{2}=4 \pi^{2}(z-i / 2)^{2}+8 \pi^{3}(z-i / 2)^{3}+\cdots
$$

The residue at $\frac{i}{2}$ is

$$
\begin{aligned}
\left.\frac{d}{d z}\left(\frac{(z-i / 2)^{2}}{\left(e^{2 \pi z}+1\right)^{2}}\right)\right|_{z=i / 2} &=\left.\frac{d}{d z}\left(\frac{1}{4 \pi^{2}+8 \pi^{3}(z-i / 2)+O\left((z-i / 2)^{2}\right)}\right)\right|_{z=i / 2} \\
&=\left.\frac{-8 \pi^{3}+O(z-i / 2)}{\left(4 \pi^{2}+O(z-i / 2)\right)^{2}}\right|_{z=i / 2} \\
&=-\frac{1}{2 \pi} .
\end{aligned}
$$

Using the fact that $-e^{2 \pi z}=e^{\pi i} e^{2 \pi z}$, an identical calculation shows that the other residue is also $-1 / 2 \pi$, so the integral equals $-2 i$.

Solution to 5.10.17: A standard application of the Rouché's Theorem [MH87, p. 421] shows that all the roots of the denominator lie in the open unit disc. By the Deformation Theorem [MH87, p. 148], therefore, the integral will not change if we replace the given contour by the circle centered at the origin with radius $R>1$. Using polar coordinates on this circle, the integral becomes

$$
\begin{aligned}
&\frac{1}{2 \pi i} \int_{0}^{2 \pi} \frac{R e^{11 i \theta} i R e^{i \theta} d \theta}{12 R^{12} e^{12 i \theta}-9 R^{9} e^{9 i \theta}+2 R^{6} e^{6 i \theta}-4 R^{3} e^{3 i \theta}+1}= \\
&\frac{1}{2 \pi} \int_{0}^{2 \pi} \frac{d \theta}{12-9 R^{-3} e^{-3 i \theta}+2 R^{-6} e^{-6 i \theta}-4 R^{-9} e^{-9 i \theta}+R^{-12}}
\end{aligned}
$$

which has the limit $\frac{1}{12}$ as $R \rightarrow \infty$. The value of the given integral is then, $\frac{1}{12}$.

Solution to 5.10.18: We make the change of variables $u=z-1$. The integral becomes

$$
\int_{|u+1|=2}(2 u+1) e^{1+1 / u} d u .
$$

Using the power series for the exponential function,

$$
e^{z /(z-1)}=e^{1+1 /(z-1)}=e \cdot e^{1 /(z-1)}=e \sum_{n=0}^{\infty} \frac{1}{n !(z-1)^{n}}
$$

we get

$$
(2 u+1) e^{1+1 / u}=e\left(2 u+3+\frac{2}{u}+\cdots\right)
$$

The residue of this function at zero, which lies inside $|u+1|=2$, is $2 e$, so the integral is $4 e \pi i$.

Solution to 5.10.19: Denote the integrand by $f$. By the Residue Theorem [MH87, p. 280], $I$ is equal to the sum of the residues of $f$ at $-1 / 2$ and $1 / 3$, which lie in the interior of $C . I$ is also the negative of the sum of the residues in the exterior of $C$, namely at 2 and $\infty$. We have

$$
\operatorname{Res}(f, 2)=\lim _{z \rightarrow 2}(z-2) f(z)=\frac{-1}{5^{5}}
$$

As $\lim _{z \rightarrow \infty} f(z)=0$

$$
\operatorname{Res}(f, \infty)=-\lim _{z \rightarrow \infty} z f(z)=0
$$

So $I=\frac{1}{5^{5}}$

Solution to 5.10.20: The numerator in the integrand is $\frac{1}{3 n}$ times the derivative of the denominator. Hence, $I$ equals $\frac{1}{3 n}$ times the number of zeros of the denominator inside $C$; that is, $I=\frac{1}{3}$.

Solution 2. For $r>1$, let $C_{r}$ be the circle $|z|=r$, oriented counterclockwise. By Cauchy's Theorem [MH87, p. 152], and using the parameterization $z=r e^{i \theta}$,

$$
\begin{aligned}
I &=\frac{1}{2 \pi i} \int_{C_{r}} \frac{z^{n-1}}{3 z^{n}-1} d z \\
&=\frac{1}{2 \pi} \int_{0}^{2 \pi} \frac{r^{n} e^{i n \theta}}{3 r^{n} e^{i n \theta}-1} d \theta .
\end{aligned}
$$

As $r \rightarrow \infty$, the integrand converges uniformly to $\frac{1}{3}$, giving $I=\frac{1}{3}$.

Solution to 5.10.21: The integrand has two singularities inside $C$, a pole of order 1 at the origin and a pole of order 2 at $-1 / 2$. Hence,

$$
\int_{C} \frac{e^{z}}{z(2 z+1)^{2}} d z=2 \pi i\left(\operatorname{Res}\left(\frac{e^{z}}{z(2 z+1)^{2}}, 0\right)+\operatorname{Res}\left(\frac{e^{z}}{z(2 z+1)^{2}},-\frac{1}{2}\right)\right)
$$

The residues can be evaluated by standard methods:

$$
\begin{aligned}
\operatorname{Res}\left(\frac{e^{z}}{z(2 z+1)^{2}}, 0\right) &=\left.\frac{e^{z}}{z(2 z+1)^{2}}\right|_{z=0}=1 \\
\operatorname{Res}\left(\frac{e^{z}}{z(2 z+1)^{2}},-\frac{1}{2}\right) &=\left.\frac{1}{4} \frac{d}{d z}\left(\frac{e^{z}}{z}\right)\right|_{z=-\frac{1}{2}}
\end{aligned}
$$



$$
=\left.\frac{1}{4}\left(\frac{e^{z}}{z}-\frac{e^{z}}{z^{2}}\right)\right|_{z=-\frac{1}{2}}=-\frac{3 e^{-\frac{1}{2}}}{2} .
$$

Hence,

$$
\int_{C} \frac{e^{z}}{z(2 z+1)^{2}} d z=2 \pi i\left(1-\frac{3}{2 \sqrt{e}}\right)
$$

Solution to 5.10.22: Let $f(z)$ denote the integrand. It has a pole of order 2 at $a$ and a pole of order 3 at $b$, and no other singularities. The contour $\Gamma$ has winding number 1 about $a$ and $-1$ about $b$. By the Residue Theorem [MH87, p. 280],

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-027.jpg?height=726&width=854&top_left_y=854&top_left_x=622)

We have

$$
\begin{aligned}
\operatorname{Res}(f, a) &=\left.\frac{d}{d z}(z-b)^{-3}\right|_{z=a}=\frac{-3}{(a-b)^{4}} \\
\operatorname{Res}(f, b) &=\left.\frac{1}{2} \frac{d^{2}}{d z^{2}}(z-a)^{-2}\right|_{z=b}=\frac{3}{(b-a)^{4}}
\end{aligned}
$$

Hence

$$
I=2 \pi i\left(\frac{-3}{(a-b)^{4}}-\frac{3}{(a-b)^{4}}\right)=\frac{-12 \pi i}{(a-b)^{4}}=-12 \pi i .
$$

Solution to 5.10.23: The function $f$ has poles of order 1 at $z=\pm 1$ and a pole of order 2 at $z=0$. These are the only singularities of $f$. The winding numbers of $\gamma$ around $-1,0,1$ are $1,2,-1$, respectively. By the Residue Theorem [MH87, p. 280],

$$
\frac{1}{2 \pi i} \int_{\gamma} f(z) d z=\operatorname{Res}(f,-1)+2 \operatorname{Res}(f, 0)-\operatorname{Res}(f, 1) .
$$

Since 1 and $-1$ are simple poles, we have

$$
\begin{gathered}
\operatorname{Res}(f, 1)=\lim _{z \rightarrow 1}(z-1) f(z)=\left.\frac{-e^{z}}{z^{2}(z+1)}\right|_{z=1}=\frac{-e}{2} \\
\operatorname{Res}(f,-1)=\lim _{z \rightarrow-1}(z+1) f(z)=\left.\frac{-e^{z}}{z^{2}(z-1)}\right|_{z=-1}=\frac{e^{-1}}{2} .
\end{gathered}
$$

To find the residue at 0 , we use power series:

$$
\begin{aligned}
\frac{e^{2}}{z^{2}\left(1-z^{2}\right)} &=\frac{1}{z^{2}}\left(1+z+\frac{z^{2}}{2}+\cdots\right)\left(1+z^{2}+z^{4}+\cdots\right) \\
&=\frac{1}{z^{2}}(1+z+\cdots)=\frac{1}{z^{2}}+\frac{1}{z}+\cdots
\end{aligned}
$$

It follows that $\operatorname{Res}(f, 0)=1$. Hence,

$$
\frac{1}{2 \pi i} \int_{\gamma} f(z) d z=\frac{e^{-1}}{2}+2+\frac{e}{2}=2+\cosh 1 .
$$

Solution to 5.10.24: The function $f(z)=\left(e^{z}-1\right) / z^{2}(z-1)$ has a pole of order 2 at $z=0$, and a simple pole at $z=1$. We have

$$
\begin{aligned}
\operatorname{Res}(f, 0) &=\left.\frac{d}{d z} \frac{e^{z}-1}{z-1}\right|_{z=0}=-1, \\
\operatorname{Res}(f, 1) &=\lim _{z \rightarrow 1} \frac{e^{z}-1}{z^{2}}=e-1, \\
\omega(C, 0) &=-2, \\
\omega(C, 1) &=2 .
\end{aligned}
$$

By the Residue Theorem [MH87, p. 280], we have then

$$
\begin{aligned}
\int_{C} \frac{e^{z}-1}{z^{2}(z-1)} d z &=2 \pi i(\operatorname{Res}(f, 0) \omega(C, 0)+\operatorname{Res}(f, 1) \omega(C, 1)) \\
&=4 \pi i e .
\end{aligned}
$$

Solution to 5.10.25: The roots of $1-2 z \cos \theta+z^{2}=0$ are $z=\cos \theta \pm$ $i \sqrt{1-\cos ^{2} \theta}=e^{\pm i \theta}$. Using the Residue Theorem, [MH87, p. 280], we get

$$
\begin{aligned}
\frac{1}{2 \pi i} \int_{|z|=2} \frac{z^{n}}{1-2 z \cos \theta+z^{2}} d z=& \frac{1}{2 \pi i} \int_{|z|=2} \frac{z^{n}}{\left(z-e^{i \theta}\right)\left(z-e^{-i \theta}\right)} d z \\
=& \operatorname{Res}\left(\frac{z^{n}}{\left(z-e^{i \theta}\right)\left(z-e^{-i \theta}\right)}, e^{i \theta}\right) \\
&+\operatorname{Res}\left(\frac{z^{n}}{\left(z-e^{i \theta}\right)\left(z-e^{-i \theta}\right)}, e^{-i \theta}\right)
\end{aligned}
$$



$$
\begin{aligned}
&=\left.\frac{z^{n}}{z-e^{-i \theta}}\right|_{z=e^{i \theta}}+\left.\frac{z^{n}}{z-e^{i \theta}}\right|_{z=e^{-i \theta}} \\
&=\frac{e^{i n \theta}}{e^{i \theta}-e^{-i \theta}}+\frac{e^{-i n \theta}}{e^{-i \theta}-e^{i \theta}} \\
&=\frac{e^{i n \theta}-e^{-i n \theta}}{e^{i \theta}-e^{-i \theta}} \\
&=\frac{\sin n \theta}{\sin \theta} .
\end{aligned}
$$

Solution to 5.10.26: Substituting $z=e^{i \theta}$, we have

$$
\begin{aligned}
I(a) &=\int_{0}^{2 \pi} \frac{d \theta}{a+\frac{e^{i \theta}+e^{-i \theta}}{2}}=2 \int_{0}^{2 \pi} \frac{e^{i \theta} d \theta}{2 a e^{i \theta}+e^{2 i \theta}+1} \\
&=\frac{2}{i} \int_{|z|=1} \frac{d z}{z^{2}+2 a z+1} .
\end{aligned}
$$

The roots of the polynomial in the denominator are $-a+\sqrt{a^{2}-1}$ and $-a-\sqrt{a^{2}-1}$, of which only the former is within the unit circle. By the Residue Theorem [MH87, p. 280],

$$
I(a)=4 \pi \operatorname{Res}\left(\frac{1}{z^{2}+2 a z+1},-a+\sqrt{a^{2}-1}\right) .
$$

Since the function in question has a single pole at $z=-a+\sqrt{a^{2}-1}$, the residue equals

$$
\left.\frac{1}{2 z+2 a}\right|_{z=-a+\sqrt{a^{2}-1}}=\frac{1}{2 \sqrt{a^{2}-1}}
$$

giving $I(a)=\frac{2 \pi}{\sqrt{a^{2}-1}}$.

Consider the function $F$ defined for $\xi \notin[-1,1]$ by

$$
F(\xi)=\int_{0}^{2 \pi} \frac{d \theta}{\xi+\cos \theta} .
$$

As $F^{\prime}(\xi)$ exists, $F$ is analytic on its domain. Combining with the previous results, we have that the function

$$
F(\xi)-\frac{2 \pi}{\sqrt{\xi^{2}-1}}
$$

is analytic and vanishes for $\xi>1$; therefore, it must be identically zero. From this, we obtain that

$$
\int_{0}^{2 \pi} \frac{d \theta}{\xi+\cos \theta}=\frac{2 \pi}{\sqrt{\xi^{2}-1}}
$$

in the domain of $F$.

Solution to 5.10.27: Let $f$ be the function defined by

$$
f(z)=\frac{i}{(z-r)(r z-1)}
$$

We have

$$
\int_{|z|=1} f(z) d z=\int_{0}^{2 \pi} \frac{d \theta}{1-2 r \cos \theta+r^{2}}
$$

For $|r|<1$,

$$
\operatorname{Res}(f(z), r)=\frac{i}{r^{2}-1}
$$

and

$$
\int_{0}^{2 \pi} \frac{d \theta}{1-2 r \cos \theta+r^{2}}=\frac{2 \pi}{1-r^{2}} .
$$

For $|r|>1$, we get

$$
\operatorname{Res}\left(f(z), \frac{1}{r}\right)=\frac{i}{1-r^{2}}
$$

and

$$
\int_{0}^{2 \pi} \frac{d \theta}{1-2 r \cos \theta+r^{2}}=\frac{2 \pi}{r^{2}-1} .
$$

Solution 2. Suppose that $r \in \mathbb{R}$ and $0<r<1$. Let $u_{0}$ be defined on the unit circle by $u_{0}(z)=1$. The solution of the corresponding Dirichlet Problem [MH93, p. 600], that is, the harmonic function on $\mathbb{D}, u$, that agrees with $u_{0}$ on $\partial \mathbb{D}$ is given by Poisson's Formula [MH87, p. 195]:

$$
u(r)=\frac{1-r^{2}}{2 \pi} \int_{0}^{2 \pi} \frac{d \theta}{1-2 r \cos \theta+r^{2}}
$$

but $u \equiv 1$ is clearly a solution of the same problem. Therefore, by unicity, we get

$$
\int_{0}^{2 \pi} \frac{d \theta}{1-2 r \cos \theta+r^{2}}=\frac{2 \pi}{1-r^{2}} .
$$

If $r>1$, consider a similar Dirichlet problem with $u_{0}(z)=1$ for $|z|=r$. We get

$$
1=u(1)=\frac{r^{2}-1}{2 \pi} \int_{0}^{2 \pi} \frac{d \theta}{1-2 r \cos \theta+r^{2}}
$$

so

$$
\int_{0}^{2 \pi} \frac{d \theta}{1-2 r \cos \theta+r^{2}}=\frac{2 \pi}{r^{2}-1} .
$$

If $r<0$, a similar argument applied to $u(-r), u(-1)$ leads to the results above, noting that $\cos (\theta-\pi)=-\cos \theta$. Solution to 5.10.28: Evaluating the integral using the Residue Theorem [MH87, p. 280], we have

$$
\begin{aligned}
\int_{0}^{\pi} \frac{\cos 4 \theta}{1+\cos ^{2} \theta} d \theta &=\Re \int_{0}^{\pi} \frac{e^{4 i \theta}}{1+\cos ^{2} \theta} d \theta \\
&=\Re \int_{0}^{\pi} \frac{e^{4 i \theta}}{1+\frac{\left(e^{i \theta}+e^{-i \theta}\right)^{2}}{4}} d \theta \\
&=4 \Re \int_{0}^{\pi} \frac{e^{4 i \theta}}{6+e^{2 i \theta}+e^{-2 i \theta}} d \theta \\
&=4 \Re \int_{|z|=1} \frac{z^{3}}{z^{2}+6 z+1} \frac{d z}{2 i z} \\
&=2 \mathfrak{R} \int_{|z|=1} \frac{z^{2}}{z^{2}+6 z+1} d z \\
&\left.=2 \mathfrak{R} 2 \pi\left(\operatorname{Res} \frac{z^{2}}{z^{2}+6 z+1},-3+2 \sqrt{2}\right)\right) \\
&=4 \pi\left(-3+\frac{17}{4 \sqrt{2}}\right)=-12 \pi+\frac{17}{\sqrt{2}} \pi .
\end{aligned}
$$

Solution to 5.10.29: As $2 \cos ^{2} x=\cos 2 x+1$, we have

$$
\begin{aligned}
I &=\frac{1}{2} \int_{0}^{2 \pi} \frac{\cos 6 \theta+1}{5-4 \cos 2 \theta} d \theta \\
&=\frac{1}{2} \Re \int_{0}^{2 \pi} \frac{e^{6 i \theta}+1}{5-4 \cos 2 \theta} d \theta \\
&=\frac{-1}{2} \Re \int_{0}^{2 \pi} \frac{e^{8 i \theta}+e^{2 i \theta}}{2 e^{4 i \theta}-5 e^{2 i \theta}+2} d \theta \\
&=\frac{1}{2} \Re i \int_{|z|=1} \frac{z^{3}+1}{2 z^{2}-5 z+2} d z .
\end{aligned}
$$

We evaluate this integral using the Residue Theorem [MH87, p. 280]. As the integrand has a simple pole at $z=1 / 2$ inside the unit circle and no others, we have

$$
I=\frac{1}{2} \mathfrak{N}\left(\left.i 2 \pi i \frac{z^{3}+1}{2(z-2)}\right|_{z=1 / 2}\right)=\frac{3 \pi}{8}
$$

Solution to 5.10.30: The integrand is holomorphic except for a pole of order three at the origin. By the Residue Theorem, we have

$$
I=2 \pi i \operatorname{Res}\left(\frac{\cos ^{3} z}{z^{3}}, 0\right) \text {. }
$$

Near $z=0$ we have

$$
\begin{aligned}
\frac{\cos ^{3} z}{z^{3}} &=\frac{\left(1-\frac{z^{2}}{2}+O\left(z^{4}\right)\right)^{3}}{z^{3}} \\
&=\frac{\left(1-z^{2}+O\left(z^{4}\right)\right)\left(1-\frac{z^{2}}{2}+O\left(z^{4}\right)\right)}{z^{3}} \\
&=\frac{1-\frac{3}{2} z^{2}+O\left(z^{4}\right)}{z^{3}}
\end{aligned}
$$

showing that the residue in question is $-\frac{3}{2} \cdot$ Hence $I=-3 \pi i$.

Solution to 5.10.31: Consider the contour in the figure

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-032.jpg?height=813&width=871&top_left_y=962&top_left_x=646)

We have

$$
\begin{aligned}
&\int_{0}^{2 \pi} \frac{1-\cos n \theta}{1-\cos \theta} d \theta=\lim _{\varepsilon \rightarrow 0} \int_{\varepsilon}^{2 \pi-\varepsilon} \frac{1-\cos n \theta}{1-\cos \theta} d \theta=\lim _{\varepsilon \rightarrow 0} \int_{\varepsilon}^{2 \pi-\varepsilon} \frac{1-e^{i n \theta}}{1-\cos \theta} d \theta, \\
&\text { the last equality because the integral of } f(\theta)=\frac{i \sin n \theta}{1-\cos \theta} \text { vanishes (since } \\
&f(2 \pi-\theta)=-f(\theta)) \text {. Next, }
\end{aligned}
$$

$$
\int_{\varepsilon}^{2 \pi-\varepsilon} \frac{1-e^{i n \theta}}{1-\cos \theta} d \theta=\int_{\Gamma_{\varepsilon}} \frac{1-z^{n}}{1-(z+1 / z) / 2} \frac{d z}{i z},
$$

where $\Gamma_{\varepsilon}$ is the almost-circle $\left\{e^{i \theta} \mid \varepsilon \leqslant \theta \leq 2 \pi-\varepsilon\right\}$, traversed counterclockwise. The latter integral equals

$$
\frac{2}{i} \int_{\Gamma_{\varepsilon}} \frac{z^{n}-1}{(z-1)^{2}} d z,
$$

which, when $\varepsilon \rightarrow 0$, tends to

$$
\pi i \frac{2}{i} \operatorname{Res}\left(\frac{z^{n}-1}{(z-1)^{2}}, 1\right)=2 \pi n
$$

so the integral in the statement of the problem has the value $2 \pi n$.

Solution to 5.10.32: Let $z=e^{i \theta}$. Then

$$
\begin{aligned}
\int_{-\pi}^{\pi} \frac{\sin n \theta}{\sin \theta} d \theta &=\int_{|z|=1} \frac{z^{n}-z^{-n}}{z-z^{-1}} \frac{d z}{i z} \\
&=\frac{1}{i} \int_{|z|=1} \frac{z^{2 n}-1}{z^{2}-1} \frac{1}{z^{n}} d z \\
&=\frac{1}{i} \int_{|z|=1}\left(1+z^{2}+\cdots+z^{2 n-2}\right) \frac{1}{z^{n}} d z \\
&=\sum_{k=0}^{n-1} \int_{|z|=1} z^{2 k-n} d z
\end{aligned}
$$

If $n$ is even, only even powers of $z$ occur, and each term vanishes. For odd $n$, we have

$$
\int_{-\pi}^{\pi} \frac{\sin n \theta}{\sin \theta} d \theta=\frac{1}{i} \int_{|z|=1} \frac{d z}{z}=2 \pi .
$$

Solution to 5.10.33: We have

$$
\begin{aligned}
C_{n}(a)+i S_{n}(a) &=\int_{-\pi}^{\pi} \frac{e^{i n \theta}}{a-\cos \theta} d \theta \\
&=-2 \int_{-\pi}^{\pi} \frac{e^{i(n+1) \theta}}{e^{2 i \theta}-2 a e^{i \theta}+1} d \theta \\
&=2 i \int_{|z|=1} \frac{z^{n}}{z^{2}-2 a z+1} d z .
\end{aligned}
$$

Let $f(z)$ denote this last integrand. Its denominator has two zeros, $a \pm \sqrt{a^{2}-1}$, of which $a-\sqrt{a^{2}-1}$ is inside the unit circle. The residue is given by

$$
\operatorname{Res}\left(f, a-\sqrt{a^{2}-1}\right)=\frac{\left(a-\sqrt{a^{2}-1}\right)^{n}}{\left(a-\sqrt{a^{2}-1}-a-\sqrt{a^{2}-1}\right)}=\frac{\left(a-\sqrt{a^{2}-1}\right)^{n}}{-2 \sqrt{a^{2}-1}} .
$$

Therefore, by the Residue Theorem [MH87, p. 280],

$$
C_{n}(a)+i S_{n}(a)=\frac{2 \pi\left(a-\sqrt{a^{2}-1}\right)^{n}}{\sqrt{a^{2}-1}}
$$

Since the right-hand side is real, this must be the value of $C_{n}(a)$, and $S_{n}(a)=0$. 

\subsection{Integrals Along the Real Axis}

Solution to 5.11.1: We have

$$
\int_{-\infty}^{\infty} f_{m}(x) \overline{f_{n}(x)} d x=\frac{1}{\pi} \int_{-\infty}^{\infty} \frac{(x-i)^{m}}{(x+i)^{m+1}} \frac{(x+i)^{n}}{(x-i)^{n+1}} d x .
$$

If $m=n$, we get

$$
\frac{1}{\pi} \int_{-\infty}^{\infty} \frac{d x}{1+x^{2}}=\left.\frac{1}{\pi} \arctan x\right|_{-\infty} ^{\infty}=1 .
$$

If $m<n$, as $x^{2}+1=(x-i)(x+i)$, we have

$$
\frac{1}{\pi} \int_{-\infty}^{\infty} \frac{1}{x^{2}+1} \frac{(x+i)^{n-m}}{(x-i)^{n-m}} d x .
$$

Since the numerator has degree 2 less than the denominator, the integral converges absolutely. We evaluate it using residue theory. Let $C_{R}=[-R, R] \cup \Gamma_{R}$ be the contour

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-034.jpg?height=480&width=826&top_left_y=1332&top_left_x=644)

We evaluate the integral over $C_{R}$. For $R>0$ sufficiently large, the integrand has a pole at $x=i$ inside the contour. Calculating the residue, we get

$$
\left.\frac{d^{n-m+1}}{d x}\left((x-i)^{n-m+1} \frac{(x+i)^{n-m-1}}{(x-i)^{n-m+1}}\right)\right|_{x=i}=0 .
$$

By the Residue Theorem [MH87, p. 280], the integral over $C_{R}$ is 0 for all such $R$. Letting $R$ tend to infinity, we see that the integral over the semicircle $\Gamma_{R}$ tends to 0 since the numerator has degree 2 less than the denominator. So

$$
\frac{1}{\pi} \int_{-\infty}^{\infty} \frac{1}{x^{2}+1} \frac{(x+i)^{n-m}}{(x-i)^{n-m}} d x=0 .
$$

Solution to 5.11.2: Consider the following contour around the pole of the function

$$
f(z)=\frac{1-e^{i|a| z}}{z^{2}}
$$

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-035.jpg?height=469&width=808&top_left_y=481&top_left_x=664)

On the larger $\operatorname{arc} z=R(\cos \theta+i \sin \theta)$, so

$$
\left|\frac{1-e^{i|a| z}}{z^{2}}\right| \leqslant \frac{1+e^{-R|a| \sin \theta}}{R^{2}}=O\left(R^{-2}\right) \quad(R \rightarrow \infty) .
$$

Then

$$
\begin{aligned}
\int_{0}^{\infty} \frac{1-\cos a x}{x^{2}} d x &=\frac{1}{2} \Re\left(\pi i \operatorname{Res}\left(\frac{1-e^{i|a| z}}{z^{2}}, 0\right)\right) \\
&=\frac{1}{2} \Re\left(\left.\pi i 2 \frac{-i|a| e^{i|a| z}}{2}\right|_{z=0}\right) \\
&=\frac{\pi|a|}{2} .
\end{aligned}
$$

Solution to 5.11.3: Let $f_{t}(z)=e^{i t z} /(z+i)^{2}$, and consider first the case $t>0$. Then $\left|f_{t}(z)\right|$ is bounded in the upper half-plane by $|z+i|^{-2}$. For $R>1$ let $C_{R}=\Gamma_{R} \cup[-R, R]$, where $\Gamma_{R}$ is the semicircle centered at the origin joining $R$ and $-R$, oriented counterclockwise. The function $f_{t}$ is holomorphic on $C_{R}$ and its interior, so, by Cauchy's Theorem, we have

$$
0=\int_{C_{R}} f_{f}(z) d z=\int_{-R}^{R} f_{t}(x) d x+\int_{\Gamma_{R}} f_{t}(z) d z .
$$

The absolute value of the second summand on the right is at most $\pi R / R^{2}$, since $\mid f_{t}(z) \leqslant R^{-2}$ on $\Gamma_{R}$. Taking the limit as $R \rightarrow \infty$ we obtain $I(t)=0(f \geqslant 0)$.

Suppose now $t<0$. Then $\left|f_{t}\right|$ is bounded in the lower half-plane by $|z+i|^{-2}$. Let $C_{R}^{\prime}$ be the reflection of $C_{R}$ with respect to the real axis, oriented clockwise. By the Residue Theorem, we have

$$
\int_{C_{R}^{\prime}} f_{t}(z) d z=-2 \pi i \operatorname{Res}\left(f_{t}(z),-i\right) .
$$

A calculation shows that the residue equals $i t e^{t}$, so

$$
\int_{C_{R}^{\prime}} f_{t}(z) d z=2 \pi t e^{t} .
$$

As $R \rightarrow \infty$, the contribution to the last integral from the semicircle tends to 0 since $\left|f_{t}\right| \leqslant(R-1)^{2}$ on the semicircle, giving $I(t)=2 \pi t e^{t}(t<0)$.

Solution to 5.11.4: For $t=0$ the integral is elementary:

$$
\int_{-R}^{R} \frac{d x}{(x+i)^{3}} d x=\left.\frac{-1}{2(x+i)^{2}}\right|_{-R} ^{R} \rightarrow 0 \quad \text { as } \quad R \rightarrow \infty,
$$

hence $F(0)=0$.

For $t<0$ the function $f_{t}(z)=\frac{e^{-i t z}}{(z+i)^{3}}$ is bounded in the upper half-plane, and in fact is $O\left(|z|^{-3}\right)$ there. We integrate $f_{t}$ around the contour $\Gamma_{R}$ consisting of the interval $[-R, R]$ on the real axis and the semicircle in the upper half-plane with center 0 and radius $R(R>1)$, oriented counterclockwise. The integral is 0 by Cauchy's Theorem [MH87, p. 152]. The length of the semicircle is $\pi R$ and the integrand is $O\left(R^{-3}\right.$ ) on it, so the contribution to the integral due to the semicircle tends to 0 as $R \rightarrow \infty$. The contribution due to the interval $[-R, R]$ tends to $F(t)$, so we conclude that $F(t)=0$ for $t<0$.

For $t>0$ we integrate $f_{t}$ around $\Gamma_{R}^{*}$, the reflection of $\Gamma_{R}$ with respect to the real axis (oriented clockwise). The integrand has one singularity in the interior of $\Gamma_{R}^{*}$, a pole of order 3 at $z=-i$. The residue there is

$$
\left.\frac{1}{2 !} \frac{d^{2}}{d z^{2}}\left(e^{-i t z}\right)\right|_{z=-i}=\frac{-t^{2} e^{-t}}{2} .
$$

By the Residue Theorem [MH87, p. 280],

$$
\int_{\Gamma_{R}^{*}} f_{t}(z) d z=-2 \pi i\left(\frac{-t^{2} e^{-t}}{2}\right)=\pi i t^{2} e^{-t} .
$$

By the same reasoning as above, the preceding integral tends to $F(t)$ as $R \rightarrow \infty$. Hence $F(t)=\pi i t^{2} e^{-t}$ for $t>0$.

Solution to 5.11.5: To see that the integral exists, notice that

$$
\lim _{x \rightarrow 0} \frac{\sin ^{2} x}{x^{2}}=1, \quad \frac{\sin ^{2} x}{x^{2}}=O\left(\frac{1}{x^{2}}\right),(x \rightarrow \infty) .
$$

As the integrand is an even function, we have

$$
\int_{-\infty}^{\infty} \frac{\sin ^{2} x}{x^{2}} d x=2 \int_{0}^{\infty} \frac{\sin ^{2} x}{x^{2}} d x
$$

Let $f(z)=\frac{1-e^{2 i z}}{z^{2}}$. Then $f$ is analytic except for a simple pole at 0 .

For $0<\varepsilon<R$, consider the contour $C_{\varepsilon, R}=\Gamma_{R} \cup \gamma_{\varepsilon} \cup[-R,-\varepsilon] \cup[\varepsilon, R]$

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-037.jpg?height=472&width=810&top_left_y=420&top_left_x=663)

by Cauchy's Theorem [MH87, p. 152]. We have, by the Residue Theorem,

$$
0=\int_{C_{\varepsilon, R}} f(z) d z=\int_{\Gamma_{R}}+\int_{[-R,-\varepsilon]}+\int_{\gamma_{\varepsilon}}+\int_{[\varepsilon, R]} .
$$

It is easy to see that on $\Gamma_{R}$ we have

$$
|f(z)|=\frac{\left|1-e^{2 i z}\right|}{z^{2}} \leqslant \frac{2}{|z|^{2}},
$$

therefore

$$
\int_{-\infty}^{\infty} \frac{1-e^{2 i x}}{x^{2}} d x=\pi i \operatorname{Res}(f, 0)=\pi i(-2 i)=2 \pi .
$$

Since $\sin ^{2}(x)=\frac{1}{2} \mathfrak{A}\left(1-e^{2 i x}\right)$, we have

$$
\int_{0}^{\infty} \frac{\sin ^{2} x}{x^{2}} d x=\frac{1}{4} \Re \int_{-\infty}^{\infty} \frac{1-e^{2 i x}}{x^{2}} d x,
$$

so

$$
\int_{0}^{\infty} \frac{\sin ^{2} x}{x^{2}} d x=\frac{\pi}{2} .
$$

Solution 2. Assuming Dirichlet's Integral

$$
\int_{0}^{\infty} \frac{\sin x}{x} d x=\frac{\pi}{2}
$$

whose evaluation can be found in [Boa87, p. 86] and [MH87, p. 313], we can use integration by parts:

$$
\int_{0}^{\infty} \frac{\sin ^{2} x}{x^{2}} d x=-\left.\frac{1}{x} \sin ^{2} x\right|_{0} ^{\infty}-\int_{0}^{\infty}-\frac{1}{x} \sin 2 x d x
$$



$$
\begin{aligned}
&=0+\int_{0}^{\infty} \frac{\sin y}{y} d y \\
&=\frac{\pi}{2}
\end{aligned}
$$

Solution to 5.11.6: The integrand is absolutely integrable, because it is bounded in absolute value by 1 near the origin and by $1 /|x|^{3}$ away from the origin. We have

$$
\begin{aligned}
\sin ^{3} x &=-\frac{1}{8 i}\left(e^{i x}-e^{-i x}\right)^{3}=-\frac{1}{8 i}\left(e^{3 i x}-e^{-3 i x}-3 e^{i x}+3 e^{-i x}\right) \\
&=\Im\left(\frac{3}{4} e^{i x}-\frac{1}{4} e^{3 i x}\right)
\end{aligned}
$$

For $0<\varepsilon<R$, consider the contour $C_{\varepsilon, R}=\Gamma_{R} \cup \gamma_{\varepsilon} \cup[-R,-\varepsilon] \cup[\varepsilon, R]$

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-038.jpg?height=477&width=824&top_left_y=968&top_left_x=648)

by Cauchy's Theorem [MH87, p. 152],

$$
0=\int_{C_{\varepsilon, R}} \frac{3 e^{i z}-e^{3 i z}}{z^{3}} d z=\int_{\Gamma_{R}}+\int_{[-R,-\varepsilon]}+\int_{\gamma_{\varepsilon}}+\int_{[\varepsilon, R]} .
$$

The integral over $\Gamma_{R}$ is bounded in absolute value by $\left(4 R^{-3}\right)(2 \pi R)$ since $\left|e^{i z}\right|$ and $\left|e^{31 z}\right|$ are bounded by 1 in the upper half-plane, so it tends to 0 as $R \rightarrow \infty$. To estimate the integral over $\gamma_{\varepsilon}$, we note that

$$
\begin{aligned}
\frac{3 e^{i z}-e^{3 i z}}{z^{3}} &=\frac{1}{z^{3}}\left(2+\frac{3(i z)^{2}}{2}-\frac{(3 i z)^{2}}{2}+O\left(z^{3}\right)\right) \\
&=\frac{2}{z^{3}}+\frac{3}{z}+O(1) \quad(z \rightarrow 0) .
\end{aligned}
$$

Hence,

$$
\begin{aligned}
\int_{\gamma_{\varepsilon}} \frac{3 e^{i z}-e^{3 i z}}{z^{3}} d z &=\int_{\pi}^{0}\left(\frac{2}{\varepsilon^{3} e^{3 i \theta}}-\frac{3}{\varepsilon e^{i \theta}}+O(1)\right) i \varepsilon e^{i \theta} d \theta \\
&=\frac{2 i}{\varepsilon^{2}} \int_{\pi}^{0} e^{-2 i \theta} d \theta+3 i \int_{\pi}^{0} d \theta+O(\varepsilon) \\
&=0-3 \pi i+O(\varepsilon) \rightarrow-3 \pi \quad \text { as } \varepsilon \rightarrow 0 .
\end{aligned}
$$

Thus,

$$
\lim _{\substack{R \rightarrow \infty \\ \varepsilon \rightarrow 0}}\left(\int_{-R}^{-\varepsilon} \frac{3 e^{i x}-e^{3 i x}}{x^{3}} d x+\int_{\varepsilon}^{R} \frac{3 e^{i x}-e^{3 i x}}{x^{3}} d x\right)=3 \pi i
$$

The integral is one-fourth the imaginary part of the preceding limit, so $\frac{3 \pi}{4}$.

Solution to 5.11.7: Let

$$
f(z)=\frac{e^{i z} z^{3}}{\left(z^{2}+1\right)^{2}}=\frac{e^{i z} z^{3}}{(z+i)^{2}(z-i)^{2}}
$$

Integrate $f$ over a closed semicircular contour $C_{R}=[-R, R] \cup \Gamma_{R}$ with radius $R$ in the upper half plane.

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-039.jpg?height=475&width=824&top_left_y=928&top_left_x=648)

The portion $[-R, R]$ along the axis gives

$$
\int_{-R}^{R} \frac{x^{3}}{\left(x^{2}+1\right)^{2}}(\cos x+i \sin x) d x
$$

whose imaginary part is

$$
\int_{-R}^{R} \frac{x^{3} \sin x}{\left(x^{2}+1\right)^{2}} d x
$$

This integral converges as $R \rightarrow \infty$ to

$$
\int_{-\infty}^{\infty} \frac{x^{3} \sin x}{\left(x^{2}+1\right)^{2}} d x
$$

To establish the convergence, note that integration by parts gives

$$
\int_{-R}^{R} \frac{x^{3} \sin x}{\left(x^{2}+1\right)^{2}} d x=\left.\frac{(-\cos x) x^{3}}{\left(x^{2}+1\right)^{2}}\right|_{-R} ^{R}+\int_{-R}^{R} \cos x \frac{d}{d x}\left(\frac{x^{3}}{\left(x^{2}+1\right)^{2}}\right) d x
$$

and

$$
\left|\frac{(-\cos x) x^{3}}{\left(x^{2}+1\right)^{2}}\right| \leqslant \frac{1}{|x|} \rightarrow 0 \quad \text { as } \quad x \rightarrow \infty \text {, }
$$

and the second term is integrable by the Comparison Test [Rud87, p. 60], $O\left(1 / x^{2}\right)$. The real part also converges by a similar reasoning, and since the integrand is odd, it converges to zero.

The integral of $f$ over $\Gamma_{R}$ is treated as follows. Let $z=R e^{i \theta}$ for $0 \leqslant \theta \leqslant \pi$. Then

$$
\int_{\Gamma_{R}} \frac{e^{i z} z^{3}}{\left(z^{2}+1\right)^{2}} d z=\int_{0}^{\pi} \frac{e^{i R(\cos \theta+i \sin \theta)} z^{3}}{\left(z^{2}+1\right)^{2}} i R e^{i \theta} d \theta .
$$

This is bounded above in absolute value for large $R$ by

$$
A \int_{0}^{\pi} e^{-R \sin \theta} d \theta
$$

for a constant $A$ and this integral tends to zero as $R \rightarrow \infty$ (Jordan's Lemma [MH87, p. 301]). Finally,

$$
\int_{C_{R}} f(z) d z=2 \pi i \operatorname{Res}(f, i)
$$

and since we have a second order pole, we have

$$
\operatorname{Res}(f, i)=\left.\frac{d}{d z}\left(\frac{e^{i z} z^{3}}{(z+i)^{2}}\right)\right|_{z=i}=\frac{1}{4 e} .
$$

Thus,

$$
\int_{C_{R}} f(z) d z=\frac{\pi i}{2 e}
$$

and so the required integral is $\frac{\pi}{2 e}$.

Solution to 5.11.8: Using an argument similar to the one in Problem 5.11.7 with the function

$$
f(z)=\frac{e^{i z} z}{\left(z^{2}+1\right)^{2}},
$$

we get

$$
\begin{aligned}
\int_{-\infty}^{\infty} \frac{x \sin x}{\left(x^{2}+1\right)^{2}} d x &=\mathfrak{H}(2 \pi i \operatorname{Res}(f, i))=\left.\mathfrak{R} 2 \pi i \frac{d}{d z}\left(\frac{e^{i z} z}{(z+i)^{2}}\right)\right|_{z=i} \\
&=\Re\left(2 \pi i \frac{1}{4 e}\right)=\frac{\pi}{2 e}
\end{aligned}
$$

Solution to 5.11.9: Consider the complex integral

$$
\int_{C_{\varepsilon, R}} \frac{e^{i z}}{z\left(z^{2}+a^{2}\right)} d z
$$

where the contour $C_{\varepsilon, R}$ is the contour described below oriented counterclockwise. The integrand has simple poles at 0 and $i a$.

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-041.jpg?height=477&width=827&top_left_y=398&top_left_x=646)

Notice that $\frac{\sin x}{x\left(x^{2}+a^{2}\right)}=\mathfrak{N}\left(\frac{e^{i x}}{x\left(x^{2}+a^{2}\right)}\right)$ for real $x$. By Jordan's Lemma [MH87, p. 301], we have

$$
\left|\int_{\Gamma_{R}} \frac{e^{i z}}{z\left(z^{2}+a^{2}\right)} d z\right| \leqslant \frac{\pi}{R} \int_{\Gamma_{R}} \frac{1}{\left(z^{2}+a^{2}\right)} d z=o(1) \quad(|R| \rightarrow \infty)
$$

so, at the limit, the contribution from the bigger semicircle to the integral above is zero. Using the Residue Theorem [MH87, p. 280], we have

$$
\begin{aligned}
\int_{C_{\varepsilon, R}} \frac{e^{i z}}{z\left(z^{2}+a^{2}\right)} d z &=2 \pi i \operatorname{Res}\left(\frac{e^{i z}}{z\left(z^{2}+a^{2}\right)}, i a\right)+\pi i \operatorname{Res}\left(\frac{e^{i z}}{z\left(z^{2}+a^{2}\right)}, 0\right) \\
&=2 \pi i \frac{e^{-a}}{i a(2 i a)}+\pi i \frac{1}{a^{2}} \\
&=-\frac{\pi i}{e^{a} a^{2}}+\frac{\pi i}{a^{2}}
\end{aligned}
$$

since $f$ is even we have,

$$
I=\frac{\pi}{2 a^{2}}\left(1-\frac{1}{e^{a}}\right) .
$$

Solution 2. Consider the function $\frac{e^{i z}-1}{z\left(z^{2}+a^{2}\right)}$ which has a removable singularity at the origin and a simple pole at $i a$. Again, we have $\frac{\sin x}{x\left(x^{2}+a^{2}\right)}=\mathfrak{H}\left(\frac{e^{i x}-1}{x\left(x^{2}+a^{2}\right)}\right)$. By the same argument as above, but with one less pole, we can use the following contour 

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-042.jpg?height=469&width=816&top_left_y=215&top_left_x=665)

and get

$$
\int_{-\infty}^{\infty} \frac{e^{i x}-1}{x\left(x^{2}+a^{2}\right)} d x=2 \pi i \operatorname{Res}\left(\frac{e^{i z}-1}{z\left(z^{2}+a^{2}\right)}, i a\right)=\frac{\pi i}{a^{2}}\left(1-e^{-a}\right),
$$

therefore,

$$
I=\frac{\pi i}{2 a^{2}}\left(1-e^{-a}\right) .
$$

Solution to 5.11.10: We have

$$
\frac{\sin x}{x-3 i}=\frac{(\sin x)(x+3 i)}{x^{2}+9}=\Re\left(\frac{x e^{i x}}{x^{2}+9}\right)+3 i \Im\left(\frac{e^{i x}}{x^{2}+9}\right) \text {. }
$$

So

$$
\int_{-R}^{R} \frac{\sin x}{x-3 i} d x=\Im \int_{-R}^{R} \frac{x e^{i x}}{x^{2}+9} d x+3 i \Im \int_{-R}^{R} \frac{e^{i x}}{x^{2}+9} d x
$$

We evaluate these integrals over the contour $C_{R}=\Gamma_{R} \cup[-R, R]$ :

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-042.jpg?height=472&width=811&top_left_y=1637&top_left_x=668)

As $z^{2}+9$ has simple zeros at $\pm 3 i$, by the Residue Theorem [MH87, p. 280],

$$
\int_{C_{R}} \frac{z e^{i z}}{z^{2}+9} d z=2 \pi i \operatorname{Res}(f, 3 i)=e^{-3} \pi i \text {. }
$$

By Jordan's Lemma [MH87, p. 301], we have

$$
\left|\int_{\Gamma_{R}} \frac{z e^{i z}}{z^{2}+9} d z\right| \leqslant \int_{\Gamma_{R}} \frac{|z|\left|e^{i z}\right|}{|z|^{2}-9}|d z| \leqslant \frac{\pi R}{R^{2}-9}=o(1) \quad(R \rightarrow \infty)
$$

so, in the limit the integral along the upper half-circle contributes nothing.

We can evaluate the second integral in the same way, getting

$$
\int_{C_{R}} \frac{e^{i z}}{z^{2}+9} d z=\frac{e^{-3} \pi}{3}
$$

Again, in the limit, the upper half-circle contributes zero, so

$$
\lim _{R \rightarrow \infty} \int_{-R}^{R} \frac{\sin x}{x-3 i} d x=e^{-3} \pi
$$

Solution to 5.11.11: Consider the function $f$ defined by

$$
f(z)=\frac{e^{i z}}{z(z-\pi)}
$$

By Cauchy's Theorem [MH87, p. 152],

$$
\int_{C_{\varepsilon, R}} f(z) d z=0
$$

where $C_{\varepsilon, R}$ is the contour $\Gamma_{R} \cup \gamma_{0} \cup[\varepsilon, \pi-\varepsilon] \cup \gamma_{\pi} \cup[\pi+\varepsilon, R]$.

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-043.jpg?height=472&width=824&top_left_y=1412&top_left_x=713)

The poles of $f$ at 0 and $\pi$ are simple; therefore,

$$
\begin{aligned}
&\lim _{\varepsilon \rightarrow 0} \int_{\gamma_{0}} f(z) d z=-\pi i \operatorname{Res}(f(z), 0)=i \\
&\lim _{\varepsilon \rightarrow 0} \int_{\gamma_{\pi}} f(z) d z=-\pi i \operatorname{Res}(f(z), \pi)=i .
\end{aligned}
$$

We also have, for $|z|=R$

$$
\frac{e^{i z}}{z(z-\pi)}=O\left(R^{-2}\right) \quad(R \rightarrow \infty)
$$

so

$$
\lim _{R \rightarrow \infty} \int_{\Gamma_{R}} f(z) d z=0 .
$$

Taking imaginary parts, we obtain

$$
\int_{-\infty}^{\infty} \frac{\sin x}{x(x-\pi)} d x=-2
$$

Solution to 5.11.12: The integral equals $\Re \int_{-\infty}^{\infty} \frac{e^{i x}}{\left(1+x^{2}\right)^{3}} d x$. For $R>1$ let $\Gamma_{R}$ be the contour consisting of the interval $[-R, R]$ on the real axis and the top half of the circle $|z|=R$, with the counterclockwise orientation. The function $f(z)=e^{i z} /\left(1+z^{2}\right)^{3}$ is analytic on and inside $\Gamma_{R}$ except for a pole of order 3 at $z=i$. By the Residue Theorem [MH87, p. 280],

$$
\int_{\Gamma_{R}} f(z) d z=2 \pi i \operatorname{Res}(f(z), i)
$$

Writing $f(z)=\frac{e^{i z}}{(z-i)^{3}(z+i)^{3}}$, we see that

$$
\begin{aligned}
2 \operatorname{Res}(f(z), i) &=\left.\frac{d^{2}}{d z^{2}}\left(\frac{e^{i z}}{(z+i)^{3}}\right)\right|_{z=i} \\
&=\left.\frac{d}{d z}\left(\frac{i e^{i z}}{(z+i)^{3}}-\frac{3 e^{i z}}{(z+i)^{4}}\right)\right|_{z=i} \\
&=\left.\left(\frac{-e^{i z}}{(z+i)^{3}}-\frac{6 i e^{i z}}{(z+i)^{4}}+\frac{12 e^{i z}}{(z+i)^{5}}\right)\right|_{z=i}=\frac{e^{-1}}{i}\left(\frac{1}{8}+\frac{3}{8}+\frac{3}{8}\right) \\
&=e^{-1}\left(\frac{-1}{8 i^{3}}-\frac{6 i}{16 i^{4}}+\frac{12}{32 i^{5}}\right) \\
&=\frac{7}{8 i e} \cdot
\end{aligned}
$$

Hence $\int_{\Gamma_{R}} f(z) d z=\frac{7 \pi}{8 e} \cdot$ Now, since $\mathfrak{I} f$ is an odd function on $\mathbb{R}$,

$$
\int_{\Gamma_{R}} f(z) d z=\int_{-R}^{R} \frac{\cos x}{\left(1+x^{2}\right)^{3}} d x+\int_{S_{R}} f(z) d z,
$$

where $S_{R}$ is the top half of $\Gamma_{R}$. As $R \rightarrow \infty$ the first term on the right converges to the integral we want, and the second term converges to 0 because $|f(z)| \leqslant$ $1 /\left(|z|^{2}-1\right)^{3}$ in the upper half-plane, and we conclude that the value of the integral is $7 \pi / 8 e$. Solution to 5.11.13: Note that $1+x+x^{2}=\left(x+\frac{1}{2}\right)^{2}+\frac{3}{4}$, so the denominator does not vanish on the real axis. The given integral is $I=\Re \int_{-\infty}^{\infty} \frac{e^{i k x}}{1+x+x^{2}}$. As $\left|e^{i k z}\right| \leqslant 1$ for $z$ in the upper half-plane, and since the denominator has degree 2, we may close the contour in the upper half-plane to get $I=\Re(2 \pi i R)$, where $R$ is the sum of the residues of $f(z)=\frac{e^{i k z}}{1+z+z^{2}}$ in the upper half-plane. In this domain $f$ has just one singularity, a simple pole at $-\frac{1}{2}+i \frac{\sqrt{3}}{2}$, with residue $R=\frac{e^{i k\left(-\frac{1}{2}+i \frac{\sqrt{3}}{2}\right)}}{2\left(-\frac{1}{2}+i \frac{\sqrt{3}}{2}\right)+1}=\frac{e^{-\frac{k \sqrt{3}}{2}} e^{-i \frac{k}{2}}}{i \sqrt{3}}$. Therefore

$$
I=\mathfrak{H}(2 \pi i R)=\frac{2 \pi}{\sqrt{3}} e^{-\frac{k \sqrt{3}}{2}} \cos \frac{k}{2} .
$$

Solution to 5.11.14: An argument similar to the one used in Problem 5.11.11 with the contour around the simple poles $-1 / 2$ and $1 / 2$, gives

$$
\begin{aligned}
\int_{-\infty}^{\infty} \frac{\cos (\pi x)}{4 x^{2}-1} d x &=\Re\left(\pi i\left(\operatorname{Res}\left(\frac{e^{\pi i z}}{4 z^{2}-1},-\frac{1}{2}\right)+\operatorname{Res}\left(\frac{e^{\pi i z}}{4 z^{2}-1}, \frac{1}{2}\right)\right)\right) \\
&=\Re \pi i\left(\frac{i}{4}+\frac{i}{4}\right)=-\frac{\pi}{2} .
\end{aligned}
$$

Solution to 5.11.15: Consider the following contour $C_{R}$ and the function

$$
f(z)=\frac{e^{i n z}}{z^{4}+1}
$$

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-045.jpg?height=483&width=829&top_left_y=1832&top_left_x=648)

We have

$$
\left|\int_{\Gamma_{R}} \frac{e^{i n z}}{z^{4}+1} d z\right| \leqslant \int_{\Gamma_{R}} \frac{\left|e^{i n z}\right|}{R^{4}-1}|d z| \leqslant \frac{\pi R}{R^{4}-1}
$$

which approaches 0 when $R \rightarrow \infty$ (note that for $z \in \Gamma_{R},\left|e^{i n z}\right|=e^{-x n z} \leqslant 1$ ). Taking the real part and using the fact that for these roots $1 / z_{0}^{3}=-z_{0}$, we have

$$
\begin{aligned}
\int_{-\infty}^{\infty} \frac{\cos n x}{x^{4}+1} d z &=\lim _{R \rightarrow \infty} \Re\left(\int_{C} \frac{e^{i n z}}{z^{4}+1} d z\right) \\
&=\Re\left(2 \pi i\left(\operatorname{Res}\left(\frac{e^{i n z}}{z^{4}+1}, e^{\pi i / 4}\right)+\operatorname{Res}\left(\frac{e^{i n z}}{z^{4}+1}, e^{3 \pi i / 4}\right)\right)\right) \\
&=-2 \pi \Re\left(\left.\frac{e^{i n z}}{4 z^{3}}\right|_{z=e^{\pi i / 4}}+\left.\frac{e^{i n z}}{4 z^{3}}\right|_{z=e^{3 \pi i / 4}}\right) \\
&=\frac{\pi e^{-\frac{n}{\sqrt{2}}}}{\sqrt{2}}\left(\cos \frac{n}{\sqrt{2}}+\sin \frac{n}{\sqrt{2}}\right) .
\end{aligned}
$$

Solution to 5.11.16: Consider the same contour $C_{R}$ as in Problem 5.11.15; which encircles two of the poles $\left(e^{\frac{\pi i}{4}}\right.$ and $\left.e^{\frac{3 \pi i}{4}}\right)$, and the function

$$
f(z)=\frac{z^{2}+1}{z^{4}+1} \text {. }
$$

We have

$$
\begin{aligned}
\int_{0}^{\infty} \frac{x^{2}+1}{x^{4}+1} d z &=\frac{1}{2}\left(2 \pi i\left(\operatorname{Res}\left(\frac{z^{2}+1}{z^{4}+1}, e^{\pi i / 4}\right)+\operatorname{Res}\left(\frac{z^{2}+1}{z^{4}+1}, e^{3 \pi i / 4}\right)\right)\right) \\
&=\pi i\left(\left.\frac{z^{2}+1}{4 z^{3}}\right|_{z=e^{\pi i / 4}}+\left.\frac{z^{2}+1}{4 z^{3}}\right|_{z=e^{3 \pi i / 4}}\right) \\
&=-\frac{\pi i}{4}\left(\left.\left(z^{2}+1\right) z\right|_{z=e^{\pi i / 4}}+\left.\left(z^{2}+1\right) z\right|_{z=e^{3 \pi i / 4}}\right) \\
&=\frac{\pi}{\sqrt{2}} .
\end{aligned}
$$

Solution to 5.11.17: We have

$$
\begin{aligned}
\int_{-\infty}^{\infty} \frac{\cos ^{3} x}{a^{2}+x^{2}} d x &=\int_{-\infty}^{\infty} \frac{\left(\frac{e^{i x}+e^{-i x}}{2}\right)^{3}}{a^{2}+x^{2}} d x \\
&=\frac{1}{8} \int_{-\infty}^{\infty} \frac{e^{3 i x}+e^{-3 i x}+3 e^{i x}+3 e^{-i x}}{a^{2}+x^{2}} d x \\
&=\frac{1}{4} \int_{-\infty}^{\infty} \frac{e^{3 i x}}{a^{2}+x^{2}} d x+\frac{3}{4} \int_{-\infty}^{\infty} \frac{e^{i x}}{a^{2}+x^{2}} d x \\
&=\frac{1}{4} I_{1}(a)+\frac{3}{4} I_{2}(a) .
\end{aligned}
$$

To evaluate $I_{1}(a)$, let $\Gamma_{R}(R>a)$ be the contour consisting of the interval $[-R, R]$ on the real axis plus the semicircle in the upper half-plane with center 0 and radius $R$, oriented counterclockwise. By Cauchy's Integral Formula [MH87, p. 167],

$$
\begin{aligned}
\int_{\Gamma_{R}} \frac{e^{3 i z}}{z^{2}+a^{2}} d \uparrow &=\int_{\Gamma_{R}} \frac{e^{3 i z}}{(z+i a)(z-i a)} d z \\
'^{\prime} &=\left.2 \pi i\left(\frac{e^{3 i z}}{z+i a}\right)\right|_{z=i a}=\frac{\pi e^{-3 a}}{a} .
\end{aligned}
$$

The contribution to the integral from the semicircle is $O\left(\frac{\pi R}{R^{2}-a^{2}}\right)$ (as $\left|e^{3 i z}\right| \leqslant 1$ for $\Im z \geqslant 0$ ), so it tends to 0 as $R \rightarrow \infty$. The contribution from the interval $[-R, R]$ tends to $I_{1}(a)$ as $R \rightarrow \infty$. It follows that $I_{1}(a)=\frac{\pi e^{-3 a}}{a}$. Exactly the same reasoning shows that $I_{2}(a)=\frac{\pi e^{-a}}{a}$. We obtain

$$
I(a)=\frac{\pi}{4 a}\left(e^{-3 a}+3 e^{-a}\right) .
$$

Solution to 5.11.18: This integral converges, by Dirichlet's Test [MH93, p. 287], since

$$
\lim _{x \rightarrow \pm \infty} \frac{x}{x^{2}+4 x+20}=0
$$

monotonically and

$$
\int_{\alpha}^{\beta} \sin x d x=O(1) \quad(\alpha \rightarrow-\infty, \beta \rightarrow \infty) .
$$

Consider the integral

$$
I=\int_{C_{R}} f(z) e^{i z} d z \text { where } f(z)=\frac{z}{z^{2}+4 z+20} .
$$

The curve $C_{R}$ is the contour

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-047.jpg?height=483&width=824&top_left_y=2065&top_left_x=648)

where $\Gamma_{R}$ is the semicircle of radius $R>0$. The integral $I$ is equal to the sum of the residues of $g(z)=f(z) e^{i z}$ inside of $C_{R} . f$ has poles at $-2 \pm 4 i$, of which only $-2+4 i$ lies inside $C_{R}$. Hence,

$$
I=2 \pi i \operatorname{Res}(g,-2+4 i)=k_{\pi i} \lim _{z \rightarrow-2+4 i} \frac{z e^{i z}}{z-(-2-4 i)}=\frac{\pi}{4}(-2+4 i) e^{-4-2 i} .
$$

We have, by Jordan's Lemma' [MH87, p. 301], when $R \rightarrow \infty$,

$$
\left|\int_{\Gamma_{R}} g(z) d z\right| \leqslant \frac{R}{R^{2}-4 R-20} \int_{\Gamma_{R}}\left|e^{i z}\right||d z| \leqslant \frac{R \pi}{R^{2}-4 R-20}=o(1) \text {. }
$$

So,

$$
\begin{aligned}
\int_{-\infty}^{\infty} f(x) \sin x d x &=\mathfrak{}\left(\int_{-\infty}^{\infty} f(x) e^{i x} d x\right) \\
&=\Im\left(\lim _{R \rightarrow \infty} \int_{C_{R}} f(z) e^{i z} d z\right) \\
&=\Im\left(\frac{\pi}{4}(-2+4 i) e^{-4-2 i}\right) \\
&=\frac{\pi}{2 e^{4}}(2 \cos 2+\sin 2) .
\end{aligned}
$$

Solution to 5.11.19: Consider the following contour avoiding the pole of the function

$$
f(z)=\frac{z+i e^{i z}}{z^{3}}
$$

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-048.jpg?height=480&width=816&top_left_y=1565&top_left_x=644)

we have

$$
\left|\int_{\Gamma_{R}} \frac{z+i e^{i z}}{z^{3}} d z\right| \leqslant \int_{\Gamma_{R}} \frac{|z|+\left|e^{i z}\right|}{R^{3}}|d z| \leqslant \frac{\pi(R+1)}{R^{2}}
$$

which approaches 0 when $R \rightarrow \infty$ (note that for $z \in \Gamma_{R},\left|e^{i z}\right|=e^{-i z} \leqslant 1$ ); and around 0 ,

$$
\frac{z+i e^{i z}}{z^{3}}=\frac{1}{z^{3}}\left(z+i\left(1+i z+\frac{(i z)^{2}}{2}+\cdots\right)=\frac{i}{z^{3}}-\frac{i}{2 z}+g(z)\right.
$$

where $g$ is analytic. Then

$$
\int_{\gamma_{\varepsilon}} \frac{z+i e^{i z}}{z^{3}} d z=i \int_{\gamma_{\varepsilon}} \frac{d z}{z^{3}}-\frac{i}{2} \int_{\gamma_{\varepsilon}} \frac{d z}{z}+\int_{\gamma_{\varepsilon}} g(z) d z=-\frac{\pi}{2}+\int_{\gamma_{\varepsilon}} g(z) d z .
$$

Now since

$$
\left|\int_{\gamma_{\varepsilon}} g(z) d z\right| \leqslant \max _{|z| \leqslant \varepsilon}|g(z)| \pi \varepsilon \rightarrow 0 \text { as } \varepsilon \rightarrow 0
$$

the integral along the real axis will approach $\frac{\pi}{2}$. Taking the real part, one gets

$$
\int_{0}^{\infty} \frac{x-\sin x}{x^{3}} d x=\frac{1}{2} \frac{\pi}{2}=\frac{\pi}{4} \text {. }
$$

Solution to 5.11.20: Denote the given integral by $I$. Consider the function $f(z)=\left(1+z+z^{2}\right)^{-2} \cdot f$ has two poles of order 2 at $z=(-1 \pm i \sqrt{3}) / 2$. We evaluate the contour integral

$$
I^{\prime}=\int_{C_{R}} f(z) d z .
$$

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-049.jpg?height=477&width=827&top_left_y=1263&top_left_x=649)

By the Residue Theorem [MH87, p. 280], for $R$ large enough,

$$
I^{\prime}=2 \pi i \operatorname{Res}\left(f(z), \frac{-1+i \sqrt{3}}{2}\right) .
$$

As $R$ tends to infinity, the integral along the upper half-circle tends to 0 since the denominator of $f(z)$ has degree 4 higher than the numerator. Hence, $I^{\prime}$ tends to $I$ as $R$ tends to infinity. Therefore,

$$
\begin{aligned}
I &=2 \pi i \operatorname{Res}\left(f(z), \frac{-1+i \sqrt{3}}{2}\right) \\
&=\left.2 \pi i \frac{d}{d z}\left(\left(z+\frac{1+i \sqrt{3}}{2}\right)^{-2}\right)\right|_{z=\frac{-1+i \sqrt{3}}{}}
\end{aligned}
$$



$$
=\frac{4 \pi}{3 \sqrt{3}} .
$$

Solution to 5.11.21: Letting $t^{2}=x$, we get

$$
\int_{0}^{\infty} \frac{x^{\alpha-1}}{1+x} d x=2 \int_{0}^{\infty} \frac{t^{2 \alpha-1}}{1+t^{2}} d t
$$

Consider the integral

$$
\int_{C_{\varepsilon, R}} \frac{z^{2 \alpha-1}}{1+z^{2}} d z,
$$

where $C_{\varepsilon, R}$ is the contour $\Gamma_{R} \cup \gamma_{\varepsilon} \cup[-R,-\varepsilon] \cup[\varepsilon, R]$ :

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-050.jpg?height=469&width=811&top_left_y=915&top_left_x=646)

where the small circle, $\gamma_{\varepsilon}$, has radius $\varepsilon$, and the large circle, $\Gamma_{R}$, radius $R$. Since $z^{2 \alpha-1}$ is defined to be $e^{(2 \alpha-1) \log z}$, we choose as our branch of the logarithm that with arguments between $-\pi / 2$ and $3 \pi / 2$ (that is, the one with the cut along the negative imaginary axis).

The integrand has a simple pole inside $C_{\varepsilon} R$ at $i$, so, by the Residue Theorem [MH87, p. 280], the integral is equal to

$$
2 \pi i \operatorname{Res}(f, i)=\frac{2 \pi i e^{(2 \alpha-1) \log i}}{2 i}=-\pi i e^{\alpha \pi i} .
$$

As $\varepsilon$ and $R$ tend to 0 and infinity, respectively, the integral on $[\varepsilon, R]$ tends to the desired integral. On the segment $[-R,-\varepsilon]$, we make the change of variables $y=-z$, getting

$$
\int_{-R}^{-\varepsilon} \frac{e^{(2 \alpha-1) \log z}}{1+z^{2}} d z=-e^{2 \alpha \pi i} \int_{\varepsilon}^{R} \frac{e^{(2 \alpha-1) \log y}}{1+y^{2}} d y,
$$

so this integral tends to a constant multiple of the desired integral. On $\Gamma_{R}$, a calculation shows that $\left|z^{2 \alpha-1}\right|=|z|^{2 \Re \alpha-1} e^{-2 \Re \alpha \text { arg } z}$, so if we assume that $\Re \alpha \geqslant 0$, we have $\left|z^{2 \alpha-1}\right| \leqslant|z|^{29 i \alpha-1}$ and

$$
\left|\int_{\Gamma_{R}} \frac{z^{2 \alpha-1}}{1+z^{2}} d z\right| \leqslant \frac{\pi R^{2 \$ \alpha \alpha}}{R^{2}-1} \text {. }
$$

This tends to 0 whenever $\mathfrak{P} \alpha<1$.

On $\gamma_{\varepsilon}$, essentially the same estimate holds:

$$
\left|\int_{\gamma_{\varepsilon}} \frac{z^{2 \alpha-1}}{1+z^{2}} d z\right| \leqslant \frac{\pi \varepsilon^{2 \Re+\alpha}}{1-\varepsilon^{2}}
$$

and this tends to 0 whenever $\mathfrak{R} \alpha>0$.

So we have

$$
\left(1-e^{2 \alpha \pi i}\right) \int_{0}^{\infty} \frac{t^{2 \alpha-1}}{1+t^{2}} d t=-\pi i e^{\alpha \pi i}
$$

Dividing through, we get

$$
\frac{-\pi i e^{\alpha \pi i}}{1-e^{2 \alpha \pi i}}=\frac{\pi}{2 \sin \pi \alpha} .
$$

Therefore, twice this is our answer, subject to the restrictions $0<\Re \alpha<1$ and I $\alpha \geqslant 0$. However, if $\Im \alpha \leqslant 0$, we may replace $\alpha$ by $\bar{\alpha}$ and obtain the above equality with $\bar{\alpha}$. Then, by taking the complex conjugate of both sides, we see that we can eliminate the second restriction.

Solution 2. Considering the slightly more complex contour of integration below, we can do away with the change of variables. So consider the integral

$$
\int_{C} \frac{z^{\alpha-1}}{z+1} d z
$$

where $C$ is

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-051.jpg?height=816&width=824&top_left_y=1674&top_left_x=648)

The origin is a branch point and the two straight lines are close to the $x$-axis. The integrand has one simple pole at $z=-1$ inside $C$. Thus,

$$
\int_{C} \frac{z^{\alpha-1}}{z+1} d z=2 \pi i \operatorname{Res}\left(\frac{z^{\alpha-1}}{z+1},-1\right)=2 \pi i \lim _{z \rightarrow-1} \frac{z^{\alpha-1}}{z+1}(z+1)=2 \pi i e^{(\alpha-1) \pi i} .
$$

On the other hand, the integral can be split in four integrals along the segments of the contour:

$$
\begin{aligned}
2 \pi i e^{(\alpha-1) \pi i}=& \int_{\Gamma_{R}} \frac{z^{\alpha-1}}{z+1} d z+\int_{\gamma_{\varepsilon}} \frac{z^{\alpha-1}}{z+1} d z+\int_{\ell_{1}} \frac{z^{\alpha-1}}{z+1} d z+\int_{\ell_{2}} \frac{z^{\alpha-1}}{z+1} d z \\
=& \int_{0}^{2 \pi} \frac{\left(R e^{i \theta}\right)^{\alpha-1}}{R e^{i \theta}+1} i R e^{i \theta} d \theta+\int_{2 \pi}^{0} \frac{\left(\varepsilon e^{i \theta}\right)^{\alpha-1}}{\varepsilon e^{i \theta}+1} i \varepsilon e^{i \theta} d \theta \\
&+\int_{\varepsilon}^{R} \frac{x^{\alpha-1}}{x+1} d x+\int_{R}^{\varepsilon} \frac{\left(x e^{2 \pi i}\right)^{\alpha-1}}{x e^{2 \pi i}+1} d x .
\end{aligned}
$$

Taking the limits as $\varepsilon \rightarrow 0$ and $R \rightarrow \infty$, we get

$$
\int_{0}^{\infty} \frac{x^{\alpha-1}}{x+1} d x+\int_{\infty}^{0} \frac{e^{2 \pi i(\alpha-1)} x^{\alpha-1}}{x+1} d x=2 \pi i e^{(\alpha-1) \pi i}
$$

or

$$
\left(1-e^{2 \pi i(\alpha-1)}\right) \int_{0}^{\infty} \frac{x^{\alpha-1}}{x+1} d x=2 \pi i e^{(\alpha-1) \pi i}
$$

Hence,

$$
\int_{0}^{\infty} \frac{x^{\alpha-1}}{x+1} d x=\frac{2 \pi i e^{(\alpha-1) \pi i}}{1-e^{(\alpha-1) 2 \pi i}}=\frac{\pi}{\sin \alpha \pi} .
$$

Solution to 5.11.22: Making the substitution $x=\sqrt{y}$ the integral becomes

$$
\frac{1}{2} \int_{0}^{\infty} \frac{y^{-1 / 4}}{1+y} d y
$$

which is equal to $\frac{\pi}{\sqrt{2}}$ by Problem 5.11.21 with $\alpha=\frac{3}{4}$.

Solution to 5.11.23: Making the substitution $x=\sqrt[5]{y}$ the integral becomes

$$
\frac{1}{5} \int_{0}^{\infty} \frac{y^{-4 / 5}}{1+y} d y
$$

which is equal to $\frac{\pi}{5 \sin \frac{\pi}{5}}$ by Problem $5.11 .21$ with $\alpha=\frac{1}{5}$.

Solution 2. We integrate the function $f(z)=1 /\left(1+z^{5}\right)$ around the counterclockwise oriented contour $\Gamma_{R}$ consisting of the segment $[0, R]$, the circular arc joining the points $R$ and $R e^{2 \pi i / 5}$, and the segment $\left[R e^{2 \pi i / 5}, 0\right]$, where $R>1$. 

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-053.jpg?height=361&width=827&top_left_y=215&top_left_x=649)

The function $f$ has one singularity inside $\Gamma_{R}$, a simple pole at $e^{\pi i / 5}$. The residue is given by

$$
\operatorname{Res}\left(f, e^{\pi i / 5}\right)=\left.\frac{1}{\frac{d}{d z}\left(1+z^{5}\right)}\right|_{z=\pi i / 5}=\frac{1}{5 e^{4 \pi i / 5}} .
$$

By the Residue Theorem [MH87, p. 280], $\int_{\Gamma_{R}} f(z) d z=\frac{2 \pi i}{5 e^{4 \pi i / 5}} \cdot$
The contribution to the integral from the segment $[0, R]$ equals $\int_{0}^{R} \frac{1}{1+x^{5}} d x$
and the contribution from the segment $\left[R e^{2 \pi i / 5}, 0\right]$ equals

$$
\begin{aligned}
-\int_{0}^{R} \frac{1}{1+\left(t e^{2 \pi i / 5}\right)^{5}} d\left(t e^{2 \pi i 5}\right) &=-e^{2 \pi i / 5} \int_{0}^{R} \frac{1}{1+t^{5}} d t \\
&=-e^{2 \pi i / 5} \int_{0}^{R} \frac{1}{1+x^{5}} d x
\end{aligned}
$$

The contribution from the circular arc is $O\left(\frac{R}{(R-1)^{5}}\right)$, so it tends to 0 as $R \rightarrow \infty$. Taking the limit as $R \rightarrow \infty$, we thus obtain

$$
\left(1-e^{2 \pi i / 5}\right) \int_{0}^{\infty} \frac{1}{1+x^{5}} d x=\frac{2 \pi i}{5 e^{4 \pi i / 5}}
$$

or

$$
\begin{aligned}
\int_{0}^{\infty} \frac{1}{1+x^{5}} d x &=\frac{2 \pi i}{5\left(e^{4 \pi i / 5}-e^{6 \pi i / 5}\right)}=\frac{2 \pi i}{5\left(e^{\pi i / 5}-e^{-\pi i / 5}\right)} \\
&=\frac{2 \pi i}{5\left(2 i \sin \frac{\pi}{5}\right)}=\frac{\pi}{5 \sin \frac{\pi}{5}} .
\end{aligned}
$$

Solution to 5.11.24: Observing that the function is even, doubling the integral from 0 to $\infty$ and making the substitution $y=x^{2 n}$, we get

$$
\int_{-\infty}^{\infty} \frac{d x}{1+x^{2 n}}=2 \int_{0}^{\infty} \frac{d x}{1+x^{2 n}}=\frac{1}{n} \int_{0}^{\infty} \frac{y^{\frac{1}{2 n}-1}}{y+1} d y=\frac{\pi}{n \sin \pi / 2 n}
$$

\section{by Problem 5.11.21.}

Solution 2. The $2 n^{t h}$ roots of $-1$ in the upper half-plane are $z_{k}=e^{i\left(\frac{\pi}{2 n}+\frac{k \pi}{n}\right)}$ for $0 \leqslant k \leqslant n-1$. 

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-054.jpg?height=474&width=816&top_left_y=210&top_left_x=641)

Therefore

$$
\int_{-\infty}^{\infty} \frac{d x}{1+x^{2 n}}=2 \pi i \sum_{k=0}^{n-1} \operatorname{Res}\left(\frac{1}{1+z^{2 n}}, z_{k}\right) .
$$

Since $z_{k}$ is a simple pole of the integrand,

$$
\operatorname{Res}\left(\frac{1}{1+z^{2 n}}, z_{k}\right)=\lim _{z \rightarrow z_{k}} \frac{z-z_{k}}{1+z^{2 n}}=\frac{1}{2 n z_{k}^{2 n-1}}=-\frac{z_{k}}{2 n},
$$

giving

$$
\begin{aligned}
\int_{-\infty}^{\infty} \frac{d x}{1+x^{2 n}} &=-\frac{i \pi}{n} \sum_{k=0}^{n-1} z_{k}-\frac{i \pi}{n} e^{i \frac{\pi}{2 n}} \sum_{k=0}^{n-1} e^{i \frac{k \pi}{n}} \\
&=-\frac{i \pi}{n} e^{i \frac{\pi}{2 n}} \frac{1-e^{i \pi}}{1-e^{i \frac{\pi}{n}}}=\frac{2 \pi i}{n} e^{i \frac{\pi}{2 n}} \frac{1}{e^{i \frac{\pi}{n}}-1}=\frac{\pi}{n \sin \frac{\pi}{2 n}}
\end{aligned}
$$

Solution 3. Let $f(z)=\frac{1}{1+z^{2 n}}$ and consider the contour in shape of a slice of pizza containing the positive real axis from 0 to $R$, the arc of circle of angle $\pi / n$ and radius of the circle from the end of the arc back to 0 , oriented in this way.

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-054.jpg?height=368&width=813&top_left_y=1830&top_left_x=648)

$f$ has one singularity inside the contour, which is a simple pole at $e^{\pi i / 2 n}$. Therefore, the integral of $f$ around such a contour equals, by the Residue Theorem [MH87, p. 280],

$$
2 \pi i \operatorname{Res}\left(\frac{1}{1+z^{2 n}}, e^{\pi i / 2 n}\right)=\frac{-\pi i e^{\pi i / 2 n}}{n} .
$$

We have, then

$$
\begin{gathered}
\int_{0}^{R} \frac{1}{1+x^{2 n}} d x+\int_{0}^{\pi / 4} \frac{1}{1+R^{2 n} e^{2 n i \theta}} i R e^{i \theta} d \theta+\int_{R}^{0} \frac{1}{1+r^{2 n} e^{2 \pi i}} e^{\pi i / n} d r \\
\quad=\left(1-e^{\pi i / n}\right) \int_{0}^{R} \frac{1}{1+x^{2 n}} d x+i R \int_{0}^{\pi / n} \frac{1}{1+R^{2 n} e^{2 n i \theta}} e^{i \theta} d \theta .
\end{gathered}
$$

The integral on the arc approaches zero as $R \rightarrow \infty$, therefore, we get

$$
\int_{-\infty}^{\infty} \frac{1}{1+x^{2 n}} d x=-2 \frac{\pi i e^{\pi i / 2 n}}{n\left(1-e^{\pi i / n}\right)}=\frac{\pi}{n \sin \frac{\pi}{2 n}} .
$$

Solution 4. Consider the complex function $f(z)=\frac{1}{z^{2 n}+1}$. Its poles are all simple and located at the points

$$
c_{k}=e^{i\left(\frac{\pi}{2 n}+2 k \frac{\pi}{2 n}\right)}=e^{i(2 k+1) \frac{\pi}{2 n}} \quad k=0,1, \ldots, 2 n-1
$$

and since $k$ and $n$ are integers, the roots are never real and calling the primitive angle $\alpha=\frac{\pi}{2 n}$ the ones located above the real axis are

$$
c_{k}=e^{i(2 k+1) \alpha} \quad k=0,1, \ldots, n-1
$$

Computing the residue of $f$ at these points we have:

$$
\operatorname{Res}\left(\frac{1}{z^{2 n+1}}, c_{k}\right)=\frac{1}{\frac{d}{d z}\left(z^{2 n}+1\right)}=\frac{1}{2 n c_{k}^{2 n-1}} \quad k=0,1, \ldots, n-1
$$

So

$$
\begin{aligned}
\operatorname{Res}\left(\frac{1}{z^{2 n+1}}, c_{k}\right) &=\frac{1}{2 n} e^{\left[i \frac{(2 k+1)(1-2 n)}{2 n} \pi\right]} \\
&=\frac{1}{2 n} e^{\left[i \frac{(2 k+1)}{2 n} \pi+i \frac{(2 k+1)(-2 n)}{2 n} \pi\right]} \\
&=\frac{1}{2 n} e^{\left[i \frac{(2 k+1)}{2 n} \pi\right]} \cdot e^{[-i(2 k+1) \pi]} \\
&=-\frac{e^{[i(2 k+1) \alpha]}}{2 n}
\end{aligned}
$$

Using the upper-semi-circle as the contour of integration, we have to add the residues on all poles in the upper-half-plane. Using the fact that $\sum_{k=0}^{n-1} z^{k}=\frac{1-z^{n}}{1-z}$ and applying the Residue Theorem:

$$
\int_{-R}^{R} \frac{1}{x^{2 n+1}} d x+\int_{C_{R}} \frac{1}{z^{2 n+1}} d z=2 \pi i \sum_{k=0}^{n-1} \operatorname{Res}\left(\frac{1}{z^{2 n+1}}, c_{k}\right)
$$



$$
\begin{aligned}
&=-\frac{\pi i}{n} e^{i \alpha} \sum_{k=0}^{n-1}\left(e^{i 2 \alpha}\right)^{k} \\
&=-\frac{\pi i}{n} e^{i \alpha} \frac{1-e^{i 2 \alpha n}}{1-e^{i 2 \alpha}} \cdot \frac{e^{-i \alpha}}{e^{-i \alpha}} \\
&=-\frac{\pi i}{n} \frac{e^{i 2 \alpha n}-1}{e^{i \alpha}-e^{-i \alpha}}=\frac{\pi}{n} \frac{2 i}{e^{i \alpha}-e^{-i \alpha}}=\frac{\pi}{n \sin \alpha}
\end{aligned}
$$

Solution to 5.11.25: Let

$$
f(z)=\frac{z^{2}}{z^{n}+1} .
$$

The answer is $2 I$, where $I=\int_{0}^{\infty} f(x) d x$.

For $R>1$ let $\Gamma_{R}$ be the straight line path from 0 to $R$ followed by the arc $R e^{i t}$ for $t \in[0,2 \pi / n]$, followed by the straight line from $R e^{2 \pi i / n}$ to 0 .

Let $\zeta=e^{\pi i / n}$. The poles of $f$ are $\zeta^{2 m+1}$ for $m \in \mathbb{Z}$, so the only pole inside $\Gamma_{R}$ is $\zeta$ and $f$ has a simple pole at $\zeta$ with residue $\frac{1}{n} \zeta^{3-n}$. By the Residue Theorem, we have,

$$
\int_{\Gamma_{R}} f(z) d z=\frac{2 \pi i}{n} \zeta^{3-n}=-\frac{2 \pi i}{n} \zeta^{3} .
$$

The integral over the first line tends to $I$ as $R \rightarrow \infty$, over the arc of circle part it approaches 0 , since the integrand is $O\left(R^{2-n}\right.$ while the arc length is $O(R)$, and the integral over the last line segment tends to $-\zeta^{6} I$, as the substitution $z=\zeta^{2} w$ shows. Thus,

$$
I-\zeta^{6} I=-\frac{2 \pi i}{n} \zeta^{3} .
$$

Now,

so

$$
\sin \frac{3 \pi}{n}=\frac{\zeta^{3}-\zeta^{-3}}{2 i}=\frac{\zeta^{6}-1}{2 i \zeta^{3}}
$$

$$
2 I=\frac{2 \pi}{n \sin \frac{3 \pi}{n}} .
$$

Solution to 5.11.26: We have

$$
\begin{aligned}
I &=\int_{0}^{\infty} \frac{x}{e^{x}+e^{-x}} d x \\
&=\frac{1}{2} \int_{-\infty}^{\infty} \frac{x}{e^{x}-e^{-x}} d x \\
&=\frac{1}{2} \int_{0}^{\infty} \frac{\log u}{(u-1 / u) u} d u \\
&=\frac{1}{4} \int_{-\infty}^{\infty} \frac{\log |u|}{u^{2}-1} d u
\end{aligned}
$$

where we used $u=e^{x}$. Integrate

$$
f(z)=\frac{\log z}{z^{2}-1}
$$

over the contour $\Gamma=\Gamma_{R} \cup \gamma_{-1} \cup \gamma_{0} \cup \gamma_{1}$.

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-057.jpg?height=477&width=816&top_left_y=556&top_left_x=665)

By Cauchy's Theorem [MH87, p. 152], we have

$$
\int_{\Gamma} f(z) d z=0
$$

also,

So we get

$$
\begin{gathered}
\int_{\gamma_{-1}} f(z) d z=-i \pi \operatorname{Res}(f,-1)=\frac{\pi^{2}}{2} \\
\int_{\gamma_{1}} f(z) d z=-i \pi \frac{\log 1}{2}=0 \\
\int_{\Gamma_{R}} f(z) d z \rightarrow 0 \text { as } R \rightarrow \infty \text { since } \lim _{R \rightarrow \infty} \frac{R \log R}{R^{2}-1}=0 \\
\int_{\gamma_{0}} f(z) d z \rightarrow 0 \text { since } \lim _{\varepsilon \rightarrow 0} \frac{\varepsilon \log \varepsilon}{\varepsilon^{2}-1}=0 .
\end{gathered}
$$

$$
\int_{-\infty}^{\infty} \frac{\log |u|}{u^{2}-1} d u=\frac{\pi^{2}}{2}
$$

and

$$
I=\frac{\pi^{2}}{8} .
$$

Solution to 5.11.27: The roots of the denominator are $1 \pm i \sqrt{3}$, and for large $|x|$, the absolute value of the integrand is of the same order of magnitude as $x^{-2}$. It follows that the integral converges. For $R>2$, let

$$
I_{R}=\int_{C_{R}} \frac{e^{-i z}}{z^{2}-2 z+4} d z,
$$

where the contour $C_{R}$ consists of the segment $[-R, R]$ together with the semicircle $\Gamma_{R}$ in the lower half-plane with the same endpoints as that segment, directed clockwise.

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-058.jpg?height=474&width=827&top_left_y=446&top_left_x=646)

The integrand has only one singularity inside $C_{R}$, a simple pole at $1-i \sqrt{3}$. By the Residue Theorem [MH87, p. 280],

$$
I_{R}=-2 \pi i \operatorname{Res}\left(\frac{e^{-i z}}{z^{2}-2 z+4}, 1-i \sqrt{3}\right) .
$$

The residue, since we are dealing with a simple pole, equals

$$
\lim _{z \rightarrow 1-i \sqrt{3}}(z-1+i \sqrt{3})\left(\frac{e^{-i z}}{z^{2}-2 z+4}\right)=\left.\frac{e^{-i z}}{z-1-i \sqrt{3}}\right|_{z=1-i \sqrt{3}}=\frac{e^{-\sqrt{3}-i}}{-2 i \sqrt{3}} .
$$

Hence, $I_{R}=\frac{\pi e^{-\sqrt{3}-i}}{\sqrt{3}}$. Since $\left|e^{-i z}\right|$ is bounded by 1 in the lower half-plane, we have, for large $R$,

$$
\left|\int_{\Gamma_{R}} \frac{e^{-i z}}{z^{2}-2 z+4} d z\right| \leqslant \frac{2 \pi R}{R^{2}-2 R-4} \rightarrow 0 \quad(R \rightarrow \infty) .
$$

Hence,

$$
\begin{aligned}
&\frac{\pi e^{-\sqrt{3}-i}}{\sqrt{3}}=\int_{-R}^{R} \frac{e^{-i x}}{x^{2}-2 x+4} d x+\int_{\Gamma_{R}} \frac{e^{-i z}}{z^{2}-2 z+4} d z \rightarrow I \quad(R \rightarrow \infty), \\
&\text { giving } I=\frac{\pi e^{-\sqrt{3}-i}}{\sqrt{3}} .
\end{aligned}
$$

Solution to 5.11.28: The integral converges absolutely since, for each $0<\varepsilon<1$, we have $\log x=o\left(x^{\varepsilon}\right)(x \rightarrow 0+)$.

Consider the function

$$
f(z)=\frac{\log z}{\left(z^{2}+1\right)\left(z^{2}+4\right)}
$$

and, for $0<\varepsilon<R$, consider the contour $C_{\varepsilon, R}=\Gamma_{R} \cup \gamma_{\varepsilon} \cup[-R,-\varepsilon] \cup[\varepsilon, R]$.

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-059.jpg?height=477&width=811&top_left_y=352&top_left_x=668)

On the small circle, we have

$$
\left|\int_{\gamma_{\varepsilon}} f(z) d z\right| \leqslant \frac{|\log \varepsilon|+\pi}{\frac{1}{2}} \pi \varepsilon=0(1) \quad(\varepsilon \rightarrow 0)
$$

and on the larger one,

$$
\left|\int_{\Gamma_{R}} f(z) d z\right| \leqslant \frac{|\log R|+\pi}{\left(R^{2}-1\right)\left(R^{2}-4\right)} \pi R=o(1) \quad(R \rightarrow \infty) .
$$

Combining with the Residue Theorem [MH87, p. 280] we get

$$
\int_{-\infty}^{0} f(z) d z+\int_{0}^{\infty} f(z) d z=2 \pi i(\operatorname{Res}(f, i)+\operatorname{Res}(f, 2 i)) .
$$

We have

$$
\operatorname{Res}(f, i)=\lim _{z \rightarrow i} f(z)(z-i)=\frac{\pi}{12}
$$

and

$$
\operatorname{Res}(f, 2 i)=\lim _{z \rightarrow 2 i} f(z)(z-2 i)=\frac{\log 2+\frac{\pi i}{2}}{-12 i}=\frac{i \log 2}{12}-\frac{\pi}{24} .
$$

We also have

$$
\begin{aligned}
\int_{-\infty}^{0} f(z) d z &=\int_{-\infty}^{0} \frac{\log (-x)+\pi i}{\left(x^{2}+1\right)\left(x^{2}+4\right)} d x \\
&=\int_{0}^{\infty} \frac{\log x}{\left(x^{2}+1\right)\left(x^{2}+4\right)} d x+\int_{0}^{\infty} \frac{\pi i}{\left(x^{2}+1\right)\left(x^{2}+4\right)} d x
\end{aligned}
$$

and

$$
\int_{0}^{\infty} f(z) d z=\int_{0}^{\infty} \frac{\log x}{\left(x^{2}+1\right)\left(x^{2}+4\right)} d x
$$

using the same contour and singularities to evaluate

$$
\int_{0}^{\infty} \frac{\pi i}{\left(x^{2}+1\right)\left(x^{2}+4\right)} d x
$$

we get $\frac{\pi^{2} i}{12}$, so

$$
2 \int_{0}^{\infty} \frac{\log x}{\left(x^{2}+1\right)\left(x^{2}+4\right)} d x+\frac{\pi^{2} i}{12}=2 \pi i\left(\frac{\pi}{12}+\frac{i \log 2}{12}-\frac{\pi}{24}\right)
$$

and

$$
\int_{0}^{\infty} \frac{\log x}{\left(x^{2}+1\right)\left(x^{2}+4\right)} d x=-\frac{\pi \log 2}{12}
$$

Solution to 5.11.29: For $0<\varepsilon<1<R$, let $C_{\varepsilon, R}$ denote the contour pictured below. Let $\log z$ denote the branch of the logarithm function in the plane slit along the negative imaginary axis.

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-060.jpg?height=469&width=811&top_left_y=1175&top_left_x=646)

By the Residue Theorem [MH87, p. 280], since $i$ is a simple pole of the integrand, we have

$$
\begin{aligned}
\int_{C_{\varepsilon, R}} \frac{(\log z)^{2}}{z^{2}+1} d z &=2 \pi i \operatorname{Res}\left(\frac{(\log z)^{2}}{z^{2}+1}, i\right) \\
&=2 \pi i \lim _{z \rightarrow i} \frac{(\log z)^{2}}{z+i} \\
&=2 \pi i \frac{\left(\frac{\pi}{2} i\right)^{2}}{2 i} \\
&=-\frac{\pi^{3}}{4}
\end{aligned}
$$

For $x$ negative, we have $\log x=\log |x|+\pi i$, so $(\log x)^{2}=(\log |x|)^{2}-\pi^{2}+$ $2 \pi i \log |x|$. Thus, the contribution to the integral from the interval $[-R,-\varepsilon]$ equals

$$
\begin{aligned}
\int_{\varepsilon}^{R} \frac{(\log x)^{2}-\pi^{2}+2 \pi i \log x}{x^{2}+1} d x=& \int_{\varepsilon}^{R} \frac{(\log x)^{2}-\pi^{2}}{x^{2}+1} d x+\int_{\Gamma_{R}} \frac{(\log z)^{2}}{z^{2}+1} d z \\
&+\int_{\gamma_{\varepsilon}} \frac{(\log z)^{2}}{z^{2}+1} d z+2 \pi i \int_{\varepsilon}^{R} \frac{\log x}{x^{2}+1} d x \\
=&-\frac{\pi^{3}}{4} .
\end{aligned}
$$

Also,

$$
\left|\int_{\Gamma_{R}} \frac{(\log z)^{2}}{z^{2}+1} d z\right| \leqslant \frac{(\log R)^{2}(\pi R)}{R^{2}-1}=o(1) \quad(R \rightarrow \infty)
$$

and

$$
\left|\int_{\gamma_{\varepsilon}} \frac{(\log z)^{2}}{z^{2}+1} d z\right| \leqslant \frac{(\log \varepsilon)^{2}(\pi \varepsilon)}{\varepsilon^{2}-1}=o(1) \quad(\varepsilon \rightarrow 0) .
$$

In the limit we then obtain, considering the real parts,

$$
2 \int_{0}^{\infty} \frac{(\log x)^{2}}{x^{2}+1} d x=-\frac{\pi^{3}}{4}+\int_{0}^{\infty} \frac{\pi^{2}}{x^{2}+1} d x=-\frac{\pi^{3}}{4}+\frac{\pi^{3}}{2}=\frac{\pi^{3}}{4}
$$

and

$$
\int_{0}^{\infty} \frac{(\log x)^{2}}{x^{2}+1} d x=\frac{\pi^{3}}{8}
$$

Solution to 5.11.30: For $\lambda=0$, we have

$$
\int_{0}^{\infty}(\operatorname{sech} x)^{2} d x=\left.\tanh x\right|_{0} ^{\infty}=1
$$

For $\lambda \neq 0$,

$$
(\operatorname{sech} x)^{2} \cos \lambda x=\mathfrak{R}\left(\frac{4 e^{i \lambda x}}{\left(e^{x}+e^{-x}\right)^{2}}\right)
$$

so we consider the function $f$ given by

$$
f(z)=\frac{e^{i \lambda z}}{\left(e^{z}+e^{-z}\right)^{2}} .
$$

This function has a simple pole inside the contour

$$
C=[-R, R] \cup[R, R+\pi i] \cup[R+\pi i,-R+\pi i] \cup[-R+\pi i,-R] .
$$

pictured below:

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-062.jpg?height=290&width=824&top_left_y=343&top_left_x=713)

We have

$$
\begin{aligned}
\int_{[R+\pi i,-R+\pi i]} f(z) d z &=-\int_{-R}^{R} \frac{e^{i \lambda(x+\pi i)} d x}{\left(-e^{x}-e^{-x}\right)^{2}} \\
&=-e^{-\lambda \pi} \int_{-R}^{R} \frac{e^{i \lambda x} d x}{\left(e^{x}+e^{-x}\right)^{2}}
\end{aligned}
$$

and

$$
\begin{aligned}
\left|\int_{[R, R+\pi i]} f(z) d z\right| &=\left|\int_{0}^{1} \frac{e^{i \lambda(R+\pi i x)} \pi i d x}{\left(e^{R+\pi i x}+e^{-R-\pi i x}\right)^{2}}\right| \\
& \leqslant \frac{\pi e^{|\lambda| \pi}}{\left(e^{R}-e^{-R}\right)^{2}}=o(1) \quad(R \rightarrow \infty)
\end{aligned}
$$

similarly, we get

$$
\int_{[-R+\pi i,-R]} f(z) d z=o(1) \quad(R \rightarrow \infty) .
$$

Therefore,

$$
\left(1-e^{-\lambda \pi}\right) \int_{-\infty}^{\infty} \frac{e^{i \lambda x} d x}{\left(e^{x}+e^{-x}\right)^{2}}=2 \pi i \operatorname{Res}\left(f(z), \frac{\pi i}{2}\right)=\frac{\lambda \pi e^{-\lambda \pi / 2}}{2}
$$

Thus,

$$
\int_{0}^{\infty}(\operatorname{sech} x)^{2} \cos \lambda x d x=\frac{\lambda \pi / 2}{\sinh (\lambda \pi / 2)}
$$

Solution to 5.11.31: If $b=0$, the integral is well known:

$$
\int_{0}^{\infty} e^{-x^{2}} d x=\frac{\sqrt{\pi}}{2}
$$

and can be computed either by doubling it up

$$
\int_{0}^{\infty} e^{-x^{2}} d x \int_{0}^{\infty} e^{-y^{2}} d y=\int_{0}^{\infty} \int_{0}^{\infty} e^{-\left(x^{2}+y^{2}\right)} d x d y
$$

and converting to polar coordinates or by using the Residue Theorem applied to the function $f(z)=e^{-z^{2}}$ and the following contour, as shown by [Cad47, Mir49]. For the details, see [MH93, p. 321-322]. We will use this result to compute the full integral (for other values of $b$ ) below.

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-063.jpg?height=623&width=824&top_left_y=553&top_left_x=648)

Now consider the function $f(z)=e^{-z^{2}}$. If $b>0$, we will consider the integral over the rectangle contour of height $b$ and if $b<0$ the symmetric one below the $x$-axis.

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-063.jpg?height=295&width=813&top_left_y=1487&top_left_x=648)

Since $f$ is entire, the integral around the contour is zero, for any value of $R$. If $b<0$ draw the contour below the $x$-axis. Evaluating the integral over each one of the two sides parallel to the $x$-axis, we have

$$
\begin{aligned}
\int_{I} f(z) d z &=\int_{-R}^{R} e^{-x^{2}} d x \\
\int_{I I I} f(z) d z &=\int_{R}^{-R} e^{-(x+b i)^{2}} d x \\
&=-\int_{-R}^{R} e^{-\left(x^{2}-b^{2}\right)} e^{2 b x i} d x \\
&=-e^{b^{2}} \int_{-R}^{R} e^{-x^{2}}(\cos (2 b x)+i \sin (2 b x)) d x
\end{aligned}
$$



$$
=-e^{b^{2}} \int_{-R}^{R} e^{-x^{2}} \cos (2 b x) d x .
$$

Along the vertical segments (II and IV), $|f(z)|=\left|e^{-(\pm R+i y)^{2}}\right|=e^{-R^{2}+y^{2}} \leqslant$ $e^{-R^{2}}$ then $\left|\int_{I I} f\right|$ and $\left|\int_{I V} f\right|$ are bounded by $b e^{-R^{2}}$, and with $b$ fixed, this goes to 0 as $R \rightarrow \infty$. Letting $R \rightarrow \infty$, the integral along the circuit becomes

$$
\int_{-\infty}^{\infty} e^{-x^{2}} d x-e^{b^{2}} \int_{-\infty}^{\infty} e^{-x^{2}} \cos (2 b x) d x=0
$$

Now using the pre-computed integral the result follows:

$$
\begin{aligned}
\int_{0}^{\infty} e^{-x^{2}} \cos (2 b x) d x &=\frac{1}{2} e^{-b^{2}} \int_{-\infty}^{\infty} e^{-x^{2}} \cos (2 b x) d x \\
&=\frac{1}{2} e^{-b^{2}} \int_{-\infty}^{\infty} e^{-x^{2}} d x \\
&=\frac{\sqrt{\pi}}{2} e^{-b^{2}} .
\end{aligned}
$$

Solution to 5.11.32: The integral is the real part of

$$
I=\int_{0}^{\infty} e^{-(1+i) x^{2}} d x=\int_{0}^{\infty} e^{-\sqrt{2} e^{i \pi / 4} x^{2}} d x=\int_{0}^{\infty} e^{-\sqrt{2}\left(e^{i \pi / 8} x\right)^{2}} d x
$$

Consider the contour $\Gamma$ in shape of a slice of pizza, like the one in Solution 3 of Prob. 5.11.24, with angle $\pi / 8$. By Cauchy's Theorem [MH87, p. 152],

$$
\int_{\Gamma} e^{-\sqrt{2} z^{2}} d z=0
$$

On the arc the integral will approach 0 when the radius $R \rightarrow \infty$, because the integrand is bound in absolute value by $\left|e^{-\sqrt{2} e^{i \pi / 4} R^{2}}\right|=e^{-R^{2}}$. Thus

$$
0=\int_{0}^{\infty} e^{-\sqrt{2} x^{2}} d z-\int_{0}^{\infty} e^{-\sqrt{2}\left(e^{i \pi / 8} x\right)^{2}} d\left(e^{i \pi / 8} x\right)
$$

or equivalently

$$
0=2^{-1 / 4} \int_{0}^{\infty} e^{-u^{2}} d u-e^{i \pi / 8} I
$$

so $I=2^{-1 / 4} e^{-i \pi / 8} \sqrt{\pi} 2$, and taking the real part, the integral is equals to

$$
2^{-5 / 4}(\cos \pi / 8) \sqrt{\pi}=\frac{(2+\sqrt{2}) \sqrt{\pi}}{2^{13 / 4}} .
$$

Solution to 5.11.33: Denote the line segment in $\mathbb{C}$ from $z_{0}$ to $z_{1}$ by $\left[z_{0}, z_{1}\right]$. Let $a, b>0$, and $C_{1}=[b,-a], C_{2}=[-a,-a-i \gamma], C_{3}=[-a-i \gamma, b-$ $i \gamma], C_{4}=[b-i \gamma, b]$ 

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-065.jpg?height=358&width=870&top_left_y=198&top_left_x=606)

We integrate $f(z)=e^{-z^{2} / 2}$ round the rectangular contour $=C_{1}+C_{2}+C_{3}+C_{4}$. (The figure shows the case $\gamma>0$.)

The reverse of $C_{1}$ is parametrized by $z=t,-a \leqslant t \leqslant b$. Thus

$$
\int_{C_{1}} f(z) d z=-\int_{-a}^{b} e^{-\frac{1}{2} t^{2}} d t .
$$

The curve $C_{2}$ is parametrized by $z=-a-i t \gamma, 0 \leqslant t \leqslant 1$. Thus

$$
\int_{C_{2}} f(z) d z=-i \gamma \int_{0}^{1} e^{-\frac{1}{2}(-a-i t \gamma)^{2}} d t=-i \gamma e^{-\frac{1}{2} a^{2}} \int_{0}^{1} e^{\frac{1}{2} 2^{2} \gamma^{2}} e^{-i a t \gamma} d t
$$

so

$$
\left|\int_{C_{2}} f(z) d z\right| \leqslant|\gamma| e^{-\frac{1}{2} a^{2}} \int_{0}^{1} e^{\frac{1}{2} t^{2} \gamma^{2}} d t \rightarrow 0, \quad \text { as } a \rightarrow \infty .
$$

The curve $C_{3}$ is parametrized by $z=t-i \gamma,-a \leqslant t \leqslant b$. Thus

$$
\int_{C_{3}} f(z) d z=\int_{-a}^{b} e^{-\frac{1}{2}(t-i \gamma)^{2}} d t .
$$

The opposite of $C_{4}$ is parametrized by $z=b-i t \gamma, 0 \leqslant t \leqslant 1$. Thus

$$
\int_{C_{4}} f(z) d z=i \gamma \int_{0}^{1} e^{-\frac{1}{2}(b-i t \gamma)^{2}} d t=i \gamma e^{-\frac{1}{2} b^{2}} \int_{0}^{1} e^{\frac{1}{2} t^{2} \gamma^{2}} e^{i b t \gamma} d t
$$

therefore

$$
\left|\int_{C_{4}} f(z) d z\right| \leqslant|\gamma| e^{-\frac{1}{2} b^{2}} \int_{0}^{1} e^{\frac{1}{2} t^{2} \gamma^{2}} d t \rightarrow 0, \quad \text { as } b \rightarrow \infty .
$$

By Cauchy's Theorem [MH87, p. 152], the integral of $f$ round the rectangular contour is zero. Thus

$$
-\int_{-a}^{b} e^{-\frac{1}{2} t^{2}} d t+\int_{C_{2}} f(z) d z+\int_{-a}^{b} e^{-\frac{1}{2}(t-i \gamma)^{2}} d t+\int_{C_{4}} f(z) d z=0
$$

Let $a, b \rightarrow \infty$. We deduce that

$$
\int_{-\infty}^{\infty} \frac{e^{-(t-i \gamma)^{2} / 2}}{\sqrt{2 \pi}} d t=\frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} e^{-\frac{1}{2} t^{2}} d t=1
$$

proving the result. 

\section{Algebra}

\subsection{Examples of Groups and General Theory}

Solution to 6.1.1: 1. The set $G$ is the set of all invertible $2 \times 2$ real matrices. If $A, B \in G$, then $|A B|=|A||B| \neq 0$ and thus $A B \in G$. Matrix multiplication is associative. The identity matrix $I$ has determinant one, and thus belongs to $G$. Finally if $A \in G$ then $A$ is invertible, with $\left|A^{-1}\right|=1 /|A| \neq 0$, and thus $G$ is closed to inverses. This proves that $G$ is a multiplicative group.

2. Let

$$
A=\left(\begin{array}{ll}
a & b \\
c & d
\end{array}\right), B=\left(\begin{array}{ll}
1 & 1 \\
0 & 1
\end{array}\right), \dot{C}=\left(\begin{array}{ll}
1 & 0 \\
1 & 1
\end{array}\right) .
$$

Then

$$
\begin{aligned}
& A B=\left(\begin{array}{ll}a & a+b \\c & c+d\end{array}\right) \quad B A=\left(\begin{array}{cc}a+c & b+d \\c & d\end{array}\right), \\
& A C=\left(\begin{array}{ll}a+b & b \\c+d & d\end{array}\right) \quad C A=\left(\begin{array}{cc}a & b \\a+c & b+d\end{array}\right) .
\end{aligned}
$$

The matrices $B$ and $C$ belong to $G$. If $A$ commutes with $B$ and with $C$ then $c=0$, $a=d, b=0$ and $A=a I$ is a scalar matrix. On the other hand any scalar matrix commutes with every element of $G$. We conclude that the center of $G$ is the set of all real $2 \times 2$ matrices $a I$ with $a \neq 0$.

3. If $A, B$ are orthogonal then $(A B)(A B)^{t}=A B B^{t} A^{t}=I$, so $O$ is closed under multiplication. The identity matrix is orthogonal. Finally, if $A$ is orthogonal then $A^{-1}=A^{t}$ and $A^{-1}\left(A^{-1}\right)^{t}=A^{t} A=I$, thus $O$ is a subgroup of $G$. If $x=\left(\begin{array}{ll}1 & 0 \\ 1 & 1\end{array}\right)$ and $y=\left(\begin{array}{ll}0 & 1 \\ 1 & 0\end{array}\right)$ then $x$ is in $G, y$ is in $O$ and

$$
x^{-1} y x=\left(\begin{array}{cc}
1 & 0 \\
-1 & 1
\end{array}\right)\left(\begin{array}{ll}
0 & 1 \\
1 & 0
\end{array}\right)\left(\begin{array}{ll}
1 & 0 \\
1 & 1
\end{array}\right)=\left(\begin{array}{cc}
1 & 1 \\
0 & -1
\end{array}\right)
$$

is not orthogonal. This proves that $O$ is not a normal subgroup of $G$.

4. $\mathbb{R}^{*}=\mathbb{R} \backslash\{0\}$ is an abelian multiplicative group. The map det : $G \rightarrow \mathbb{R}^{*}$ is a group homomorphism which is clearly surjective, and nontrivial.

Solution to 6.1.2: 1. The set $G$ is a subset of the multiplicative group $G L_{3}(\mathbb{R})$ of nonsingular matrices. It is easy to verify that the product of two elements of $G$ has ones on the diagonal, and zeros below the diagonal. Moreover the identity matrix belongs to $G$. To verify closure to inverses, let $A \in G$. Then the characteristic polynomial of $A$ is $(x-1)^{3}$. By Cayley-Hamilton's Theorem [HK61, p. 194], $(A-I)^{3}=0$, therefore $A^{-1}=A^{2}-3 A+3 I$. From the closure property, $A^{2} \in G$, and it is easy to see that $A^{2}-3 A+3 I$ has ones on the diagonal, and zeros below the diagonal, so $A^{-1} \in G$. Thus $G$ is a group.

2. Let $A=\left(\begin{array}{lll}1 & a & b \\ 0 & 1 & c \\ 0 & 0 & 1\end{array}\right)$ be in the center of $G$, so that $A$ commutes with every member of $G$. Then the products

$$
\begin{aligned}
&\left(\begin{array}{lll}
1 & a & b \\
0 & 1 & c \\
0 & 0 & 1
\end{array}\right)\left(\begin{array}{lll}
1 & 1 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right)=\left(\begin{array}{ccc}
1 & 1+a & b \\
0 & 1 & c \\
0 & 0 & 1
\end{array}\right) \\
&\left(\begin{array}{lll}
1 & 1 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right)\left(\begin{array}{lll}
1 & a & b \\
0 & 1 & c \\
0 & 0 & 1
\end{array}\right)=\left(\begin{array}{ccc}
1 & 1+a & b+c \\
0 & 1 & c \\
0 & 0 & 1
\end{array}\right)
\end{aligned}
$$

show that $c=0$. And the products

$$
\begin{aligned}
\left(\begin{array}{lll}
1 & a & b \\
0 & 1 & c \\
0 & 0 & 1
\end{array}\right)\left(\begin{array}{lll}
1 & 0 & 0 \\
0 & 1 & 1 \\
0 & 0 & 1
\end{array}\right)=\left(\begin{array}{ccc}
1 & a & a+b \\
0 & 1 & 1+c \\
0 & 0 & 1
\end{array}\right) \\
\left(\begin{array}{lll}
1 & 0 & 0 \\
0 & 1 & 1 \\
0 & 0 & 1
\end{array}\right)\left(\begin{array}{lll}
1 & a & b \\
0 & 1 & c \\
0 & 0 & 1
\end{array}\right)=\left(\begin{array}{ccc}
1 & a & b \\
0 & 1 & 1+c \\
0 & 0 & 1
\end{array}\right)
\end{aligned}
$$

show that $a=0$. Finally the products

$$
\begin{aligned}
&\left(\begin{array}{lll}
1 & a & b \\
0 & 1 & c \\
0 & 0 & 1
\end{array}\right)\left(\begin{array}{lll}
1 & 0 & x \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right)=\left(\begin{array}{ccc}
1 & a & x+b \\
0 & 1 & c \\
0 & 0 & 1
\end{array}\right) \\
&\left(\begin{array}{lll}
1 & 0 & x \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right)\left(\begin{array}{lll}
1 & a & b \\
0 & 1 & c \\
0 & 0 & 1
\end{array}\right)=\left(\begin{array}{ccc}
1 & a & b+x \\
0 & 1 & c \\
0 & 0 & 1
\end{array}\right)
\end{aligned}
$$

show that the center of $G$ is the set of all matrices

$$
\left(\begin{array}{lll}
1 & 0 & x \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right), \quad x \in \mathbb{R}
$$

Solution to 6.1.3: 1. $S_{3}$ is a nonabelian group.

2. The Klein four group $V=\mathbb{Z}_{2} \times \mathbb{Z}_{2}$ is a finite abelian group that is not cyclic.

$3 . \mathbb{Z}$ is an infinite additive group; the subgroup $5 \mathbb{Z}$ is a subgroup of index 5 in $\mathbb{Z}$.

4. $\mathbb{Z}_{4}$ and $\mathbb{Z}_{2} \times \mathbb{Z}_{2}$ are nonisomorphic finite groups of order 4 .

5. $S_{3}$ has a subgroup $H=\langle\alpha\rangle$, where $\alpha$ is any transposition in $S_{3}$, which is not normal in $S_{3}$.

6. The alternating group $A_{5}$ is nonabelian and has no normal subgroups other than the full group and the identity.

7. The additive group $\mathbb{Z}$ has a normal subgroup $H=3 \mathbb{Z}$; the factor group $\mathbb{Z} / 3 \mathbb{Z}$ has order 3 , and is not isomorphic to any subgroup of $\mathbb{Z}$.

8. The left (also right) cosets of $H$ in $G$ form a partition of $G$, and for any $g \in G$, $g H=H$ iff $g \in H$. Let $H$ have index 2 in $G$. Then there are two left cosets $H$ and $G \backslash H$, and these are also the two right cosets of $H$ in $G$. Fix $x \in G$. If $x \in H$ then $x H=H=H x$. If $x \notin H$ then $x H=G \backslash H=H x$. This proves that $H$ is normal in $G$.

Solution to 6.1.4: 1. If $x, y \in G(R)$ have inverses $x^{\prime}, y^{\prime} \in R$ respectively, then $x y\left(y^{\prime} x^{\prime}\right)=\left(y^{\prime} x^{\prime}\right) x y=1$ proving that $x y$ has an inverse. The identity 1 has itself as an inverse. If $x$ has inverse $x^{\prime}$, then $x^{\prime}$ has inverse $x$. Associativity of multiplication is given. Therefore $G(R)$ is a multiplicative group.

2. Let $x=a+i b$ have inverse $y=u+i v$, with $a, b, u, v$ integers. Then $x y=1$, and taking moduli $|x||y|=1$ and $|x|^{2}|y|^{2}=1$. Since $a, b, u$ and $v$ are integers, $|x|^{2}=a^{2}+b^{2}=1$, giving $a=\pm 1, b=0$ or $a=0, b=\pm 1$. The units are $1, i,-1,-i$. This group is cyclic, generated by $i$, and so isomorphic to $\mathbb{Z}_{4}$.

Solution to 6.1.5: Let $G$ be the group of symmetries of the network and let $D$ be the triangle with vertices $P_{0}, P_{2}$, and $P_{3}$.

Let $\alpha$ and $\beta$ be $180^{\circ}$ degree rotations about the midpoints of the segments $P_{0} P_{3}$ and $P_{0} P_{1}$, respectively, and let $\tau$ be a reflection in the line extending the segment $P_{0} P_{2}$. Let $x$ be a point in the plane. With this notation, it is easy to see that:

$$
\begin{aligned}
\alpha \beta(x) &=x+(0,1) \\
\alpha \tau \alpha \tau(x) &=x+(2,0)
\end{aligned}
$$

Claim 1: $\alpha, \beta, \tau$ generate the group $G$.

Proof: Note that the four triangles $D, \alpha(D), \alpha \tau(D)$, and $\alpha \tau \alpha(D)$ tile a one-bytwo rectangle. Applying the symmetries $\alpha \beta$ and $\alpha \tau \alpha \tau$ we may tile the plane with copies of this rectangle. Finally, any element of $G$ which fixes $D$ setwise must be the identity. Note also that $\alpha, \beta$, and $\tau$ satisfy the relations:

$$
\begin{gathered}
\alpha^{2}=\beta^{2}=\tau^{2}=1 \\
{[\alpha \beta, \tau]=1}
\end{gathered}
$$

the second relation holds because the line of reflection of $\tau$ is parallel to the translation given by $\alpha \beta$.

We will show that the abstract group $G^{\prime}$ with presentation

$$
G^{\prime}=<\alpha, \beta, \tau \mid \alpha^{2}=\beta^{2}=\tau^{2}=1,[\alpha \beta, \tau]=1>
$$

is isomorphic to $G$. We first prove that $G^{\prime}$ surjects onto $G$ via the natural map by finding a single preimage for every element of $G$.

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-070.jpg?height=867&width=564&top_left_y=987&top_left_x=778)

Claim 2: The set of elements $N=\left\{(\alpha \beta)^{p}(\alpha \tau)^{2 q} Z\right\}$ in $G^{\prime}$, where $p$ and $q$ are integers and where $Z=1, \alpha, \alpha \tau$, or $\alpha \tau \alpha$, injects and surjects onto the elements of $G$.

Proof: Surjectivity follows from Claim 1. Injectivity is also clear as there is a bijection between $N$ and the triangles of the network. We will refer elements of $N$ as being in normal form.

Claim 3: Any word in $\alpha, \beta$, and $\tau$ can be put in normal form using the relations of $G^{\prime}$.

Proof: To do this we first apply the recursive algorithm below. Note that the relation $[\alpha \beta, \tau]=1$ can be rewritten to obtain $\tau \beta=\beta \alpha \tau \alpha$ and $\tau \alpha \beta=\alpha \beta \tau$. We are given a word of the form $(\alpha \beta)^{r} W(\alpha, \beta, \tau)$ : Step 1: Move all $\beta$ 's in $W$ to the left:

$$
\begin{aligned}
&\beta \beta=1 \\
&\tau \beta=\beta \alpha \tau \alpha \\
&\alpha \beta-\text { use step } 2 .
\end{aligned}
$$

Step 2: Move all $\alpha \beta$ 's in the new word to the left using:

$\alpha \alpha \beta$ - delete the $\alpha \alpha$ and go back to step 1 .

$$
\tau \alpha \beta=\alpha \beta \tau
$$

$\beta \alpha \beta-$ go back to step one and move the $\beta$ which is on the left.

This procedure can be shown to terminate by applying induction to the correct lexicographic order. Thus we have shown that any word can be put in the form $(\alpha \beta)^{p} W(\alpha, \tau)$, where $W$ is a word in $\alpha$ and $\tau$ only. But, since $\alpha^{2}=\tau^{2}=1$, we may assume that $W(\alpha, \tau)=(\alpha \tau)^{r} Q$, where $Q=1, \alpha$, or $\tau$. It is straightforward to place $W$ in the desired normal form by considering the possible values of $r$ and $Q$. It follows that $G^{\prime}$ is isomorphic to $G$. This group is isomorphic to the pmg group of [CM57, p. 45].

Solution to 6.1.6: If $a, b \in G$, then $a>0$ and $b \neq 1$, so $a^{\log b} \in G$. Therefore, the operation $*$ is well defined.

Identity. The constant $e$ is the identity since $a * e=a^{10 g e}=a^{1}=a$ and $e * a=e^{\log a}=a$.

Associativity. We have

$$
(a * b) * c=c^{\log \left(e^{\log a \log b}\right)}=e^{\log a \log b \log c}=a^{\log \left(e^{\log b \log c}\right)}=a *(b * c) .
$$

Invertibility. Since $a \neq 1, e^{1 / \log a}$ exists and is an element of $G$. A calculation shows that $a * e^{1 / \log a}=e^{1 / \log a} * a=e$.

Solution 2. The map log : $G \rightarrow \mathbb{R} \backslash\{0\}$ is a bijection that transforms the operation $*$ into multiplication; that is, $\log (a * b)=(\log a)(\log b)$. Since $\mathbb{R} \backslash\{0\}$ forms a group with respect to multiplication, $G$ is a group with respect to $*$.

Solution to 6.1.7: 1. Let $a \in G$. The centralizer $C_{a}=\left\{x \in G \mid x a x^{-1}=a\right\}$ of $a$ is a subgroup of $G$.

We have,

$$
x a x^{-1}=y a y^{-1} \Leftrightarrow y^{-1} x a=a y^{-1} x \Leftrightarrow y^{-1} x \in C_{a} \Leftrightarrow x C_{a}=y C_{a},
$$

which shows that two conjugates $x a x^{-1}, y a y^{-1}$ of $a$ are equal if and only if the left cosets $x C_{a}, y C_{a}$ are equal. Therefore the number of distinct conjugates of $a$ is equal to the number of distinct cosets of $C_{a}$ in $G$. Thus $|C(a)|$ divides $|G|$.

2. Each element $\sigma \in S_{n}$ can be written as a product of disjoint cycles $\sigma=\sigma_{1} \ldots \sigma_{k}$; this representation is unique apart from the ordering of the cycles. Include the cycles of length 1 , and denote the length of $\sigma_{i}$ by $r_{i}$, which we assume are in increasing order. The cycle pattern of $\sigma$ is then the sequence $\left\{r_{1}, \ldots, r_{k}\right\}$. Thus the elements of

$$
S_{3}=\left\{(1)(2)(3),(1)(23),(2)\left(\begin{array}{ll}
1 & 3
\end{array}\right),(3)(12),(123),\left(\begin{array}{lll}
1 & 3 & 2
\end{array}\right)\right\}
$$

have cycle patterns $\{1,1,1\},\{1,2\},\{1,2\},\{1,2\},\{3\},\{3\}$. In $S_{n}$ two permutations are conjugate if and only if they have the same cycle pattern. The conjugacy classes in $S_{3}$ are therefore the classes $\{e\},\{(12),(13),(23)\}$ and $\left\{\left(\begin{array}{ll}1 & 23),(132)\} \text {. }\end{array}\right.\right.$ This shows that conjugacy classes do not all have the same number of elements. 3. Denote the order of $G$ by $n$. The conjugacy class of the identity has one element, namely the identity. The second conjugacy class then has $n-1$ elements, and thus $n-1$ divides $n$. We deduce that $n=2$.

Solution to 6.1.8: Fix an element $a \in G$ different from the identity, and consider the map $\varphi: G \backslash\{e\} \rightarrow G \backslash\{e\}$ defined by $\varphi(c)=c^{-1} a c$. The map is onto, so, since $G$ is finite, it is one-to-one. As $\varphi(a)=a$ and $a^{-2} a a^{2}=a$, it must be that $a^{2}=e$. Thus, all elements of $G$, other than the identity, have order 2 . Then, if $a$ and $b$ are in $G$, we have

$$
a b=a(a b)^{2} b=a^{2}(b a) b^{2}=b a
$$

in other words, $G$ is commutative, and it follows that $G$ has order 2.

Solution 2. Since $G$ is finite, it has an element of prime order $p$. Hence, every element of $G$, other than the identity, has order $p$. Since $G$ is a $p$-group, it has a nontrivial central element. Therefore, all elements are central; in other words, $G$ is abelian. Hence, $G$ has order 2.

Solution 3. By our hypothesis, any two nonidentity elements are conjugate. Hence, there are two conjugacy classes: The class containing the identity and the class containing all the other elements of $G$. Letting $n$ be the order of $G$, we see that the second conjugacy class must contain $n-1$ elements. But, by the class equation, we know that the order of any conjugacy class divides the order of the group, so $(n-1) \mid n$. Solving for $n>0$, we see that the only possible solution is $n=2$.

Solution to 6.1.9: Since $G$ is finite, every element of $G$ has finite order. Since any two elements of $G \backslash\{e\}$ are related by an automorphism of $G$, all such elements have the same order, say $q$. Since all powers of an element of $G \backslash\{e\}$ have order $q$ or 1, $q$ is prime. By Sylow's Theorems [Her75, p. 91], the order of $G$ is a power of $q$. Therefore the center $Z$ of $G$ contains an element other than $e$. Since the center of $G$ is invariant under all automorphisms, $Z$ is the whole of $G$, i.e. $G$ is abelian.

Solution to 6.1.11: Consider the group $G^{\prime}=\left\{a^{n} b^{m} \mid 0 \leqslant n \leqslant r-1,0 \leqslant\right.$ $m \leqslant s-1\}$. As the order of any element of $G^{\prime}$ divides the order of $G^{\prime}$, we have $\left|G^{\prime}\right|=r s$. This shows that $(a b)^{k}$ is never the identity for $0<k<r s$. Clearly we have $(a b)^{r s}=a^{r} b^{s}=e$, so the order of $a b$ is $r s$.

Solution to 6.1.12: 1. Suppose $X \subset Y$. If $z \in C(Y)$ then $z y=y z$ for all $y \in Y$, and thus $z x=x z$ for all $x \in X$, so $z \in C(X)$. 2. Let $x \in X$. If $z \in C(X)$ then $z x=x z$; thus $x$ commutes with each member of $C(X)$, that is, $x \in C(C(X))$.

3. Replace $X$ by $C(X)$ in (2): we get that $C(X) \subset C(C(C(X))$ ). Further, using (2) in (1), we conclude that $C(C(C(X))) \subset C(X)$.

Solution to 6.1.13: Let $g, h \in H$ and let $x \in D \backslash H$. Then

$$
h^{-1} g^{-1}=x h x^{-1} x g x^{-1}=x h g x^{-1}=(h g)^{-1}=g^{-1} h^{-1}
$$

and we can conclude immediately from this that $H$ is abelian.

Let $x \in D \backslash H$. We have $[\mathrm{D}: \mathrm{H}]=2$, so $x^{2} \in H$. By hypothesis, $x x^{2} x^{-1}=x^{-2}$ or $x^{4}=1$. Therefore, $x$ has order 1,2 , or 4 . But $n$ is odd, so 4 does not divide the order of $D$, so, by Lagrange's Theorem [Her75, p. 41], $x$ cannot have order 4. By our choice of $x, x \neq 1$, so $x$ cannot have order 1 . Hence, $x$ has order 2.

\subsection{Homomorphisms and Subgroups}

Solution to 6.2.1: Let $f: \mathbb{Z}_{2} \times \mathbb{Z}_{2} \rightarrow S_{3}$ be a homomorphism. Then $\left(\mathbb{Z}_{2} \times \mathbb{Z}_{2}\right) / \operatorname{ker} f \simeq \operatorname{im} f$. Denote $|\operatorname{ker} f|$ by $k$, and $|\operatorname{im} f|$ by $l$. Then $k=1,2$ or 4 , and $l=1,2,3$ or 6 . Therefore $4 / k=l$. The only solutions are $k=4, l=1$ and $k=2, l=2$. The first case is realized by the map which sends each element of $\mathbb{Z}_{2} \times \mathbb{Z}_{2}$ to the identity of $S_{3}$. Now

$$
\begin{aligned}
\mathbb{Z}_{2} \times \mathbb{Z}_{2} &=\{(0,0),(0,1),(1,0),(1,1)\}, \\
S_{3} &=\{e,(12),(13),(23),(123),(132)\} .
\end{aligned}
$$

Each element of $\mathbb{Z}_{2} \times \mathbb{Z}_{2}$, not the identity, has order 2 ; denote these by $z_{1}, z_{2}, z_{3}$. The cycles in $S_{3}$ of length 2 have order 2 ; denote these by $y_{1}, y_{2}, y_{3}$. We therefore consider the map $f$ given by

$$
f(0,0)=f\left(z_{1}\right)=e, \quad f\left(z_{2}\right)=f\left(z_{3}\right)=y_{1} .
$$

It is routine to verify that $f$ is a homomorphism. By 'varying' the $z s$ and the $y s$ we find nine such homomorphisms. Thus there are ten homomorphisms from $\mathbb{Z}_{2} \times \mathbb{Z}_{2}$ to $S_{3}$.

Solution to 6.2.2: 1. Let $C_{g}$ denote the conjugacy class of $g$ and $Z_{g}$ the centralizer of $g$. Using the Orbit Stabilizer Theorem [Fra99, Thm. 16.16], [Lan94, Prop. I.5.1], $\left|Z_{g}\right| \cdot\left|C_{g}\right|=|G|$ for every element $g$. Hence $\sum_{g \in C}\left|Z_{g}\right|=|G|$ for every conjugacy class $C$, and $|X|=\sum_{g \in G}\left|Z_{g}\right|=c|G|$.

2. In this case $G=S_{5}$, so $|G|=5 !=120$. The number of conjugacy classes is the number of partitions of 5 , namely 7 . So there are $7 \cdot 120=840$ pairs of commuting permutations.

Solution to 6.2.3: A matrix $A$ is a solution of $x^{3}=x^{2}$, or equivalently $x^{2}(x-1)=$ 0 , if and only if all its Jordan blocks are solution to the same equation. So each Jordan block must have eigenvalues 0 and 1 , and the possibilities are (0), (1), $\left(\begin{array}{ll}0 & 1 \\ 0 & 0\end{array}\right)$

The conjugacy type of a matrix is determined by the multiplicity of the Jordan blocks. Let $a, b, c$ be the multiplicities of the blocks above, respectively. Then the answer is the number of nonnegative integer solutions of $a+b+2 c=5$. For fixed $c \in\{0,1,2\}$, there are $6-2 c$ solutions to $a+b=5-2 c$. Thus the answer is

$$
(6-2 \cdot 0)+(6-2 \cdot 1)+(6-2 \cdot 2)=12
$$

Solution to 6.2.4: Let $n=\left[\mathbb{C}^{*}: H\right]$. By Lagrange's Theorem [Her75, p. 41], the order of any element of $\mathbb{C}^{*} / H$ divides $n$. So $x^{n} \in H$ for all $x \in \mathbb{C}^{*}$. Therefore, $\left(\mathbb{C}^{*}\right)^{n}=\mathbb{C} \subset H$.

Solution to 6.2.5: 1. The number of conjugates of $H$ in $G$ is $|G: N(H ; G)|$ where $N(H ; G)=\left\{g \in G \mid g^{-1} H g=H\right\}$ is the normalizer of $H$. As $H \subset N(H ; G)$, we have $|G: H| \geqslant|G: N(H ; G)|$.

2. By Problem 6.4.19, there is a normal subgroup of $G, N$, contained in $H$, such that $G / N$ is finite. By Part 1 we can find a coset $N g \in G / N$ such that $N g$ is not contained in any conjugate $y^{-1} H y / N$ of $H / N$ in $G / N$. Then $g \notin y^{-1} H y$ for any $y \in G$.

Solution to 6.2.6: We will prove a more general result. Let $G=\langle g\rangle,|G|=n$, and $\alpha \in$ Aut $G$. As $\alpha(g)$ also generates $G$, we have $\alpha(g)=g^{k}$ for some $1 \leqslant k<n$, $(k, n)=1$. Conversely, $x \mapsto x^{k}$ is an automorphism of $G$ for $(k, 1)=1$. Let $\mathbb{Z}_{n}$ be the multiplicative group of residue classes modulo $n$ relatively prime to $n$. If $\bar{k}$ denotes the residue class containing $k$, we can define $\Phi:$ Aut $G \rightarrow \mathbb{Z}_{n}$ by

$$
\Phi(\alpha)=\bar{k} \quad \text { iff } \quad \alpha(g)=g^{k} .
$$

It is clear that $\Phi$ is an isomorphism. As $\mathbb{Z}_{n}$ is an abelian group of order $\varphi(n)$ ( $\varphi$ is Euler's totient function [Sta89, p. 77], [Her75, p. 43]), so is Aut $G$. When $n$ is prime, these groups are also cyclic.

Solution to 6.2.7: 1. If $g^{-1} \varphi(g)=h^{-1} \varphi(h)$ for some $g, h \in G$, then $\varphi(g) \varphi(h)^{-1}=g h^{-1}$, so, by hypothesis, $g h^{-1}=1$ and $g=h$. Thus, there are $|G|$ elements of that form, so they must constitute all of $G$.

2. Using Part 1, we have, for $z=g^{-1} \varphi(g) \in G$,

$$
\varphi(z)=\varphi\left(g^{-1}\right) \varphi^{2}(g)=\varphi\left(g^{-1}\right) g=z^{-1}
$$

so $g \mapsto g^{-1}$ is an automorphism of $G$, which implies that $G$ is abelian. For any $z \in G, z \neq 1$, we have $z^{-1}=\varphi(z) \neq z$, so $G$ has no element of order 2 , and $|G|$ is odd.

Solution to 6.2.8: Let $G$ be the group. If $G$ is not abelian and $a$ is an element not in the center, then the map $x \mapsto a^{-1} x a$ is the desired automorphism. If $G$ is cyclic, say of order $m$, and $n$ is an integer larger than 1 and relatively prime to $m$, then the map $x \mapsto x^{n}$ is the desired automorphism. If $G$ is any finite abelian group, then, by the Structure Theorem [Her75, p. 109], it is a direct product of cyclic groups. If one of the factors has order at least 3 , we get the desired automorphism by using the preceding one in that factor and the identity in the other factors. If every factor has order 2 , we get the desired automorphism by permuting any two of the factors.

Solution to 6.2.9: We show that $u$ is an isomorphism from $\operatorname{ker} f$ onto ker $g$. Let $y \in \operatorname{ker} f$. We have

$$
g(u(y))=u(y)-u(v(u(y)))=u(y-v(u(y)))=u(f(y))=u(0)=0 .
$$

Hence, $u$ maps $\operatorname{ker} f$ into $\operatorname{ker} g$. Let $y \in \operatorname{ker} f$ with $u(y)=0$; then

$$
0=f(y)=y-v(u(y))=y .
$$

Thus, $u$ is injective. Let $x \in \operatorname{ker} g$. Then $0=g(x)=x-u(v(x))$, so $u(v(x))=x$. Therefore, $u$ is onto if $v(x) \in \operatorname{ker} f$. However, an argument identical to the first one shows that this is the case, so we are done.

Solution to 6.2.10: Given an element $\beta \in G$, by the surjectivity of $h$, there is an element $\gamma \in G$ such that

$$
\gamma-u(v(\gamma))=u(\beta)
$$

now build $\alpha=\beta+v(\gamma)$, for this element

$$
\begin{aligned}
f(\alpha) &=\beta+v(\gamma)-v(u(\beta+v(\gamma))) \\
&=\beta+v(\gamma)-v(u(\beta)+u(v(\gamma))) \\
&=\beta+v(\gamma)-v(\gamma) \\
&=\beta
\end{aligned}
$$

showing that $f$ is surjective.

Solution to 6.2.12: 1. Suppose this is not the case, then there exists $h_{1} \in H_{1} \backslash H_{2}$ and $h_{2} \in H_{2} \backslash H_{1}$. Since they both belong to the group $H_{1} \cup H_{2}$, the element $h_{1} h_{2} \in H_{1} \cup H_{2}$. In both cases, if $h_{1} h_{2} \in H_{1}$ we get a contradiction with $h_{2}=h_{1}^{-1}\left(h_{1} h_{2}\right) \in H_{1}$ or if $h_{1} h_{2} \in H_{2}$ we get the contradiction with $h_{1}=\left(h_{1} h_{2}\right) h_{2}^{-1} \in H_{2}$.

2. Consider the product group $G=\mathbb{Z}_{2}^{n-1}$, and the subgroups

$$
H_{i}=\left\{\left(x_{1}, \ldots, x_{n-1}\right) \in G \mid x_{i}=0\right\} .
$$

Then $H_{1} \cup \cdots \cup H_{n-1}=G \backslash\{(1,1, \ldots, 1)\}$. Let $H_{n}=\left\{\left(x_{1}, \ldots, x_{n-1}\right) \in G \mid x_{1}+\right.$ $\left.x_{2}=0\right\}$. Then $(1,1, \ldots, 1) \in H_{n}$, so $H_{1} \cup H_{2} \cdots H_{n}=G$, no $H_{i}$ is contained in any other, since they are distinct groups of the same order.

Solution to 6.2.13: 1. The statement is false. Let $G=S_{3}=\langle a, b| a^{3}=b^{2}=$ $\left.1, b a=a^{2} b\right\rangle$ and $H=\{1, b\}$, so that $|G: H|=3$. Let $x=a b \in G$. Then $x^{3}=a b \notin H$. 2. The statement is true. The $n+1$ cosets $H, H a, H a^{2}, \ldots, H a^{n}$ are not all distinct. There are integers $0 \leqslant i<j \leqslant n$ such that $H a^{i}=H a^{j}$. This means that $a^{j-i} \in H$, proving the result.

Solution to 6.2.14: For each nonzero rational $s$, the map $f: \mathbb{Q} \rightarrow \mathbb{Q}$ given by $x \mapsto s x$ is a bijection and satisfies $f(x+y)=f(x)+f(y)$. Thus $f$ is an automorphism of $\mathbb{Q}$.

Conversely, let $f$ be an automorphism of $\mathbb{Q}$. Then $f(0)=0$, and $s=f(1)$ is a nonzero rational. The inductive step $f(n+1)=f(n)+f(1)=n s+s$ proves that $f(n)=n s$ for $n \geqslant 1$. Further as $0=f(n+(-n))=f(n)+f(-n)$ we see that $f(-n)=-f(n)=-n s$ for $n \geqslant 1$. Thus $f(n)=n s$ for $n \in \mathbb{Z}$. Now let $x=m / n$ where $n>0$ and $m \in \mathbb{Z}$. Then $m s=f(m)=f(n x)=n f(x)$. Thus $f(x)=s x$.

Solution to 6.2.15: If the origin were a limit point of $\Gamma$ then $\Gamma$ would be dense in $\mathbb{R}$, since $\Gamma$ contains all integer multiples of its elements. But then $\Gamma$ would equal $\mathbb{R}$, since it is closed, which is impossible because $\Gamma$ is countable as a set $\Gamma=\{m \alpha+n \beta \mid m, n \in \mathbb{Z}\}$. Hence the origin is not a limit point of $\Gamma$.

Therefore $\Gamma$ contains a smallest positive number $\gamma$. If $x$ is in $\Gamma$ and $n$ is the largest integer such that $n \gamma \leqslant x$, then $x-n \gamma$ is in $\Gamma$, and $0 \leqslant x-n \gamma<\gamma$, implying that $x-n \gamma=0$. Therefore $\Gamma=\gamma \mathbb{Z}$, from which the desired conclusion is immediate.

Solution to 6.2.16: Suppose $H$ is a proper subgroup of $G$ of finite index. Let $N=G / H$, so that $N$ is a finite non trivial abelian group. Let $p$ be a prime dividing the order of $N$, and $n \in N$ an element of order $p$. Suppose $n$ is the image of the class of $a / b$ in $G$. Let $n_{k}$ be the image of the class of $a / p^{k} b$. Then $n_{k}$ has order $p^{k+1}$, because $p^{k} n_{k}=n$, so $p^{k+1} n_{k}=0$ and $d n_{k}$ does not equal zero for any proper divisor $d$ of $p^{k+1}$. This is not possible since every element of $N$ has order dividing $|N|$.

Solution to 6.2.17: The only homomorphism is the trivial one. Suppose $\varphi$ is a nontrivial homomorphism. Then $\varphi(a)=m \neq 1$ for some $a, m \in \mathbb{Q}$. We have

$$
a=\frac{a}{2}+\frac{a}{2}=\frac{a}{3}+\frac{a}{3}+\frac{a}{3}+\cdots
$$

but $m$ is not the $n^{\text {ih }}$ power of a rational number for every positive $n$. For example $3 / 5=1 / 5+1 / 5+1 / 5$ but $\sqrt[3]{\frac{3}{5}} \notin \mathbb{Q}^{+}$

Solution to 6.2.18: For $i=1,2$ let $S_{i}$ denote the set of right cosets of $H_{i}, G / H_{i}$. Then $G$ acts on $S_{i}$ on the left. Hence $G$ acts on $S_{1} \times S_{2}$, and the stabilizer of the trivial pair $\left(H_{1}, H_{2}\right)$ of cosets is $H_{1} \cap H_{2}$ Hence $\left[G: H_{1} \cap H_{2}\right]$ is the size of the $G$-orbit of $\left(H_{1}, H_{2}\right) \in S_{1} \times S_{2}$, so $\left[G: H_{1} \cap H_{2}\right] \leqslant 9$. On the other hand, $H_{1}$ and $\mathrm{H}_{2}$ are distinct subgroups of the same index, so $H_{1}$ is not contained in $H_{2}$, and $\left[G: H_{1} \cap H_{2}\right]$ is a proper multiple of $\left[G: H_{1}\right]=3$. Thus it equals 6 or 9 . Both are possible. The index is 6 if $H_{1}=\{1,(12)\}$ and $H_{2}=\{1,(13)\}$ in the symmetric group $G=S_{3}$. The index is 9 if $H_{1}$ and $H_{2}$ are distinct subgroups of order 3 in $G=\mathbb{Z} / 3 \times \mathbb{Z} / 3$.

Solution to 6.2.19: By assumption, $H$ has only finitely many right cosets, say $H, x_{1} H, \ldots, x_{n} H$, whose union is $G$. Hence, $K$ is the union of the sets $K \cap H, K \cap x_{1} H, \ldots, K \cap x_{n} H$, some of which may be empty, say $K=(K \cap H) \cup\left(K \cap x_{1} H\right) \cup \cdots \cup\left(K \cap x_{m} H\right)$ (the notation being so chosen that $K \cap x_{j} H=\emptyset$ if and only if $j>m$ ). If $K \cap x_{j} H \neq \emptyset$, then we may assume $x_{j}$ is in $K$ (since $y$ is in $x H$ if and only if $y H=x H$ ). After making this assumption, we have $K=x_{j} K$, so that $K \cap x_{j} H=x_{j} K \cap x_{j} H=x_{j}(K \cap H)$, whence

$$
K=(K \cap H) \cup x_{1}(K \cap H) \cup \cdots \cup x_{m}(K \cap H) .
$$

This shows $K \cap H$ has only finitely many right cosets in $K$, the desired conclusion.

Solution to 6.2.20: For each $g \in G$, let $S_{g}=\{a \in G \mid a g=g a\}$. Each $S_{g}$ is a nontrivial subgroup of $G$, because $g \in S_{g}$ and $S_{e}=G$. The intersection of all $S_{g}$, $g \in G$, is the center of $G$. So $H$ is a subset of the center of $G$.

Solution to 6.2.21: Let $N=6^{k}$. Let $G_{i}=C_{6}^{i} \times S_{3}^{k-i}$ for $0 \leqslant i \leqslant k$. These groups are not isomorphic because the center of $G_{i}$ is $C_{6}^{i}$.

Solution 2. Let $N=2^{2 k}$. Let $G_{i}=C_{2^{i}} \times C_{2^{2 k-i}}$ for $0 \leqslant i \leqslant k$. These groups are not isomorphic because the exponent of $C_{i}$ is $2^{2 k-i}$.

Solution to 6.2.22: Since $G$ is finitely generated, $\operatorname{Hom}\left(G, S_{k}\right)$ is finite (bounded by $\left.(k !)^{n}\right)$, where $S_{k}$ denotes the symmetric group on $k$ numbers $1,2, \ldots, k$. For any subgroup $H$ of index $k$ in $G$, we can identify $G / H$ with this set of symbols, sending the coset $H$ to 1 . Then the left action of $G$ on $G / H$ determines an element of $\operatorname{Hom}\left(G, S_{k}\right)$ such that $H$ is the stabilizer of 1 . Thus, the number of such $H$ 's is, at most, $(k !)^{n}$.

\subsection{Cyclic Groups}

Solution to 6.3.1: 1. Let $G$ be the subgroup of $\mathbb{Q}$ generated by the nonzero numbers $a_{1}, \ldots, a_{r}$, and let $q$ be a common multiple of the denominators of $a_{1}, \ldots, a_{r}$. Then each $a_{i}$ has the form $p_{j} / q$ with $p_{j} \in \mathbb{Z}$, and, accordingly, $G=\frac{1}{q} G_{0}$, where $G_{0}$ is the subgroup of $\mathbb{Z}$ generated by $p_{1}, \ldots, p_{r}$. Since all subgroups of $\mathbb{Z}$ are cyclic, it follows that $G$ is cyclic, that is, $G=\frac{p}{q} \mathbb{Z}$, where $p$ is a generator of $G_{0}$

2. Let $\pi: \mathbb{Q} \rightarrow \mathbb{Q} / \mathbb{Z}$ be the quotient map. Suppose $G$ is a finitely generated subgroup of $\mathbb{Q} / \mathbb{Z}$, say with generators $b_{1}, \ldots, b_{r}$. Let $G_{1}$ be the subgroup of $\mathbb{Q}$ generated by $1, \pi^{-1}\left(b_{1}\right), \ldots, \pi^{-1}\left(b_{r}\right)$. Then $\pi\left(G_{1}\right)=G$, and $G_{1}$ is cyclic by Part 1. Hence, $G$ is cyclic. Solution to 6.3.2: The subgroup of $\mathbb{Q} / \mathbb{Z}$ generated by the coset of $1 / t$ is a cyclic subgroup of order $t, H_{0}$ say. Let $H$ be any cyclic subgroup of order $t$, and $r+\mathbb{Z}$ be one of its generators, where $0<r<1$. Then $t r=p$ is an integer, and $r=p / t$. This implies that the coset of $r$ is in $H_{0}$, hence that $H$ is a subset of $H_{0}$. Since $H$ and $H_{0}$ both have order $t$, we have $H=H_{0}$, as desired.

Solution to 6.3.3: Suppose $G=\left\{0, g_{1}, g_{2}, \ldots\right\}$ is countable, and, for each $n \in \mathbb{N},\left\{g_{1}, \ldots, g_{n}\right\}$ is contained in an infinite cyclic subgroup of $G$. Then the subgroup generated by $\left\{g_{1}, \ldots, g_{n}\right\}, H_{n}$, is infinite cyclic, and $H_{n} \subset H_{n+1}$. Let $H_{n}$ be generated by $h_{n}$. We have $h_{n}=h_{n+1}^{q_{n}}$ for some nonzero $q_{n} \in \mathbb{Z}$. Let $r_{n}=q_{1} q_{2} \cdots q_{n-1}$, and let $\varphi_{n}: H_{n} \rightarrow \mathbb{Q}$ be the morphism verifying $\varphi_{n}\left(h_{n}\right)=1 / r_{n}$. Then we have

$$
\varphi_{n+1}\left(h_{n}\right)=\varphi_{n+1}\left(h_{n+1}^{q_{n}}\right)=\frac{q_{n}}{r_{n+1}}=\frac{1}{r_{n}}=\varphi_{n}\left(h_{n}\right) .
$$

Since $h_{n}$ generates $H_{n}, \varphi_{n}=\left.\varphi_{n+1}\right|_{H_{n}}$. Thus there is a unique homomorphism $\varphi: G \rightarrow \mathbb{Q}$ such that $\varphi_{n}=\left.\varphi\right|_{H_{n}}$ for all $n \in \mathbb{N}$. As each $\varphi_{n}$ is injective and the subgroups $H_{n}$ cover $G, \varphi$ is an isomorphism from $G$ onto a subgroup of $\mathbb{Q}$. The converse is clear.

Solution to 6.3.4: 1. Let $G=\langle c\rangle$. We have $a=c^{r}$ and $b=c^{s}$ for some positive odd integers $r$ and $s$, so $a b=c^{r+s}$ with $r+s$ even, and $a b$ is a square.

2. Let $G=\mathbb{Q}^{*}$, the multiplicative group of the rational numbers, $a=2$, and $b=3$. Then $a b=6$, and none of these is a square in $G$.

Solution to 6.3.6: Let $e$ be the identity in $G$, and let $N=\{e, a\}$ be the normal subgroup of order 2 . If $x \in G$, then $x^{-1} a x \in N$ and certainly does not equal $e$, so it equals $a$. Thus, $x a=a x$ for all $x \in G$. The quotient group $G / N$ has order $p$ and so is cyclic. Let $x$ be any element not in $N$. Then the coset of $x$ in $G / N$ has order $p$, so, in particular, the order of $x$ itself is not 2. But the order of $x$ divides $2 p$, so it must be $p$ or $2 p$. In the latter case, $G$ is the cyclic group generated by $x$. In the former case, since $x a=a x$, we have $(x a)^{p}=x^{p} a^{p}=a$, so $(x a)^{2 p}=a^{2}=e$, and $x a$ has order $2 p$, which means $G$ is the cyclic group generated by $x a$.

Solution to 6.3.8: It follows that every Sylow subgroup is normal. From this it follows that if $a$ and $b$ are in distinct Sylow subgroups $a$ and $b$ commute. Indeed $a b a^{-1} b^{-1}$ lies in the intersection of the two groups which is trivial. Hence, it suffices to establish the result when $G$ has order $p^{n}$ for a prime $p$. Let $a \in G$ be an element of largest order, say $p^{k}$. Let $H=(a)$. If $b \in G$ and $b$ has order $p^{i} \leqslant p^{k}$, then $b \in H$ because $|(b)|=p^{i}$ and $H$ contains the unique subgroup of $G$ of order $p^{i}$. Thus if $c \notin H$ then $c$ has order strictly bigger than $p^{k}$ which contradicts the maximality of the order of $a$. Thus $H=G$ and $G$ is cyclic. 

\subsection{Normality, Quotients, and Homomorphisms}

Solution to 6.4.1: 1. For each $x \in G$ the map $h \mapsto x h$ is a bijection between $H$ and the left coset $x H$. Thus, $|H|=|x H|$. Further, the left cosets of $H$ form a partition of $G$, arising from the equivalence relation on $G$ given by $x \sim y$ iff $x^{-1} y \in H$. Thus, the number of left cosets of $H$ in $G$ is given by $|G| /|H|$. The same holds for right cosets: here the partition of $G$ is given by the relation $x \sim y$ iff $x y^{-1} \in H$. This proves the result.

2. The group of symmetries of the square is the dihedral group $D_{4}$ of order 8 . Take the square with vertices $P_{4}=\{1, i,-1,-i\}$ in the complex plane. Let $r$ denote the rotation of the about the origin through the angle $\pi / 2$. Then $\left\{1, r, r^{2}, r^{3}\right\}$ is a normal subgroup of $D_{4}$. Let $s$ be the reflection in the real axis. The elements of $D_{4}$ are

$$
1, r, r^{2}, r^{3}, s, r s, r^{2} s, r^{3} s
$$

Further, $s^{2}=1$, and $r s$ is also a reflection, so $(r s)^{2}=1$ or $s r=r^{-1} s=r^{3} s$. We obtain the presentation

$$
D_{4}=\left\langle r, s \mid r^{4}=1=s^{2}, s r=r^{-1} s\right\rangle .
$$

Let $H=\{1, s\}$. Then $r H=\{r, r s\} \neq H r=\{r, s r\}$.

Solution to 6.4.3: The subgroup $H$ is normal only if $a H a^{-1}=H$ or $a H=H a$ for all $a \in G$. Since $H$ has only one left coset different from itself, it will suffice to show that this is true for a fixed $a$ which is a representative element of this coset. Since $H$ has the same number of right and left cosets, there exists a $b$ such that $H$ and $b H$ form a partition of $G$. Since cosets are either disjoint or equal and $H \cap a H=\emptyset$, we must have that $a H=H b$. But then $a \in H b$, so $H b=H a$.

Solution to 6.4.5: The number of Sylow 2-groups is odd, and divides 112, therefore it divides 7 , thus, it is 1 or 7 . If there is only one Sylow 2-group, it's normal, so we may assume otherwise.

Let $G$ act on the set of seven 2-groups by conjugation, so it maps into $S_{7}$. This is a nontrivial map, since the action is transitive. If it is not an injection, then the kernel is a nontrivial normal subgroup, so we may assume it is an injection.

Since 16 divides $|G|$ and does not divide $\left|A_{7}\right|$, the image of $G$ is not entirely inside $A_{7}$. Therefore the composite of $G \rightarrow S_{7} \rightarrow S_{7} / A_{7}=\mathbb{Z}_{2}$ is onto, and its kernel is a normal subgroup (of index 2).

Solution to 6.4.6: The groups of order $\leqslant 6$ are $\{1\}, \mathbb{Z}_{2}, \mathbb{Z}_{3}, \mathbb{Z}_{4}, \mathbb{Z}_{2} \times \mathbb{Z}_{2}, \mathbb{Z}_{5}$, $\mathbb{Z}_{6}$, and the symmetric group $S_{3}$. Suppose that $G$ is a group with the mentioned property. Let $S$ be a Sylow-2 subgroup of $G$. Some Sylow-2 subgroup of $G$ must contain (a subgroup isomorphic to) $\mathbb{Z}_{4}$, and all Sylow 2-subgroups are conjugate, so $S$ contains $\mathbb{Z}_{4}$. Similarly $S$ contains $\mathbb{Z}_{2} \times \mathbb{Z}_{2}$. Thus $S$ cannot equal either, so 8 divides the order of $S$, which divides the order of $G$. On the other hand, $G$ has subgroups of orders 3 and 5 , so 3 and 5 also divide $|G|$. Since $8,3,5$ are pairwise relatively prime, their product 120 divides $|G|$. Thus $|G| \geqslant 120$. On the other hand, the group $G=\mathbb{Z}_{4} \times \mathbb{Z}_{5} \times S_{3}$ of order 120 has the property: $\mathbb{Z}_{2}$ is isomorphic to a subgroup of $\mathbb{Z}_{4}$, and $\mathbb{Z}_{2}$ and $\mathbb{Z}_{3}$ are isomorphic to subgroups of $S_{3}$, so $\mathbb{Z}_{2} \times \mathbb{Z}_{2}$ and $\mathbb{Z}_{6} \cong \mathbb{Z}_{2} \times \mathbb{Z}_{3}$ are isomorphic to subgroups of $G$. The other desired inclusions are obvious.

Solution to 6.4.7: Denote by $h_{i} H$ (and $k_{i} K$ ) a coset of $H$ (and $K$ ) and suppose

$$
G=h_{1} H \cup \cdots \cup h_{r} H \cup k_{1} K \cup \cdots \cup k_{s} K .
$$

Since all of the cosets of $K$ are equal or disjoint and since the index of $K$ in $G$ is infinite, there is a $k \in K$ such that

$$
k K \subset h_{1} H \cup \cdots \cup h_{r} H .
$$

Therefore, for $1 \leqslant i \leqslant s$,

$$
k_{i} K \subset k_{i} k^{-1} h_{1} H \cup \cdots \cup k_{i} k^{-1} h_{r} H .
$$

This implies that $G$ can be written as the union of a finite number of cosets of $H$, contradicting the fact that the index of $H$ in $G$ is infinite. Hence, $G$ cannot be written as the finite union of cosets of $H$ and $K$.

Solution to 6.4.8: 1. First, note that the multiplication rule in $G$ reads

$$
\left(\begin{array}{cc}
a & b \\
0 & a^{-1}
\end{array}\right)\left(\begin{array}{cc}
a_{1} & b_{1} \\
0 & a_{1}^{-1}
\end{array}\right)=\left(\begin{array}{cc}
a a_{1} & a b_{1}+b a_{1}^{-1} \\
0 & a^{-1} a_{1}^{-1}
\end{array}\right)
$$

which gives $\left(\begin{array}{cc}a & b \\ 0 & a^{-1}\end{array}\right)^{-1}=\left(\begin{array}{cc}a^{-1} & -b \\ 0 & a\end{array}\right)$. This makes it clear that $N$ is a subgroup, and if $\left(\begin{array}{ll}1 & \beta \\ 0 & 1\end{array}\right)$ is in $N$, then

$$
\begin{aligned}
\left(\begin{array}{cc}
a & b \\
0 & a^{-1}
\end{array}\right)^{-1}\left(\begin{array}{ll}
1 & \beta \\
0 & 1
\end{array}\right)\left(\begin{array}{cc}
a & b \\
0 & a^{-1}
\end{array}\right) &=\left(\begin{array}{cc}
a^{-1} & -b \\
0 & a
\end{array}\right)\left(\begin{array}{cc}
a & b+\beta a^{-1} \\
0 & a^{-1}
\end{array}\right) \\
&=\left(\begin{array}{cc}
1 & \beta a^{-2} \\
0 & 1
\end{array}\right) \in N,
\end{aligned}
$$

proving that $N$ is normal.

By the first equation, the map from $G$ onto $\mathbb{R}_{+}$(the group of positive reals under multiplication) given by $\left(\begin{array}{cc}a & b \\ 0 & a^{-1}\end{array}\right) \mapsto a$ is a homomorphism whose kernel is $N$ (which by itself proves that $N$ is a normal subgroup). Hence, $G / N$ is isomorphic to $\mathbb{R}_{+}$, which is isomorphic to the additive group $\mathbb{R}$.

2. To obtain the desired normal subgroup majorizing $N$, we can take the inverse image under the homomorphism above of any nontrivial proper subgroup of $\mathbb{R}_{+}$. If we take the inverse image of $\mathbb{Q}_{+}$, the group of positive rationals, we get the proper normal subgroup

$$
N^{\prime}=\left\{\left(\begin{array}{cc}
a & b \\
0 & a^{-1}
\end{array}\right) \mid a \in \mathbb{Q}+\right\}
$$



\section{of $G$, which contains $N$ properly.}

Solution to 6.4.9: $H_{1}=\{m(1,2)+n(4,1) \mid m, n \in \mathbb{Z}\}$. By inspection, we see that a transversal for the quotient group $G_{1}$, a set containing one representative of each coset, consists of the lattice points inside the parallelogram with edges connecting $(0,0),(1,2),(5,3)$ and the origin. A glance at the figure shows that

$$
G_{1}=\{(0,0),(1,1),(2,1),(3,1),(2,2),(3,2),(4,2)\}
$$

where we have abbreviated $(a, b)=H_{1}+(a, b)$. Thus $G_{1}$ is an abelian group of order 7 , isomorphic to the cyclic group $\mathbb{Z}_{7}$. In the same way we see that

$$
G_{2}=\{(0,0),(1,1),(1,2),(2,2),(2,3),(3,3),(3,4)\} \text {, }
$$

so that $G_{2}$ is isomorphic to the cyclic group $\mathbb{Z}_{7}$. Thus $G_{1} \simeq G_{2}$.

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-081.jpg?height=450&width=447&top_left_y=1041&top_left_x=530)

$G_{1}$

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-081.jpg?height=448&width=442&top_left_y=1039&top_left_x=1145)

$G_{2}$

Solution to 6.4.10: Let $H=\{1, b\}$ be a normal subgroup of order 2. The group $G$ contains an element of order 10 , or $G$ contains an element of order 5 , or all elements (not 1) of $G$ have order 2. If $G$ has an element of order 10 , or if $x^{2}=1$ for all $x \in G$, then $G$ is abelian. When $G$ has an element of order $5, a$, let $K=$ $\left\{1, a, a^{2}, a^{3}, a^{4}\right\}$. Since $|H|,|K|$ are coprime, we have $H \cap K=1$. Thus $b \notin K$ and $G=K \cup K b=\left\{1, a, a^{2}, a^{3}, a^{4}, b, a b, a^{2} b, a^{3} b, a^{4} b\right\}$. Now $\{a, a b\}=$ $a H=H a=\{a, b a\}$ whence $a b=b a$. It is now easy to see that $G$ is abelian, for each element of $G$ has the form $a^{i} b^{j}$, and $a^{i} b^{j} a^{k} b^{l}=a^{i+k} b^{j+l}=a^{k} b^{l} a^{i} b^{j}$.

Solution to 6.4.11: Let $x \in N_{1}$ and $y \in N_{2}$, since $N_{1}$ is normal, $x\left(y x^{-1} y^{-1}\right) \in$ $N_{1}$ and by the same token $\left(x y x^{-1}\right) y^{-1} \in N_{2}$. So $x y x^{-1} y^{-1} \in N_{1} \cap N_{2}=\{e\}$ and $x y=y x$. A similar argument works for the other pairs of subgroups, showing that elements of $N_{i}$ commutes with elements of $N_{J}$ for all $i \neq j$. Now suppose that $x, y \in N_{1}$. Since $N_{1}$ is normal in $N_{2} N_{3}=G$, there exist $x_{2} \in N_{2}$ and $x_{3} \in N_{3}$ such that $x=x_{2} x_{3}$. By the previous remarks, $y$ commutes with $x_{2}$ and $x_{3}$ so $x y=x_{2} x_{3} y=y x_{2} x_{3}=y x$, and we can now see that the elements of $N_{i}$ commutes will all elements of $N_{j}$ for all $i$ and $j$, since $G=N_{1} N_{2}, \mathrm{G}$ is abelian. Applying the Second Isomorphism Theorem [Fra99, §3.1], we obtain,

$$
N_{2} \simeq \frac{N_{2}}{\{e\}} \simeq \frac{N_{2}}{N_{1} \cap N_{2}} \simeq \frac{N_{1} N_{2}}{N_{1}}=\frac{G}{N_{1}}=\frac{N_{3} N_{1}}{N_{1}} \simeq N_{3} .
$$

The other two isomorphisms $N_{1} \simeq N_{3}$ and $N_{1} \simeq N_{2}$ are obtained in the same way.

Solution to 6.4.13: Let $\left\{a_{1} H, a_{2} H, \ldots, a_{n} \frac{n}{m} H\right\}$ be the set of distinct cosets of $H$. $G$ acts on on this set by left multiplication and any $g \in G$ permutes these $n / m$ cosets. This group action defines a map

$$
\varphi: G \rightarrow S_{\frac{n}{n}}
$$

from $G$ to the permutation group on $n / m$ objects. There are two cases to consider depending on $\varphi$ being injective or not.

If $\varphi$ is not injective, then $\operatorname{ker} \varphi$ is a normal subgroup $K \neq\{e\}$; and $K \neq G$, as well, because if $g \notin H, g H \neq H$; so $\mathrm{g}$ is not a trivial permutation.

If $\varphi$ is injective, then $|\varphi(G)|=n$, and $\varphi(G)$ is a subgroup of $S_{\frac{n}{m}}$. But

$$
\left[S_{\frac{n}{m}}: \varphi(G)\right]=\left|S_{\frac{n}{m}}\right| /|\varphi(G)|=\left(\frac{n}{m}\right) ! / n<2
$$

So $\left[S_{\frac{n}{m}}: \varphi(G)\right]=1$, that is, $G$ is isomorphic to $S_{\frac{n}{m}}$, and in that case, $A_{\frac{n}{m}}$ is a nontrivial normal subgroup.

Solution to 6.4.14: Let $H$ be a subgroup of $G$ of index 3. $G$ acts by left multiplication on the left cosets $\{g H\}$ of the subgroup $H$. This gives a homomorphism of $G$ into the symmetric group of degree 3, the group of permutations of these cosets. The subgroup $H$ is the stabilizer of one element of this set of cosets, namely the coset $1 H$. This homomorphism cannot map onto the entire symmetric group, since this symmetric group has a subgroup of index 2 , which would pull back to a subgroup of $G$ of index 2 . Thus, it must map onto the cyclic subgroup of order 3 , and the group $H$ is then the kernel of this homomorphism.

Solution to 6.4.15: For each element $g \in G$ consider the inner automorphism $\psi_{g}(h)=g h g^{-1}$. Defined in this way the map $g \mapsto \psi_{g}$ defines a homomorphism $G \rightarrow A$. It is nontrivial because $G$ is not abelian, and injective since $G$ is simple. Let $B$ be its image, so $G \simeq B$. If $\alpha \in A$ and $g, h \in G$, we have

$$
\alpha\left(\psi_{g}(h)\right)=\alpha\left(g h g^{-1}\right)=\alpha(g) \alpha(h) \alpha(g)^{-1}=\psi_{\alpha(g)}(\alpha(h)),
$$

so $\alpha \circ \psi_{g}=\psi_{\alpha(g)} \circ \alpha$ in $A$. Thus, $\alpha \circ \psi_{g} \circ \alpha^{-1}=\psi_{\alpha(g)}$ and $B$ is normal in $A$.

Solution to 6.4.16: 1 . The cycles of $\lambda_{g}$ are the right cosets $\langle g\rangle x$ of the subgroup $\langle g\rangle$ in $G$, so the lengths of each one is the order of $g$, and the number of cycles is $[G:\langle g\rangle]$. If the order of $g$ is odd, then each cycle has odd length, so each cycle is even, and so is $\lambda_{g}$. If the order of $g$ is even, then each cycle is of even length and, therefore, odd. Also,

$$
[G:\langle g\rangle] \cdot|\langle g\rangle|=|G|
$$

is odd. As $[G:\langle g\rangle]$ is the number of cycles, $\lambda_{g}$ is odd.

2. Let $\varphi: G \rightarrow\{-1,1\}$ be defined by $g \mapsto \varphi(g)=$ sign of $\lambda_{g} . \varphi$ is a morphism and, by Part 1 , its kernel is $N$. So $N$ is a normal subgroup of $G$ with index 1 or 2 . By Cauchy's Theorem [Her75, p. 61], $G$ has an element of order 2, which is not in $N$, so $N$ has order 2 .

Solution to 6.4.17: Let $g=x y x^{-1} y^{-1}$ be a commutator. It suffices to show that conjugation by $g$ fixes every element of $N$. As $N$ is cyclic, Aut $(\mathrm{N})$ is abelian, and, because $N$ is normal, conjugation by any element of $G$ is an automorphism of $N$. Let $\varphi_{x}$ be the automorphism of conjugation by $x$. We have $\varphi_{x} \varphi_{y}=\varphi_{y} \varphi_{x}$. Hence, for $n \in N, g n g^{-1}=\varphi_{x} \circ \varphi_{y} \circ \varphi_{x}^{-1} \circ \varphi_{y}^{-1}(n)=n$.

Solution to 6.4.18: We will show that if the index of $N$ in $G$ is not finite and equal to a prime number, then there is a subgroup $H$ properly between $N$ and $G$. Since any nontrivial proper subgroup of $G / N$ is the image of such a subgroup, we need only look at subgroups of $G / N$.

Suppose first that the index of $N$ in $G$ is infinite, and let $g$ be an element of $G / N$. If $g$ is a generator of $G / N$, then $G / N$ is isomorphic to $\mathbb{Z}$, and the element $g^{2}$ generates a proper nontrivial subgroup of $G / N$. Otherwise, $g$ generates such a subgroup.

Suppose that the index of $N$ in $G$ is finite but not a prime number. Let $p$ be any prime divisor of the index. By Cauchy's Theorem [MH87, p. 152], there is an element of order $p$ in $G / N$. This element cannot generate the whole group, so it generates a nontrivial proper subgroup of $G / N$.

Solution to 6.4.19: Let the index of $A$ in $G$ be $n$. $G$ acts by left multiplication on the cosets $g A$, and this gives a homomorphism into the group of permutations of the cosets, which has order $n$ !. The kernel, $N$, of this homomorphism is contained in $A$, so the index of $N$ in $G$ is, at most, $n !$ !

Solution to 6.4.20: 1. False. Consider the integers $\mathbb{Z}$ and the subgroup of the multiples of $n>1$, then the quotient $H \simeq \mathbb{Z}_{n}$ is finite cyclic but $G \not \subset H \times K$ because the product has torsion and $\mathbb{Z}$ is free abelian.

2. False. If $H$ is a countable direct product of infinite cyclic groups and $G$ is a free abelian group mapping onto $H$ with kernel $K, G$ is not isomorphic to the direct product of $H$ and $K$. For if $G$ were isomorphic to $H \times K, H$ would be isomorphic to a subgroup of $G$. All subgroups of $G$ are free abelian groups, but $H$ is not. 

\section{$6.5 S_{n}, A_{n}, D_{n}, \ldots$}

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-084.jpg?height=79&width=1394&top_left_y=351&top_left_x=360)
$|A|=a d-b c=1$. If $a=1$ then $d=1+b c$ and $X$ is one of

$$
\left(\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right),\left(\begin{array}{ll}
1 & 0 \\
1 & 1
\end{array}\right),\left(\begin{array}{ll}
1 & 1 \\
0 & 1
\end{array}\right),\left(\begin{array}{ll}
1 & 1 \\
1 & 0
\end{array}\right) \text {. }
$$

If $a=0$ then $b c=1$ and $X$ is one of

$$
\left(\begin{array}{ll}
0 & 1 \\
1 & 0
\end{array}\right),\left(\begin{array}{ll}
0 & 1 \\
1 & 1
\end{array}\right) \text {. }
$$

Hence $|G|=6$

Let

$$
A=\left(\begin{array}{ll}
0 & 1 \\
1 & 1
\end{array}\right), B=\left(\begin{array}{ll}
0 & 1 \\
1 & 0
\end{array}\right) .
$$

It is easily verified that $A^{3}=I=B^{2}, B A=A^{2} B$. The subgroup $\langle A\rangle$ has index 2 in $G$. Moreover, $B \notin\langle A\rangle$ since $B$ has order 2 and elements of $\langle A\rangle$ have order 1 or 3 . Therefore we have the coset decomposition

$$
G=\langle A\rangle \cup\langle A\rangle B=\left\{I, A, A^{2}, B, A B, A^{2} B\right\} .
$$

The relations $A^{3}=I=B^{2}, B A=A^{2} B$ completely determine the multiplication in $G$.

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-084.jpg?height=57&width=1347&top_left_y=1468&top_left_x=405)
order 2. Moreover $\beta \alpha=\alpha^{2} \beta$. As above

$$
\begin{aligned}
S_{3} &=\left\{1, \alpha, \alpha^{2}, \beta, \alpha \beta, \alpha^{2} \beta\right\} \\
&=\left\langle\alpha, \beta \mid \alpha^{3}=1=\beta^{2}, \beta \alpha=\alpha^{2} \beta\right\rangle
\end{aligned}
$$

Thus the correspondence $A^{i} B^{j} \leftrightarrow \alpha^{i} \beta^{j}$ is an isomorphism between $G$ and $S_{3}$.

Solution to 6.5.2: Think of $S_{4}$ as permuting the set $\{1,2,3,4\}$. For $1 \leqslant i \leqslant 4$, let $G_{i} \subset S_{4}$ be the set of all permutations which fix $i$. Clearly, $G_{i}$ is a subgroup of $S_{4}$; since the elements of $G_{i}$ may freely permute the three elements of the set $\{1,2,3,4\} \backslash\{i\}$, it follows that $G_{i}$ is isomorphic to $S_{3}$. Thus, the $G_{i}$ 's are the desired four subgroups isomorphic to $S_{3}$.

Similarly, for $i, j \in\{1,2,3,4\}, i \neq j$, let $H_{i j}$ be the set of permutations which fix $i$ and $j$. Again $H_{i j}$ is a subgroup of $S_{4}$, and since its elements can freely permute the other two elements, each must be isomorphic to $S_{2}$. Since for each pair $i$ and $j$ we must get a distinct subgroup, this gives us six such subgroups.

Finally, note that $S_{2}$ is of order 2 and so is isomorphic to $\mathbb{Z}_{2}$. Therefore, any subgroup of $S_{4}$ which contains the identity and an element of order 2 is isomorphic to $S_{2}$. Consider the following three subgroups: $\{1,(12)(34)\},\{1,(13)(24)\}$, and $\{1,(14)(23)\}$. None of these three groups fix any of the elements of $\{1,2,3,4\}$, so they are not isomorphic to any of the $H_{i j}$. Thus, we have found the final three desired subgroups.

Solution to 6.5.3: Let $\sigma$ be a 5-cycle and $\tau$ a 2-cycle. By renaming the elements of the set, we may assume $\sigma=\left(\begin{array}{lll}1 & 2345\end{array}\right)$ and $\tau=\left(\begin{array}{ll}a & b\end{array}\right)$. Recall that if $\left(i_{1} i_{2} \cdots i_{n}\right)$ is a cycle and $\sigma$ is any permutation, then

$$
\sigma\left(i_{1} i_{2} \cdots i_{n}\right) \sigma^{-1}=\left(\sigma\left(i_{1}\right) \sigma\left(i_{2}\right) \cdots \sigma\left(i_{n}\right)\right) .
$$

Letting $\sigma$ act repeatedly on $\tau$ as above, we get the five transpositions $(a+i b+i)$, $1 \leqslant i \leqslant 5$, where we interpret $a+i$ to be $a+i-5$ if $a+i>5$. Fixing $i$ such that $a+i-5=1$ and letting $c=b+i$, we see that $G$ contains the five transpositions $(1 c),(2 c+1), \ldots,(5 c+4)$. Since $a \neq b, c \neq 1$.

Let $d=c-1$. Since $d$ does not equal 0 or 5 , these five transpositions can be written as $(c+n d c+(n+1) d), 0 \leqslant n \leqslant 4$. By the Induction Principle [MH93, p. 7] the equation above shows that $G$ contains the four transpositions

$$
(1 c+n d)=(1 c+(n-1) d)(c+(n-1) d c+n d)(1 c+(n-1) d),
$$

$1 \leqslant n \leqslant 4$. Since they are distinct, it follows that $G$ contains the four transpositions ( 1 2), (1 3), (1 4), and (1 5). Applying the equation a third time, we see that $(i j)=(1 i)(1 j)(1 i)$ is an element of $G$ for all $i$ and $j, 1 \leqslant i, j \leqslant 5$. Hence, $G$ contains all of the 2-cycles. Since every element in $S_{n}$ can be written as the product of 2-cycles, we see that $G$ is all of $S_{n}$.

Solution to 6.5.5: We use the notation $1^{\alpha_{1}} 2^{\alpha_{2}} \ldots$ to denote the cycle pattern of a permutation in disjoint cycles form; this means that the permutation is a product of $\alpha_{1}$ cycles of length $1, \alpha_{2}$ cycles of length $2, \ldots$ (Thus the pattern of, say, (8)(3 4$)(56)(271) \in S_{8}$ is denoted by $1^{1} 2^{2} 3^{1}$.) The order of a permutation in disjoint cycles form is the least common multiple of the orders of its factors. The order of a cycle of length $r$ is $r$. The possible cycle patterns of elements of $S_{7}$ are

$$
\begin{aligned}
& 1^{7} \\
& 1^{5} 2^{1} ; \quad 1^{3} 2^{2} \quad 1^{1} 2^{3} ; \\
& 1^{4} 3^{1} ; \quad 1^{2} 2^{1} 3^{1} ; \quad 2^{2} 3^{1} ; \quad 1^{1} 3^{2} ; \\
& 1^{3} 4^{1} ; \quad 1^{1} 2^{1} 4^{1} ; \quad 3^{1} 4^{1} \\
& 1^{2} 5^{1} ; \quad 2^{1} 5^{1} \text {; } \\
& 1^{1} 6^{1} \text {; } \\
& 7^{1} \text {. }
\end{aligned}
$$

We see that possible orders of elements of $S_{7}$ are $1,2,3,4,5,6,7,10,12$.

Solution to 6.5.6: 1. $\sigma=(1234)(56789)$ has order $\operatorname{lcm}\{4,5\}=20$.

2. If $\sigma=\sigma_{1} \cdots \sigma_{k}$ is written as a product of disjoint cycles $\sigma_{i}$ of length $r_{i}$, then the order of $\sigma$ is $\operatorname{lcm}\left\{r_{1}, \ldots, r_{k}\right\}$. In order that this lcm is 18 , some $r_{i}$ must have a factor $3^{2}$, and some $r_{i}$ a factor 2 . As $1<r_{i} \leqslant 9$ and $r_{1}+\cdots+r_{k} \leqslant 9$ this is impossible. Solution to 6.5.7: The order of a $k$-cycle is $k$, so the smallest $m$ which simultaneously annihilates all 9-cycles, 8-cycles, 7-cycles, and 5-cycles is $2^{3} \cdot 3^{2} \cdot 5 \cdot 7=$ 2520. Any $n$-cycle, $n \leqslant 9$, raised to this power is annihilated, so $n=2520$.

To compute $n$ for $A 9$, note that an 8-cycle is an odd permutation, so no 8-cycles are in $A_{9}$. Therefore, $n$ need only annihilate 4-cycles (since a 4-cycle times a transposition is in A9), 9-cycles, 7-cycles, and 5-cycles. Thus, $n=2520 / 2=$ 1260.

Solution to 6.5.8: We have $1111=11 \times 101$, the product of two primes. So $G$ is cyclic, say $G=\langle a\rangle$. From $1111>999$, it follows that $a$, when written as a product of disjoint cycles, has no cycles of length 1111 . Therefore, all cycles of $a$ have lengths 1,11 , and 101. Let there be $x, y$, and $z$ cycles of lengths, respectively, 1,11 , and 101. If $x>0$, then $a$ has a fixed point, and this is then the desired fixed point for all of $G$. So assume that $x=0$. Then $11 x+101 z=999$. It follows that $2 z \equiv 9(\bmod 11)$, so $z \equiv 10 \quad(\bmod 11)$, and, therefore, $z \geqslant 10$. But then $999=11 y+101 z \geqslant 1010$, a contradiction.

Solution to 6.5.9: For $\sigma$ in $S_{n}$, let $\varepsilon_{\sigma}=0$ if $\sigma$ is even and $\varepsilon_{\sigma}=1$ if $\sigma$ is odd. Let $\tau$ denote the cycle $(n+1 n+2)$ in $S_{n+2}$, and regard the elements of $S_{n}$ as elements of $S_{n+2}$ in the obvious way. The map of $S_{n}$ into $S_{n+2}$ defined by $\sigma \mapsto \sigma \tau^{\varepsilon_{\sigma}}$ is then an isomorphism of $S_{n}$ onto its range, and the range lies in $A_{n+2}$.

Solution to 6.5.10: Call $i$ and $j \in\{1,2, \ldots, n\}$ equivalent if there exists $\sigma \in G$ with $\sigma(i)=j$. (This is clearly an equivalence relation.) For each $i$, the set $G_{i}=\{\sigma \in G \mid \sigma(i)=i\}$ is a subgroup of $G$, and the map $G \rightarrow\{1,2, \ldots, n\}$, $\sigma \mapsto \sigma(i)$, induces a bijection from the coset space $G / G_{i}$ to the equivalence class of $i$. Hence, for each $i$, the size of its equivalence class equals $\left[G: G_{i}\right]$, which is a power of $p$. Choosing one $i$ from each equivalence class and summing over $i$, one finds that all these powers of $p$ add up to $n$, since $p$ does not divide $n$, one of these powers has to be $p^{0}=1$. This corresponds to an equivalence class that contains a single element $i$, and this $i$ satisfies $\sigma(i)=i$ for all $\sigma \in G$.

Solution to 6.5.13: To determine the center of

$$
D_{n}=\left\langle a, b \mid a^{n}=b^{2}=1, b a=a^{-1} b\right\rangle
$$

( $a$ is a rotation by $2 \pi / n$ and $b$ is a flip), it suffices to find those elements which commute with the generators $a$ and $b$. Since $n \geqslant 3, a^{-1} \neq a$. Therefore,

$$
a^{r+1} b=a\left(a^{r} b\right)=\left(a^{r} b\right) a=a^{r-1} b
$$

so $a^{2}=1$, a contradiction; thus, no element of the form $a^{r} b$ is in the center. Similarly, if for $1 \leqslant s<n, a^{s} b=b a^{s}=a^{-s} b$, then $a^{2 s}=1$, which is possible only if $2 s=n$. Hence, $a^{s}$ commutes with $b$ if and only if $n=2 s$. So, if $n=2 s$, the center of $D_{n}$ is $\left\{1, a^{s}\right\}$; if $n$ is odd the center is $\{1\}$.

Solution to 6.5.14: The number of Sylow 2-subgroups of $D_{n}$ is odd and divides n. Each Sylow 2-subgroup is cyclic of order 2 , since $2^{1}$ is the largest power of 2 dividing the order of the group. By considering the elements of $D_{n}$ as symmetries of a regular $n$-gon, we see that there are $n$ reflections through axes dividing the $n$-gon in half, and each of these generates a different subgroup of order 2 . Thus, the answer is that there are exactly $n$ Sylow 2-subgroups in $D_{n}$ when $n$ is odd.

\subsection{Direct Products}

Solution to 6.6.3: Suppose $\mathbb{Q}$ was the direct sum of two nontrivial subgroups $A$ and $B$. Fix $a \neq 0$ in $A$ and $b \neq 0$ in $B$. We can write $a=a_{0} / a_{1}$ and $b=b_{0} / b_{1}$, where the $a_{i}$ 's and $b_{i}$ 's are nonzero integers. Since $A$ and $B$ are subgroups, na $a \in A$ and $n b \in B$ for all integers $n$. In particular, $\left(a_{1} b_{0}\right) a \in A$ and $\left(b_{1} a_{0}\right) b \in B$. But

$$
\left(a_{1} b_{0}\right) a=\left(a_{1} b_{0}\right) a_{0} / a_{1}=a_{0} b_{0}=\left(b_{1} a_{0}\right) b_{0} / b_{1}=\left(b_{1} a_{0}\right) b .
$$

Hence, $A$ and $B$ have a nontrivial intersection, a contradiction.

Solution to 6.6.4: Let $C_{m}$ denote the cyclic group of order $m$ for $m \in \mathbb{N}$. By the Structure Theorem for abelian groups [Her75, p. 109], if $G$ is a finite abelian group, there exist unique nonnegative integers $n_{p^{r}}(G)$ for each prime number $p$ and each nonnegative integer $r$ such that $G$ is isomorphic to

$$
\prod_{p} \prod_{r} c_{p^{r}}^{n_{p_{r}}(G)} \text {. }
$$

If $H$ is another abelian group, $G \times H$ is isomorphic to

![](https://cdn.mathpix.com/cropped/2022_10_26_723dd7f537f2370baeeag-087.jpg?height=143&width=1051&top_left_y=1590&top_left_x=537)

Hence, $n_{p^{r}}(G \times H)=n_{p^{r}}(G)+n_{p^{r}}(H)$. Now this and the fact that $A \times B$ is isomorphic to $A \times C$ yield the identities $n_{p^{r}}(B)=n_{p^{r}}(C)$ for all primes $p$ and all nonnegative integers $r$. We conclude $B$ and $C$ are isomorphic.

Solution to 6.6.5: Let $\pi_{3}: A \rightarrow G_{1} \times G_{2}$ be the natural projection map. We have $\operatorname{ker} \pi_{3}=\left\{\left(1,1, g_{3}\right) \in A\right\}$. Let $N_{3}=\left\{g_{3} \mid\left(1,1, g_{3}\right) \in \operatorname{ker} \pi_{3}\right\}$. Since $\operatorname{ker} \pi_{3}$ is a normal subgroup of $A, N_{3}$ is normal in $G_{3}$. Let $A^{\prime}=G_{1} \times G_{2} \times G_{3} / N_{3}$. Since $\pi_{3}$ is onto, for any $\left(g_{1}, g_{2}\right) \in G_{1} \times G_{2}$, there exists $g_{3} \in G_{3}$ such that $\left(g_{1}, g_{2}, g_{3}\right) \in A$, and, thus, $\left(g_{1}, g_{2}, \overline{g_{3}}\right) \in A^{\prime}$.

Define the map $\varphi: G_{1} \times G_{2} \rightarrow G_{3} / N_{3}$ by $\varphi\left(g_{1}, g_{2}\right)=\overline{g_{3}}$, where $g_{3}$ is such that $\left(g_{1}, g_{2}, g_{3}\right) \in A$. This is well defined, for if $\left(g_{1}, g_{2}, g_{3}\right)$ and $\left(g_{1}, g_{2}, h_{3}\right)$ are both in $A$, then $\left(1,1, g_{3} h_{3}^{-1}\right) \in A$, so $g_{3} h_{3}^{-1} \in N_{3}$, which, in turn, implies that $\overline{g_{3}}=\overline{h_{3}}$. The map $\varphi$ is clearly a homomorphism. Furthermore, since $\pi_{1}: A \rightarrow G_{2} \times G_{3}$ is onto, if $g_{3} \in G_{3}$, there exist $g_{1} \in G_{1}$ and $g_{2} \in G_{2}$ such that $\left(g_{1}, g_{2}, g_{3}\right) \in A$. Thus, $\varphi\left(g_{1}, g_{2}\right)=\overrightarrow{g_{3}}$ so $\varphi$ is onto. Therefore, $\varphi\left(G_{1} \times\{1\}\right)$ and $\varphi\left(\{1\} \times G_{2}\right)$ are subgroups of $G_{3} / N_{3}$ which commute with each other. If these two subgroups were equal to one another and to $G_{3} / N_{3}$, then $G_{3} / N_{3}$ would be abelian. As $G_{3}$ is generated by its commutator subgroup, this would imply $G_{3} / N_{3}$ to be trivial or, equivalently, $G_{3}=N_{3}$, which we assumed not to be the case. So we may assume that $\varphi\left(\{1\} \times G_{2}\right) \neq G_{3} / N_{3}$. Pick $\overline{g_{3}} \in G_{3} / N_{3} \backslash \varphi\left(\{1\} \times G_{2}\right)$. Since $\pi_{2}: A \rightarrow G_{1} \times G_{3}$ is onto, there exists a $g_{2} \in G_{2}$ such that $\left(1, g_{2}, g_{3}\right) \in A$. Hence, $\varphi\left(1, g_{2}\right)=\overline{g_{3}}$, contradicting our choice of $\overline{83}$.

Therefore, we must have that $N_{3}=G_{3}$, so $\{1\} \times\{1\} \times G_{3} \subset A$. Similar arguments show that $\{1\} \times G_{2} \times\{1\}$ and $G_{1} \times\{1\} \times\{1\}$ are contained in $A$ and, thus, $A=G$.

Solution to 6.6.6: Yes, we will show that there exist a $Y \subset C$ such that $C=A+Y$. Choose $Y=X \cap C$, then $A \cap Y \subset A \cap X=0$. Now to see the decomposition, observe that for all $c \in C \subset B$, there exist an $a \in A$ and $x \in X$ such that $c=a+x$, but $x=c-a \in C$ so $x \in C \cap X=Y$, thus showing that for all $c \in C$ there exist an $a \in A$ and $x \in Y$ such that $c=a+x$.

Solution to 6.6.7: The obvious isomorphism $F: \operatorname{Aut}(G) \times \operatorname{Aut}(H) \rightarrow \operatorname{Aut}(G \times H)$ is defined by

$$
F\left(\alpha_{G}, \alpha_{H}\right)(g, h)=\left(\alpha_{G}(g), \alpha_{H}(h)\right) .
$$

Since $\alpha_{G}$ and $\alpha_{H}$ are automorphisms of $G$ and $H$, it is clear that $F\left(\alpha_{G}, \alpha_{H}\right)$ is an automorphism of $G \times H$. Let us prove now that $F$ is an isomorphism.

- $F$ is injective: Let $F\left(\alpha_{G}, \alpha_{H}\right)=\mathrm{id}_{G \times H}$. Then $\alpha_{G}=\mathrm{id}_{G}$ and $\alpha_{H}=\mathrm{id}_{H}$ by definition of $F$. Hence, $\operatorname{ker} F$ is trivial so $F$ is injective.

- $F$ is surjective: Choose $\alpha \in \operatorname{Aut}(G \times H)$. Define $\alpha_{G}, \alpha_{H}$ by

$$
\alpha_{G}(g)=\pi_{G}\left(\alpha\left(g, \mathrm{id}_{H}\right)\right) \quad \text { and } \quad \alpha_{H}(h)=\pi_{H}\left(\alpha\left(\mathrm{id}_{G}, h\right)\right)
$$

where $\pi_{G}$ and $\pi_{H}$ are the quotient maps. Thus,

$$
\alpha(g, h)=\left(\alpha_{G}(g), \alpha_{H}(h)\right)=F\left(\alpha_{G}, \alpha_{H}\right)(g, h) .
$$

Since the situation is symmetric between $G$ and $H$; and $G$ is finite, we need only show that $\alpha_{G}$ is injective. Let $\alpha_{G}(g)=\mathrm{id}_{G}$. Then $\alpha\left(g, \mathrm{id}_{H}\right)=$ (id $_{G}, h$ ) for some $h \in H$. Suppose $n=|G|$. Then

$$
\left(\mathrm{id}_{G}, h^{n}\right)=\alpha\left(g, \mathrm{id}_{H}\right)^{n}=\alpha\left(g^{n}, \mathrm{id}_{H}\right)=\left(\mathrm{id}_{G}, \mathrm{id}_{H}\right) .
$$

Hence, $h^{n}=\mathrm{id}_{H}$, so the order of $h$ divides $|G|$. But, by Lagrange's Theorem [Her75, p. 41], the order of $h$ also divides $|H|$, which is relatively prime to $|G|$, so the order of $h$ is one and $h=\mathrm{id}_{H}$.

Solution to 6.6.8: We only have to determine where the automorphism send its generators $(1,0)$ and $(0,1)$, and these can be mapped into $(a, b)$ and $(c, d)$, respectively, if and only if,

$$
a \notin p \mathbb{Z} / p^{2} \mathbb{Z}, \quad c \in p \mathbb{Z} / p^{2} \mathbb{Z} \quad \text { and } \quad d \neq 0 \in \mathbb{Z}_{p}
$$

If $\psi$ is an automorphism that takes $(1,0)$ to $(a, b)$ and $(0,1)$ to $(c, d)$, then the $(a, b)$ must not be killed by $p$, so $a \notin p \mathbb{Z} / p^{2} \mathbb{Z}$ and $(c, d)$ must be killed by $p$, so $c \in p \mathbb{Z} / p^{2} \mathbb{Z}$. Moreover $(c, d)$ should not be multiple of $p(a, b)=(p a, 0)$, so $d \neq 0$

On the other hand, if $(a, b)$ and $(c, d)$ satisfy the conditions, then it is easy to find an automorphism, mapping the generators there, since $(a, b)$ is killed by $p^{2}$ and $(c, d)$ is killed by $p$. The condition on $a$ implies that $(a, b)$ has order $p^{2}$. If $(c, d)$ were a a multiple of $(a, b)$, then since $c \in p \mathbb{Z} / p^{2} \mathbb{Z}$, the element $(c, d)$ would be a multiple of $p(a, b)=(p a, 0)$, which is impossible, since $d \neq 0 \in \mathbb{Z}_{p}$. This makes the # $\psi(G)>p^{2}$ and by the Lagrange's Theorem the number has to be $p^{3}$. Thus $\psi$ is surjective, but $G$ is finite, so then $\psi$ is injective and then an automorphism.

We need now to count all possibilities for $a, b, c$ and $d$ that satisfy the conditions. First there are $p^{2}-p$ choices for $a, p$ total for $b, p$ for $c$ and $p-1$ choices for $d$, and these are all independent, so in total there are $p^{3}(p-1)^{2}$ automorphisms of $G$.

\subsection{Free Groups, Generators, and Relations}

Solution to 6.7.2: Suppose $\mathbb{Q}$ is finitely generated, with generators

$$
\alpha_{1} / \beta_{1}, \ldots, \alpha_{k} / \beta_{k} \quad \alpha_{i}, \beta_{i} \in \mathbb{Z}
$$

Then any element of $\mathbb{Q}$ can be written as $\sum n_{i} \alpha_{i} / \beta_{i}$, where the $n_{i}$ 's are integers. This sum can be written as a single fraction with denominator $s=\beta_{1} \cdots \beta_{k}$. Consider a prime $p$ which does not divide $s$. Then we have $1 / p=r / s$ for some integer $r$, or $p r=s$, contradicting the fact that $p$ does not divide $s$. Hence, $\mathbb{Q}$ cannot be finitely generated.

Solution 2. Suppose $\mathbb{Q}$ is finitely generated, then using the solution to Part 1 of Problem 6.3.1, $\mathbb{Q}$ is cyclic, which is a contradiction.

Solution to 6.7.3: Let $A$ be the matrix

$$
A=\left(\begin{array}{ccc}
15 & 3 & 0 \\
3 & 7 & 4 \\
18 & 14 & 8
\end{array}\right)
$$

that represents the relations of the group $G$, following [Hun96, p. 343] we perform elementary row and column operations on $A$, to bring it into diagonal form.

$$
\left(\begin{array}{ccc}
15 & 3 & 0 \\
3 & 7 & 4 \\
18 & 14 & 8
\end{array}\right) \rightarrow\left(\begin{array}{ccc}
15 & 3 & 0 \\
3 & 7 & 4 \\
12 & 0 & 0
\end{array}\right) \rightarrow\left(\begin{array}{ccc}
3 & 3 & 0 \\
3 & 7 & 4 \\
12 & 0 & 0
\end{array}\right) \rightarrow\left(\begin{array}{ccc}
3 & 3 & 0 \\
0 & 4 & 4 \\
12 & 0 & 0
\end{array}\right)
$$



$$
\rightarrow\left(\begin{array}{ccc}
3 & 3 & 0 \\
0 & 0 & 4 \\
12 & 0 & 0
\end{array}\right) \rightarrow\left(\begin{array}{ccc}
0 & 3 & 0 \\
0 & 0 & 4 \\
12 & 0 & 0
\end{array}\right) \rightarrow\left(\begin{array}{ccc}
12 & 0 & 0 \\
0 & 3 & 0 \\
0 & 0 & 4
\end{array}\right)
$$

that is, $G \simeq \mathbb{Z}_{12} \times \mathbb{Z}_{3} \times \mathbb{Z}_{4}$.

1 . $G$ is then the direct product of two cyclic groups $\mathbb{Z}_{12} \times \mathbb{Z}_{12}$.

2. $G$ is also the direct product of the cyclic groups of prime power orders, namely $\mathbb{Z}_{3} \times \mathbb{Z}_{3} \times \mathbb{Z}_{4} \times \mathbb{Z}_{4}$

3. Using the decomposition $G \simeq \mathbb{Z}_{12} \times \mathbb{Z}_{12}$, we can see that for any element $(p, q)$ of order 2 of $G, p$ and $q$ have to be either 0 or 6 , so it has three elements of order 2 , which are $(6,0),(0,6)$ and $(6,6)$.

Solution to 6.7.4: $x^{5} y^{3}=x^{8} y^{5}$. implies $x^{3} y^{2}=1$. Then $x^{5} y^{3}=x^{3} y^{2}$ and $x^{2} y=1$. Hence, $x^{3} y^{2}=x^{2} y$, so $x y=1$. But then $x y=x^{2} y$, so we have that $x=1$. This implies that $y=1$ also, and $G$ is trivial.

Solution to 6.7.5: We start with the given relations

$$
a^{-1} b^{2} a=b^{3} \quad \text { and } \quad b^{-1} a^{2} b=a^{3}
$$

that we will call first and second, respectively. From the first one, we get $a^{-1} b^{4} a=b^{6}$ and

$$
a^{-2} b^{4} a^{2}=a^{-1} b^{6} a=\left(a^{-1} b^{2}\right) b^{4} a=b^{3}\left(a^{-1} b^{4} a\right)=b^{3} b^{6}=b^{9}
$$

using the relation just obtained and the second one, we get

$$
b^{9}=\left(a^{-2}\right) b^{4} a^{2}=b a^{-3} b^{-1} b^{4}\left(a^{2}\right)=b a^{-3} b^{-1} b^{4} b a^{3} b^{-1}=b a^{-3} b^{4} a^{3} b^{-1}
$$

and we conclude that

$$
a^{-3} b^{4} a^{3}=b^{9}=a^{-2} b^{4} a^{2}
$$

from which we obtain $a b^{4}=b^{4} a$. This, combined with the square of the first relation, $\left(a^{-1} b^{4} a=b^{6}\right)$, gives $b^{2}=1$ and substitution back into the first shows that $b=1$; then, substituting that into the second relation, we see that $a=1$, so the group is trivial.

Solution 2. Using the same labels for the first and second relations, as above, we can conjugate the first one by $a^{2}$ to obtain the new relation $a b^{2} a^{-1}=a^{2} b^{3} a^{-2}$. At the same time multiplying the second relation by $b$ on the left and $a^{-2}$ on the right, we get $a^{2} b a^{-2}=b a$ and multiplying three copies of it back to back we get $a^{2} b^{3} a^{-2}=(b a)^{3}$. Getting these two last relations together we see that

$$
a b^{2} a^{-1}=(b a)^{3} \quad *
$$

and similarly we can show that

$$
b a^{2} b^{-1}=(a b)^{3} \quad \text { * }
$$

Now consider this last relation $(x)$ :

$$
\begin{array}{rlr}
b a^{2} b^{-1} & =(a b)^{3} & \\
& =a(b a)^{2} b & \\
& =a\left(a b^{2} a^{-2} b^{-1}\right) b & \\
& =a^{2} b^{2} a^{-2} & \\
& =\left(a^{2} b\right) b a^{-2} & \\
& =\left(b a^{3}\right) b a^{-2} & \\
& =b a\left(a^{2} b a^{-2}\right) & \\
& =b a b a & \\
& =(b a)^{2} &
\end{array}
$$

so $a b^{-1}=b a$ and similarly we can show that $b a^{-1}=a b$. These two together combined show that $a^{2}=b^{-2}$, and substituting back into the second relation, we see that $a=1$ and the group is trivial.

Solution 3. We have

$$
\begin{aligned}
a^{-2} b^{-2} a^{2} &=a^{-1} b^{-3} a=a^{-1} b^{-1}\left(b^{-2} a\right)=a^{-1} b^{-1} a b^{-3} \\
&=a^{-1} b^{-3} a=\left(a^{-1} b^{-2}\right) b^{-1} a=b^{-3} a^{-1} b^{-1} a \\
b^{-2} a^{-2} b^{2} &=b^{-1} a^{-3} b=b^{-1} a^{-1}\left(a^{-2} b\right)=b^{-1} a^{-1} b a^{-3} \\
&=b^{-1} a^{-3} b=\left(b^{-1} a^{-2}\right) a^{-1} b=a^{-3} b^{-1} a^{-1} b \\
a^{-2} b^{2} a^{2} &=a^{-1} b^{3} a=a^{-1} b\left(b^{2} a\right)=a^{-1} b a b^{3} \\
&=a^{-1} b^{3} a=\left(a^{-1} b^{2}\right) b a=b^{3} a^{-1} b a \\
b^{-2} a^{2} b^{2} &=b^{-1} a^{3} b=b^{-1} a\left(a^{2} b\right)=b^{-1} a b a^{3} \\
&=b^{-1} a^{3} b=\left(b^{-1} a^{2}\right) a b=a^{3} b^{-1} a b
\end{aligned}
$$

so we get

$$
\begin{aligned}
a^{-2} b^{-2} a^{2} b^{2} &=\left(a^{-2} b^{-2} a^{2}\right) b^{2}=a^{-1} b^{-1} a b^{-3} b^{2}=a^{-1} b^{-1} a b^{-1} \\
&=a^{-2}\left(b^{-2} a^{2} b^{2}\right)=a^{-2} a^{3} b^{-1} a b=a b^{-1} a b \\
b^{-2} a^{-2} b^{2} a^{2} &=\left(b^{-2} a^{-2} b^{2}\right) a^{2}=b^{-1} a^{-1} b a^{-3} a^{2}=b^{-1} a^{-1} b a^{-1} \\
&=b^{-2}\left(a^{-2} b^{2} a^{2}\right)=b^{-2} b^{3} a^{-1} b a=b a^{-1} b a
\end{aligned}
$$

therefore, $b a^{-1} b a=b a b a^{-1}$ or $a^{-1} b a=a b a^{-1}$ and $a^{-2} b a^{2} b^{-1}=1$. Thus, $b a^{2}=a^{2} b$ so the original relations give $b^{2}=b^{3}, a^{2}=a^{3}$ so $a=b=1$ and the group is trivial.

Solution 4. By induction we show that

$$
a^{-n} b^{2^{n} m} a^{n}=b^{3^{n} m}
$$

in particular $a^{-3} b^{8} a^{3}=b^{27}$, and then substituting the second relation in, we get $b^{-1} a^{2} b^{8} a^{2} b=b^{27}$, but we know from the general relation obtained by induction that $a^{-2} b^{4 \cdot 2} a^{-2}=b^{18}$, so $b^{18}=b^{27}$ or $b^{9}=1$. Similarly we obtain $a^{9}=1$, now using the general relation again for $n=9$ and $m=1$ we get

$$
b^{2^{9}}=a^{-9} b^{2^{9}} a^{-9}=b^{3^{9}}
$$

but since $3^{9}-2^{9} \equiv 1 \quad(\bmod 9)$, we have $b=1$ and the group is trivial.

Solution 5. In this one we will go further and show that the group

$$
\left\langle a, b \mid a=\left[a^{m}, b^{n}\right], b=\left[b^{p}, a^{q}\right]\right\rangle
$$

is trivial for all $m, n, p, q \in \mathbb{Z}$. Here $[x, y]=x^{-1} y^{-1} x y$, and for $m=p=2$ and $n=q=1$, we have our original group.

The two relations can be written as

$$
\begin{aligned}
&a^{-q} b^{p} a^{q}=b^{p+1} \\
&b^{-n} a^{m} b^{n}=a^{m+1}
\end{aligned}
$$

By induction and raising the first relation to the $k^{t h}$-power shows that

$$
a^{-q} b^{k p} a^{q}=b^{k(p+1)} \quad k \in \mathbb{Z}
$$

and we can write

$$
b^{-k(p+1)} a^{l} b^{k(p+1)}=a^{-q} b^{-k p} a^{l} b^{k p} a^{q} \quad k, l \in \mathbb{Z}
$$

and in the same fashion we can obtain from the second relation

$$
b^{-r n} a^{m^{r}} b^{r n}=a^{(m+1)^{r}} \quad r \geqslant 1 \quad \dagger
$$

Now taking $k=n, l=m^{p+1}$ and $r=p+1$ in the two equations above and equating the right-hand sides we get

$$
b^{-n p} a^{m^{(p+1)}} b^{n p}=a^{(m+1)^{(p+1)}}
$$

and the relation $\dagger$ by itself is

$$
b^{-n p} a^{m^{p}} b^{n p}=a^{(m+1)^{p}}
$$

so raising this last relation to $m+1$ and equating with the left-hand side of the previous one we end up with

$$
a^{m^{(p+1)}}=a^{m^{p}(m+1)}=a^{m^{(p+1)}} a^{m^{p}}
$$

which shows that

$$
a^{m^{p}}=1 \ddagger
$$

Now $a^{m^{p}}=\left(a^{m}\right)^{m^{p-1}}$ so substituting back into the first relation we see that

$$
a^{(m+1)} m^{p-1}=1
$$

that is,

$$
a^{m^{p}} a^{m^{p-1}}=1
$$

and using $\ddagger$ again

$$
a^{m^{p-1}}=1
$$

continuing in this way we eventually get to $a^{m^{0}}=1$, that is $a=1$ and the group is trivial.

Solution to 6.7.6: Since $a^{2}=b^{2}=1$, every element of $G$ can be expressed in (at least) one of the forms $a b \ldots a b, b a \ldots b a, a b \ldots a b a, b a \ldots b a b$. But $a b \ldots a b=(a b)^{n}$ for some $n \geqslant 0, b a \ldots b a=(b a)^{n}=(a b)^{-n}, a b \ldots a b a=$ $(a b)^{n} a$, and $b a \ldots b a b=(a b)^{-n} b a a=(a b)^{1-n} a$. Let $H$ be the cyclic subgroup of $G$ generated by $a b$. Then $G=H \cup H a$, so either $H$ is a cyclic subgroup of index 2 in $G$ or $G=H$ is itself cyclic. In the latter case, $G$ is either infinite cyclic or finite cyclic of even order, so $G$ has a cyclic subgroup of index 2 .

Solution to 6.7.8: We use the Induction Principle [MH93, p. 7]. For $n=1$, the result is obvious.

Suppose $g_{1}, \ldots, g_{n}$ generate the group $G$ and let $H$ be a subgroup of $G$. If $H \subset\left\langle g_{2}, \ldots, g_{n}\right\rangle$, by the induction hypothesis, $H$ is generated by $n-1$ elements or fewer. Otherwise let

$$
y=g_{1}^{m_{1}} \cdots g_{n}^{m_{n}} \in H
$$

be such that $\left|m_{1}\right|$ is minimal but nonzero. We can assume, without loss of generality, that $m_{1}>0$. For any $z \in H$,

$$
z=g_{1}^{k_{1}} \cdots g_{n}^{k_{n}}
$$

there are integers $q$ and $r$ such that $k_{1}=q m_{1}+r$ and $0 \leqslant r<m_{1}$. Then the exponent of $g_{1}$ in $z y^{-q}$ is $r$, and, by the choice of $m_{1}$, we obtain $r=0$. Hence,

$$
H=\langle y, K\rangle \text { where } K=H \cap\left\langle g_{2}, \ldots, g_{n}\right\rangle \text {. }
$$

By the induction hypothesis, $K$ is generated by, at most, $n-1$ elements, and the result follows.

Solution to 6.7.10: By the Structure Theorem for finite abelian groups [Her75, p. 109], there are integers $m_{1}, \ldots, m_{k}, m_{j} \mid m_{j+1}$ for $1 \leqslant j \leqslant k-1$, such that $A$ is isomorphic to $\mathbb{Z}_{m_{1}} \oplus \cdots \oplus \mathbb{Z}_{m_{k}}$. We identify $A$ with this direct sum. Clearly $m=m_{k}$. Therefore, $S$ must contain the elements of the form $(0,0, \ldots, 1)$ and $(0, \ldots, 0,1,0, \ldots, 1)$, where the middle 1 is in the $j^{t h}$ position, $1 \leqslant j \leqslant k-1$. Hence, using elements in $S$, we can generate all the elements of $A$ which are zero everywhere except in the $j^{\text {th }}$ position. These, in turn, clearly generate $A$, so $S$ generates $A$ as well. 

\subsection{Finite Groups}

Solution to 6.8.1: Any group with one element must be the trivial group.

By Lagrange's Theorem [Her75, p. 41], any group with prime order is cyclic and so abelian. Therefore, by the Structure Theorem for abelian groups [Her75, p. 109 ], every group of orders 2,3 , or 5 is isomorphic to $\mathbb{Z}_{2}, \mathbb{Z}_{3}$, or $\mathbb{Z}_{5}$, respectively. If a group $G$ has order 4, it is either cyclic, and so abelian, or each of its elements has order 2. In this case, we must have $1=(a b)^{2}=a b a b$ or $b a=a b$, so the group is abelian. Then, again by the Structure Theorem for abelian groups, a group of order 4 must be isomorphic to $\mathbb{Z}_{4}$ or $\mathbb{Z}_{2} \times \mathbb{Z}_{2}$. These two groups of order 4 are not isomorphic since only one of them has an element of order 4.

Since groups of different orders can not be isomorphic, it follows that all of the groups on this list are distinct.

Solution to 6.8.2: By the Solution to the Problem 6.8.1 all groups of order up to 5 are abelian and the group of symmetries of the triangle, $D_{3}$, has 6 elements and is nonabelian.

Solution to 6.8.3: Let $G$ be a group of order 6. By Cauchy's Theorem there exist an element $a$ of order 3 , and an element $b$ of order 2. $H=\langle a\rangle$ is a subgroup of $G$ and, since $b$ has order 2 and 2 does not divide $|H|, b \notin H$. Therefore

$$
G=H \cup H b=\left\{1, a, a^{2}, b, a b, a^{2} b\right\} .
$$

The multiplication of $G$ is determined if we know which of the six elements of $G$ is equal to $b a$. We find that

$$
\begin{array}{lll}
b a \neq 1 & \text { else } & a=b^{-1}=b, \\
b a \neq a & \text { else } & b=1, \\
b a \neq a^{2} & \text { else } & b=a, \\
b a \neq b & \text { else } & a=1
\end{array}
$$

Two cases remain.

Suppose $b a=a b$. Then $(a b)^{n}=a^{n} b^{n}$ for all $n \geqslant 1$. The order of $a b$ is 2,3 or 6. Now $(a b)^{2}=a^{2} b^{2}=a^{2} \neq 1$, and $(a b)^{3}=a^{3} b^{3}=b \neq 1$. Hence $a b$ has order 6. Thus $G$ is cyclic, and $G \simeq \mathbb{Z}_{6}$.

Suppose $b a=a^{2} b$. In this case $G \simeq S_{3}$. For $S_{3}$ has an element $\alpha=\left(\begin{array}{ll}1 & 23\end{array}\right)$ of order 3 , and an element $\beta=\left(\begin{array}{ll}1 & 2\end{array}\right)$ of order 2 which satisfy $\beta \alpha=\alpha^{2} \beta$. Since $\langle\alpha\rangle$ has index 2 in $S_{3}$ and $\beta \notin\langle\alpha\rangle$, it follows that

$$
S_{3}=\left\{1, \alpha, \alpha^{2}, \beta, \alpha \beta, \alpha^{2} \beta\right\} .
$$

Then the correspondence $a^{i} b^{j} \leftrightarrow \alpha^{i} \beta^{j}$ is an isomorphism between $G$ and $S_{3}$.

Solution to 6.8.4: 1 . The Klein four-group $V=\mathbb{Z}_{2} \times \mathbb{Z}_{2}$ is a noncyclic group of order 4 , by the arguments of the Solution to Problem $6.8 .1$ this is the only one of order 4. Alternatively to show uniqueness up to isomorphism, let $G$ be a noncyclic group of order 4 . Then the order of any non-identity element is equal to 2, whence $g=g^{-1}$ for all $g \in G$. Thus $G$ is abelian. For if $x, y \in G$ then $x y=x^{-1} y^{-1}=(y x)^{-1}=y x$.

Let $G=\{1, a, b, c\}$, and consider $a b$. We see that

$$
\begin{array}{lll}
a b \neq 1 & \text { else } & b=a^{-1}=a, \\
a b \neq a & \text { else } & b=1, \\
a b \neq b & \text { else } & a=1 .
\end{array}
$$

Hence $a b=c$. The multiplication of $G$ is then completely determined using the fact that each element of $G$ appears exactly once in any row or column in the multiplication table. This completes the proof of uniqueness.

2. Let $G=\{1, a, b, c\}$ be the noncyclic group of order 4. If $\varphi \in \operatorname{Aut}(G)$, then $\varphi(1)=1$ and $\tilde{\varphi}$, the restriction of $\varphi$ to $X=\{a, b, c\}$, is a permutation of $X$, i.e., $\widetilde{\varphi} \in S_{3}$. Moreover, for $\varphi, \psi \in \operatorname{Aut}(G), \widetilde{\varphi \psi}=\widetilde{\varphi} \tilde{\psi}$. Thus the map $f: \varphi \mapsto \widetilde{\varphi}$ is a group homomorphism $\operatorname{Aut}(G) \rightarrow S_{3}$.

If $\varphi \neq \psi \in \operatorname{Aut}(G)$, then $\varphi(1)=\psi(1)$ and $\varphi, \psi$ differ on $X$, so $f$ is injective.

To show that $f$ is surjective, let $\sigma$ be a permutation of $X$, and let $\hat{\sigma}$ be the extension of $\sigma$ to $G$ given by $\hat{\sigma}(1)=1$. Then $\hat{\sigma}$ is a bijection $G \rightarrow G$. It is left to show that $\hat{\sigma}$ is a homomorphism.

Let $x, y \in G$. If $x=1$ or $y=1$ then $\hat{\sigma}(x y)=\hat{\sigma}(x) \hat{\sigma}(y)$, since $\hat{\sigma}(1)=1$. If $x=y$, then $\hat{\sigma}(x y)=\hat{\sigma}\left(x^{2}\right)=\hat{\sigma}(1)=(\hat{\sigma}(x))^{2}=\hat{\sigma}(x) \hat{\sigma}(y)$, since $g^{2}=1$ for all $g \in G$. Finally for the case when $x, y$ are distinct elements of $X$ we note that the product of two of $a, b, c$ is the third, i.e.,

$$
a b=b a=c, b c=c b=a, c a=a c=b .
$$

Hence $\hat{\sigma}(a) \hat{\sigma}(b)=\hat{\sigma}(c)=\hat{\sigma}(a b)$. The other cases are similar. Thus the map $f: \operatorname{Aut}(G) \rightarrow S_{3}$ is an isomorphism.

Solution to 6.8.5: By the Structure Theorem for finitely generated abelian groups [Her75, p. 109], there are three: $\mathbb{Z}_{8}, \mathbb{Z}_{2} \times \mathbb{Z}_{4}$, and $\mathbb{Z}_{2} \times \mathbb{Z}_{2} \times \mathbb{Z}_{2}$.

1. $\left(\mathbb{Z}_{15}\right)^{*}=\{1,2,4,7,8,11,13,14\}$. By inspection, we see that every element is of order 2 or 4 . Hence, $\left(\mathbb{Z}_{15}\right)^{*} \simeq \mathbb{Z}_{2} \times \mathbb{Z}_{4}$

2. $\left(\mathbb{Z}_{17}\right)^{*}=\{1,2, \ldots, 16\}=\{\pm 1, \pm 2, \ldots, \pm 8\}$, passing to the quotient $\left(\mathbb{Z}_{17}\right)^{*} /\{\pm 1\}=\{1,2, \ldots, 8\}$ which is generated by 3 , so $\left(\mathbb{Z}_{17}\right)^{*} \simeq \mathbb{Z}_{8}$.

3. The roots form a cyclic group of order 8 isomorphic to $\mathbb{Z}_{8}$.

4. $\mathbf{F}_{8}$ is a field of characteristic 2, so every element added to itself is 0 . Hence,

$$
\mathbf{F}_{8}^{+} \simeq \mathbb{Z}_{2} \times \mathbb{Z}_{2} \times \mathbb{Z}_{2} .
$$

5. $\left(\mathbb{Z}_{16}\right)^{*}=\{1,3,5,7,9,11,13,15\} \simeq \mathbb{Z}_{2} \times \mathbb{Z}_{4}$ Solution to 6.8.6: Order 24: The groups $S_{4}$ and $S_{3} \times \mathbb{Z}_{4}$ are nonabelian of order 24. They are not isomorphic since $S_{4}$ does not contain any element of order 24 but $S_{3} \times \mathbb{Z}_{4}$ does.

Order 30: The groups $D_{3} \times \mathbb{Z}_{5}$ and $D_{5} \times \mathbb{Z}_{3}$ have different numbers of Sylow 2 -subgroups, namely 3 and 5 , respectively.

Order 40: There are two examples where the Sylow 2-subgroup is normal: The direct product of $\mathbb{Z}_{5}$ with a nonabelian group of order 8 . Such order 8 groups are the dihedral group of symmetries of the square (which has only two elements of order 4 ), and the group of the quaternions $\{\pm 1, \pm i, \pm j, \pm k\}$ (which has six elements of order 4). There are also several other examples where the Sylow 2subgroup is not normal.

Solution to 6.8.7: Consider the following eight groups of order 36:

$\mathbb{Z}_{2}^{2} \times \mathbb{Z}_{3}^{2}, \mathbb{Z}_{2}^{2} \times \mathbb{Z}_{9}, \mathbb{Z}_{4} \times \mathbb{Z}_{3}^{2}, \mathbb{Z}_{4} \times \mathbb{Z}_{9}, \mathbb{Z}_{6} \times S_{3}, S_{3} \times S_{3}, \mathbb{Z}_{2} \times D_{2 \cdot 9}, \mathbb{Z}_{3} \times A_{1}$.

The first four are abelian and pairwise nonisomorphic because each pair has either distinct 2-Sylow subgroups or distinct 3-Sylow subgroups. They are not isomorphic to the last four, since the latter are not abelian.

Of the last four, only $\mathbb{Z}_{2} \times D_{2.9}$ has a cyclic 3-Sylow subgroup, only $\mathbb{Z}_{3} \times A_{1}$ has a normal 2-Sylow subgroup, and only $S_{3} \times S_{3}$ has trivial center. Thus the last four are pairwise nonisomorphic.

Solution to 6.8.8: Let $G$ be a group of order $p^{2}$. The conjugacy classes of $G$ form a partition of $G$, as the ones associated with the equivalence relation $x \sim y$ iff there is $z$ such that $y=z x z^{-1}$. Let $Z$ denote the center of $G$. Then $x \in Z$ iff $C(x)=\{x\}$, where $C(x)$ denotes a conjugacy class. Thus $|Z|$ is the number of singleton conjugacy classes. Now $|C(g)|$ is a divisor of $|G|$, for each $g \in G$. Denote the non-singleton conjugacy classes by $C_{1}, \ldots, C_{k}$, where $k \geqslant 0$. Then $|G|=|Z|+\left|C_{1}\right|+\cdots+\left|C_{k}\right|$. If $|Z|=1$ we obtain the contradiction that $p$ divides 1 . Thus $|Z|>1$.

If $G$ is cyclic then $G$ is abelian. So we suppose that $G$ is not cyclic. Therefore, each element of $G$, except 1 , has order $p$. Select $x \neq 1$ in $Z$, and $y \notin\langle x\rangle$. Then $|\langle x\rangle|=p=|\langle y\rangle|$ and thus, by Lagrange's theorem, $\langle x\rangle \cap\langle y\rangle=\{1\}$. We deduce that the elements $x^{i} y^{j},(1 \leqslant i, j \leqslant p)$ are distinct, and therefore $\langle x\rangle\langle y\rangle=G$. As $\langle x\rangle \subset Z$ we see that each element of $\langle x\rangle$ commutes with each element of $\langle y\rangle$.

Write $H=\langle x\rangle, K=\langle y\rangle ; H \times K$ is the direct product group. We define a bijection $f: H \times K \rightarrow G$ given by $f(h, k)=h k$. This map is an isomorphism as

$$
\begin{aligned}
f\left\{(h, k)\left(h^{\prime}, k^{\prime}\right)\right\} &=f\left(h h^{\prime}, k k^{\prime}\right) \\
&=h h^{\prime} k k^{\prime} \\
&=h k h^{\prime} k^{\prime} \quad(u \in H \quad \text { commutes with } \quad v \in K) \\
&=f(h, k) f\left(h^{\prime} k^{\prime}\right) .
\end{aligned}
$$

Now $H$ and $K$ are both isomorphic to the cyclic group $\mathbb{Z}_{p}$ of order $p$. Hence $G$ is isomorphic to the abelian group $\mathbb{Z}_{p} \times \mathbb{Z}_{p}$, which proves that $G$ is abelian. (In fact, this proves that a group of order $p^{2}$ is either $\mathbb{Z}_{p^{2}}$ or $\mathbb{Z}_{p} \times \mathbb{Z}_{p}$.) Solution to 6.8.9: The number of Sylow 3-subgroups is congruent to $1 \bmod 3$ and divides 5; hence, there is exactly one such subgroup, which is normal in the group. It is an abelian group of order 9. The abelian groups of this order are the cyclic group of order 9 and the direct product of two cyclic groups of order 3.

The number of Sylow 5-subgroups is congruent to $1 \bmod 5$ and divides 9 ; hence, there is exactly one such subgroup, which is the (normal) cyclic group of order 5. The Sylow 3-subgroup and the Sylow 5-subgroup intersect trivially so their direct product is contained in the whole group, and a computation of the order shows that the whole group is exactly this direct product. Therefore, there are, up to isomorphism, two possibilities

$$
\mathbb{Z}_{9} \times \mathbb{Z}_{5}, \quad \mathbb{Z}_{3} \times \mathbb{Z}_{3} \times \mathbb{Z}_{5}
$$

Solution to 6.8.10: Every element of $G$ of order 7 generates a Sylow 7-subgroup. Therefore $G$ contains more than one Sylow 7-subgroup. The number of Sylow 7 -subgroups of $G$ is congruent to 1 modulo 7 , and any two of them have a trivial intersection. Hence the number must be 8 (since $G$, with 56 elements, is too small to accommodate 15 or more Sylow 7-subgroups). Thus $G$ has $8 \times 6=48$ elements of order 7.

If $P$ is a Sylow 2-subgroup of $G$, then $P$ is contained in the complement of the set of elements of order 7 , and that complement has cardinality $56-48=8$. Therefore $G$ has only one Sylow 2-subgroup.

Since $G$ has a unique Sylow 2-subgroup $P$, the subgroup $P$ is normal in $G$. Let $g$ be an element of $G$ of order 7 , and let $g$ act on $P$ by conjugation: $h \mapsto g h g^{-1}$ $(h \in P)$. If this automorphism were the identity, then $P$ would commute with the subgroup generated by $g$, and it would follow that $G$ is the direct product of $P$ and the subgroup generated by $g$. In that case $G$ would have only 6 elements of order 7 , a contradiction. Hence the automorphism of $P$ induced by $g$ is not the identity. That automorphism therefore has order 7 . Being a permutation of the 7-element set $P \backslash\{1\}$, it must be a cyclic permutation of that set. It follows that all elements of $P \backslash\{1\}$ have the same order. Since $P$ contains an element of order 2 , all of its nonidentity elements must have order 2.

Solution to 6.8.11: The prime factorization of the order is $80=2^{4} \cdot 5^{1}$. Since there are five partitions of 4 and one of 1 , by the Structure Theorem for finite abelian groups [Her75, p. 109], there are five non-isomorphic groups of order 80 , which are:

$$
\begin{aligned}
&\mathbb{Z}_{2} \times \mathbb{Z}_{2} \times \mathbb{Z}_{2} \times \mathbb{Z}_{2} \times \mathbb{Z}_{5} \\
&\mathbb{Z}_{2} \times \mathbb{Z}_{2} \times \mathbb{Z}_{4} \times \mathbb{Z}_{5} \\
&\mathbb{Z}_{2} \times \mathbb{Z}_{8} \times \mathbb{Z}_{5} \\
&\mathbb{Z}_{4} \times \mathbb{Z}_{4} \times \mathbb{Z}_{5} \\
&\mathbb{Z}_{16} \times \mathbb{Z}_{5}
\end{aligned}
$$

Solution to 6.8.12: If $p=2$, then the group is either cyclic and so isomorphic to $\mathbb{Z}_{4}$, or every element has order 2 and so is abelian and isomorphic to $\mathbb{Z}_{2} \oplus \mathbb{Z}_{2}$; for details see the Solution to Problem 6.8.1.

Now suppose $p>2$ and let $G$ have order $2 p$. By Sylow's Theorems [Her75, p. 91], the $p$-Sylow subgroup of $G$ must be normal, since the number of such subgroups must divide $2 p$ and be congruent to $1 \bmod p$. Since the $p$-Sylow subgroup has order $p$, it is cyclic; let it be generated by $g$. A similar argument shows that the number of 2-Sylow subgroups is odd and divides $2 p$; hence, there is a unique, normal 2-Sylow subgroup, or there are $p$ conjugate 2-Sylow subgroups. Let one of the 2-Sylow subgroups be generated by $h$.

In the first case, the element $g h g^{-1} h^{-1}$ is in the intersection of the 2-Sylow and the $p$-Sylow subgroups since they are both normal; these are cyclic groups of different orders, so it follows that $g h g^{-1} h^{-1}=1$, or $h g=g h$. Since $g$ and $h$ must generate $G$, we see that $G$ is abelian and isomorphic to $\mathbb{Z}_{2} \oplus \mathbb{Z}_{p}$.

In the second case, a counting argument shows that all the elements of $G$ can be written in the form $g^{i} h^{j}, 0 \leqslant i<p, 0 \leqslant j<2$. Since all the elements of the form $g^{i}$ have order $p$, it follows that all the 2-Sylow subgroups are generated by the elements $g^{i} h$. Hence, all of these elements are of order 2 ; in particular, $g h g h=1$, or $h g=g^{-1} h$. Thus, $G=\left\langle g, h \mid g^{p}=h^{2}=1, h g=g^{-1} h\right\rangle$ and so $G$ is the dihedral group $D_{n}$.

Solution to 6.8.13: By Cayley's Theorem [Her75, p. 71], every group of order $n$ is isomorphic to a subgroup of $S_{n}$, so it is enough to show that $S_{n}$ is isomorphic to a subgroup of $\mathbb{O}(n)$. For each $\sigma \in S_{n}$, consider the matrix $A_{\sigma}=\left(a_{i j}\right)$, where $a_{\sigma(i) i}=1$ and all other entries are zero. Let $\varphi$ be defined by $\sigma \mapsto \varphi(\sigma)=A_{\sigma}$. The matrix $A_{\sigma}$ has exactly one 1 in each row and column. Hence, both the rows and columns form an orthonormal basis of $\mathbb{R}^{n}$, so $A_{\sigma}$ is orthogonal. $\varphi$ maps $S_{n}$ into $\mathbb{O}(n)$. Let $A_{\sigma}=\left(a_{i j}\right)$ and $B_{\tau}=\left(b_{i j}\right)$. Then

$$
A_{\sigma} B_{\tau}=\left(c_{i j}\right)=\left(\sum_{k=1}^{n} a_{i k} b_{k j}\right) .
$$

An element of this matrix is 1 if and only if $i=\sigma(k)$ and $k=\tau(j)$ for some $k$; equivalently, if and only if $i=\sigma(\tau(j))$. Hence, $c_{\sigma(\tau(i)) i}=1$ and all the other entries are 0 . Therefore, $\left(c_{i j}\right)=A_{\sigma \cdot \tau}$, so $\varphi$ is a homomorphism.

If $A_{\sigma}$ equals the identity matrix, then $\sigma(i)=i$ for $1 \leqslant i \leqslant n$, so $\sigma$ is the identity permutation. Thus, $\varphi$ has trivial kernel and is one-to-one hence an isomorphism.

Solution to 6.8.14: Suppose $P$ and $Q$ are Sylow subgroups corresponding to different primes. If $x \in P$ and $y \in Q$ then $x y x^{-1} \in Q$ (since $Q$ is normal), therefore $[x, y]=x y x^{-1} y^{-1} \in Q$. Similarly, using the normality of $P,[x, y] \in$ $P$. But $P \cap Q=\{1\}$ since $P$ and $Q$ have coprime orders, so $x y=y x$. The union of the Sylow subgroups generate a subgroup of $G, H$ say, whose order is divisible by the order of each Sylow subgroup, therefore by the order of $G$, and we get $G=H$. Since $G$ is generated by a set of commuting elements, it is abelian. Solution to 6.8.15: Any counterexample is clearly nonabelian. In the symmetric group $S_{3}$ we have $S=A_{3}$, a subgroup. But if $G=S_{4}$, then $S$ contains the squares of all 4-cycles, hence all transpositions. But the transpositions generate $S_{4}$, so if $S$ were a subgroup it would equal $S_{4}$. However 4-cycles are not in $S$ since $S$ has no elements of order 8 .

Solution 2. Let $G$ be any simple nonabelian group of even order. Since $S$ is obviously invariant under conjugation $\left(a^{-1} S a=S\right.$ for all $\left.a\right)$, if it were a subgroup it would be either $\{1\}$ or $G$. If $S=\{1\}$ then $a b a b=a a b b$ for all $a, b$, implying that $G$ is abelian, a contradiction. If $S=G$ then the squaring map is surjective, hence injective (since $G$ is finite), contradicting the fact that a group of even order has elements of order 2 . Hence $S$ is not a subgroup.

Solution to 6.8.16: For each element $x \in X, G_{x}=\{g \in G \mid g x=x\}$ is a subgroup of $G$ of index $|X|$, by transitivity. Since the identity is in each $G_{x}$, $\cup_{x \in X} G_{x}$ contains at most

$$
|X|(|G| /|X|-1)+1=|G|-|X|+1
$$

elements. Therefore at least $|X|-1$ elements of $G$ fix no elements of $X$.

Solution to 6.8.17: Suppose $b \in G$ is not in the center of $G$. Let $P=\left\{g b g^{-1}\right\}$. Then $G$ clearly acts transitively on $P$. The cardinality of $P$ equals $|G| /|C(b)|$ where $C(b)$ is the centralizer of $b$. Since $\{e, b\} \subset C(b), b \neq e$ and $C(b) \neq G$, the result follows.

Solution 2. Since $G$ is non-Abelian its order is not prime, so it has a nontrivial proper subgroup $H$ (for example, a subgroup of prime order, obtained either from a Sylow Theorem [Her75, p. 91], or as a subgroup of the cyclic subgroup generated by a nonidentity element of $G$ ). Then $G$ acts transitively by left multiplication on the left cosets of $H$.

Solution to 6.8.18: 1 . For any set $X$ let $S_{X}$ be the group of bijections $\sigma: X \rightarrow X$. If $X$ is a finite set with $n$ elements then $S_{X}$ is isomorphic to $S_{n}$, the group of all permutations of $\{1,2, \ldots, n\}$.

For any group $G$ and any element $g \in G$, define a mapping $\varphi_{g}: G \rightarrow G$ by $\varphi_{g}(x)=g x$ for all $x \in G$. For any $g, h, x \in G,\left(\varphi_{g} \cdot \varphi_{h}\right)(x)=\varphi_{g}\left(\varphi_{h}(x)\right)=$ $\varphi_{g}(h x)=g(h x)=(g h) x=\varphi_{g h}(x)$. This shows that $\varphi_{g h}=\varphi_{g} \cdot \varphi_{h}$.

It follows that, for each $g \in G, \varphi_{g}: G \rightarrow G$ is a bijection, with inverse $\varphi_{g^{-1}}$. In other words $\varphi_{g} \in S_{G}$ for all $g \in G$, defining a map $\varphi: G \rightarrow S_{G}$. Since $\varphi_{g h}=\varphi_{g} \cdot \varphi_{h}, \varphi: G \rightarrow S_{G}$ is a group homomorphism. If $\varphi_{g}=\varphi_{h}$ for two elements $g, h \in G$ then $g=\varphi_{g}(1)=\varphi_{h}(1)=h$, showing that $\varphi: G \rightarrow S_{G}$ is injective. Therefore, $G$ is isomorphic to the subgroup $\varphi(G)$ of $S_{G}$.

2. Since any group of order $n$ embeds in $S_{n}$, it suffices to embed $S_{n}$ into the group of even permutations of $n+2$ objects. Let $\varepsilon: S_{n} \rightarrow \mathbb{Z}_{2}$ be the homomorphism that maps even permutations to 0 and odd permutations to 1 .

Define $\theta: S_{n} \rightarrow S_{n+2}$ by $\theta(\sigma)=\sigma \cdot(n+1, n+2)^{\varepsilon(\sigma)}$. Since the transposition $(n+1, n+2)$ commutes with each element $\sigma \in S_{n}, \theta$ is a homomorphism, clearly injective. Since $\sigma$ and $(n+1, n+2)^{\varepsilon(\sigma)}$ have the same parity, their product $\theta(\sigma)$ is even.

Solution to 6.8.19: Case 1. Suppose $a$ and $b$ commute. Then the elements $a^{i} b^{j}$, $0 \leqslant i<p, 0 \leqslant j<p$ are distinct, have order $p$ (if $i, j$ are not both zero).

Case 2. Suppose $a, b$ do not commute. Then the $p^{2}-1$ elements $a^{i} b^{j} a^{-i}$ and $a^{k}$, $0 \leqslant i<p, 0<j<p, 0<k<p$ are distinct and have order $p$. Indeed, if $a^{i} b^{j} a^{-i}=b^{k}$ then we would have a homomorphism from (a) into Aut $(b)$. Since the first group has order $p$ and the second has order $p-1$ this morphism must be trivial, which is impossible since $a$ and $b$ do not commute. We have $a^{i} b^{j} a^{-i} \neq a^{k}$ because $(a)$ and $(b)$ are distinct groups.

Solution to 6.8.20: Suppose $G$ contains an element whose order is not 2 . Then the map $g \mapsto-g$ is an automorphism of $G$ of order 2, and it follows by Lagrange's Theorem [Her75, p. 41] that Aut $(G)$ has even order. By the Structure Theorem for finite abelian groups [Her75, p. 109], it only remains to consider the groups $G=\left(\mathbb{Z}_{2}\right)^{r}(r=1,2, \ldots)$, plus the trivial group. If $G$ is trivial or $r=1$, then $\operatorname{Aut}(G)$ is trivial. If $r \geqslant 2$, then $G$ has the automorphism

$$
\left(x_{1}, x_{2}, x_{3}, x_{4}, \ldots, x_{r}\right) \mapsto\left(x_{2}, x_{1}, x_{3}, x_{4}, \ldots, x_{r}\right)
$$

of order 2, so Lagrange's Theorem again shows that $\operatorname{Aut}(G)$ has even order.

So $\operatorname{Aut}(G)$ has odd order if and only if $G$ is trivial or $G \simeq \mathbb{Z}_{2}$. In both cases $\operatorname{Aut}(G)$ is trivial. Observe that it is not necessarily true that $\operatorname{Aut}(G \times H)=$ $\operatorname{Aut}(G) \times \operatorname{Aut}(H)$ even if $G$ and $H$ are cyclic of orders equal to distinct powers of a prime.

Solution to 6.8.21: Let $p$ be a prime dividing $n$ and $\varphi(n)$.The expression for $\varphi(n)$ shows that either $p^{2} \mid n$ or there is a different prime number $q$ such that $q \mid n$ and $p \mid(q-1)$. If $p^{2} \mid n$, then $\mathbb{Z}_{p} \times \mathbb{Z}_{p} \times \mathbb{Z}_{n / p^{2}}$ is a noncyclic group of order $n$. In the second case, let $G$ be the subgroup of $G L\left(\mathbb{F}_{q}\right)$ of matrices of the form $\left(\begin{array}{cc}a & b \\ 0 & 1\end{array}\right)$ where $a^{p}=1$. Since $\mathbb{F}_{q}^{*}$ is cyclic of order $q-1$, there are $p$ solutions to $a^{p}=1$ in $\mathbb{F}_{q}$. Therefore $|G|=p q$. If $a^{p}=1$ and $a \neq 1$, then, $\left(\begin{array}{ll}a & 0 \\ 0 & 1\end{array}\right)\left(\begin{array}{ll}1 & 1 \\ 0 & 1\end{array}\right) \neq\left(\begin{array}{ll}1 & 1 \\ 0 & 1\end{array}\right)\left(\begin{array}{ll}a & 0 \\ 0 & 1\end{array}\right)$, so $G$ is not abelian. Therefore, $G \times \mathbb{Z}_{n / p q}$ has order $n$ and is not abelian, so it is not cyclic either.

\subsection{Rings and Their Homomorphisms}

Solution to 6.9.2: 1. Let $N=\left(\begin{array}{ll}1 & 1 \\ 0 & 1\end{array}\right)$. If $A, B \in R$, then $(A+B) N=A N+B N=$ $N A+N B=N(A+B)$, so $A+B \in R$. If $A, B \in R$ then $(A B) N=A(B N)=$ $A(N B)=(A N) B=(N A) B=N(A B)$, so $A B \in R$. And it is trivial to verify that the matrix, $-I$ also commutes with $\mathrm{N}$, so it belongs to the $R$, implying that $R$ is a subring.

2. A simple calculation shows that the matrix $A=\left(\begin{array}{l}a b \\ c d\end{array}\right)$ belongs to $R$ iff $a=$ $a+c, a+b=b+d, c+d=d$, that is, iff $A$ has the form $\left(\begin{array}{l}a b \\ 0\end{array}\right)$. Define

![](https://cdn.mathpix.com/cropped/2022_10_26_a807eeb357d0f35abe4eg-001.jpg?height=76&width=1385&top_left_y=344&top_left_x=367)
$\psi\left(x^{2}\right)=\left(\begin{array}{ll}0 & 1 \\ 0 & 0\end{array}\right)^{2}=0$, so $\psi$ induces a homomorphism $\mathbb{Q}[x] /\left(x^{2}\right\rangle \rightarrow R$. Since $\psi(a+b x)=\left(\begin{array}{ll}a & b \\ 0 & a\end{array}\right)$, this homomorphism is an isomorphism, and the result follows. Solution to 6.9.4: Let $\varphi^{\prime}: \mathbb{C}^{n} \rightarrow \mathbb{C}$ be a ring homomorphism and $e_{1}=(1,0,0, \ldots, 0), e_{2}=(0,1,0, \ldots, 0), \ldots, e_{n}=(0,0,0, \ldots, 1)$, then $e_{i} e_{j}=0$ for all $i \neq j$ and if $\varphi\left(e_{1}\right)=\cdots=\varphi\left(e_{n}\right)=0$,

$$
\varphi\left(x_{1}, \ldots, x_{n}\right)=\varphi\left(x_{1}, 0, \ldots, 0\right) \varphi\left(e_{1}\right)+\cdots+\varphi\left(0,0, \ldots, x_{n}\right) \varphi\left(e_{n}\right)=0
$$

that is, $\varphi$ is identically zero.

Suppose now that $\varphi$ is a nontrivial homomorphism, then $\varphi\left(e_{i}\right) \neq 0$ for some $i$ and in this case $\varphi\left(e_{i}\right)=\varphi\left(e_{i} e_{i}\right)=\varphi\left(e_{i}\right) \varphi\left(e_{i}\right)$ and $\varphi\left(e_{i}\right)=1$. At the same time $0=\varphi\left(e_{i} e_{j}\right)=\varphi\left(e_{i}\right) \varphi\left(e_{j}\right)$ we conclude that $\varphi\left(e_{j}\right)=0$ for all $j \neq i$, and $\varphi$ is determined by its value on the $i^{i h}$ coordinate.

$$
\begin{aligned}
\varphi\left(x_{1}, \ldots, x_{i}, \ldots, x_{n}\right) &=\varphi\left(0, \ldots, x_{i}, \ldots, 0\right) \varphi\left(e_{i}\right) \\
&=\varphi\left(0, \ldots, x_{i}, \ldots, 0\right) 1 \\
&=\varphi\left(0, \ldots, x_{i}, \ldots, 0\right)
\end{aligned}
$$

So for every homomorphism $\sigma: \mathbb{C} \rightarrow \mathbb{C}$ we can create $n$ such homomorphisms from $\mathbb{C}^{n}$ to $\mathbb{C}$ by composing $\varphi\left(x_{1}, \ldots, x_{n}\right)=\sigma\left(\pi_{i}\left(x_{1}, \ldots, x_{n}\right)\right)$ where $\pi_{i}$ is the projection on the $i^{i h}$ coordinate, and the argument above shows that all arise in this way. We observe here that is probably the best that can be done, since homomorphims from $\mathbb{C}$ to $\mathbb{C}$ cannot be easily classified.

Solution to 6.9.5: Let $R$ contain $k$ elements $(k<\infty)$ and consider the ring

$$
S=\underbrace{R \times R \times \cdots \times R}_{k \text { copies }}
$$

Let $R=\left\{r_{1}, \ldots, r_{k}\right\}$ and $\alpha=\left(r_{1}, \ldots, r_{k}\right) \in S$. Now consider the collection of elements $\alpha, \alpha^{2}, \alpha^{3}, \ldots$. Since $S$ is also a finite ring, by the Pigeonhole Principle [Her75, p. 127], there exist $n$ and $m$ sufficiently large with $\alpha^{n}=\alpha^{m}$. Coordinatewise, this means that $r_{i}^{n}=r_{i}^{m}$ for $1 \leqslant i \leqslant k$, and we are done.

Solution to 6.9.6: If $a x=0$ (or $x a=0$ ) with $a \neq 0$, then $a x a=0 a$ or $a 0=0$. If $b$ is as in the text, then $a(b+x) a=a$, so, by uniqueness of $b, b=b+x$ and $x=0$. Thus, there are no zero divisors.

Fix $a$ and $b$ such that $a b a=a$. If $x \in R$, then $x a b a=x a$ and, as there are no zero divisors, $x a b=x$, so $a b$ is a right identity. Similarly, $a b a x=a x$ implies $b a x=x$ and $b a$ is a left identity. Since any right identity is equal to any right identity, we get $a b=b a=1$. Since $b=a^{-1}, R$ is a division ring. Solution to 6.9.7: Since $(R,+)$ is a finite abelian group with $p^{2}$ elements, by the Structure Theorem for finite abelian groups [Her75, p. 109], it is isomorphic to either $\mathbb{Z}_{p^{2}}$ or $\mathbb{Z}_{p} \times \mathbb{Z}_{p}$. In the first case, there is an element $x \in R$ such that every element of $R$ can be written as $n x$, for $1 \leqslant n \leqslant p^{2}$. Since all elements of this form commute, it follows that $R$ is abelian.

In the second case, every nonzero element must have additive order $p$. Let $x \in R$ be any element not in the additive subgroup generated by 1 . Then it too must have additive order $p$. Thus, a counting argument shows that every element of $R$ can be written in the form $n+k x, 1 \leqslant n \leqslant p, 1 \leqslant k \leqslant p$. Since all elements of this form commute, it follows that $R$ is commutative.

Solution to 6.9.8: Assume $R$ is not the zero ring, and let $A$ be the center of $R$. Then 0,1 are distinct elements of $A$. Hence $\# A \geqslant 2$. The quotient of additive groups $R / A$ has less than $8 / 2=4$ elements, so it must be cyclic. Thus there exists $x \in R$ such that $R=\{a+m x \mid a \in A, m \in \mathbb{Z}\}$. Such expressions commute (so in fact $A=R$ ).

The result is the best possible, the ring of upper triangular $2 \times 2$ matrices over the field of two elements is an example of a noncommutative ring of order 8 , since

$$
\left(\begin{array}{ll}
1 & 0 \\
0 & 0
\end{array}\right)\left(\begin{array}{ll}
0 & 1 \\
0 & 0
\end{array}\right) \not \equiv\left(\begin{array}{ll}
0 & 1 \\
0 & 0
\end{array}\right)\left(\begin{array}{ll}
1 & 0 \\
0 & 0
\end{array}\right) \quad(\bmod 2) .
$$

Solution to 6.9.9: $R$ is trivially an additive subgroup and closed under multiplication and since $\mathbb{C}$ is a field, any subring is an integral domain, finishing the first part. Now consider two factorizations of the integer 10 in $R, 10=2 \cdot 5$ and $10=(1+3 i) \cdot(1-3 i)$. The norm $|z|^{2}=a^{2}=9 b^{2}$ of any $z \in R$ is an integer, and if $|z|^{2}<9$ then $b=0$, so $z$ is a real integer. This implies, in particular, that 2 has no non-trivial factorizations in $R$. If $R$ were a unique factorization domain, then 2 would divide $1+3 i$ or $1-3 i$, but can't since $(1 \pm 3 i) / 2$ are not in $R$.

Solution to 6.9.10: One can embed $R$ in the field of quotients of $R$; then the finite subgroup of $R^{*}$ is a finite subgroup of the multiplicative group of the field; it is a finite abelian group, and so can be written as a direct product of $\mathbb{Z}_{p^{n}}$ for various primes $p$. If there are two such factors for the same $p$, then there are at least $p$ elements of the field satisfying the equation $x^{p}-1=0$. However, in a field, due to the uniqueness of factorization in the polynomial ring, there are, at most, $n$ solutions to any $n^{\text {th }}$ degree polynomial equation in one variable. Thus, in the factorization of our group, each prime $p$ occurs at most once, therefore, any such group is cyclic.

Solution to 6.9.11: Consider the map $\alpha: R \rightarrow R$ defined by $\alpha(x)=a x$. If $\alpha(x)=\alpha(y)$ then $a x=a y$, so $a(x-y)=0$. Since $a$ is not a left zero divisor this implies $x=y$. Therefore $\alpha: R \rightarrow R$ is one-to-one. Since $R$ is finite, $\alpha$ is also onto, so there is an element $b \in R$ such that $a b=1$.

Similarly, using the fact that $a$ is not a right zero divisor, there is an element $c \in R$ such that $c a=1$. Therefore $b=(c a) b=c(a b)=c$ satisfies $a b=b a=1$. Solution to 6.9.12: $1 \Rightarrow 2$ : There exist $v_{1} \neq v_{2}$ such that $u v_{1}=u v_{2}=1$; thus, $u\left(v_{1}-v_{2}\right)=0$ and $u$ is a zero divisor.

$2 \Rightarrow 3$ : Suppose that $u$ is a unit with inverse $v$. If the $u v=0$ then $w=(v u) w=v(u w)=v 0=0$ and, therefore, $u$ is not a left zero divisor.

$3 \Rightarrow 1$ : Let $v$ be a right inverse for $u$, that is, $u v=1$. Since $u$ is not a unit $v u \neq 1$ implying $v u-1 \neq 0$. Now consider the element $v^{\prime}=i+(v u-1) \neq v$, and we have

$$
\begin{aligned}
u v^{\prime} &=u v+u(v u-1) \\
&=1-(u v) u-u \\
&=1+u-u=1
\end{aligned}
$$

showing that $u$ has more than one right inverse.

Solution to 6.9.13: The identity element of such a ring would belong to the additive group of the ring, which is a torsion group; thus there is some finite $n$ such that if you add the identity element, 1 , to itself $n$ times, you get 0 . In other words, the ring would have some finite characteristic $n$. But this implies that the additive order of every element of the ring divides $n$, and this is false, for example, for the element $1 /(n+1)$

Solution to 6.9.14: Consider separately the cases: $a>1, a=1, a<1$.

- If $a>1$ then $a-1>0$; multiplying this by $a$ (which is $>0$ ) we get $a^{2}-a>0$. Adding 1 , we get $a^{2}-a+1>1>0$.

- If $a=1$ then $a^{2}-a+1=1>0$.

- If $a<1$, then $1-a>0$. If $a \neq 0$, then $a^{2}>0$; the sum of positives is positive, so $a^{2}-a+1>0$. If $a=0$ then $a^{2}-a+1=1>0$.

Solution to 6.9.15: The degree 2 polynomial $x^{2}+y^{2}-1$ does not factor into the product of two linear ones (since the circle $x^{2}+y^{2}=1$ is not a union of two lines). This implies that the ideal $\left\langle x^{2}+y^{2}-1\right\rangle$ is prime and, thus, the ring $R=\mathbb{Q}[x, y] /\left\langle x^{2}+y^{2}-1\right\rangle$ is an integral domain.

Consider now the stereographic projection $(x, y) \mapsto(1, y /(x+1)$ ) (at half of the speed of the standard one, in order to make the expressions simpler) of the circle from the point $(-1,0)$ to the line $(1, t)$. It provides a homomorphism $t=y /(x+1)$ of $\mathbb{Q}(t)$ to the field of fractions of $R$. The inverse homomorphism is given by the formulas $x=\left(1-t^{2}\right) /\left(1+t^{2}\right)$ and $y=2 t /\left(1+t^{2}\right)$.

Solution to 6.9.16: For any set $S$ of primes, let $R_{S}$ be the set of rational numbers of the form $a / b$ where $a, b$ are integers and $b$ is a product of powers of primes in $S$. It is clear that $R_{S}$ contains 0 and 1 , and is closed under addition, multiplication, and additive inverses. Hence $R_{S}$ is a subring. If $S$ and $T$ are distinct sets of primes, say with $p \in S \backslash T$, then $1 / p \in R_{S}$ but $1 / p \notin R_{T}$, so $R_{S} \neq R_{T}$. Since there are infinitely many primes, we obtain at least $2^{\aleph_{0}}$ subrings in this way. On the other hand, as $\mathbb{Q}$ is countable, its number of subsets is $2^{N_{0}}$, therefore the set of subrings of $\mathbb{Q}$ has cardinality $2^{\aleph_{0}}$.

\section{$6.10$ Ideals}

Solution to 6.10.1: For two matrices in $A$ we have

$$
\left(\begin{array}{ll}
a & b \\
0 & c
\end{array}\right)\left(\begin{array}{cc}
a_{1} & b_{1} \\
0 & c_{1}
\end{array}\right)=\left(\begin{array}{cc}
a a_{1} & a b_{1}+b c_{1} \\
0 & c c_{1}
\end{array}\right),
$$

![](https://cdn.mathpix.com/cropped/2022_10_26_a807eeb357d0f35abe4eg-004.jpg?height=75&width=1385&top_left_y=800&top_left_x=367)
in $A$. Call these ideals $\mathfrak{I}_{1}, \mathfrak{I}_{2}, \mathfrak{I}_{3}$, respectively. We'll show that, together with $\{0\}$ and $A$, these are the only ideals in $A$.

Consider any ideal $\mathfrak{I} \neq\{0\}$. As $A$ contains all scalar multiples of the identity matrix, $\mathfrak{I}$ is closed under scalar multiplication. From the equalities

$$
\begin{gathered}
\left(\begin{array}{ll}
1 & 0 \\
0 & 0
\end{array}\right)\left(\begin{array}{ll}
a & b \\
0 & c
\end{array}\right)\left(\begin{array}{ll}
0 & 0 \\
0 & 1
\end{array}\right)=\left(\begin{array}{ll}
0 & b \\
0 & 0
\end{array}\right) \\
\left(\begin{array}{ll}
a & b \\
0 & c
\end{array}\right)\left(\begin{array}{ll}
0 & 1 \\
0 & 0
\end{array}\right)=\left(\begin{array}{ll}
0 & a \\
0 & 0
\end{array}\right),\left(\begin{array}{ll}
0 & 1 \\
0 & 0
\end{array}\right)\left(\begin{array}{ll}
a & b \\
0 & c
\end{array}\right)=\left(\begin{array}{ll}
0 & c \\
0 & 0
\end{array}\right),
\end{gathered}
$$

it follows that $\mathfrak{I}$ contains a nonzero matrix in $\mathfrak{I}_{1}$, hence that $\mathfrak{I}$ contains $\mathfrak{I}_{1}$. Suppose $\mathfrak{I} \neq \mathfrak{I}_{1}$. Every matrix in $\mathfrak{I}$ is the sum of a diagonal matrix and a matrix in $\mathfrak{I}_{1}$. There are three cases:

- All diagonal matrices in $\mathfrak{I}$ have the form $\left(\begin{array}{ll}a & 0 \\ 0 & 0\end{array}\right)$. Then $\mathfrak{I}$ contains all such matrices, since it contains a nonzero one, and we conclude that $\mathfrak{I}=\mathfrak{I}_{2}$.

- All diagonal matrices in $\mathfrak{I}$ have the form $\left(\begin{array}{l}0 \\ 0 \\ 0\end{array}\right)$. Then, by a similar argument, $\mathfrak{J}=\mathfrak{I}_{3}$.

- I contains a matrix of the form $\left(\begin{array}{ll}a & 0 \\ 0 & c\end{array}\right)$ with $a \neq 0 \neq c$. Since

$$
\left(\begin{array}{ll}
1 & 0 \\
0 & 0
\end{array}\right)\left(\begin{array}{ll}
a & 0 \\
0 & c
\end{array}\right)=\left(\begin{array}{ll}
a & 0 \\
0 & 0
\end{array}\right),\left(\begin{array}{ll}
0 & 0 \\
0 & 1
\end{array}\right)\left(\begin{array}{ll}
a & 0 \\
0 & c
\end{array}\right)=\left(\begin{array}{ll}
0 & 0 \\
0 & c
\end{array}\right),
$$

we conclude in this case that $\mathfrak{I}$ contains both $\mathfrak{I}_{2}$ and $\mathfrak{I}_{3}$, hence that $\mathfrak{I}=A$.

Solution to 6.10.2: Assume that $\mathfrak{I}$ is a nontrivial ideal. Let $M_{i j}$ be the $n \times n$ matrix with 1 in the $(i, j)^{\ell h}$ position and zeros elsewhere. Choose $A \in \mathfrak{I}$ such that $a=a_{i j} \neq 0$. Then, for $1 \leqslant k \leqslant n, M_{k i} A M_{j k}$ is a matrix which has $a$ in the $(k, k)^{t h}$ entry and 0 elsewhere. Since $\mathfrak{I}$ is an ideal, $M_{k i} A M_{j k} \in \mathfrak{I}$. The sum of these matrices is $a I$ and so this matrix is also in $\mathfrak{J}$. However, since $F$ is a field, $a$ is invertible, so $\mathfrak{I}=M_{n}(\mathbf{F})$. Solution to 6.10.3: We have see in the Solution to Problem 6.10.2 that $M_{n \times n}(F)$ has no nontrivial proper ideals. $M_{n \times n}(\mathbf{F})$ is an $\mathbf{F}$-vector field, and if we identify $\mathbf{F}$ with $\{a \mathfrak{I} \mid a \in \mathbf{F}\}$, we see that any ring homomorphism induces a vector space homomorphism. Hence, if $M_{n \times n}(F)$ and $M_{(n+1) \times(n+1)}(F)$ are isomorphic as rings, they are isomorphic as vector spaces. However, they have different dimensions $n^{2}$ and $(n+1)^{2}$, respectively, so this is impossible.

Solution to 6.10.4: Since the kernel of $h$ is a two sided ideal in $R$, it suffices to show that every ideal $\mathfrak{J}$ in $R$ is either trivial ( $h$ is injective) or all of $R$ ( $h$ is zero).

Assume $\mathfrak{J}$ is a non-trivial two sided ideal in $R$. Suppose $A \in \mathfrak{J}$ with $a_{i j} \neq 0$ for some $0 \leqslant i, j \leqslant n$. If we multiply $A$ on the left by the elementary matrix $E_{j i}$ we get the matrix $B=E_{j i} A$ which has only one nonzero entry $\left(b_{i j}=a_{i j}\right)$. If we multiply $B$ on the left by $\left(1 / a_{i j}\right) I$, we get $E_{i j}$, therefore $E_{i j} \in \mathfrak{J}$. Multiplying $E_{i j}$ on the left and on the right by elementary matrices, we produce all elementary matrices, so these are in $\mathfrak{J}$. As they generate $R$ we have $\mathfrak{J}=R$.

Solution to 6.10.5: Each element of $F$ induces a constant function on $X$, and we identify the function with the element of $\mathbf{F}$. In particular, the function 1 is the unit element in $R(X, \mathrm{~F})$.

Let $\mathfrak{I}$ be a proper ideal of $R(X, \mathbb{F})$. We will prove that there is a nonempty subset $Y$ of $X$ such that $\mathfrak{I}=\{f \in R(X, F) \mid f(x)=0$ for all $x \in Y\}=\mathfrak{I}_{Y}$. Suppose not. Then either $\mathfrak{I} \subset \mathfrak{I}_{Y}$ for some set $Y$ or, for every point $x \in X$, there is a function $f_{x}$ in $\mathfrak{I}$ such that $f_{x}(x)=a \neq 0$. In the latter case, since $\mathfrak{I}$ is an ideal and $F$ is a field, we can replace $f_{x}$ by the function $a^{-1} f_{x}$, so we may assume that $f_{x}(x)=1$. Multiplying $f_{x}$ by the function $g_{x}$, which maps $x$ to 1 and all other points of $X$ to 0 , we see that $\mathfrak{I}$ contains $g_{x}$ for all points $x \in X$. But then, since $X$ is finite, $\mathfrak{I}$ contains $\sum g_{x} \equiv 1$, which implies that $\mathfrak{I}$ is not a proper ideal.

Hence, there is a nonempty set $Y$ such that $\mathfrak{I} \subset \mathfrak{I}_{Y}$. Let $Y$ be the largest such set. As for every $x \notin Y$, there is an $f_{x} \in \mathfrak{I}$ such that $f_{x}(x) \neq 0$ (otherwise we would have $\mathfrak{I} \subset \mathfrak{I}_{Y \cup\{x\}}$ ) by an argument similar to the above, $\mathfrak{I}$ contains all the functions $g_{x}, x \notin Y$. But, from these, we can construct any function in $\mathfrak{I}_{Y}$, so $\mathfrak{I}_{Y} \subset \mathfrak{J}$.

Let $\mathfrak{I}$ and $\mathfrak{J}$ be two ideals, and the associated sets be $Y$ and $Z$. Then $\mathfrak{I} \subset \mathfrak{J}$ if and only if $Z \subset Y$. Therefore, an ideal is maximal if and only if its associated set is as small as possible without being empty. Hence, the maximal ideals are precisely those ideals consisting of functions which vanish at one point of $X$.

Solution to 6.10.6: If $f, g \in E(U, W)$ then so is the homomorphism $f+g$ since its image on $U$ is contained in $f U+g U$. If $Y$ is a subspace and $h \in E(W, Y)$, then $h \circ f \in E(U, Y)$ since $f U \subseteq W+X$ for some finite dimensional subspace $X$ and $h(f(U)) \subseteq h(W)+h(X)$. Thus $E(W, Y) E(U, W) \subseteq E(U, Y)$. From this we see that $E(U, U)$ is a ring with left ideal $E(V, U)$ and right ideal $E(U, 0)$. Also, $E(U, U) \subseteq E(V, V)=E(0,0)$ so these latter two sets are also right and left ideals. The conclusion follows. Solution to 6.10.7: Let $\mathfrak{I}=\left\langle a^{n}-1, a^{m}-1\right\rangle$ and $\mathfrak{J}=\left\langle a^{d}-1\right\rangle$. For $n=r d$ the polynomial $x^{n}-1$ factors into $\left(x^{d}-1\right)\left(x^{r(d-1)}+x^{r(d-2)}+\cdots+x^{r}+1\right)$. Therefore, in $R, a^{n}-1=\left(a^{d}-1\right)\left(a^{r(d-1)}+a^{r(d-2)}+\cdots+a^{r}+1\right)$. A similar identity holds for $a^{m}-1$. Hence, the two generators of $\mathfrak{I}$ are in $\mathfrak{J}$, so $\mathfrak{I} \subset \mathfrak{J}$.

Since $d=\operatorname{gcd}\{n, m\}$, there exist positive integers $x, y$ such that $x n-y m=d$. A calculation gives

$$
\begin{aligned}
a^{d}-1=& a^{d}-1-a^{d+y m}+a^{x n} \\
=&-a^{d}\left(a^{y m}-1\right)+a^{x n}-1 \\
=&-a^{d}\left(a^{m}-1\right)\left(a^{y(m-1)}+\cdots+a^{y}+1\right) \\
&+\left(a^{n}-1\right)\left(a^{x(n-1)}+\cdots+a^{x}+1\right) .
\end{aligned}
$$

Hence, $a^{d}-1$ is in $\mathfrak{J}$, so $\mathfrak{J} \subset \mathfrak{I}$ and the two ideals are equal.

Solution to 6.10.8: 1. If there is an ideal $\mathfrak{I} \neq R$ of index, at most, 4, then there is also a maximal ideal of index, at most, $4, \mathfrak{M}$, say. Then $R / \mathfrak{M}$ is a field of cardinality less than 5 containing an element $\alpha$ with $\alpha^{3}=\alpha+1$, namely $\alpha=a+\mathfrak{M}$. By direct inspection, we see that none of the fields $\mathbf{F}_{2}, \mathbf{F}_{3}, \mathbf{F}_{4}$ contains such an element. Therefore, $\mathfrak{I}=R$.

2. Let $R=\mathbb{Z}_{5}, a=2(\bmod 5)$, and $\mathfrak{I}=\{0\}$.

Solution 2. Since $R / \mathfrak{I}$ has order less than 5 , two of the elements $0,1, a, a^{2}, a^{3}$ have the same image in $R / \mathfrak{J}$. Then $\mathfrak{I}$ contains one of their differences, that is, one of

$$
1, a, a^{2}, a^{3}, a-1, a^{2}-1, a^{3}-1, a(a-a), a\left(a^{2}-1\right), a^{2}(a-1) .
$$

But all these elements are units, since

$$
a(a-1)(a+1)=a\left(a^{2}-1\right)=1, \quad a^{3}-1=a .
$$

Therefore, $\mathfrak{I}$ contains a unit, so $\mathfrak{I}=R$.

Solution to 6.10.9: Let $\mathfrak{I}$ be such an ideal. Consider $\varphi: R \rightarrow R / \mathfrak{I}$, the quotient map. Since $R / \mathfrak{I}$ is a three element ring with 1 , it must be isomorphic to $\mathbb{Z}_{3}$. If $u \in R^{*}$ is a unit then so is $\varphi(u)$. Hence, $\varphi(u)=\pm 1$, and $\varphi\left(u^{2}\right)=1$. As the squares of the units generate the additive group of $R$, this uniquely determines $\varphi$ so there is, at most, one such $\mathfrak{I}$.

Solution to 6.10.10: Let $\mathfrak{M}$ be the maximal ideal, so in particular $2 \notin \mathfrak{M}$. If $a \in 1+\mathfrak{M}$, then $a \notin \mathfrak{M}$, so the ideal ( $a$ ) of $R$ is not contained in any maximal ideal, so $(a)=(1)$, and $a$ is a unit. By assumption, $R^{*}$ is trivial, so $1+\mathfrak{M}$ has at most one element, and $\mathfrak{M}$ has at most one element. Thus $\mathfrak{M}=\{0\}$, and $R$ is a field. Hence $1=\#\left(R^{*}\right)=\# R-1$, and $\# R=2$. Therefore $R$ is the field of two elements, $\mathbb{Z}_{2}$, which can easily be verified to satisfy the condition.

Solution to 6.10.11: It suffices to show that if $a b-b a$ is any generator of $\mathfrak{I}$ and if $c$ is any element of $R$, then $a b c-b a c$ is in $\mathfrak{J}$. By the definition of $\mathfrak{I}, a(b c)-(b c) a$ is an element of $\mathfrak{I}$. Further, since $\mathfrak{I}$ is a left ideal, $b(c a-a c)=b c a-b a c$ is an element of $\mathfrak{J}$. Therefore, $a b c-b a c=a b c-b c a+b c a-b a c$ is in $\mathfrak{I}$, and we are done.

Solution to 6.10.12: Using the direct sum decomposition, there are elements $u_{i} \in \mathfrak{I}_{i}$ such that

$$
1=u_{1}+u_{2}+\cdots+u_{n} .
$$

If $a_{1} \in \mathfrak{I}_{1}$ then

$$
a_{1}=a_{1} \cdot 1=\left(u_{1}+u_{2}+\cdots+u_{n}\right)=a_{1} u_{1}+a_{2} u_{2}+\cdots+a_{n} u_{n} .
$$

Therefore $\left(a_{1} u_{1}-a_{1}\right)+a_{2} u_{2}+\cdots+a_{n} u_{n}=0$. Since the $i$ th summand is in $\mathfrak{I}_{i}$ and $R=\mathfrak{I}_{1} \oplus \mathfrak{I}_{2} \oplus \cdots \oplus \mathfrak{I}_{n}$, each summand is zero, i.e., $a_{1} u_{1}=a_{1}$ and $a_{1} u_{j}=0$ for $j \neq 1$. Similarly we get $a_{i} u_{i}=a_{i}$ and $a_{i} u_{j}=0$ for $j \neq i$.

Solution to 6.10.13: The fact that this map is a ring homomorphism follows from the fact that the inclusion map $R \rightarrow S$ is a ring homomorphism mapping $m R$ into $m S$. Let $1=a n+b m$, with integers $a, b$. Let $r \in R$ be in the kernel. Then $r=m s$ for some $s \in S$, so $r=(a m+b n) r=m(a r+b n s)$. Since $R$ has index $n$ in $S$, we have $n s \in R$ and so $r \in m R$. This shows that the map is an injection. Now suppose $s \in S$. Then $s=(a m+b n) s \equiv b(n s) \bmod m S$. As $n s \in R$, the map is a surjection.

Solution to 6.10.15: We have $\mathfrak{I}=\langle i\rangle$ and $\mathfrak{J}=\langle j\rangle$, for some $i, j \in R$. Suppose first that $\mathfrak{I}+\mathfrak{J}=R$. Then $1 \in \mathfrak{I}+\mathfrak{J}$, so $1=r i+s j$ for some $r, s \in R$. Therefore, the greatest common divisor of $i$ and $j$ is 1 . Now $\mathfrak{I} \mathfrak{J}$ and $\mathfrak{I} \cap \mathfrak{J}$ are both ideals and, clearly, have generators $i j$ and $k$, respectively, where $k$ is the least common multiple of $i$ and $j$. But the greatest common divisor of $i$ and $j$ is 1 , so $i j=k$, and $\mathfrak{I} \mathfrak{J}=\mathfrak{I} \cap \mathfrak{J}$. Since every implication in the previous argument can be reversed, if $\mathfrak{I} \mathfrak{J}=\mathfrak{I} \cap \mathfrak{J}$, then $\mathfrak{I}+\mathfrak{J}=R$ and we are done.

\subsection{Polynomials}

Solution to 6.11.1: If $P(z)$ is a polynomial of degree $n$ with $\alpha$ as a root, then $z^{n} P(1 / z)$ is a polynomial of degree at most $n$ with $1 / \alpha$ as a root, multiplying by an appropriate term $z^{k}$, we have a polynomial of degree $n$.

Solution to 6.11.2: We have

$$
x^{3}+2 x^{2}+7 x+1=\left(x-\alpha_{1}\right)\left(x-\alpha_{2}\right)\left(x-\alpha_{3}\right) .
$$

Equating coefficients we get

$$
\begin{aligned}
\alpha_{1}+\alpha_{2}+\alpha_{3} &=-2 \\
\alpha_{1} \alpha_{2}+\alpha_{2} \alpha_{3}+\alpha_{3} \alpha_{1} &=7 .
\end{aligned}
$$



\section{Therefore}

$$
\begin{aligned}
\alpha_{1}^{2}+\alpha_{2}^{2}+\alpha_{3}^{2} &=\left(\alpha_{1}+\alpha_{2}+\alpha_{3}\right)^{2}-2\left(\alpha_{1} \alpha_{2}+\alpha_{2} \alpha_{3}+\alpha_{3} \alpha_{1}\right) \\
&=-10
\end{aligned}
$$

For $i=1,2,3$

$$
\alpha_{i}^{3}+2 \alpha_{i}^{2}+7 \alpha_{i}+1=0
$$

Adding these equations we get

$$
\left(\alpha_{1}^{3}+\alpha_{2}^{3}+\alpha_{3}^{3}\right)+2\left(\alpha_{1}^{2}+\alpha_{2}^{2}+\alpha_{3}^{2}\right)+7\left(\alpha_{1}+\alpha_{2}+\alpha_{3}\right)+3=0
$$

hence,

$$
\alpha_{1}^{3}+\alpha_{2}^{3}+\alpha_{3}^{3}=31
$$

Solution to 6.11.3: Since $\zeta$ is a primitive seventh root of unity, we have

$$
\zeta^{6}+\zeta^{5}+\cdots+\zeta+1=0
$$

Dividing this by $\zeta^{3}$, we get

$$
\left(\zeta^{3}+\zeta^{-3}\right)+\left(\zeta^{2}+\zeta^{-2}\right)+\left(\zeta+\zeta^{-1}\right)+1=0
$$

As $\left(\zeta+\zeta^{-1}\right)^{2}=\left(\zeta^{2}+\zeta^{-2}\right)+2$ and $\left(\zeta+\zeta^{-1}\right)^{3}=\left(\zeta^{3}+\zeta^{-3}\right)+3\left(\zeta+\zeta^{-1}\right)$ the above equation becomes, letting $\alpha=\left(\zeta+\zeta^{-1}\right)$,

$$
\alpha^{3}+\alpha^{2}-2 \alpha-1=0
$$

Solution to 6.11.4: 1. Let $x=\sqrt{5}+\sqrt{7}$. Squaring and rearranging successively, we get

$$
\begin{aligned}
x-\sqrt{5} &=\sqrt{7} \\
x^{2}-2 \sqrt{5} x+5 &=7 \\
x^{2}-2 &=2 \sqrt{5} x \\
x^{4}-24 x^{2}+4 &=0
\end{aligned}
$$

This calculation shows that $\sqrt{5}+\sqrt{7}$ is a root of $f(x)=x^{4}-24 x^{2}+4$.

2. If $f$ had a linear factor, then it would have a rational root, but a calculation shows that none of $\pm 1, \pm 2$ is such a root (if $p / q$ in lowest terms is a root of $a_{n} x^{n}+\cdots+a_{0}$ then, $p \mid a_{0}$ and $\left.q \mid a_{n}\right)$. Suppose now that for some $a, b, c, d \in \mathbb{Z}$,

$$
f(x)=\left(x^{2}+a x+b\right)\left(x^{2}+c x+d\right) .
$$

Since the coefficient of $x^{3}$ in $f$ is zero, $c=-a$, so we have

$$
f(x)=\left(x^{2}+a x+b\right)\left(x^{2}-a x+d\right) .
$$

As the coefficient of $x$ in $f$ is zero, we get $a d-a b=0$. If $a=0$, then $f(x)=$ $\left(x^{2}+b\right)\left(x^{2}+d\right)=x^{4}+(b+d) x^{2}+b d$, but the equations $b d=4, b+d=-24$. have no integer solutions. If $b=d$, then $f(x)=\left(x^{2}+a x+b\right)\left(x^{2}-a x+b\right)=$ $x^{4}+\left(2 b-a^{2}\right) x^{2}+b^{2}$, so $b^{2}=4$ and $2 b-a^{2}=-24$, which also have no solutions in $\mathbb{Z}$.

Solution to 6.11.5: It is easy to see that $\sqrt{2}+\sqrt[3]{3}$ is a zero of a monic polynomial $p \in \mathbb{Z}[x]$ (use the process described on Problem 6.11.4.) If it were a rational number, it would have to be an integer, since its denominator would divide the leading coefficient of $p$. As $\sqrt{2}+\sqrt[3]{3}$ is strictly between 2 and 3 , it must be irrational.

Solution 2. Suppose $\sqrt{2}+\sqrt[3]{3} \in \mathbb{Q}$. Then $\mathbb{Q}(\sqrt{2})=\mathbb{Q}(\sqrt[3]{3})$. However, this contradicts the fact that the fields $\mathbb{Q}(\sqrt{2})$ and $\mathbb{Q}(\sqrt[3]{3})$ have degrees 2 and 3 over $\mathbb{Q}$, respectively, since by Eisenstein Criterion [Her75, p. 160], the polynomials $x^{2}-2$ and $x^{3}-3$ are irreducible over $\mathbb{Q}$.

Solution to 6.11.6: Suppose that $\omega$ is a primitive $k^{t h}$ root of unity and that $\omega_{i}=\omega^{i}$ for $1 \leqslant i \leqslant k$. Let $P(z)=a_{0}+a_{1} z+\cdots+a_{j} z^{j}(j<k)$; we have

$$
\frac{1}{k} \sum_{i=1}^{k} P\left(\omega^{i}\right)=\frac{1}{k} \sum_{i=1}^{k} \sum_{r=0}^{j} a_{r} \omega^{i r}=\frac{1}{k} \sum_{r=0}^{j} a_{r} \sum_{i=1}^{k} \omega^{i r}
$$

Since $\omega^{k}=1$, we have $\omega^{r k}-1=0$ for $1 \leqslant r \leqslant j$. Factoring and replacing 1 by $\omega^{r k}$, we get

$$
0=\left(\omega^{r}-1\right)\left(\omega^{r k}+\omega^{r(k-1)}+\cdots+\omega^{r}\right) .
$$

Since $r<k$ and $\omega$ is a primitive root of unity, $\omega^{r} \neq 1$. Therefore,

$$
\omega^{r k}+\omega^{r(k-1)}+\cdots+\omega^{r}=0 .
$$

Substituting this into the above equality gives

$$
\frac{1}{k} \sum_{i=1}^{k} P\left(\omega^{i}\right)=\frac{1}{k} \sum_{i=1}^{k} a_{0} \omega^{0 i}=a_{0}=P(0) .
$$

Solution to 6.11.7: By the Euclidean Algorithm, the vector space $V=\mathbb{Q}[x] /\langle f\rangle$ has dimension $d=\operatorname{deg}(f)$ : Therefore, the infinitely many equivalence classes

$$
\bar{x}^{2}, \bar{x}^{3}, \bar{x}^{5}, \ldots
$$

are linearly dependent in $V$, so we can let $q_{i}$ be a finite collection of rational numbers not all zero and satisfying

$$
q_{2} \bar{x}^{2}+q_{3} \bar{x}^{3}+q_{5} \bar{x}^{5}+\cdots=0 .
$$

This means that

$$
q_{2} x^{2}+q_{3} x^{3}+q_{5} x^{5}+\cdots=f(x) g(x)
$$

for some nonzero $g \in \mathbb{Q}[x]$

Solution to 6.11.8: First, note that each polynomial $p(x)$ in $\mathbb{Z}[x]$ is congruent modulo $x-7$ to a unique integer, namely the remainder one obtains by using the division algorithm to divide $x-7$ into $p(x)$. (Only integer coefficients arise in the process, because the coefficient of $x$ in $x-7$ is 1 .) If $p(x)$ lies in $\mathfrak{I}$, then so does the preceding remainder. However, the only members of $\mathfrak{I} \cap \mathbb{Z}$ are the integers that are divisible by 15 . In fact, if $k$ is in $\mathfrak{I} \cap \mathbb{Z}$, say $k=(x-7) q(x)+15 r(x)$, then $k=15 r(7)$. Hence, we get a well defined map from $\mathbb{Z}[x] / \mathfrak{I}$ into $\mathbb{Z}_{15}$ by sending $p(x)+\mathfrak{I}$ to $k+15 \mathbb{Z}$, where $k$ is the remainder one gets when dividing $p(x)$ by $x-7$. The map is clearly a homomorphism. If $p(x)$ is not a unit in $\mathfrak{I}$, then the remainder is clearly not divisible by 15 , from which we conclude that the map is one-to-one. The map is obviously surjective. It is, thus, the required isomorphism.

Solution 2. The map $\varphi: \mathbb{Z}[x] \rightarrow \mathbb{Z}[x]$ defined by $\varphi(p(x))=p(x+7)$ is a ring automorphism and it maps $\mathfrak{I}$ onto the ideal generated by $x$ and 15 . The quotient ring $\mathbb{Z}[x] / \varphi(\mathfrak{J})$ is isomorphic to $\mathbb{Z}_{15}$ under the map $p(x) \mapsto p(0)+15 \mathbb{Z}$, implying the desired conclusion.

Solution to 6.11.9: $\mathfrak{I}$ is prime. To show this we will prove that the quotient ring $\mathbb{Z}[x] / \mathfrak{J}$ is a field. Since $5 \in \mathfrak{I}$, this quotient ring is isomorphic to $\mathbb{Z}_{5}[x] /\left(x^{3}+x+1\right\rangle$. So it suffices to show that $x^{3}+x+1$ is irreducible $(\bmod 5)$. If it were reducible, it would have a linear factor, and, hence, a zero. But we can evaluate this polynomial for each $x \in \mathbb{Z}_{5}$ as follows:

\begin{tabular}{c|c}
$x$ & $x^{3}+x+1$ \\
\hline & \\
0 & 1 \\
1 & 3 \\
2 & 1 \\
3 & 1 \\
4 & 4
\end{tabular}

Since there is no zero, the polynomial is irreducible, and the quotient ring

$$
\frac{\mathbb{Z}_{5}[x]}{\left\langle x^{3}+x+1\right\rangle}
$$

is a field.

Solution to 6.11.12: Case 1: $p=2$. In this case, define a map of $F_{2}[x]$ into itself by $\varphi(1)=1$ and $\varphi(x)=x+1$, and extend it in the obvious way. Since constants are fixed and $\varphi(x+1)=x$, it is clear that this is a ring isomorphism. Further, $\varphi\left(x^{2}-2\right)=(x-1)^{2}-2=x^{2}+1=x^{2}-3$; we see that $\varphi$ maps the ideal $\left\langle x^{2}-2\right\rangle$ onto the ideal $\left\langle x^{2}-3\right\rangle$. It follows immediately from this that the two rings $\mathbf{F}_{2}[x] /\left\langle x^{2}-2\right\rangle$ and $\mathbf{F}_{2}[x] /\left\langle x^{2}-3\right\rangle$ are isomorphic.

Case 2: $p=5$. By checking all the elements of $F_{5}$, we see that $x^{2}-2$ and $x^{2}-3$ are both irreducible polynomials in $\mathrm{F}_{5}[x]$. Therefore, the ideals they generate are maximal and the quotient rings $\mathbf{F}_{5}[x] /\left\langle x^{2}-2\right\rangle$ and $\mathbf{F}_{5}[x] /\left\langle x^{2}-3\right\rangle$ are fields. The Euclidean Algorithm [Her75, p. 155] shows that each is a finite field with 25 elements. Since finite fields of the same order are isomorphic, the quotient rings in this case are isomorphic.

Case 3: $p=11$. In this case, checking all the elements of $\mathbf{F}_{11}$ shows that $x^{2}-2$ is irreducible, but $x^{2}-3=(x-5)(x+5)$ is not. Hence, the quotient ring $\mathbf{F}_{11}[x] /\left\langle x^{2}-2\right\rangle$ is a field, whereas $\mathbf{F}_{11}[x] /\left\langle x^{2}-3\right\rangle$ is not, so the two quotient rings are not isomorphic in this case.

Solution to 6.11.13: Since $x-3$ is a monic, given any polynomial $r(x)$ in $\mathbb{Z}[x]$, there exist polynomials $t(x)$ and $s(x)$ such that $r(x)=t(x)(x-3)+s(x)$ and $\operatorname{deg} s(x)<\operatorname{deg}(x-3)=1$. Hence, $s(x)$ is a constant, and so it is congruent modulo 7 to some $\alpha, 0 \leqslant \alpha \leqslant 6$. Hence, $r(x)-\alpha=t(x)(x-3)+(s(x)-\alpha)$, and the right-hand term is clearly an element of $\mathfrak{I}$.

In the special case where $r(x)=x^{250}+15 x^{14}+x^{2}+5$, we have, by the Euclidean Algorithm [Her75, p. 155], $r(x)=t(x)(x-3)+\alpha$. Substituting $x=3$, we get $r(3)=\alpha$. Since we only need to know $\alpha$ modulo 7, we reduce $r(3) \bmod$ 7 using the fact that $n^{7} \equiv n(\bmod 7)$, getting $r(3) \equiv 3^{4}+3^{2}+3^{2}+5 \equiv 6$ $(\bmod 7)$. Hence, $\alpha=6$ is the desired value.

Solution to 6.11.14: Let $\varphi: \mathbb{Z}[x] \rightarrow \mathbb{Z}_{13}$ be the unique ring homomorphism such that $\varphi(x)=4$. A polynomial $\alpha(x) \in \mathbb{Z}[x]$ is in the kernel of $\varphi$ if and only if $\alpha(4) \equiv 0(\bmod 13)$. This occurs if and only if $\alpha(x) \equiv(x-4) \beta(x)(\bmod 13)$ for some $\beta(x) \in \mathbb{Z}[x]$, i.e., exactly when $\alpha(x)=(x-4) \beta(x)+13 \gamma(x)$ for some $\gamma(x) \in \mathbb{Z}[x]$, in other words if and only if $\alpha(x) \in \mathfrak{I}$.

Set $f(x)=\left(x^{26}+x+1\right)^{73} \in \mathbb{Z}[x] ;$ then $f(x)-m \in \mathfrak{I}$ if and only if $\varphi(f(x)-m)=0$, which holds if and only if $f(4) \equiv m(\bmod 13)$.

By Fermat's Little Theorem [Sta89, p. 80], [Her75, p. 44], if $a \in \mathbb{Z}$ is not divisible by the prime $p$ then $a^{p-1} \equiv 1(\bmod p)$. This gives $\left(4^{26}+4+1\right) \equiv$ $\left(4^{2}+5\right) \equiv 8(\bmod 13)$, and $f(4) \equiv 8^{73} \equiv 8(\bmod 13)$. So $m=8$ is the unique integer in the range $0 \leqslant m \leqslant 12$ such that $\left(x^{26}+x+1\right)^{73}-m \in \mathfrak{J}$.

Solution to 6.11.15: Let $f(x)=x^{3}-2$, and denote by $\sqrt[3]{2}$ the real cube root of 2. Then $\mathbb{Q}(\sqrt[3]{2})$ is an algebraic extension of $\mathbb{Q}$ generated by a root of $f$, hence isomorphic to $K=\mathbb{Q}[x] /\left(x^{3}-2\right)$. Since $\mathbb{Q}(\sqrt[3]{2}) \subset \mathbb{R}$, and $x^{3}-2$ has only one real root, $x^{3}-2$ does not factor completely over $K$.

Solution to 6.11.16: 1. Let $K$ be the set of $f \in \mathbb{R}[x]$ for which $f(2)=f^{\prime}(2)=$ $f^{\prime \prime}(2)=0 . K$ is an ideal in the ring $\mathbb{R}[x]$ if

(i) $K$ is closed under addition and negation, and $0 \in K$, (ii) $g f$ and $f g$ belong to $K$ whenever $g \in \mathbb{R}[x]$ and $f \in K$.

Condition (i) is easily verified. For condition (ii), let $g \in \mathbb{R}[x], f \in K$. Then $(g f)^{\prime}=g^{\prime} f+g f^{\prime},(g f)^{\prime \prime}=g^{\prime \prime} f+2 g^{\prime} f^{\prime}+g f^{\prime \prime}$. We see that $(g f)(2)=$ $(g f)^{\prime}(2)=(g f)^{\prime \prime}(2)=0$, so that $g f$, and also $f g$, belongs to $K$. Thus $K$ is an ideal of $\mathbb{R}[x]$.

If $K$ is an (nonzero) ideal of $R=\mathbb{R}[x]$, let $a$ be a nonzero element of $K$ of lowest degree. If $b \in K$ then there exist $q, r \in R$ such that $b=a q+r$ with $r=0$ or $\operatorname{deg}(r)<\operatorname{deg}(a)$. Now $r=b-a q \in K$, so $\operatorname{deg}(r) \geqslant \operatorname{deg}(a)$. That is, $r=0$ and $b=a q$. We conclude that $K=a R$ is generated by $a$. Two generators of $K$ are associates and there is a unique monic generator. Expand in powers of $(x-2)$ :

$$
a=a_{0}+a_{1}(x-2)+a_{2}(x-2)^{2}+a_{3}(x-2)^{3}+\cdots+a_{n}(x-2)^{n} .
$$

Then $a_{0}=a(2)=0, a_{1}=a^{\prime}(2)=0, a_{2}=a^{\prime \prime}(2) / 2 !=0$. The monic generator is $(x-2)^{3}$.

2. The function $f(x)=x^{2}-6 x+8$ satisfies $f(2)=0=f^{\prime}(3)$. If $g(x)=x$ then $(g f)^{\prime}(3) \neq 0$. Thus the stated condition does not define an ideal.

Solution to 6.11.17: Let $\varphi: \mathbb{Z}[x] \rightarrow \mathbb{Z}[x]$ be any automorphism. Since $\varphi$ is determined by the value of $x$, every element of $\mathbb{Z}[x]$ must be a polynomial in $\varphi(x)$. In order to get $x$ in the image of $\varphi$, we see that $\varphi(x)$ must be of the form $\pm x+\alpha$ for some constant $\alpha$.

Solution to 6.11.19: The multiplicative group of units $\mathbb{Z}_{p}^{*}$ has $p-1$ elements. Therefore, $a^{p-1}=1$ for each $a \in \mathbb{Z}_{p}$, whence $a^{p}-a=0$ for all $a \in \mathbb{Z}_{p}$. The polynomial $x^{p}-x$ has $p$ distinct roots, thus factorizes as

$$
x^{p}-x=x(x-1) \ldots(x-(p-1)) \text {. }
$$

If $f(x)=u p_{i}^{\alpha_{1}} \ldots p_{k}^{\alpha_{k}}$, where $u$ is a unit and $p_{i}$ are distinct monic irreducible polynomials, then we see that the gcd of $f$ and $g$ is equal to the product of the distinct linear factors of $f$.

Solution to 6.11.20: Let $p=u_{1} a_{1}^{\alpha_{1}} \ldots a_{k}^{\alpha^{k}}, q=u_{2} a_{1}^{\beta_{1}} \ldots a_{k}^{\beta_{k}}$ where $a_{i}$ are monic irreducible in $\mathrm{F}[x]$, and $\alpha_{i}, \beta_{i} \geqslant 0, u_{i}$ units of $\mathrm{F}$. If $\gamma_{i}=\min \left\{\alpha_{i}, \beta_{i}\right\}$ then $h=a_{i}^{\gamma_{1}} \ldots a_{k}^{\gamma_{k}}$ is the $\operatorname{gcd}$ of $p$ and $q$ over $\mathbf{F}$.

The units of $\mathbf{F}[x]$ are the nonzero constants, and remain units in $\mathbf{K}[x]$. If $a_{i}=$ $\prod_{j} b_{i j}^{\alpha_{i j}}$ is the irreducible factorization of $a_{i}$ over $\mathbf{K}$ then $p=u_{1} \prod_{i} \prod_{j} b_{i j}^{\alpha_{i j}}$ is the irreducible factorization of $p$ over $\mathbf{K}$, by uniqueness of factorization. We thus obtain the irreducible factorizations of $p$ and $q$ over $\mathbf{K}$ by further factorizing the $a_{i}$ over $\mathbf{K}$.

If $a$ and $b$ are distinct irreducible polynomials over $\mathbf{F}$ then their gcd over $\mathbf{F}$ is 1. There exist polynomials $s, t$ over $\mathbf{F}$ with $s a+t b=1$. Thus, if $d \in \mathbf{K}[x]$ is the $\operatorname{gcd}$ of $a$ and $b$ over $\mathbf{K}$, then $d \mid 1$, and so $d$ is a unit of $\mathbf{K}[x]$, and $d=1$ as $d$ is chosen monic. Let $h, h^{\prime}$ be the gcds of $p$ and $q$ over $\mathrm{F}, \mathrm{K}$ respectively. Then $h$ divides $h^{\prime}$. By the above we cannot get any extra factors over $\mathbf{K}$ in $h^{\prime}$. Thus the gcd of $p$ and $q$ over $\mathbf{K}$ is the same as their $\operatorname{gcd}$ over $\mathbf{F}$.

Solution to 6.11.21: If the fraction $p / q$ in lowest terms is a zero of $x^{10}+x^{9}+x^{8}+$ $\cdots+x+1$, then $p \mid 1$ and $q \mid 1$, so the possible rational zeros are $\pm 1$. A calculation shows that neither of these is a zero, so the given polynomial is irreducible over $\mathbb{Q}$. Now $-1$ is a zero of the second polynomial, so it is reducible over $\mathbb{Q}$.

Solution to 6.11.22: Note that $539=7^{2} \cdot 11,511=7 \cdot 23$, and $847=7 \cdot 11^{2}$. Thus, all the coefficients except the leading one are divisible by 7 , but the constant term is not divisible by $7^{2}$. Since 7 is a prime, by Eisenstein Criterion [Her75, p. $160]$, the polynomial is irreducible in $\mathbb{Z}[x]$.

Solution to 6.11.23: By the Gauss Lemma [BML97, p. 85], an integral polynomial that can be factored over rationals can be factored into polynomials of the same degree over the integers. Since $\pm 1$ are not roots (and they are the only ones possible because $a_{0}=a_{n}=1$ ), there are no linear terms and the only possible factorizations are in polynomials of degree 2.

- $\left(x^{2}+a x+1\right)\left(x^{2}+b x+1\right)$

- $\left(x^{2}+a x-1\right)\left(x^{2}+b x-1\right)$

In the first, case we get $x^{4}+(a+b) x^{3}+(2+a b) x^{2}+(a+b) x+1$, which implies that the coefficients of the terms of degree 1 and 3 , are the same, a contradiction. The other case is analogous, showing that the polynomial is irreducible over $\mathbb{Q}$.

Solution to 6.11.24: We will use Eisenstein Criterion [Her75, p. 160]. Let $x=y+1$. We have

$$
\begin{aligned}
x^{p-1}+x^{p-2}+\cdots+1 &=(y+1)^{p-1}+(y+1)^{p-2}+\cdots+1 \\
&=\frac{(y+1)^{p}-1}{(y+1)-1} \\
&=\frac{\sum_{k=0}^{p}\left(\begin{array}{c}
p \\
k
\end{array}\right) y^{k}-1}{y} \\
&=\sum_{k=1}^{p}\left(\begin{array}{c}
p \\
k
\end{array}\right) y^{k-1} \\
&=y^{p-1}+p y^{p-2}+\cdots+p .
\end{aligned}
$$

Since the prime $p$ divides all the coefficients except the first and $p^{2}$ does not divide the last, it follows that the polynomial is irreducible in $\mathbb{Q}[x]$. Therefore, the given polynomial must also be irreducible, since if it were not, the same change of variables would give a factorization of the new polynomial. Solution to 6.11.25: Put $x=y+1$ to get

$$
f(x)=\frac{x^{5}-1}{x-1}+5 x=\frac{(y+1)^{5}-1}{y}+5 y+5 .
$$

The coefficients of $y^{3}, y^{2}, y$, and 1 are integers divisible by $p=5$ and the constant term is 10 , which is not divisible by $p^{2}$. Thus, by Eisenstein Criterion [Her75, p. 160], $f$ is irreducible over $\mathbb{Q}$.

Solution to 6.11.26: By the Gauss Lemma [BML97, p. 85], it is enough to show that $f(x)$ is irreducible over the integers. Suppose $f(x)=g(x) h(x)$, where $g(x)$ and $h(x)$ have positive degrees and integer coefficients, say

$$
g(x)=b_{j} x^{j}+b_{j-1} x^{j-1}+\ldots+b_{0}, \quad h(x)=c_{k} x^{k}+c_{k-1} x^{k-1}+\ldots+c_{0} .
$$

The we can see that $b_{0} c_{0}=25$, which can only happens in two ways, either $b_{0}=25, c_{0}=1$ or $b_{0}=5, c_{0}=5$.

Case 1: $b_{0}=25, c_{0}=1$.

Then $75=b_{1} c_{0}+c_{1} b_{0}=b_{1}+25 c_{1}$, so 25 divides $b_{1}$. Hence $-100=b_{2} c_{0}+$ $b_{1} c_{1}+b_{0} c_{2}=b_{2}+b_{1} c_{1}+25 c_{1}$, so 25 divides $b_{2}$. Continuing in this way, we find that 25 also divides $b_{3}$ and $b_{4}$. However 16 , the leading coefficient of $f(x)$, is given by

$$
16=b_{4} c_{1}+b_{3} c_{2}+b_{2} c_{3}+b_{1} c_{4}
$$

(since $j<5, k<5$ ), and 25 does not divide 16 . Therefore Case 1 is impossible. Case 2: $b_{0}=5, c_{0}=5$.

Then $75=5 b_{1}+5 c_{1},-100=5 b_{2}+b_{1} c_{1}+5 c_{2}$. The first of these equalities tells us that if either $b_{1}$ or $c_{1}$ is divisible by 5 then both are divisible by 5 . By the second equality, 5 divides $b_{1} c_{1}$. Hence 5 divides both $b_{1}$ and $c_{1}$.

Similarly, we have $50=5 b_{3}+b_{2} c_{1}+b_{1} c_{2}+5 c_{3}$ and $-125=5 b_{4}+b_{3} c_{1}+$ $b_{2} c_{2}+b_{1} c_{3}+5 b_{4}$. Exactly the same reasoning as before shows that 5 divides $b_{2}$ and $c_{2}$, again a contradiction with $(*)$, since 5 does not divide 16 .

Solution to 6.11.27: Let $f(x)=x^{3}+x+2$. A calculation shows that 2 is a zero of $f(x)$ over $\mathbb{Z}_{3}$, but 0 and 1 are not. Hence, we get the factorization $f(x)=(x-2)\left(x^{2}+2 x+2\right)=(x-2) g(x)$. Clearly, 0 and 1 are not roots of $g(x)$ since they are not roots of $f(x)$; another calculation shows that 2 is not a root of $g(x)$. Hence, $g(x)$ is irreducible, and the above factorization is the desired one.

Solution to 6.11.28: Let $f=x^{4}+x^{3}+x+3$. A calculation shows that $f$ has no zeros in $\mathbb{Z}_{5}$, so it contains no linear factors. Consider a product of two monic quadratics. Now $3=3 \cdot 1=2$ - 4. If $f=\left(x^{2}+a x+1\right)\left(x^{2}+b x+3\right)$, then, equating coefficients of powers of $x$, we get $a+b=1,4+a b=0$ and $3 a+b=1$. This equation has no solutions in $\mathbb{Z}_{5}$. If $f=\left(x^{2}+a x+2\right)\left(x^{2}+b x+4\right)$, then $a+b=1,1+a b=0$ and $4 a+2 b=1$. This equation has no solutions in $\mathbb{Z}_{5}$. Thus $x^{4}+x^{3}+x+3$ is irreducible in $\mathbb{Z}_{5}[x]$. Solution to 6.11.29: 1 . There are five distinct monic irreducible polynomials of degree $1: x, x+1, x+2, x+3, x+4$. Multiplying these in pairs we obtain reducible monic polynomials of type $p^{2}$ or type $p_{1} p_{2}$ with $p_{1} \neq p_{2}$. There are $5+\left(\begin{array}{l}5 \\ 2\end{array}\right)=15$ distinct reducible monic polynomials of degree 2 . The number of monic polynomials $x^{2}+a x+b$ is 25 . Therefore there are 10 monic irreducible polynomials of degree 2 .

2. There are 5 monic irreducible polynomials of degree 1 denoted $p$, and 10 of degree 2 denoted $q$. A reducible polynomial of degree 3 has the form $p^{3}$ (5 of these), or $p_{1}^{2} p_{2}\left(5 \times 4=20\right.$ of these), or $p_{1} p_{2} p_{3}\left(\left(\begin{array}{l}5 \\ 3\end{array}\right)=10\right.$ of these), or $p q$ $(5 \times 10=50$ of these $)$. There are $5^{3}=125$ monic polynomials $x^{3}+a x^{2}+b x+c$ of degree 3. Thus there are $125-85=40$ monic irreducible polynomials of degree 3.

Solution to 6.11.30: $x^{4}+1=\left(x^{2}+1\right)^{2}-2 x^{2}=\left(x^{2}-\sqrt{2} x+1\right)\left(x^{2}+\sqrt{2} x+1\right)$. Thus, $x^{4}+1$ is reducible over the real numbers.

The above expression is the irreducible factorization of $x^{4}+1$ over $\mathbb{R}$. The irreducible factorization over $\mathbb{Q}$ can be obtained from this by suitably grouping factors and can only be $x^{4}+1$ itself. Thus $x^{4}+1$ is irreducible over the rationals.

The field $\mathbf{F}_{16}$ has characteristic 2 ; that is, $1+1=0$. Thus $x^{4}+1=(x+1)^{4}$ is reducible over $\mathbf{F}_{16}$.

Solution to 6.11.31: In this solution we use the fact that a polynomial of degree 2 or 3 is reducible over a field iff it has a root there.

Decomposing $x^{4}-4:$

- over $\mathbb{R}$, we have

$$
x^{4}-4=(x-\sqrt{2})(x+\sqrt{2})\left(x^{2}+2\right),
$$

where $x^{2}+2$ is irreducible.

- over $\mathbb{Z}$, we have

$$
x^{2}-4=\left(x^{2}-2\right)\left(x^{2}+2\right)
$$

where both $x^{2}-2$ and $x^{2}+2$ are irreducible.

- over $\mathbb{Z}_{3}$, we have

$$
x^{4}-4=x^{4}-1=(x-1)\left(x^{3}+x^{2}+x+1\right)=(x-1)(x+1)\left(x^{2}+1\right)
$$

and $x^{2}+1$ is irreducible.

Decomposing $x^{3}-2$ :

- over $\mathbb{R}$, we have

$$
x^{3}-2=(x-\sqrt[3]{2})\left(x^{2}+\sqrt[3]{2} x+\sqrt[3]{4}\right)
$$

and $x^{2}+\sqrt[3]{2} x+\sqrt[3]{4}$ is irreducible. - over $\mathbb{Z}, x^{3}-2$ has no roots, and is irreducible.

- over $\mathbb{Z}_{3}$, we have

$$
x^{3}-2=x^{3}+1=(x+1)\left(x^{2}-x+1\right)=(x+1)(x+1)(x+1)=(x+1)^{3} .
$$

Solution to 6.11.32: Suppose, to the contrary, that $x^{p}-a$ has nontrivial factors $f(x)$ and $g(x)$ in $\mathbf{F}[x]$. Let $\mathbf{K}$ be a splitting field of $x^{p}-a$. Then, in $\mathbf{K}$, there are elements $a_{1}, \ldots, a_{p}$ such that $x^{p}-a=\left(x-a_{1}\right) \cdots\left(x-a_{p}\right)$. We may assume without loss of generality that $f(x)=\left(x-a_{1}\right) \cdots\left(x-a_{k}\right)$ and $g(x)=\left(x-a_{k+1}\right) \cdots\left(x-a_{p}\right)$. Therefore, $A=a_{1} \cdots a_{k}=\pm f(0)$ and $B=a_{k+1} \cdots a_{p}=\pm g(0)$ are both elements of $\mathbf{F}$. Further, since the $a_{j}$ 's are zeros of $x^{p}-a, a_{j}^{p}=a$ for all $j$. Hence, $A^{p}=a^{k}$ and $B^{p}=a^{p-k}$. Since $k$ and $p$ are relatively prime, there exist integers $x$ and $y$ such that $k x+p y=1$. Let $r=-y$ and $s=x+y$. Then $A^{s} / B^{r}$ is an element of $\mathrm{F}$ and

$$
\left(\frac{A^{s}}{B^{r}}\right)^{p}=\frac{a^{s} k}{a^{r p-r k}}=a^{k x+p y}=a .
$$

Hence, $a$ is a $p^{t h}$ power, contradicting our assumptions. Therefore, $x^{p}-a$ must be irreducible over $\mathbf{F}$.

Solution to 6.11.33: Suppose $g(x)$ (or $h(x)$ ) is not irreducible. Then $x^{4}+1$ has a linear factor and, hence, a zero. In other words, there is an element $a \in \mathbb{Z}_{p}$ with $a^{4}=-1$. It follows that $a^{8}=1$, and since $a^{4} \neq 1$ ( $p$ is odd), $a$ has order 8 in the multiplicative group $\mathbb{Z}_{p}^{*}$ of the field with $p$ elements. But $\mathbb{Z}_{p}^{*}$ is a group of order $p-1 \equiv 2(\bmod 4)$, so 8 cannot divide $p-1$, and we have a contradiction.

Solution to 6.11.34: Let $\operatorname{deg} f=n$. Then the collection of all real polynomials of deg $\leqslant n-1$ is an $m$-dimensional vector space. Hence, any collection of $n+1$ polynomials of degree $\leqslant n-1$ is linearly dependent. By the Euclidean Algorithm [Her75, p. 155], we have

$$
\begin{aligned}
& x^{2^{0}}=q_{0}(x) f(x)+r_{0}(x) \quad \text { with } \operatorname{deg} r_{0}<n \\
& x^{2^{1}}=q_{1}(x) f(x)+r_{1}(x) \text { with } \operatorname{deg} r_{1}<n \\
& x^{2^{n}}=q_{n}(x) f(x)+r_{n}(x) \text { with } \operatorname{deg} r_{n}<n .
\end{aligned}
$$

The polynomials $r_{0}, \ldots, r_{n}$ are linearly dependent, so, for some $\alpha_{i} \in \mathbb{R}$, we have

$$
\sum \alpha_{i} r_{i}(x)=0 .
$$

Therefore,

and $f(x) \mid p(x)$.

$$
p(x)=\sum \alpha_{i} x^{2^{i}}=f(x) \sum \alpha_{i} q_{i}(x)
$$

Solution to 6.11.35: Fix $f \in R \backslash F$ of least degree $n$, say. Choose polynomials $f_{1}, f_{2}, \ldots, f_{n-1}$ in $R$ such that $\operatorname{deg} f_{j} \equiv j \quad(\bmod n)$ and such that each $f_{j}$ is of least degree with this property, $1 \leqslant j \leqslant n-1$, if such a polynomial exists, otherwise take $f_{j}=0$. Let $f_{n}=f$. We will prove that $R=\mathbf{F}\left[f_{1}, f_{2}, \ldots, f_{n}\right]$. Suppose not, and fix $g \in R \backslash F\left[f_{1}, f_{2}, \ldots, f_{n}\right]$ of least degree, and suppose $\operatorname{deg} g \equiv j \quad(\bmod n)$. For some $k \geqslant 0, \operatorname{deg} g=\operatorname{deg}\left(f_{n}^{k} f_{j}\right)$. Hence, $g-\alpha f_{n}^{k} f_{j}$ is of lower degree than $g$ for some $\alpha \in \mathbf{F}$, and, by the minimality of $g$, must lie in $\mathbf{F}\left[f_{1}, f_{2}, \ldots, f_{n}\right]$. However, this implies that $g \in \mathbf{F}\left[f_{1}, f_{2}, \ldots, f_{n}\right]$ as well.

\subsection{Fields and Their Extensions}

Solution to 6.12.1: Since the $2 \times 2$ matrices over a field form a ring, to show that $R$ is a commutative ring with 1 it suffices to show that it is closed under addition and multiplication, commutative with respect to multiplication, and contains $I$. The first and the last are obvious, and the second follows almost as quickly from

$$
\left(\begin{array}{rr}
a & -b \\
b & a
\end{array}\right)\left(\begin{array}{rr}
c & -d \\
d & c
\end{array}\right)=\left(\begin{array}{rr}
a c-b d & -a d-b c \\
a d+b c & a c-b d
\end{array}\right)=\left(\begin{array}{rr}
c & -d \\
d & c
\end{array}\right)\left(\begin{array}{rr}
a & -b \\
b & a
\end{array}\right) .
$$

The inverse of a nonzero element in $R$ is given by

$$
\frac{1}{a^{2}+b^{2}}\left(\begin{array}{rr}
a & b \\
-b & a
\end{array}\right)
$$

which lies in $R$ provided that $a^{2}+b^{2} \neq 0$. If $\mathbf{F}=\mathbb{Q}$, then $a^{2}+b^{2}>0$ for $a$ and $b$ not both equal to 0 ; hence, every nonzero element of $R$ has an inverse, so $R$ is a field. If $F=\mathbb{C}$, then the matrix

$$
\left(\begin{array}{rr}
i & -1 \\
1 & i
\end{array}\right)
$$

has no inverse since $i^{2}+1^{2}=0$. Therefore, in this case, $R$ is not a field. Similarly, if $\mathbf{F}=\mathbb{Z}_{5}$, we have that $2^{2}+1^{2}=0$, so there exists a noninvertible matrix in $R$ and so $R$ is not a field. Finally, if $F=\mathbb{Z}_{7}$, the equation $a^{2}+b^{2}=0$ has no nonzero solutions, so every nonzero element of $R$ has an inverse and $R$ is a field.

Solution to 6.12.2: Let $R$ be a finite integral domain. Let $0 \neq b \in R$ and enumerate the elements of $R$ by $c_{1}, c_{2} \ldots, c_{n}$. Since $R$, has no zero divisors, cancellation shows that the elements $b c_{i}$ are distinct. Since there are $n$ of them, it follows that there is an element $c_{i_{0}}$ such that $b c_{i_{0}}=1$. Hence, $c_{i_{0}}$ is the inverse of $b$ and we are done.

Solution to 6.12.3: Since $a$ and $b$ are algebraic over $\mathbf{F}$, there exist integers $n$ and $m$ such that $[\mathbf{F}(a): \mathbf{F}]=n$ and $[\mathbf{F}(b): \mathbf{F}]=m$. Because $b$ is algebraic over $\mathbf{F}$, it must also be algebraic over $\mathbf{T}=\mathbf{F}(a)$ of degree, at most, $m$. Hence, $[\mathbf{T}(b): \mathbf{F}(a)] \leqslant m$, which implies

$$
[\mathbf{T}(b): \mathbf{F}]=[\mathbf{T}(b): \mathbf{F}(a)][\mathbf{F}(a): \mathbf{F}] \leqslant n m .
$$

Therefore, $\mathbf{T}(b)$ is a finite extension of $\mathbf{F}$. $\mathbf{T}(b)$ contains $a+b$ and so contains $\mathbf{F}(a+b)$; the latter must, therefore, be a finite extension of $\mathbf{F}$, so $a+b$ is algebraic over $\mathbf{F}$.

Solution to 6.12.4: From the fact that the group is finite, we can see that all elements are in the unit circle, otherwise consecutive powers would make an infinite sequence.

All elements of the group are roots of unity (maybe of different degrees), since a high enough power will end up in 1 (the order of an element always divides the order of the group). We will prove that they are all roots of unity of the same degree. Let $\alpha$ be the element with smallest positive argument $(\arg \alpha \in(0,2 \pi))$. We will show that this element generates the whole group. Suppose that there is an element $\beta$ that is not in the group generated by $\alpha$. There is a $p \in \mathbb{N}$ such that

$$
\arg \alpha^{p}<\arg \beta<\arg \alpha^{p+1}
$$

therefore,

$$
\arg \left(\beta \alpha^{-p}\right)<\arg \alpha
$$

which contradicts the minimality of arg $\alpha$. We conclude then that the group is generated by $\alpha$. See also the Solution to Problem 6.12.5.

Solution to 6.12.5: Let $G$ be a finite subgroup of $\mathrm{F}^{*}$ of order $n$. By the Structure Theorem for finite abelian groups [Her75, p. 109], there are integers $m_{1}\left|m_{2}\right| \cdots \mid m_{k}$ such that $G$ is isomorphic to $\mathbb{Z}_{m_{1}} \times \cdots \times \mathbb{Z}_{m_{k}}$. To show that $G$ is cyclic, it suffices to show that $m_{k}=n$. Suppose that $m_{k}<n$. From the structure of $G$ we know that $g^{m_{k}}=1$ for every $g \in G$. Hence, the polynomial $x^{m_{k}}-1$ has $n$ roots, contradicting the fact that a polynomial over a field has no more roots than its degree. Hence, $m_{k}=n$ and $G$ is cyclic.

Solution to 6.12.6: Let $\mathbf{K}_{+}$denote the additive group of $\mathbf{K}$. By the Structure Theorem on finitely generated abelian groups [Her75, p. 109], $K_{+} \cong \mathbb{Z}^{n} \oplus T$, where $n \geqslant 0$ and $T$ is a finite abelian group. Then $2 \mathbf{K}_{+} \cong(2 \mathbb{Z})^{n} \oplus(2 T)$. If $n>0$, then $2 \mathbf{K}$ is an ideal of $\mathbf{K}$ not equal to $\{0\}$ or $\mathbf{K}$, but this contradicts the assumption that $\mathrm{K}$ is a field. Thus $n=0$, and $\mathrm{K} \cong T$ is finite.

Solution to 6.12.7: Since $x^{3}-2$ is irreducible over $\mathbb{Q}[x],\left\langle x^{3}-2\right\rangle$ is a maximal ideal in $\mathbb{Q}[x]$, so $\mathbf{F}=\mathbb{Q}[x] /\left\langle x^{3}-2\right\rangle$ is a field. Using the relationship $\bar{x}^{3}=2$, we get that every element of $\mathbf{F}$ can be written in the form $a+b \bar{x}+c \bar{x}^{2}$, where $a, b, c \in \mathbb{Q}$. Further, such a representation is unique, since otherwise we could find $a, b, c \in \mathbb{Q}$, not all 0 , with $a+b \bar{x}+c \bar{x}^{2}=0$. On pulling back to $\mathbb{Q}[x]$, we find that $x^{3}-2$ divides $a+b x+c x^{2}$, a contradiction. Consider the map $\varphi: \mathbf{F} \rightarrow \mathbf{F}$ given by $\varphi(a)=a$ if $a \in \mathbb{Q}$, and $\varphi(\sqrt[3]{2})=\bar{x}$. Since $(\sqrt[3]{2})^{3}=2$ and $\bar{x}^{3}=2$ this extends to a ring epimorphism in the obvious way. It is also one-to-one, since if $\varphi(a+b \sqrt[3]{2}+c \sqrt[3]{4})=0$ then $a+b \bar{x}+c \bar{x}^{2}=0$, so $a=b=c=0$. Hence, $F$ is the isomorphic image of a field, and it is field.

Further, by the isomorphism we see that every element can be expressed uniquely in the desired form. In particular, $(1-\sqrt[3]{2})(-1-\sqrt[3]{2}-\sqrt[3]{4})=1$.

Solution to 6.12.8: Consider the ring homomorphism $\varphi: \mathbb{Z} \rightarrow \mathbb{F}$ that satisfies $\varphi(1)=1$. Since $F$ is finite, $\operatorname{ker} \varphi \neq 0$. Since $\mathbf{F}$ is a field, it has characteristic $p$, where $p$ is a prime number. Hence, $\operatorname{ker} \varphi$ contains the ideal $\langle p\rangle$, which is maximal. Therefore, $\operatorname{ker} \varphi=\langle p\rangle$, and the image of $\mathbb{Z}$ is a subfield of $F$ isomorphic to $\mathbb{Z}_{p}$. Identify $\mathbb{Z}_{p}$ with this subfield. Then $F$ is a vector space over $\mathbb{Z}_{p}$, and it must be of finite dimension since $\mathbf{F}$ is finite. Let $\operatorname{dim} \mathbf{F}=r$. A counting argument shows that $\mathbf{F}$ has $p^{r}$ elements.

Solution to 6.12.9: Let $a$ be any element of $\mathbf{K}$. Then $\mathbf{L}$ contains $a^{2}$ and $(a+1)^{2}$, hence it contains

$$
(a+1)^{2}-a^{2}-1=2 a .
$$

\section{Since $2 \neq 0, \mathbf{L}$ contains $a$.}

If $\mathbf{K}$ is the finite field of order $2^{n}$, then the multiplicative group of $\mathbf{K}$ is cyclic of order $2^{n}-1$, an odd number. The homomorphism $a \mapsto a^{2}$ on this group has a trivial kernel, hence it is surjective, therefore, every element of $K$ is a square. In this case, then, $\mathbf{L}=\mathbf{K}$.

To obtain a field of characteristic 2 in which the equality $\mathbf{L}=\mathbf{K}$ can fail, consider $\mathbf{K}=\mathbf{F}_{2}(x)$, the field of rational functions with coefficients in $\mathbf{F}_{2}$, the field of order 2. For $r(x)$ in $\mathrm{F}_{2}(x)$ we have $r(x)^{2}=r\left(x^{2}\right)$, so that the subfield $\mathbf{L}=\left\{r\left(x^{2}\right) \mid r(x) \in \mathbf{K}\right\}$ is the desired counterexample.

Solution to 6.12.10: Since $F$ has characteristic $p$, it contains a subfield isomorphic to $\mathbb{Z}_{p}$, which we identify with $\mathbb{Z}_{p}$. For $j \in \mathbb{Z}_{p}, \alpha+j$ is an element of $F(\alpha)$. Using the identity $(\alpha+j)^{p}=\alpha^{p}+j^{p}$, we get

$$
f(\alpha+j)=\alpha^{p}-\alpha+3+j^{p}-j=0 .
$$

Therefore, $f$ has $p$ roots in $F(\alpha)$, which are clearly distinct.

Solution to 6.12.11: The polynomial $x^{10}-2$ is irreducible over $\mathbb{Q}$, by Eisenstein's Criterion. Hence

$$
\mathbb{Q}(\sqrt[10]{2})=\mathbb{Q}[x] /\left\langle x^{10}-2\right\rangle
$$

has degree 10 over $\mathbb{Q}$.

Let $\varphi$ be an automorphism of $\mathbf{K}$. It is well known that $\varphi$ acts like the identity on $\mathbb{Q}$. Let $\alpha=\sqrt[10]{2}$. Then $\alpha$ and $-\alpha$ are the only roots of $x^{10}-2$ in $\mathbb{R}$ and hence in $\mathbf{K}$. Therefore, either $\varphi(\alpha)=\alpha$ or $\varphi(\alpha)=-\alpha$. In the first case $\varphi$ is the identity. It is easy to see that $p(\alpha) \mapsto p(-\alpha)(p \in \mathbb{Q}[x])$ defines an automorphism of $\mathbf{K}$. Thus, $\mathbf{K}$ has exactly two automorphisms. Solution to 6.12.12: Notice that $\mathbb{Q}[x]$ is an Euclidean domain, so the ideal $\langle f, g\rangle$ is generated by a single polynomial, $h$ say. $f$ and $g$ have a common root if and only if that root is also a root of $h$. We can use the Euclidean Algorithm to find $h=x^{2}+3 x+1$, which has roots $-3 / 2 \pm \sqrt{5} / 2$. Therefore, $f$ and $g$ have exactly two common roots, both in $\mathbb{Q}(\sqrt{5})$.

Solution to 6.12.13: Let $x$ be an element of $F$ that is not in $\mathbb{Q}$. Then $x$ satisfies an equation $a x^{2}+b x+c=0$ with $a, b$, and $c \in \mathbb{Q}$. Completing the square, we see that $\left(x+\frac{a}{2}\right)^{2} \in \mathbb{Q}$, whereas $\left(x+\frac{a}{2}\right) \notin \mathbb{Q}$. Let $\xi=\left(x+\frac{a}{2}\right)$. As $\xi^{2} \in \mathbb{Q}$, we can write it as $\frac{c^{2}}{d^{2}} m$, where $c, d, m \in \mathbb{Z}$ and $m$ is square free (i.e., $m$ has no multiple prime factors). As $\mathbf{F}=\mathbb{Q}(\xi)=\mathbb{Q}\left(\frac{d}{c} \xi\right)$, we get an isomorphism $\mathbf{F} \rightarrow \mathbb{Q}(\sqrt{m})$ by sending $r+s\left(\frac{d}{c} \xi\right)$ to $r+s \sqrt{m}$. The uniqueness of $m$ follows from the fact that the elements of $F$ that are not in $\mathbb{Q}$ but whose squares are in $\mathbb{Q}$ are those of the form $k \xi$ for some nonzero $k \in \mathbb{Q}$.

Solution to 6.12.14: Let $p_{1}=2, p_{2}=3, \ldots$, be the prime numbers and $F_{i}=\mathbb{Q}\left(\sqrt{p_{i}}\right)$. Claim: The fields $\mathbf{F}_{i}$ are pairwise nonisomorphic. Indeed, if $F_{i}$ were isomorphic to $\mathbf{F}_{j}$, then there would exist $r \in \mathbf{K}_{j}$ such that $r^{2}=p_{i}$. Write such an $r$ in the form

$$
r=a+b \sqrt{p_{j}} \quad a, b \in \mathbb{Q} .
$$

Then

$$
r^{2}=a^{2}+b^{2} p_{j}+2 a b \sqrt{p_{j}}=p_{i}
$$

if and only if $a b=0$. Therefore, either $p_{i}=c^{2}$ or $p_{j}=c^{2}$ for some $c \in \mathbb{Q}$, which contradicts the primality of $p_{i}$ and $p_{j}$.

\section{Solution to 6.12.15:}

$$
\begin{aligned}
\left(\cos \frac{\theta}{3}+i \sin \frac{\theta}{3}\right)^{3} &=\cos \theta+i \sin \theta \\
& \Rightarrow 3 \sin \frac{\theta}{3}-4 \sin ^{3} \frac{\theta}{3}=\sin \theta \\
& \Rightarrow E_{\theta} \supset F_{\theta}
\end{aligned}
$$

All the possibilities can occur. For example

$$
\begin{array}{lll}
\operatorname{dim}_{F_{\theta}} E_{\theta}=1 & \text { if } & \theta=\frac{\pi}{2} \\
\operatorname{dim}_{F_{\theta}} E_{\theta}=2 & \text { if } & \theta=\pi \\
\operatorname{dim}_{F_{\theta}} E_{\theta}=3 & \text { if } & \theta=\frac{\pi}{6}
\end{array}
$$

In the last example, $4 x^{3}-3 x+1 / 2$ or $8 x^{3}-6 x+1$ is irreducible because $\pm 1$, $\pm 1 / 2, \pm 1 / 4$, and $\pm 1 / 8$ are not roots of the above polynomial. Solution to 6.12.16: We first prove, using the Induction Principle [MH93, p. 7], that there are $n$ algebraically independent ([BML97, p. 73]) real numbers. The case $n=1$ is obvious. Suppose $\alpha_{1}, \ldots, \alpha_{n-1}$ are algebraically independent. Then $\mathbb{Q}\left(\alpha_{1}, \ldots, \alpha_{n-1}\right)$ is countable, and so is the the set of numbers algebraic over it. Let $\alpha_{n}$ be transcendent over $\mathbb{Q}\left(\alpha_{1}, \ldots, \alpha_{n-1}\right)$. Then clearly $\alpha_{1}, \ldots, \alpha_{n-1}, \alpha_{n}$ are independent too.

Let $\psi: \mathbb{Q}\left(t_{1}, \ldots, t_{n}\right) \rightarrow \mathbb{R}$ be defined by

$$
\frac{p\left(t_{1}, \ldots, t_{n}\right)}{q\left(t_{1}, \ldots, t_{n}\right)} \mapsto \frac{p\left(\alpha_{1}, \ldots, \alpha_{n}\right)}{q\left(\alpha_{1}, \ldots, \alpha_{n}\right)} .
$$

$\psi$ is clearly a homomorphism. If

$$
\frac{p_{1}\left(\alpha_{1}, \ldots, \alpha_{n}\right)}{q_{1}\left(\alpha_{1}, \ldots, \alpha_{n}\right)}=\frac{p_{2}\left(\alpha_{1}, \ldots, \alpha_{n}\right)}{q_{2}\left(\alpha_{1}, \ldots, \alpha_{n}\right)}
$$

then $\left(p_{1} q_{2}-q_{2} p_{1}\right)\left(\alpha_{1}, \ldots, \alpha_{n}\right)=0$ and the independence of the $\alpha$ 's gives $p_{1} / q_{1}=p_{2} / q_{2}$, so $\psi$ is injective, therefore it is an isomorphism over its range.

Solution to 6.12.18: We'll show that $\mathrm{K}\left[t^{a}, t^{b}\right]$ is a unique factorization domain, UFD for short, if and only if one of $a$ and $b$ divides the other. Let $d=\operatorname{gcd}(a, b)$, $a^{\prime}=a / d$ and $b^{\prime}=b / d$. Then $\mathbf{K}\left[t^{a}, t^{b}\right]$ is the image of $\mathrm{K}\left[u^{a^{\prime}}, u^{b^{\prime}}\right]$ under the injective $K$-algebra homomorphism $K[u] \rightarrow K[t]$ mapping $u$ to $t^{d}$, so we may reduce to the case that $\operatorname{gcd}(a, b)=1$. If $a=1$ or $b=1$, then $\mathrm{K}\left[t^{a}, t^{b}\right]=\mathbf{K}[t]$ is a UFD. It remains to show that if $a, b>1$ and $\operatorname{gcd}(a, b)=1$, then $\mathbf{K}\left[t^{a}, t^{b}\right]$ is not a UFD.

First we show that $t^{a}$ is an irreducible element of $\mathrm{K}\left[t^{a}, t^{b}\right]$. Since $t$ is an irreducible element of the UFD K[t $t$ with unit group $K^{*}$, the only ways to factor $t^{a}$ into nonunits of $\mathbf{K}[t]$ are as $\left(c t^{m}\right)\left(c^{-1} t^{a-m}\right)$ for some $c \in \mathbf{K}^{*}$ and $0<m<a$. But $m$ and $a-m$ cannot both be nonnegative integer combinations of $a$ and $b$ (they would have to be positive multiples of $b$, and their sum then could not be $a$ ), so $c t^{m}$ and $c^{-1} t^{a-m}$ cannot both lie in $\mathbf{K}\left[t^{a}, t^{b}\right]$. Thus $t^{a}$ is irreducible in $\mathbf{K}\left[t^{a}, t^{b}\right]$. Similarly $t^{b}$ is irreducible in $\mathrm{K}\left[t^{a}, t^{b}\right]$. Moreover $t^{a}$ and $t^{b}$ are not associate in $\mathbf{K}\left[t^{a}, t^{b}\right]$ because even in $\mathbf{K}[t]$ it is not true that each of them divides the other. Now $t^{a b}$ can be factored in $\mathrm{K}\left[t^{a}, t^{b}\right]$ either as $\left(t^{a}\right)^{b}$ or as $\left(t^{b}\right)^{a}$, violating unique factorization.

As an alternative to the argument in the previous paragraph, one could note that since $\operatorname{gcd}(a, b)=1$, there exist $r, s \in \mathbb{Z}$ with $r a+s b=1$, so the fraction field of $\mathbf{K}\left[t^{a}, t^{b}\right]$ (viewed as a subfield of $\mathbf{K}(t)$ ) contains $t=\left(t^{a}\right)^{r}\left(t^{b}\right)^{s}$, even though $\mathbf{K}\left[t^{a}, t^{b}\right]$ obviously does not contain $t$ if $a, b>1$. But $t$ is integral over $\mathbf{K}\left[t^{a}, t^{b}\right]$ because it satisfies the polynomial equation $x^{a}-t^{a}=0$. Hence $\mathrm{K}\left[t^{a}, t^{b}\right]$ is not integrally closed (in its fraction field). UFD's are integrally closed, so $\mathrm{K}\left[t^{a}, t^{b}\right]$ is not a UFD.

Solution to 6.12.19: 1. A $2 \times 2$ matrix over $F$ is invertible if and only if the first column is nonzero and the second is not a $\mathbf{F}$-multiple of the first. This gives $|F|^{2}-1=p^{2 n}-1$ possibilities for the first column of an invertible matrix, and, given the first column, $|\mathbf{F}|^{2}-|\mathbf{F}|=p^{2 n}-p^{n}$ for the second, hence the result. See also Problem 7.1.3 for a more general solution.

2. The map $\mathbf{F} \rightarrow G$ sending $a$ to $\left(\begin{array}{ll}1 & a \\ 0 & 1\end{array}\right)$ is easily checked to be an injective group homomorphism. Its image is a subgroup $S$ of $G$ that is isomorphic to the additive group of $\mathbf{F}$ and that, consequently, has order $p^{n}$. By 1., this is the largest power of $p$ dividing the order of $G$. Hence, $S$ is, in fact, a $p$-Sylow subgroup of $G$. Since all $p$-Sylow subgroups of a finite group are conjugate (and hence isomorphic), this implies the result.

Solution to 6.12.20: Assume $g \in G L_{n}\left(\mathbf{F}_{p}\right)$ has order $p^{k}$, and let $m(x)$ be the minimal polynomial of $g-1$. By Fermat's Little Theorem [Sta89, p. 80], [Her75, p. 44], $(x-1)^{p}=x^{p}-1(\bmod p)$, and iteration gives $(x-1)^{p^{\ell}}=x^{p^{\ell}}-1(\bmod$ $p)$ for all positive integers $\ell$. Since $g^{p^{k}}-1=0$, also $(g-1)^{p^{k}}=0$, so $m(x)$ divides $(x-1)^{p^{k}}$. By the same token, $m(x)$ does not divide $(x-1)^{p^{k-1}}$. Therefore $m(x)=(x-1)^{j}$ with $p^{k-1}<j \leqslant p^{k}$. But $j=\operatorname{deg} m(x) \leqslant n$, so the desired inequality follows.

For the other direction, assume first $p^{k-1}<n \leqslant p^{k}$. Let $g$ be the $n \times n$ matrix with 1 in each position on the main diagonal and immediately above the main diagonal, 0 elsewhere. Then $g-1$ is nilpotent of index $n$, so

$$
\begin{aligned}
&0=(g-1)^{p^{k}}=g^{p^{k}}-1, \\
&0 \neq(g-1)^{p^{k-1}}=g^{p^{k-1}}-1 .
\end{aligned}
$$

It follows that the order of $g$ divides $p^{k}$ but does not divide $p^{k-1}$, so it must equal $p^{k}$.

Finally, if $n>p^{k}$, we can get a $g$ of order $p^{k}$ by taking the direct sum of the preceding $g$ (corresponding to $n=p^{k}$ ) and an identity matrix of size $\left(n-p^{k}\right) \times$ $\left(n-p^{k}\right)$.

Solution to 6.12.21: 1 . We can write down the general commutator:

$$
\begin{gathered}
\left(\begin{array}{cc}
a & b \\
0 & a^{-1}
\end{array}\right)\left(\begin{array}{cc}
c & d \\
0 & c^{-1}
\end{array}\right)\left(\begin{array}{cc}
a^{-1} & -b \\
0 & a
\end{array}\right)\left(\begin{array}{cc}
c^{-1} & -d \\
0 & c
\end{array}\right) \\
=\left(\begin{array}{cc}
a c & a d+b c^{-1} \\
0 & a^{-1} c^{-1}
\end{array}\right)\left(\begin{array}{cc}
a^{-1} c^{-1} & -a^{-1} d-b c \\
0 & a c
\end{array}\right) \\
=\left(\begin{array}{cc}
1 & -c d-a b c^{2}+a^{2} c d+a b \\
0 & 1
\end{array}\right)
\end{gathered}
$$

The upper-right entry can be rewritten as $a b\left(1-c^{2}\right)-c d\left(1-a^{2}\right)$. This is 0 if $F$ is the field of two or the field of three elements. Otherwise we can take $d=0$ and $c$ such that $1-c^{2} \neq 0$. Then, by suitably choosing $a$ and $b$, we can make the upper-right entry equal any element of $\mathbf{F}$. Conclusion: the commutators are

![](https://cdn.mathpix.com/cropped/2022_10_26_a807eeb357d0f35abe4eg-022.jpg?height=84&width=1394&top_left_y=2342&top_left_x=360)
three elements, in which case the identity is the only commutator. In either case, the commutators form a group, which is the commutator subgroup. 2. Except in the trivial cases $p=2$ or $3, k=1$, the commutator subgroup is isomorphic to the additive group of $\mathbf{F}$, which is a vector space of dimension $k$ over the field of $p$ elements. The fewest number of generators is thus $k$.

Solution to 6.12.22: The zero element of $\mathbf{F}_{p}$ obviously has a unique square root and a unique cube root. Let $F_{p}^{*}$ denote the multiplicative group of nonzero elements of $\mathbf{F}_{p}$. It is a cyclic group of order $p-1$. Since $p-1$ is even, the homomorphism $x \rightarrow x^{2}$ of $F_{p}^{*}$ into itself has a kernel of order 2 , which means that its range has order $(p-1) / 2$. There are, thus, $1+(p-1) / 2$ elements of $\mathbf{F}_{p}$ with square roots.

If $p-1$ is not divisible by 3 , the homomorphism $x \rightarrow x^{3}$ of $\mathbf{F}_{p}^{*}$ into itself has a trivial kernel, and so every element of $\mathbf{F}_{p}$ has a cube root. If 3 divides $p-1$, then the preceding homomorphism has a kernel of order 3 , so its range has order $(p-1) / 3$. In this case, there are $1+(p-1) / 3$ elements of $F_{p}$ with cube roots.

Solution to 6.12.23: All functions are polynomials. A polynomial with the value 1 at 0 and 0 elsewhere is $p(x)=1-x^{q-1}$; from this one, we can construct any function by considering sums $\sum f_{i} \cdot p\left(x-x_{i}\right)$. Thus, there are $q^{q}$ such functions, and that many polynomials. Another way is to observe that all polynomials of degree, at most, $q-1$ define nonzero functions unless the polynomial is the zero polynomial.

Solution to 6.12.24: Let $K$ be that subfield. The homomorphism of multiplicative groups $\mathbf{F}^{*} \rightarrow \mathbf{K}^{*}$ sending $x$ to $x^{3}$ has a kernel of order, at most, 3, so $\left|\mathbf{K}^{*}\right| \geqslant$ $\left|\mathbf{F}^{*}\right| / 3$, that is, $(|\mathbf{F}|-1) /(|\mathbf{K}|-1) \leq 3$. Also, if the extension degree [F : $\mathbf{K}$ ] equals $n$, then $n \geqslant 2$, so $|\mathbf{F}|=|\mathbf{K}|^{n} \geqslant|\mathbf{K}|^{2}$, and $(|\mathbf{F}|-1) /(|\mathbf{K}|-1) \geqslant|\mathbf{K}|+1$, with equality if and only if $n=2$. Thus, $3 \geqslant|\mathbf{K}|+1$, which gives $|\mathbf{K}|=2, n=2$, and $|\mathbf{F}|=2^{2}=4$.

Solution to 6.12.25: As $A$ has dimension at least 2 as a vector space over $\mathbb{C}$, it contains an element $a$ which is not in the subspace spanned by the identity element $1 \in A$. Since $A$ has finite dimension over $\mathbb{C}$, there exists a complex number $\lambda$ such that $(a-\lambda 1) x=0$ for some nonzero $x \in A$. Let $\mathfrak{I}$ be the ideal generated by $b=a-\lambda 1$, and $\mathfrak{J}=\{x \in A \mid b x=0\}$ the annihilator of $b$. We have $\mathfrak{I} \cap \mathfrak{J}=\{0\}$ since all the elements of $\mathfrak{I} \cap \mathfrak{J}$ have zero square. As the dimensions of $\mathfrak{I}$ and $\mathfrak{J}$ add up to the dimension of $A$ we must have $A=\mathfrak{I} \oplus \mathfrak{J}$ as vector spaces over $\mathbb{C}$. Since $\mathfrak{I}$ and $\mathfrak{J}$ are ideals in $A, A=\mathfrak{I} \oplus \mathfrak{J}$ as rings. Let $1=e+f$ with $e \in \mathfrak{I}$ and $f \in \mathfrak{J}$. Then $e^{2}=e, f^{2}=f$ and neither of them is zero or 1 .

Solution to 6.12.26: Suppose that $A^{*}$ has order 5. We have $-1=1$ in $A$, since otherwise $A^{*}$ would have even order. Hence we have a homomorphism $\varphi: \mathrm{F}_{2}[x] \rightarrow A$ sending $x$ to a generator of $A^{*}$. The kernel of $\varphi$ is generated by a polynomial dividing $x^{5}-1$ in $F_{2}[x]$, but not dividing $x-1$, since the image of $x$ is not 1 . Now $x^{5}-1=(x-1)\left(x^{4}+x^{3}+x^{2}+x+1\right)$, and the latter is irreducible over $\mathbf{F}_{2}$, since otherwise it would have a factor of degree $\leqslant 2$, and $\mathbf{F}_{2}$ or $\mathbf{F}_{4}$ would contain a nontrivial fifth root of unity. Hence $\operatorname{ker} \varphi$ equals $\left(x^{4}+x^{3}+x^{2}+x+1\right)$ or $\left((x-1)\left(x^{4}+x^{3}+x^{2}+x+1\right)\right)$ and $A$ contains a subring $B=\mathrm{F}_{2}[x] / \operatorname{ker} \varphi$ isomorphic to $\mathrm{F}_{2^{4}}$ or $\mathrm{F}_{2} \times \mathrm{F}_{2^{4}}$. In either case, $\left|B^{*}\right|=15$, a contradiction.

Solution to 6.12.27: The relations imply $a b+b a=1$. In particular $a \neq 0 \neq b$. Multiplying the equality $a b+b a=1$ (from either side) by $a$ gives $a b a=a$, so $a b \neq 0 \neq b a$. Multiplying the same equality by $b$ gives $b a b=b$. The threeequalities $a b+b a=1, a b a=a, b a b=b$, imply that the four elements $a, b$, $a b, b a$ generate $R$ as a vector space over $K$. It is asserted that these four elements are linearly independent. In fact, suppose $u, v, w, x$ are in $\mathrm{K}$ and

$$
u a+v b+w a b+x b a=0 \text {. }
$$

Multiplying on the right by $b$, we get $u a b+x b=0$. Multiply this on the left by $a$ to get $x a b=0$, whence $x=0$. Thus $u a b=0$, whence $u=0$. Similarly $v=w=0$. Thus $R$ has dimension 4 over $\mathbf{K}$.

Let $T$ be the quotient of the noncommutative ring $K[X, Y]$ by the two-sided ideal generated by $X^{2}, Y^{2}$ and $(X+Y)^{2}-1$. It follows from the above that $T$ has dimension 4 and maps onto $R$, hence is isomorphic to $R$.

Now let $A=\left(\begin{array}{ll}0 & 1 \\ 0 & 0\end{array}\right)$ and $B=\left(\begin{array}{ll}0 & 0 \\ 1 & 0\end{array}\right)$. Then $A^{2}=B^{2}=0$ and $(A+B)^{2}=$ 1 and the matrices $A, B, A B, B A$ span $M_{2}(\mathbf{K})$. It follows from the above that $M_{2}(\mathrm{~K}) \simeq T \simeq R$

\subsection{Elementary Number Theory}

Solution to 6.13.1: Let the six people be Aline, Ana, Laura, Lucia, Manuel, and Stephanie. Fix one of them, Manuel, say. The five girls form two sets: $X$ (Manuel's friends) and $Y$ (the others). By the Pigeonhole Principle [Her75, p. 127], one of these sets has cardinality at least 3 . Suppose it is $X$ that contains at least three elements, say \{Aline, Laura, Stephanie\} $\subset X$. If Aline, Laura, and Stephanie are pairwise strangers, we are done. Otherwise, two of them are friends of each other, Stephanie and Aline, say. Then Manuel, Aline, and Stephanie are mutual friends. If the set with three or more elements is $Y$, a similar argument leads to the same conclusion.

Solution to 6.13.2: Suppose not. Changing signs, if necessary, we may assume that $M>0$ where $M$ is the maximum of the $u_{m, n}$ 's. Define the set

$$
A=\left\{(m, n) \mid u_{m, n}=M\right) \subset\{2,3, \ldots, N-1\} \times\{2,3, \ldots, M-1\} .
$$

and choose $(m, n) \in A$ with $m$ minimal. Since $(m-1, n) \notin A$, we have

$$
\frac{1}{4}\left(u_{m-1, n}+u_{m+1, n}+u_{m, n-1}+u_{m, n+1}\right)<\frac{1}{4}(M+M+M+M)=M=u_{m, n},
$$

which contradicts the relation. Solution to 6.13.3: There are $\left(\begin{array}{l}5 \\ 3\end{array}\right)$ possibilities for the range, so the answer is $10 \mathrm{~N}$, where $N$ is the number of surjective functions from $\{1,2,3,4,5\}$ to a given 3 -element set. The total number of functions $\{1,2, \ldots, 5\} \rightarrow\{1,2,3\}$ is $3^{5}$, from which we subtract $\left(\begin{array}{l}3 \\ 2\end{array}\right)$ (the number of 2-element subsets of $\left.\{1,2,3\}\right)$ times $2^{5}$ (the number of functions mapping into that subset), but then (according to the Principle of Inclusion-Exclusion) we must add back $\left(\begin{array}{l}3 \\ 1\end{array}\right)$ (the number of functions mapping into a 1-element subset of $\{1,2,3\})$. Thus

$$
N=3^{5}-\left(\begin{array}{l}
3 \\
2
\end{array}\right) 2^{5}+\left(\begin{array}{l}
3 \\
1
\end{array}\right) 1^{5}=150
$$

and the answer is $10 N=1500$.

Solution to 6.13.5: Let $x=0 . a_{1} a_{2} \ldots$ in base 3. If $a_{j}=1$ for some $j$, choose the smallest such $j$, and define

$$
x_{-}=0 . a_{1} a_{2} \ldots a_{j-1} 022222 \ldots, \quad x_{+}=0 . a_{1} a_{2} \ldots a_{j-1} 200000 \ldots
$$

These are the numbers from the Cantor set closest to $x$. Then $f\left(x_{-}\right)=f\left(x_{+}\right)$so $f$ is constant on $\left[x_{-}, x_{+}\right]$.

It suffices then to prove the inequality for $x=\sum a_{j} 3^{-j} \geqslant y=\sum b_{j} 3^{-j}$ with $a_{j}, b_{j} \in\{0,2\}$. Let $j_{*}$ be the smallest $j$ with $a_{j} \neq b_{j}$. Then $|x-y| \geqslant 3^{-j_{*}}$. On the other hand, we have,

$$
|f(x)-f(y)=| \sum_{j \geqslant j_{*}} \frac{a_{j}-b_{j}}{2} 2^{-j} \mid \leqslant \sum_{j \geqslant j_{*}} 2^{-j}=2 \cdot 2^{-j_{*}} .
$$

Combining, we obtain,

$$
|f(x)-f(y)| \leqslant 2 \cdot 2^{-j_{*}} \leqslant 2\left(3^{-j_{*}}\right)^{(\log 2) /(\log 3)} \leqslant 2|x-y|^{(\log 2) /(\log 3)} .
$$

Solution to 6.13.6: If $a=0$, the congruence has the trivial solution $x=0$. For $1 \leqslant a \leqslant p-1$, if $x^{2} \equiv a \quad(\bmod p)$, we have

$$
(p-x)^{2}=p^{2}-2 x p+x^{2} \equiv a \quad(\bmod p)
$$

so, for $a \neq 0$, there are two solutions of the quadratic congruence in each complete set of residues mod p. We conclude, then, that the total number is $1+(p-1) / 2=(p+1) / 2$.

Solution to 6.13.7: By Fermat's Little Theorem [Sta89, p. 80], [Her75, p. 44], we have, raising both sides of the congruence $-1 \equiv x^{2} \quad(\bmod p)$ to the power $(p-1) / 2$

$$
(-1)^{\frac{p-1}{2}} \equiv x^{p-1} \equiv 1 \quad(\bmod p)
$$

which implies that $(p-1) / 2$ is even, and the result follows. Solution to 6.13.8: Let $f(n)=2^{n}+n^{2}$. It suffices to show that $f(n)$ is composite if $n \equiv i \quad(\bmod 6)$ for $i \neq 3$. If $n$ is even, then $f(n)$ is an even number larger than 2 , a composite. Let $n=6 k+1$ for some integer $k$. We have

$$
\begin{aligned}
f(n) &=2^{6 k+1}+36 k^{2}+12 k+1 \\
& \equiv(-1)^{6 k} 2+1 \\
& \equiv 0(\bmod 3),
\end{aligned}
$$

so $f(n)$, being a multiple of 3 larger than 3 , is a composite. For $n=6 k+5$ we get

$$
\begin{aligned}
f(n) &=2^{6 k+5}+36 k^{2}+60 k+25 \\
& \equiv(-1)^{6 k} 2^{5}+25 \\
& \equiv 57 \\
& \equiv 0(\bmod 3),
\end{aligned}
$$

and again $f(n)$ is composite.

Solution to 6.13.9: 1. See the Solution to Part 1 of Problem 6.1.4.

2. The congruence $k a \equiv 1 \quad(\bmod n)$ is equivalent to the equation $k a=m n+1$ for some integer $m$. As all integer linear combinations of $k$ and $n$ are multiples of $\operatorname{gcd}\{k, n\}$, the first congruence has a solution iff $\operatorname{gcd}\{k, n\}=1$.

3. Let $\varphi$ be Euler's totient function [Sta89, p. 77], [Her75, p. 43]. As $\varphi$ is multiplicative, we have

$$
\varphi(n)=\varphi(p) \varphi(q)=(p-1)(q-1) .
$$

Solution to 6.13.10: Let $p(t)=3 t^{3}+10 t^{2}-3 t$ and $n / m \in \mathbb{Q} ; \operatorname{gcd}\{n, m\}=1$. We can assume $m \neq \pm 1$. If $p(n / m)=k \in \mathbb{Q}$, then $m \mid 3$ and $n \mid k$. Therefore, we have $m=\pm 3$.

Suppose $m=3$. We have

$$
p\left(\frac{n}{3}\right)=n\left(\frac{n^{2}}{9}+10 \frac{n}{9}-1\right) .
$$

This expression represents an integer exactly when $n^{2}+10 n=n(n+10) \equiv 0$ $(\bmod 9)$. As $\operatorname{gcd}\{n, 3\}=1$, this means $n+10 \equiv 0(\bmod 9)$, that is, $n \equiv 8$ $(\bmod 9)$.

A similar argument for the case $m=-3$ shows that the numbers $n /(-3)$ with $n \equiv 1(\bmod 9)$ produce integer values in $p$.

Solution to 6.13.11:

$$
\begin{aligned}
\left(\begin{array}{c}
1 / 2 \\
n
\end{array}\right) &=\frac{\left(\frac{1}{2}\right)\left(\frac{1}{2}-1\right) \cdots\left(\frac{1}{2}-(n-1)\right)}{n !}=\frac{(-1)^{n-1} \cdot 3 \cdot 5 \cdots(2 n-1)}{2^{n} n !} \\
&=\frac{(-1)^{n-1} \cdot 2 \cdot 3 \cdot 4 \cdot 5 \cdots 2 n}{\left(2^{n} n !\right)^{2}}=\frac{(-1)^{n-1}}{2^{2 n}} \cdot\left(\begin{array}{c}
2 n \\
n
\end{array}\right) .
\end{aligned}
$$

Solution to 6.13.12: A counting argument shows that the power of 2 which divides $n$ ! is given by

$$
\sum_{k \geqslant 1}\left\lfloor\frac{n}{2^{k}}\right\rfloor \text {, }
$$

where $\lfloor x\rfloor$ denotes the largest integer less than or equal to $x$. Since $c_{n}=\frac{(2 n) !}{(n !)^{2}}$, to show that $c_{n}$ is even it suffices to show that

$$
\sum_{k \geqslant 1}\left\lfloor\frac{2 n}{2^{k}}\right\rfloor>2 \sum_{k \geqslant 1}\left\lfloor\frac{n}{2^{k}}\right\rfloor .
$$

Suppose $2^{r} \leqslant n<2^{r+1}$. For $k \leqslant r$, there is an $r_{k}, 0 \leqslant r_{k}<1$, such that

$$
\frac{n}{2^{k}}=\left\lfloor\frac{n}{2^{k}}\right\rfloor+r_{k}
$$

or

so

$$
\frac{2 n}{2^{k}}=2\left\lfloor\frac{n}{2^{k}}\right\rfloor+2 r_{k}
$$

$$
\left\lfloor\frac{2 n}{2^{k}}\right\rfloor \geqslant 2\left\lfloor\frac{n}{2^{k}}\right\rfloor
$$

and equality holds if and only if $n$ is a power of 2 . For $k=r+1$, we have that $\left\lfloor n / 2^{r+1}\right\rfloor=0$ while $\left\lfloor 2 n / 2^{r+1}\right\rfloor=1$. Finally, for $k>r$, the terms in both sums are 0 . Hence, we see that the above inequality holds. Further, we see that the left side is 2 or more greater than the right side (i.e., $c_{n}$ is divisible by 4 ) if and only $n$ is not a power of 2 .

Solution to 6.13.13: We may assume that $n>3$. Converting the sum into a single fraction, we get

$$
\frac{n ! / 1+n ! / 2+\cdots+n ! / n}{n !} .
$$

Let $r$ be such that $2^{r} \mid n$ ! but $2^{r+1}$ does not divide $n$ !, and $s$ be such that $2^{s}$ is the largest power of 2 less than or equal to $n$. Since $n>3, r>s>0$. The only integer in $1, \ldots, n$, divisible by $2^{s}$ is $2^{s}$. Hence, for $1 \leqslant k \leqslant n, n ! / k$ is divisible by $2^{r-s}$, and every term except 1 is divisible by $2^{r-s+1}$. So

$$
\frac{n ! / 1+n ! / 2+\cdots+n ! / n}{n !}=\frac{2^{r-s}(2 j+1)}{2^{r} k}=\frac{2 j+1}{2^{s} k}
$$

for some integers $j$ and $k$. The numerator is odd and the denominator is even, so this fraction is never an integer.

Solution 2. Let $n \geqslant 4$. The Bertrand's Postulate asserts that there is at least one prime number in the interval $(n / 2, n]$, [HW79, Chap. 22]. Let $p$ be the largest such prime. Converting the sum into a single fraction, as above, we get

$$
\frac{n ! / 1+n ! / 2+\cdots+n ! / n}{n !}
$$

Clearly $p$ divides the denominator and all summands in the numerator except $n ! / p$, since $p>n / 2$.

Solution to 6.13.14: Recall that if $p_{1}, p_{2}, \ldots$ is the sequence of prime numbers and $x=\prod p_{i}^{\xi_{i}}$ and $y=\prod p_{i}^{\eta_{i}}$, we have

$$
\operatorname{gcd}\{x, y\}=\prod p_{i}^{\min \left\{\xi_{i}, \eta_{i}\right\}} \quad \operatorname{lcm}\{x, y\}=\prod p_{i}^{\max \left\{\xi_{i}, \eta_{i}\right\}} .
$$

Let

$$
a=\prod p_{i}^{\alpha_{i}} \quad b=\prod p_{i}^{\beta_{i}} \quad c=\prod p_{i}^{\gamma_{i}}
$$

we have

$$
\begin{aligned}
& \operatorname{gcd}\{a, \operatorname{lcm}\{b, c\}\}=\prod p_{i}^{\min \left\{\alpha_{i}, \max \left\{\beta_{i}, \gamma_{i}\right)\right\}} \\
& =\prod p_{i}^{\max \left\{\min \left\{\alpha_{i}, \beta_{i}\right\}, \min \left\{\alpha_{i}, \gamma_{i}\right\}\right\}} \\
& =\operatorname{lcm}\{\operatorname{gcd}\{a, b\}, \operatorname{gcd}\{a, c\}\} \text {. }
\end{aligned}
$$

Solution to 6.13.15: There are nine prime numbers $\leqslant 25$ :

$$
\begin{gathered}
p_{1}=2, \quad p_{2}=3, \quad p_{3}=5, \quad p_{4}=7, \quad p_{5}=11, \\
p_{6}=13, \quad p_{7}=17, \quad p_{8}=19, \quad p_{9}=23 .
\end{gathered}
$$

By unique factorization, for each $1 \leqslant a \leqslant 25$ there is an integer sequence $v(a)=\left(v_{j}(a)\right)_{j=1}^{9}$ with $a=\prod_{j=1}^{9} p_{j}^{v_{j}(a)}$. The 10 sequences $v\left(a_{i}\right) \in \mathbb{Q}^{9}$ must be linearly dependent, so

$$
\sum_{i=1}^{10} n_{i} v_{j}\left(a_{i}\right)=0
$$

for all $j$, for some rational numbers $n_{i}$ which are not all 0 . Multiplying by a common multiple of the denominators, we can assume that the $n_{i}$ 's are integers.

So

$$
\prod_{i=1}^{10} a_{i}^{n_{i}}=\prod_{j=1}^{9} p_{j}^{\sum_{i=1}^{10} n_{i} v_{j}\left(a_{i}\right)}=1, .
$$

as required.

Solution to 6.13.16: Denote the given number by $n$ and let $n=a^{13}$. By counting digits, we see that $n<10^{26}$, so $a<100$. As $8^{13}=7934527488$, we have $80^{13}<n$ and $a>80$. Note that $n \equiv 9(\bmod 10)$. The integers $c<10$ such that $c^{k} \equiv 9(\bmod 10)$ for some $k$ are 3,7 and 9. But $3^{4} \equiv 7^{4} \equiv 9^{4} \equiv 1 \quad(\bmod 10)$ and $13=3 \cdot 4+1$, so $c^{13} \equiv c^{3 \cdot 4} c \equiv c \equiv 9 \quad(\bmod 10)$ so $c=9$. Hence, $a=89$ or $a=99$. As 3 does not divide $n, 3$ does not divide $a$. Hence, $a=89$.

Solution to 6.13.17: Since $17 \equiv 7(\bmod 10)$,

$$
A \equiv 7^{17^{17}} \quad(\bmod 10) .
$$

Since $(7,10)=1$, we can apply Euler's Theorem [Sta89, p. 80], [Her75, p. 43]:

$$
7^{\varphi(10)} \equiv 1 \quad(\bmod 10) .
$$

The numbers $k$ such that $1 \leqslant k \leqslant 10$ and $(k, 10)=1$ are precisely $1,3,7$, and 9 , so $\varphi(10)=4$. Now $17 \equiv 1 \quad(\bmod 4)$, so $17^{17} \equiv 1 \quad(\bmod 4)$. Thus,

$$
7^{17^{17}} \equiv 7^{1} \equiv 7 \quad(\bmod 10)
$$

and the rightmost decimal digit of $A$ is 7 .

Solution to 6.13.18: As $23 \equiv 3(\bmod 10)$, it suffices to find $3^{23^{23^{23}}}(\bmod 10)$. We have $\varphi(10)=4$, where $\varphi$ is the Euler's totient function, and, by Euler's Theorem [Sta89, p. 80], [Her75, p. 43], $3^{r} \equiv 3^{s} \quad(\bmod 10)$ when $r \equiv s \quad(\bmod 4)$. So we will find $23^{23^{23}}(\bmod 4)$. We have $23 \equiv 3(\bmod 4)$, so $23^{23^{23}} \equiv 3^{23^{23}}$ $(\bmod 4)$. As $-1 \equiv 3 \quad(\bmod 4), 3^{23^{23}} \equiv(-1)^{23^{23}} \equiv-1 \quad(\bmod 4)$, because $23^{23}$ is odd. Hence, $23^{23^{23}} \equiv 3 \quad(\bmod 4)$, and $3^{23^{23^{23}}} \equiv 3^{3} \equiv 7 \quad(\bmod 10)$.

Solution to 6.13.19: Let

$$
\begin{aligned}
&N_{0}=\{0,1\} \cup\{4,5,6\} \cup\{11,12,13,14,15\} \cup \cdots \\
&N_{1}=\{2,3\} \cup\{7,8,9,10\} \cup\{16,17,18,19,20,21\} \cup \cdots
\end{aligned}
$$

We have

$$
N_{0} \cap N_{1}=\emptyset, \quad N_{0} \cup N_{1}=\mathbb{Z}_{+}
$$

and, clearly, neither can contain an arithmetic progression.

Solution to 6.13.20: Consider the ring $\mathbb{Z}_{a^{k}-1}$. Since $a>1,\left(a, a^{k}-1\right)=1$, so $a \in \mathbb{Z}_{a^{k}-1}^{*}$. Further, it is clear that $k$ is the least integer such that $a^{k} \equiv 1$ (mod $a^{k}-1$ ), so $k$ is the order of $a$ in $\mathbb{Z}_{a^{k}-1}^{*}$. Hence, by Lagrange's Theorem [Her75, p. 41], $k$ divides the order of the group $\mathbb{Z}_{a^{k}-1}^{*}$, which is $\varphi\left(a^{k}-1\right)$.

Solution to 6.13.21: Let $N$ be the desired greatest common divisor. By Fermat's Little Theorem [Sta89, p. 80], [Her75, p. 44], we have

$$
n^{13} \equiv\left(n^{6}\right)^{2} n \equiv\left(n^{3}\right)^{2} n \equiv n^{4} \equiv n^{2} \equiv n \quad(\bmod 2) .
$$

Hence, $2 \mid\left(n^{13}-n\right)$ for all $n$, so $2 \mid N$. An identical calculation shows that $p \mid N$ for $p \in\{3,5,7,13\}$. Since these are all prime, their product, 2730, divides $N$. However, $2^{13}-2=8190=3 \cdot 2730$, so $N$ is either 2730 or $3 \cdot 2730$. As $3^{13}-3=3\left(3^{12}-1\right)$ is not divisible by $9, N=2730$.

Solution to 6.13.22: Let

$$
n=p_{1}^{k_{1}} p_{2}^{k_{2}} \cdots p_{n}^{k_{n}}
$$

be the factorization into a product of prime powers $\left(p_{1}<p_{2}<\cdots<p_{n}\right)$ for $n$. The positive integer divisors of $n$ are then the numbers

$$
p_{1}^{j_{1}} p_{2}^{j_{2}} \cdots p_{n}^{j_{n}} \quad 0 \leqslant j \leqslant k_{j} .
$$

It follows that $d(n)$ is the number of $r$-tuples $\left(j_{1}, j_{2}, \ldots, j_{r}\right)$ satisfying the preceding conditions. In other words,

$$
d(n)=\left(k_{1}+1\right)\left(k_{2}+1\right) \cdots\left(k_{n}+1\right),
$$

which is odd iff each $k_{i}$ is even; in other words, iff $n$ is a perfect square.

Solution to 6.13.23: Only 2 and 8 do not have the required property. This can be proved in four steps as follows.

1. If the last digit of $n$ is 0 or 5 , every power of $n$ has the same last digit, so in particular $n^{n}$ does. Hence 0 and 5 have the required property.

2. Suppose $n$ is even and not divisible by 5 . Then $n^{n}$ is even and, by Fermat's Little Theorem [Sta89, p. 80], [Her75, p. 44], $n^{n} \bmod 5$ is determined by $n$ $\bmod 5$ and $n \bmod$ 4. By the Chinese Remainder Theorem [Sta89, p. 72] ( $n$ $\bmod 5, n \bmod 4)$ can be any pair in $\{1,2,3,4\} \times\{0,2\}$, so $n^{n}$ can be 1 or $4 \bmod 5$, and $n^{n}$ has last digit 6 or 4.

3. Suppose $n$ is odd and not divisible by 5 . Then $n^{n}$ is odd, and $n^{n} \bmod 5$ is determined by $(n \bmod 5, n \bmod 4)$ in $\{1,2,3,4\} \times\{1,3\}$. Therefore $n^{n} \bmod$ 5 can be anything in $\{1,2,3,4\}$, and $n^{n}$ has last digit $1,7,3$, or 9 .

4. Finally, each of the values $0,1,3,4,5,6,7,9$ occurs infinitely often, since the arguments above show that $n^{n} \bmod 10$ has period 20.

Solution 2. Let $\bar{n}=n \bmod 10$ denote the last digit of $n$. If $n=10 m+k$ with $0 \leqslant k \leqslant 9$, then

$$
\overline{n^{n}}=\overline{k^{n}}=\overline{k^{10 m+k}} .
$$

For fixed $k$, we get a sequence as $m$ increases. Each term is the image under $x \mapsto \overline{\overline{k^{10}} x}$ of the previous term, so once a value in the sequence is repeated, we can predict the rest of the sequence, which will be periodic. After computing $a_{k}=\overline{k^{k}}$ and $b_{k}=\overline{k^{10}}$ for $k$ up to 9 , we can immediately write down, for each $k$, the sequence that starts with $a_{k}$ (or $\overline{10^{10}}$ if $k=0$ ) and then iteratively multiply by $b_{k}$ and reduce modulo 10:

$$
\begin{array}{rll}
\overline{10^{10}}, \overline{20^{20}}, \ldots & =0,0,0,0,0,0, \ldots & \left(b_{0}=0\right) \\
\overline{1^{1}}, \overline{11^{11}}, \ldots & =1,1,1,1,1,1, \ldots & \left(b_{1}=1\right) \\
\overline{2^{2}}, \overline{12^{12}}, \ldots & =4,6,4,6,4,6, \ldots & \left(b_{2}=4\right) \\
\overline{3^{3}}, \overline{13^{13}}, \ldots & =7,3,7,3,7,3, \ldots & \left(b_{3}=9\right) \\
\overline{4^{4}}, \overline{14^{14}}, \ldots & =6,6,6,6,6,6, \ldots & \left(b_{4}=6\right) \\
\overline{5^{5}}, \overline{15^{15}}, \ldots & =5,5,5,5,5,5, \ldots & \left(b_{5}=5\right) \\
\overline{6^{6}}, \overline{16^{16}}, \ldots & =6,6,6,6,6,6, \ldots & \left(b_{6}=6\right)
\end{array}
$$



$$
\begin{array}{ll}
7^{7}, 17^{17}, \ldots=3,7,3,7,3,7, \ldots & \left(b_{7}=9\right) \\
\overline{8^{8}}, \overline{18^{18}}, \ldots=6,4,6,4,6,4, \ldots & \left(b_{8}=4\right) \\
\overline{9^{9}}, \overline{19^{19}}, \ldots=9,9,9,9,9,9, \ldots & \left(b_{9}=1\right)
\end{array}
$$

Thus $0,1,3,4,5,6,7,9$ occur infinitely often as $\overline{n^{n}}$, and 2,8 do not occur at all.

Solution to 6.13.24: We need to count the number of solutions of $x^{3}=1$ in the ring $\mathbb{Z}_{N}$. By the Chinese Remainder Theorem [Sta89, p. 72], $\mathbb{Z}_{N}$ is isomorphic, as a ring, to $\prod_{p \in\{2,3,5,7,11,13\}} \mathbb{Z}_{p}$. Hence the total number of solutions is $\prod_{p \in\{2,3,5,7,11,13\}} n_{p}$, where $n_{p}$ is the number of solutions of $x^{3}=1$ in each $\mathbb{Z}_{p}$. On the other hand $n_{p}$ is the number of elements of order dividing 3 in the multiplicative group $\left(\mathbb{Z}_{p}\right)^{*}$. Since $\left(\mathbb{Z}_{p}\right)^{*}$ is cyclic of order $p-1$, we have $n_{p}=3$ if 3 divides $p-1$, and 1 otherwise. Therefore, the answer is

$$
n_{2} n_{3} n_{5} n_{7} n_{11} n_{13}=1 \cdot 1 \cdot 1 \cdot 3 \cdot 1 \cdot 3=9 .
$$



\section{Linear Algebra}

\subsection{Vector Spaces}

Solution to 7.1.1: 1 . Is sufficient. The evaluation map that assigns to each polynomial its value at 1 is a nonzero linear functional on the linear space of polynomials of degree at most 4. Like all such functionals, its null space, having codimension 1 , has dimension 3. The four polynomials being in that 3 dimensional null space are therefore a dependent set.

2. Is not sufficient. The four polynomials $1,1+x, 1+x^{2}, 1+x^{3}$ form a counterexample.

Solution to 7.1.2: Following by induction on $k$, we can easily see that the function $F^{(k)}$ is a linear combination of the $m n$ functions $f^{(i)} g(j)$ for $0 \leqslant i<n, 0 \leqslant j<$ $m$, with constant coefficients. Hence, the $m n+1$ functions $F^{(0)}, \ldots, F^{(m n)}$ are linearly dependent over $\mathbb{C}$.

Solution to 7.1.3: 1. Every element of $V$ can be uniquely written in the form $a_{1} v_{1}+\cdots+a_{n} v_{n}$, where the $v_{i}$ 's form a basis of $V$ and the $a_{i}$ 's are elements of F. Since $\mathbf{F}$ has $q$ elements it follows that $V$ has $q^{n}$ elements.

2. A matrix $A$ in $G L_{n}(F)$ is nonsingular if and only if its columns are linearly independent vectors in $F^{n}$. Therefore, the first column $A_{1}$ can be any nonzero vector in $\mathbf{F}^{n}$, so there are $q^{n}-1$ possibilities. Once the first column is chosen, the second column, $A_{2}$, can be any vector which is not a multiple of the first, that is, $A_{2} \neq c A_{1}$, where $c \in \mathbf{F}$, leaving $q^{n}-q$ choices for $A_{2}$. In general, the $i^{t h}$ column $A_{i}$ can be any vector which cannot be written in the form $c_{1} A_{1}+c_{2} A_{2}+\cdots+c_{i-1} A_{i-1}$ where $c_{j} \in \mathbf{F}$. Hence, there are $q^{n}-q^{i-1}$ possibilities for $A_{i}$. By multiplying these together we see that the order of $G L_{n}(F)$ is then $\left(q^{n}-1\right)\left(q^{n}-q\right) \cdots\left(q^{n}-q^{n-1}\right)$.

3. The determinant clearly induces a homomorphism from $G L_{n}(F)$ onto the multiplicative group $\mathbf{F}^{*}$, which has $q-1$ elements. The kernel of the homomorphism is $S L_{n}(F)$, and the cosets with respect to this kernel are the elements of $G L_{n}(F)$ which have the same determinant. Since all cosets of a group must have the same order, it follows that the order of $S L_{n}(F)$ is $\left|G L_{n}(F)\right| /(q-1)$.

Solution to 7.1.4: Suppose $v \in V$ is nonzero. Let $S_{v}=\{A \in$ End $V \mid A v=v\}$. Choose $w \in V$ so that $\{v, w\}$ is a basis. With respect to this basis, $S_{v}$ corresponds

![](https://cdn.mathpix.com/cropped/2022_10_26_a807eeb357d0f35abe4eg-034.jpg?height=75&width=1385&top_left_y=770&top_left_x=370)
if $v$ and $v^{\prime}$ are $F$-multiples of each other, and otherwise $S_{v} \cap S_{v^{\prime}}=\{I\}$, since if an endomorphism fixes a basis, it is the identity. There are $\left(q^{2}-1\right) /(q-1)=$ $q+1$ nonzero vectors in $V$ modulo the action of $F^{*}$, so the total number of endomorphisms fixing some nonzero vector is $(q+1) q^{2}-q$, where the $-q$ is there to avoid counting the identity $q+1$ times. Thus the answer is $q^{3}+q^{2}-q$.

Solution 2. We are asked for the set of endomorphisms having 1 as an eigenvalue. There is a bijection between this set and the set of endomorphisms of determinant zero, taking $A$ to $A-I$. Therefore we need to count the number of solutions to $a d-b c=0$ with $a, b, c, d \in F$.

The number of solutions with $a \neq 0$ is $(q-1) q^{2}$, since for each choice of $a \in F^{*}$ and $b, c \in F$, solving the equation for $d$ yields a unique solution. The number of solutions with $a=0$ equals $q(2 q-1)$, since $d$ is arbitrary, and there are $2 q-1$ pairs $(b, c)$ with $b=0$ or $c=0$ (there are $q$ with $b=0$, and $q$ with $c=0$, but $(b, c)=(0,0)$ has been double counted). Thus the answer is $(q-1) q^{2}+q(2 q-1)=q^{3}+q^{2}-q$.

Solution to 7.1.5: If $p$ is prime then the order of $G L_{2}\left(\mathbb{Z}_{p}\right)$ is the number of ordered bases of a two-dimensional vector space over the field $\mathbb{Z}_{p}$, namely $\left(p^{2}-1\right)\left(p^{2}-p\right)$, as in the solution to Part 2 of Problem 7.1.3 above.

A square matrix $A$ over $\mathbb{Z}_{p^{n}}$ is invertible when $\operatorname{det}(A)$ is invertible modulo $p^{n}$, which happens exactly when $\operatorname{det}(A)$ is not a multiple of $p$. Let $\rho(A)$ denote the matrix over $\mathbb{Z}_{p}$ obtained from $A$ by reducing all its entries modulo $p$. We have $\operatorname{det}(\rho(A)) \equiv \operatorname{det}(A) \quad(\bmod p)$, thus

$$
A \in G L_{2}\left(\mathbb{Z}_{p^{n}}\right) \text { iff } \rho(A) \in G L_{2}\left(\mathbb{Z}_{p}\right),
$$

giving a surjective homomorphism

$$
\rho: G L_{2}\left(\mathbb{Z}_{p^{n}}\right) \rightarrow G L_{2}\left(\mathbb{Z}_{p}\right) .
$$

The kernel of $\rho$ is composed of the $2 \times 2$ matrices that reduce to the Identity modulo $p$ so the diagonal entries come from the set $\left\{1, p+1,2 p+1, \ldots, p^{n}-\right.$ $p+1)$ and the off-diagonal are drawn from the set that reduce to 0 modulo $p$, that is, $\left\{0, p, 2 p, \ldots, p^{n}-p\right\}$. Both sets have cardinality $p^{n-1}$, so the order of the kernel is $\left(p^{n-1}\right)^{4}$, and order of $G L_{2}\left(\mathbb{Z}_{p}\right)$ is

$$
p^{4 n-4}\left(p^{2}-1\right)\left(p^{2}-p\right)=p^{4 n-3}(p-1)\left(p^{2}-1\right) .
$$

Solution to 7.1.6: The multiplicative group $F_{p^{n}}^{*}$ of the field of $p^{n}$ elements is cyclic; let $g$ be a generator. Then the map $T: \mathbf{F}_{p^{n}} \rightarrow \mathbf{F}_{p^{n}}$ sending $x$ to $g x$ is an $\mathbf{F}_{p}$-linear map acting as a single cycle on the nonzero elements of $\mathbf{F}_{p^{n}}$. The matrix of $T$ with respect to a basis for $\mathbf{F}_{p^{n}}$ over $\mathbf{F}_{p}$ has the desired property.

Solution to 7.1.7: Let $\mathbf{F}=\{0,1, a, b\}$. The lines through the origin can have slopes $0,1, a, b$, or $\infty$, so $S$ has cardinality 5 . Let $L_{s}$ be the line through the origin with slope $y$. Suppose $\gamma \in G$ fixes all these lines, to be specific say

$$
\gamma=\left(\begin{array}{ll}
x & y \\
z & w
\end{array}\right) .
$$

Then

$$
\gamma L_{0}=L_{0}
$$

implies that

$$
\gamma(1,0)^{t}=(x, z)^{t}=(c, 0)^{t}
$$

for some $c \neq 0$. Thus, $z=0$. Similarly, the invariance of $L_{\infty}$ implies $y=0$ and of $L_{1}$ implies $x=w$. Then $\operatorname{det}(\gamma)=x^{2}=1$ and since $\mathbf{F}$ has characteristic 2 , we must have $x=1$ and $\gamma$ is the identity.

Solution to 7.1.8: $1 . \mathbf{F}[x]$ is a ring under polynomial addition and multiplication because $\mathbf{F}$ is a ring. The other three axioms of vector addition - associativity, uniqueness of the zero, and inverse - are trivial to verify; as for scalar multiplication, there is a unit (same as in $\mathbf{F}$ ) and all four axioms are trivial to verify, making it a vector field.

2. To see this, observe that the set $\left\{1, x, x^{2}, \ldots, x^{n}\right\}$ form a basis for this space, because any linear combination will be zero, if and only if, all coefficients are zero, by looking at the degree on both sides.

3. An argument as above shows that

$$
a_{0} 1+a_{1}(x-a)+\cdots+a_{n}(x-a)^{n}=0
$$

only if the coefficients are all zero.

Solution to 7.1.9: For any two finite-dimensional subspaces $X$ and $Y$,

$$
\operatorname{dim}(X+Y)+\operatorname{dim}(X \cap Y)=\operatorname{dim}(X)+\operatorname{dim}(Y),
$$

which you get by applying the Rank-Nullity Theorem [HK61, p. 71] to the quotient map $X \rightarrow(X+Y) / Y$ (in this case the dimension of the kernel of the map is $\operatorname{dim}(X \cap Y)$, and the dimension of the range is $\operatorname{dim}(X+Y)-\operatorname{dim}(Y))$. Thus,

$$
\begin{aligned}
\operatorname{dim}(U)+\operatorname{dim}(V)+\operatorname{dim}(W)-\operatorname{dim}(U+V+W) \\
&=\operatorname{dim}(U)+\operatorname{dim}(V)-\operatorname{dim}(U+V)+\operatorname{dim}((U+V) \cap W)) \\
& \geqslant \operatorname{dim}(U)+\operatorname{dim}(V)-\operatorname{dim}(U+V) \\
&=\operatorname{dim}(U \cap V)
\end{aligned}
$$

Observe that it is not true for subspaces of a vector space $(U+V) \cap W=$ $(U \cap W)+(V \cap W)$

Solution to 7.1.10: Let $Y$ denote the given intersection. Then $Y$ is a subspace of $V$ and, clearly, $W \subset Y$. Suppose that there exists a nonzero vector $v \in Y \backslash W$. Since $v$ is not in $W$, a set consisting of $v$ and a basis for $W$ is linearly independent. Extend this to a basis of $V$, and let $Z$ be the $(n-1)$-dimensional subspace obtained by omitting $v$ from this basis. Then $W \subset Z$, so $Z$ is a term in the intersection used to define $Y$. However, $v$ is not in $Z$, so $v$ cannot be an element of $Y$, a contradiction. Hence, $Y \subset W$ and we are done.

Solution to 7.1.11: We use the Induction Principle [MH93, p. 7] on the dimension of $V$. If $\operatorname{dim} V=1$, the only proper subspace is $\{0\}$, so $V$ is clearly not the union of a finite number of proper subspaces.

Now suppose the result is true for dimension $n-1$ and that there is a $V$, $\operatorname{dim} V=n$, with

$$
V=\bigcup_{i=1}^{k} w_{i} \text {, }
$$

where we may assume $\operatorname{dim} W_{i}=n-1,1 \leqslant i \leqslant k$. Suppose that there existed a subspace $W$ of $V$ of dimension $n-1$ which was not equal to any of the $W_{i}$ 's. We have

$$
W=\bigcup_{i=1}^{n}\left(W \cap W_{i}\right) .
$$

But $\operatorname{dim}\left(W \cap W_{i}\right) \leqslant n-2$, and this contradicts our induction hypothesis.

Therefore, to complete the proof, it remains to show that such a subspace $W$ exists. Fix a basis $x_{1}, \ldots, x_{n}$ of $V$. For each $\alpha \in \mathbf{F}, \alpha \neq 0$, consider the $(n-1)$-dimensional subspace given by

$$
W_{\alpha}=\left\{a_{1} x_{1}+\cdots+a_{n} x_{n} \mid a_{1}+\cdots+a_{n-1}+\alpha a_{n}=0\right\} .
$$

Any two of these subspaces intersect in a subspace of dimension, at most, $n-2$, so they are distinct. Since there are infinitely many of these, because $\mathbf{F}$ is infinite, we can find $W$ as desired.

Solution 2. Suppose that $V=\cup_{1 \leqslant i \leqslant k} V_{i}$. After discarding superfluous $V_{i}$ 's, we may assume that

$$
V \neq \bigcup_{i \neq i_{0}} V_{i} \quad \text { for all } \quad 1 \leqslant i_{0} \leqslant k .
$$

Then $k \geqslant 2$, and there must be vectors $v_{1}, v_{2}$ in $V$ such that

$$
\text { (1) } v_{1} \in V_{1} \backslash \bigcup_{i \neq 1} V_{i} \text { and } \quad \text { (2) } v_{2} \in V_{2} \backslash \bigcup_{i \neq 2} V_{i} \text {. }
$$

Let $\left(x_{s}\right)$ be sequence of distinct, nonzero elements of the field. Then, for each $s$, the vector $u_{s}=v_{1}+x_{s} v_{2}$ does not lie in $V_{1} \cup V_{2}$ (if $u_{s} \in V_{1}$, then $v_{2}=\left(u_{s}-v_{1}\right) / x_{s} \in V_{1}$, contradicting (1); similarly, $u_{s} \notin V_{2}$.) It follows that, for all $s, u_{s} \in \cup_{i \neq 1,2} V_{i}$. Since the vectors $u_{s}$ are all distinct, it follows that, for some $s \neq s^{\prime}$ and $i \neq 1,2, u_{s}$ and $u_{s^{\prime}}$ lie in $V_{i}$. But then $v_{2}=\left(u_{s}-u_{s^{\prime}}\right) /\left(x_{s}-x_{s^{\prime}}\right) \in V_{i}$, contradicting (2). Hence, $V \neq \cup_{1 \leqslant i \leqslant k} V_{i}$.

Solution to 7.1.12: Note first that if $A$ and $B$ are matrices and $C$ is an invertible matrix, then

$$
A B=B A \text { iff } C^{-1} A C C^{-1} B C=C^{-1} B C C^{-1} A C \text {. }
$$

Also, if $D_{1}, \ldots, D_{n}$ are linearly independent matrices, so are the matrices $C^{-1} D_{1} C, \ldots, C^{-1} D_{n} C$. We may then assume that $A$ is in Jordan Canonical Form [HK61, p. 247].

$$
\text { A direct calculation shows that if } \tilde{A}=\left(\begin{array}{cccc}
a & 1 & \cdots & 0 \\
& \ddots & \ddots & \\
& & & 1 \\
0 & & & a
\end{array}\right) \text { is a } k \times k \text { Jordan }
$$

$$
\begin{aligned}
& \text { block, then } \tilde{A} \text { commutes with } \tilde{B}=\left(\begin{array}{llll}b_{1} & b_{2} & & b_{k} \\& \ddots & \ddots & b_{2} \\0 & & & b_{1}\end{array}\right) \text {. }
\end{aligned}
$$

Therefore, by block multiplication, $A$ commutes with any matrix of the form

$$
B=\left(\begin{array}{ccc}
\tilde{B}_{1} & & \\
& \ddots & \\
& & \tilde{B}_{r}
\end{array}\right)
$$

where the $\tilde{B}_{r}$ 's have the form of $\tilde{B}$ and the same dimension as the Jordan blocks of $A$. Since there are $n$ variables in $B, \operatorname{dim} C(A) \geqslant n$.

Solution to 7.1.13: $\operatorname{tr}(A B-B A)=0$, so $S$ is contained in the kernel of the trace. Since the trace is a linear transformation from $M_{n \times n}(\mathbb{R})=\mathbb{R}^{n^{2}}$ onto $\mathbb{R}$, its kernel must have dimension, at most, $n^{2}-1$. Therefore, it suffices to show that $S$ contains $n^{2}-1$ linearly independent matrices.

Let $M_{i j}$ denote the matrix with a 1 in the $(i, j)^{t h}$ coordinate and $0^{\prime}$ 's elsewhere. A calculation shows that for $i \neq j, M_{i j}=M_{i k} M_{k j}-M_{k j} M_{i k}$, so $M_{i j}$ is in $S$. Similarly, for $2 \leqslant j \leqslant n, M_{11}-M_{j j}=M_{1 j} M_{j 1}-M_{j 1} M_{1 j}$. Together, these $n^{2}-1$ matrices are clearly a linearly independent set. Solution to 7.1.14: Let $f, g \in S$ and let $r$ and $s$ be scalars. Then, for any $v \in A$, $(r f+s g)(v)=f(r v)+g(s v) \in A$, since $A$ is a vector subspace and $f$ and $g$ fix $A$. Similarly $r f+s g$ fixes $B$, so $r f+s g \in S$ and $S$ is a vector space.

To determine the dimension of $S$, it suffices to determine the dimension of the space of matrices which fix $A$ and $B$. To choose a basis for $V$, let $A^{\prime}$ denote a complementary subspace of $A \cap B$ in $A$ and let $B^{\prime}$ denote a complementary subspace of $A \cap B$ in $B$. Then, since $A+B=V, r=a+b-n$ is the dimension of $A \cap B$. Further, $\operatorname{dim} A^{\prime}=a-r$ and $\operatorname{dim} B^{\prime}=b-r$. Take one basis in each of the spaces $A^{\prime}, B^{\prime}$, and $A \cap B$. The union of these bases form a basis for $V$. Since any endomorphism which leaves $A$ and $B$ invariant must also fix $A \cap B$, its matrix in this basis must have the form

$$
\left(\begin{array}{ccc}
* & * & * \\
0 & * & 0 \\
0 & 0 & *
\end{array}\right)
$$

which has, at most, $a^{2}+b^{2}+n^{2}-a n-b n$ nonzero entries, so the dimension of $S$ is $a^{2}+b^{2}+n^{2}-a n-b n$.

Solution to 7.1.15: Suppose there are scalars such that

$$
a_{0} x+a_{1} T x+\cdots+a_{k} T^{k} x+\cdots+a_{m-1} T^{m-1} x=0
$$

applying $T^{m-1}$ to both sides, we get, since $T 0=0$,

$$
a_{0} T^{m-1} x+a_{1} T^{m} x+\cdots+a_{k} T^{m-1+k} x+\cdots+a_{m-1} T^{m-1+m-1} x=0
$$

so

$$
a_{0} T^{m-1} x=0
$$

and $a_{0}=0$. By the Induction Principle [MH93, p. 7] (multiplying by $T^{m-k-1}$ ) we see that all $a_{k}=0$ and the set is linearly independent.

Solution to 7.1.16: We may assume $\alpha_{1}<\cdots<\alpha_{n}$. Suppose that there are constants $c_{1}, \ldots, c_{n}$ such that $c_{1} e^{\alpha_{1} t}+\cdots+c_{n} e^{\alpha_{n} t} \equiv 0$. If not all the coefficients vanish, we have, without loss of generality, $c_{n} \neq 0$. Thus,

$$
c_{1} e^{\left(\alpha_{1}-\alpha_{n}\right) t}+\cdots+c_{n-1} e^{\left(\alpha_{n-1}-\alpha_{n}\right) t}+c_{n} \equiv 0 .
$$

Letting $t \rightarrow \infty$, and noting that $e^{\left(\alpha_{l}-\alpha_{n}\right) t} \rightarrow 0$ for $1 \leqslant i \leqslant n-1$, we get $c_{n}=0$, a contradiction.

Solution 2. This proof is by induction. Let $P(n)$ denote the statement that for all distinct real numbers $\alpha_{1}, \ldots, \alpha_{n}$, the functions $e^{\alpha_{1} t}, \ldots, e^{\alpha_{n} t}$ are linearly independent.

Then $P(1)$ is true since $e^{\alpha_{1} t} \not \equiv 0$.

Assume $P(n)$ is true for some fixed $n \geqslant 1$, and let $\alpha_{1}, \ldots, \alpha_{n+1}$ be $n+1$ distinct real numbers. Suppose for scalars $\lambda_{i}, \lambda_{1} e^{\alpha_{1} t}+\cdots+\lambda_{n+1} e^{\alpha_{n+1} t} \equiv 0$. Then $\lambda_{1}+\lambda_{2} e^{\beta_{2} t}+\cdots+\lambda_{n+1} e^{\beta_{n+1} t} \equiv 0$, where $\beta_{j}=\alpha_{j}-\alpha_{1}, j=2, \ldots, n+1$ are distinct nonzero real numbers. Differentiating with respect to $t$, we obtain $\lambda_{2} \beta_{2} e^{\beta_{2} t}+\cdots+\lambda_{n+1} \beta_{n+1} e^{\beta_{n+1} t} \equiv 0$. By the induction hypothesis, $\lambda_{j} \beta_{j}=0$ for $j=2, \ldots, n+1$. Thus $\lambda_{j}=0, j=2, \ldots, n+1$. Substituting in the first equation above gives $\lambda_{1}=0$, proving $P(n+1)$.

Solution to 7.1.17: Let $P$ be the change of basis matrix from $\left(a_{i}\right)$ to $\left(b_{i}\right)$. A straightforward calculation shows that $I+2 P$ is the matrix taking $\left(a_{i}\right)$ to $\left(a_{i}+2 b_{i}\right)$. Now $(I+2 P) v=\lambda v$ implies that $P v=\frac{1}{2}(\lambda-1) v$. So if $\lambda$ is an eigenvalue of $I+2 P$, then $\frac{1}{2}(\lambda-1)$ is an eigenvalue of $P$, and they correspond to the same eigenvectors. The reverse also holds, so there is a one-to-one correspondence between the eigenvalues of $P$ and those of $I+2 P$. As $\left(a_{i}\right)$ and $\left(b_{i}\right)$ are orthonormal bases, $P$ is orthogonal and therefore, all the eigenvalues of $P$ are $\pm 1$. But this implies that the only possible eigenvalues of $I+2 P$ are 3 and $-1$. Hence, 0 is not an eigenvalue of $I+2 P$, so it is an invertible matrix and, thus, $\left(a_{i}+2 b_{i}\right)$ is a basis. Further, det $P=(-1)^{\alpha} 1^{\beta}$, where $\alpha$ and $\beta$ are the algebraic multiplicities of $-1$ and 1 as eigenvalues of $P$. Thus, $\operatorname{det}(I+2 P)=(-1)^{\alpha} 3^{\beta}$. Since we are given that det $P>0, \alpha$ is even and, thus, $\operatorname{det}(I+2 P)$ is positive as well. Therefore, $\left(a_{i}+2 b_{i}\right)$ has the same orientation as $\left(a_{i}\right)$.

\subsection{Rank and Determinants}

Solution to 7.2.1: 1. Let $A=\left(a_{i j}\right)$, and $R_{i}$ denote the $i^{i h}$ row of $A$. Let $r$, $1 \leqslant r \leqslant m$, be the row rank of $A$, and $S_{i}=\left(b_{i 1}, \ldots, b_{i n}\right), 1 \leqslant i \leqslant r$, be a basis for the row space. The rows are linear combinations of the $S_{i}$ 's:

$$
R_{i}=\sum_{j=1}^{r} k_{i j} S_{j}, \quad 1 \leqslant i \leqslant m .
$$

For $1 \leqslant l \leqslant n$, isolating the $l^{t h}$ coordinate of each of these equations gives

$$
\begin{aligned}
a_{1 l} &=k_{11} b_{1 l}+\cdots+k_{1 r} b_{r l} \\
a_{2 l} &=k_{21} b_{1 l}+\cdots+k_{2 r} b_{r l} \\
& \vdots \\
a_{m l} &=k_{m 1} b_{1 l}+\cdots+k_{m r} b_{r l} .
\end{aligned}
$$

Hence, for $1 \leqslant l \leqslant n$ the $l^{t h}$ column of $A, C_{l}$, is given by the equation

$$
C_{l}=\sum_{j=1}^{r} b_{j l} K_{j},
$$

where $K_{j}$ is the column vector $\left(k_{1} j, \ldots, k_{m j}\right)^{l}$. Hence, the space spanned by the columns of $A$ is also spanned by the $r$ vectors $K_{j}$, so its dimension is less than or equal to $r$. Therefore, the column rank of $A$ is less than or equal to its row rank. In exactly the same way, we can show the reverse inequality, so the two are equal. 2. Using Gaussian elimination we get the matrix

$$
\left(\begin{array}{cccc}
1 & 0 & 3 & -2 \\
0 & 1 & -4 & 4 \\
0 & 0 & 2 & 0 \\
0 & 0 & 0 & 2 \\
0 & 0 & 0 & 0
\end{array}\right)
$$

so the four columns of $M$ are linearly independent.

3. If a set of rows of $M$ is linearly independent over $\mathbf{K}$, then clearly it is also independent over $\mathbf{F}$, so the rank of $M$ over $\mathbf{K}$ is, at most, the rank of $M$ over F; but looking at the Gaussian elimination process one can see that it involves operations in the sub-field only, so the rank over both fields is the same.

Solution to 7.2.2: Let $v$ be a nonzero right null vector of $A$ and $u$ a left null vector of $A$ such that $u \cdot v=0$. Since $A$ is diagonalizable, we can choose a basis $\left\{v_{1}, v_{2}, \ldots, v_{n}\right\}$ of $\mathbb{R}^{n}$ consisting of eigenvectors of $A$. Since the null space of $A$ has dimension 1 , one of these basis vectors is a multiple of $v$, so we can assume $v_{1}=v$. Let $\lambda_{j}$ be the eigenvalue corresponding to $v_{j}, j=2, \ldots, n$. Then $\lambda_{j} \neq 0$, and for $j>1$,

$$
u v_{j}=\frac{1}{\lambda_{j}} u\left(A v_{j}\right)=\frac{1}{\lambda_{j}}(u A) v_{j}=0 .
$$

Hence $u$ is orthogonal to each of the basis vectors $v_{1}, \ldots, v_{n}$, so $u=0$.

Solution to 7.2.3: As $A^{t} A V \subset A^{t} V$, it is sufficient to prove that $\operatorname{dim} A^{t} A V=$ $\operatorname{dim} A^{t} V$. We know that $\operatorname{rank} A=\operatorname{dim}(\operatorname{Im} A)$ and that

$$
\operatorname{dim}(\operatorname{Im} A)+\operatorname{dim}(\operatorname{ker} A)=n
$$

Similar formulas hold for $A^{t} A$. Therefore, it is enough to show that $\operatorname{ker} A$ and $\operatorname{ker} A^{t} A$ have the same dimension. In fact, they are equal. Clearly, $\operatorname{ker} A \subset \operatorname{ker} A^{t} A$. Conversely, take any $v \in \operatorname{ker} A^{t} A$. Then

$$
0=\left\langle A^{t} A v, v\right\rangle=\langle A v, A v\rangle,
$$

so $\|A v\|=0$. Hence, $v \in \operatorname{ker} A$ and we are done.

Solution to 7.2.4: Since $1-P-Q$ is invertible, $P$ has the same rank as

$$
P(1-P-Q)=P-P^{2}-P Q=-P Q .
$$

Similarly, $Q$ has the same rank as

$$
(1-P-Q) Q=Q-P Q-Q^{2}=-P Q,
$$

so $P$ and $Q$ have the same rank. Solution to 7.2.5: The upper $k \times k$ square minor $A(k, k)$ of $A(m, n)$ is the Vandermonde matrix, which determinant is $\prod_{0 \leqslant i<j<k}(j-i)$. If $k \leqslant p$, this determinant is not zero $(\bmod p)$, which shows that $\operatorname{rank} A(m, n) \geqslant \min \{m, n, p\}$. Now if $A(m, n)$ has at most $p$ distinct columns $(\bmod p)$, so $\operatorname{rank} A(m, n) \leqslant p$. Since $\operatorname{rank} A(n, n) \leqslant \min \{n, m\}$, we have $\operatorname{rank} A(m, n)=\min \{m, n, p\}$.

Solution to 7.2.6: Let $V=K^{n}$, and consider the two quotient spaces $A V / A^{2} V$ and $A^{2} V / A^{3} V$. The first has dimension $\operatorname{rank} A-\operatorname{rank} A^{2}$, and the second has dimension $\operatorname{rank} A^{2}-\operatorname{rank} A^{3}$. The matrix $A$ induces a linear transformation of $A V / A^{2} V$ onto $A^{2} V / A^{3} V$, so $\operatorname{dim}\left(A^{2} V / A^{3} V\right) \leqslant \operatorname{dim}\left(A V / A^{2} V\right)$, as desired.

Solution to 7.2.7: 1. and 2. Since $T$ is symmetric it is diagonalizable, so $\mathbb{R}^{n}$ can be written as the direct sum of the eigenspaces of $T$. It suffices to show that any eigenspace has dimension, at most, 1 . For if this is the case, then the kernel has dimension, at most, 1, and, by the Rank-Nullity Theorem [HK61, p. 71], T has rank at least $n-1$, and there must be $n$ distinct eigenspaces, so there are $n$ distinct eigenvalues associated with them.

Let $\lambda \in \mathbb{R}$, and consider the system of equations $T x=\lambda x$. The first equation is $a_{1} x_{1}+b_{1} x_{2}=\lambda x_{1}$. Since $b_{1} \neq 0$, we can solve for $x_{2}$ in terms of $x_{1}$. Suppose that we can solve the first $i-1$ equations for $x_{2}, \ldots, x_{i}$ in terms of $x_{1}$. Then, since $b_{i} \neq 0$, we can solve the $i^{t h}$ equation $b_{i-1} x_{i-1}+a_{i} x_{i}+b_{i} x_{i+1}=\lambda x_{i}$ for $x_{i+1}$ in terms of $x_{1}$. Therefore, by the Induction Principle [MH93, p. 7], we can solve the first $n-1$ equations for $x_{2}, \ldots, x_{n}$ in terms of $x_{1}$.

The last equation, $b_{n-1} x_{n-1}+a_{n} x_{n}=\lambda x_{n}$, is either consistent with this or is not. If not, $\lambda$ is not an eigenvalue; if it is, then $\lambda$ is an eigenvalue and we have one degree of freedom in determining eigenvectors. Hence, in either case the associated eigenspace has dimension, at most, 1 and we are done.

Solution 2. 1. The submatrix one obtains by deleting the first row and the first column is upper-triangular with nonzero diagonal entries, so its determinant is nonzero. Thus, the first $n-1$ columns of $T$ are linearly independent.

2. By the Spectral Theorem [HK61, p. 335], [Str93, p. 235], $\mathbb{R}^{n}$ has a basis consisting of eigenvectors of $T$. If $\lambda$ is an eigenvalue of $T$, then $T-\lambda I$ has rank $n-1$ by Part 1, so $\operatorname{ker}(T-\lambda I)$ has dimension 1. Since the eigenspaces span $\mathbb{R}^{n}$ and each has dimension 1 , there must be $n$ of them.

Solution to 7.29: 1. Write the characteristic polynomial of $A$, $\operatorname{det}(A-\lambda I)$, as $(-1)^{r} \lambda^{r}+c_{1} \lambda^{r-1}+\cdots+c_{r}$. Since the entries of $A$ are integers, each $c_{k}$ is an integer, and $c_{r}=\operatorname{det} A$. If $\lambda$ is an integer eigenvalue, then $\operatorname{det}(A-n I)=0$, so

$$
\operatorname{det} A=(-1)^{r-1} n^{r}+c_{1} n^{r-1}+\cdots+c_{r-1} n
$$

showing that $n$ divides det $A$.

2. Under the given hypotheses, $n$ is an eigenvalue with eigenvector $(1,1, \ldots, 1)^{t}$, so Part 1 applies. Solution to 7.2.10: Since $A$ has rank $n-1$ it has $n-1$ linearly independent rows, and the remaining row is a linear combination of those $n-1$. Interchanging rows and corresponding columns of $A$, we can assume without loss of generality that rows 1 through $n-1$ of $A$ are linearly independent. Thus, there are real numbers $c_{1}, \ldots, c_{n-1}$ such that, letting $a_{j k}$ denote the entries of $A$, we have

$$
a_{n k}=\sum_{j=1}^{n-1} c_{j} a_{j k}, \quad k=1, \ldots, n .
$$

It is asserted that the matrix obtained from $A$ by deletion of its last row and column has rank $n-1$. Assume not. Then there are real numbers $b_{1}, \ldots, b_{n-1}$, not all 0 , such that

$$
\sum_{j=1}^{n-1} b_{j} a_{j k}=0, \quad k=1, \ldots, n-1 .
$$

By $(*)$ and the symmetry of $A$,

$$
a_{j n}=\sum_{k=1}^{n-1} c_{k} a_{j k}, \quad j=1, \ldots, n .
$$

Hence

$$
\sum_{j=1}^{n-1} b_{j} a_{j n}=\sum_{j=1}^{n-1} \sum_{k=1}^{n-1} b_{j} c_{k} a_{j k}=\sum_{k=1}^{n-1} c_{k} \sum_{j=1}^{n-1} b_{j} a_{j k}=0,
$$

i.e., $(* *)$ also holds for $k=n$, contradicting the linear independence of the first $n-1$ rows of $A$.

Solution to 7.2.11: We use the Induction Principle [MH93, p. 32] in the order of the matrix. If $n=2$,

$$
A=\left(\begin{array}{ll}
1 & x_{1} \\
1 & x_{2}
\end{array}\right)
$$

which has determinant $\left(x_{2}-x_{1}\right)$.

Suppose the result holds for all $k<n$, and let $A$ be the $n \times n$ Vandermonde matrix, see the Solution to Problem 7.2.11 or [HK61, p. 125]. Treating the indeterminates $x_{1}, \ldots, x_{n-1}$ as constants and expanding the determinant of $A$ along the last row, we see that $\operatorname{det} A$ is an $(n-1)^{t h}$ degree polynomial in $x_{n}$, which can have, at most, $n-1$ roots. If we let $x_{n}=x_{i}$ for $1 \leqslant i \leqslant n-1, A$ would have two identical rows, so $\operatorname{det} A$ would equal 0 . Hence, the $x_{i}$ 's are the roots of $\operatorname{det} A$ as a polynomial in $x_{n}$. In other words, there exists a constant $c>0$ such that

$$
\operatorname{det} A=c \prod_{i=1}^{n-1}\left(x_{n}-x_{i}\right)
$$

$c$ is the coefficient of the $x_{n}^{n-1}$ term, which, when we expand the determinant, is equal to the determinant of the $(n-1) \times(n-1)$ Vandermonde matrix. So, by the induction hypothesis,

$$
\operatorname{det} A=\prod_{j<i \leqslant n-1}\left(x_{i}-x_{j}\right) \prod_{i=1}^{n-1}\left(x_{n}-x_{i}\right)=\prod_{i>j}\left(x_{i}-x_{j}\right) .
$$

Solution to 7.2.12: 1. As shown in the solution of Problem 7.2.11, the determinant of the matrix is

$$
\prod_{i>j}\left(a_{i}-a_{j}\right)
$$

which is nonzero if the $a_{i}$ are all different.

2. The function $f$ given by

$$
f(x)=\sum_{i=0}^{n} \frac{\left(x-a_{0}\right) \cdots\left(x-a_{i-1}\right) b_{i}\left(x-a_{i+1}\right) \cdots\left(x-a_{n}\right)}{\left(a_{i}-a_{0}\right) \cdots\left(a_{i}-a_{i-1}\right) b_{i}\left(a_{i}-a_{i+1}\right) \cdots\left(a_{i}-a_{n}\right)}
$$

has degree $n$ and takes $f\left(a_{i}\right)$ into $b_{i}$. Now, if $\psi(x)$ is another such polynomial of degree $n$, the polynomial

$$
f(x)-\psi(x)
$$

has degree $n$ with $n+1$ different roots (the $a_{i}$ 's), so it has to be the zero polynomial and $f$ is unique.

Solution to 7.2.13: Define the matrix $C=A^{-1} B$, one can immediatly see that it is also unitary. And we can also verify that $A+B=A(I+C)$. Since $A$ is unitary, its eigenvalues have absolute value 1 . Multiplying them together shows that $|\operatorname{det} A|=1$. If $\zeta_{1}, \ldots, \zeta_{n}$ are the eigenvalues of $C$, listed with their multiplicities, then $\left|\zeta_{i}\right|=1$, so the eigenvalues of $I+C$ are $1+\zeta_{1}, \ldots, 1+\zeta_{n}$, so

$$
|\operatorname{det}(I+C)|=\left|1+\zeta_{1}\right| \cdots\left|1+\zeta_{n}\right| \leqslant 2 \cdot 2 \cdots 2=2^{n} \text {. }
$$

Hence

$$
|\operatorname{det}(A+B)|=|\operatorname{det} A||\operatorname{det}(I+C)| \leqslant 2^{n} .
$$

Solution to 7.2.14: Consider the function $v(t)=\left(1, t, t^{2}\right)$. To show that $v\left(t_{1}\right)$, $v\left(t_{2}\right)$, and $v\left(t_{3}\right)$ form a basis for $\mathbb{R}^{3}$ whenever the $t_{i}$ 's are distinct, it will suffice to show that the matrix which has these vectors as rows has nonzero determinant. But this matrix is

$$
\left(\begin{array}{lll}
1 & t_{1} & t_{1}^{2} \\
1 & t_{2} & t_{2}^{2} \\
1 & t_{3} & t_{3}^{2}
\end{array}\right)
$$

which is the $3 \times 3$ Vandermonde matrix, see the Solution to Problem 7.2.11 or [HK61, p. 125]. Its determinant is given by

$$
\left(t_{3}-t_{2}\right)\left(t_{3}-t_{1}\right)\left(t_{2}-t_{1}\right)
$$

which is nonzero whenever the $t_{i}$ 's are distinct.

Solution to 7.2.15: Let $G$ be the matrix with entries

$$
G_{i j}=\int_{a}^{b} f_{i}(x) f_{j}(x) d x .
$$

If the determinant of $G$ vanishes, then $G$ is singular; let $a$ be a nonzero $n$-vector with $G a=0$. Then

$$
0=a^{T} G a=\sum_{i=1}^{n} \sum_{i=j}^{n} \int_{a}^{b} a_{i} f_{i}(x) a_{j} f_{j}(x) d x=\int_{a}^{b}\left(\sum_{i=1}^{n} a_{i} f_{i}(x)\right)^{2} d x,
$$

so, since the $f_{i}$ 's are continuous functions, the linear combination $\sum a_{i} f_{i}$ must vanish identically. Hence, the set $\left\{f_{i}\right\}$ is linearly dependent on $[a, b]$. Conversely, if $\left\{f_{i}\right\}$ is linearly dependent, some $f_{i}$ can be expressed as a linear combination of the rest, so some row of $G$ is a linear combination of the rest and $G$ is singular.

Solution to 7.2.16: Identify $M_{2 \times 2}$ with $\mathbb{R}^{4}$ via

$$
\left(\begin{array}{ll}
a & b \\
c & d
\end{array}\right) \leftrightarrow\left(\begin{array}{l}
a \\
b \\
c \\
d
\end{array}\right)
$$

and decompose $L$ into the multiplication of two linear transformations,

$$
M_{2 \times 2} \simeq \mathbb{R}^{4} \stackrel{L_{A}}{\longrightarrow} \mathbb{R}^{4} \stackrel{L_{B}}{\longrightarrow} \mathbb{R}^{4} \simeq M_{2 \times 2}
$$

where $L_{A}(X)=A X$ and $L_{B}(X)=X B$. is

The matrices of these two linear transformations on the canonical basis of $\mathbb{R}^{4}$

$$
L_{A}=\left(\begin{array}{rrrr}
1 & 0 & 2 & 0 \\
0 & 1 & 0 & 2 \\
-1 & 0 & 3 & 0 \\
0 & -1 & 0 & 3
\end{array}\right) \quad \text { and } L_{B}=\left(\begin{array}{llll}
2 & 0 & 0 & 0 \\
1 & 4 & 0 & 0 \\
0 & 0 & 2 & 0 \\
0 & 0 & 1 & 4
\end{array}\right)
$$

then $\operatorname{det} L=\operatorname{det} L_{A} \cdot \operatorname{det} L_{B}=(9+6+2(2+3)) \cdot(2 \cdot 32)=2^{6} \cdot 5^{2}$, and to compute the trace of $L$, we only need the diagonal elements of $L_{A} \cdot L_{B}$, that is,

$$
\operatorname{tr} L=2+4+6+12=24 \text {. }
$$

Solution to 7.2.17: Let $X=\left(x_{i j}\right)$ be any element of $M_{3}(\mathbb{R})$. A calculation gives

$$
T(X)=\left(\begin{array}{ccc}
x_{11} & 3 x_{12} / 2 & x_{13} \\
3 x_{21} / 2 & 2 x_{22} & 3 x_{23} / 2 \\
x_{31} & 3 x_{32} / 2 & x_{33}
\end{array}\right) .
$$

It follows that the basis matrices $M_{i j}$ are eigenvectors of $T$. Taking the product of their associated eigenvalues, we get $\operatorname{det} T=2(3 / 2)^{4}=81 / 8$.

Solution to 7.2.18: Since the minimal polynomial of $A$ splits into distinct linear factors, $\mathbb{R}^{3}$ has a basis $\left\{v_{1}, v_{2}, v_{3}\right\}$ of eigenvectors of $A$. Since $\operatorname{det} A=32$, two of those, say $v_{1}$ and $v_{2}$, are associated with the eigenvalue 4 , and one, $v_{3}$, is associated with the eigenvalue 2 . Now consider the nine matrices $E_{i j}, 1 \leqslant i, j \leqslant 3$, whose $i^{t h}$ column is the vector $v_{j}$ and whose other columns are zero. Since the $v_{i}$ 's are linearly independent, the matrices $E_{i j}$ are linearly independent in $M_{3 \times 3}$ and form a basis of $M_{3 \times 3}$. Further, a calculation shows that $A E_{i j}=\lambda_{j} E_{i j}$, where $\lambda_{1}=\lambda_{2}=4$ and $\lambda_{3}=2$. Hence, $M_{3 \times 3}$ has a basis of eigenvectors of $L_{A}$, so it follows that tr $L_{A}=6 \cdot 4+3 \cdot 2=30$.

Solution to 7.2.20: We have

$$
\operatorname{dim} \text { range } T=\operatorname{dim} M_{7 \times 7}-\operatorname{dim} \operatorname{ker} T=49-\operatorname{dim} \operatorname{ker} T
$$

so it suffices to find the dimension of $\operatorname{ker} T$; in other words, the dimension of the subspace of matrices that commute with $A$. Let $E_{+}$be the eigenspace of $A$ for the eigenvalue 1 and $E_{-}$be the eigenspace of $A$ for the eigenvalue $-1$. Then $\mathbb{R}^{7}=E_{+} \oplus E_{\ldots}$. A matrix that commutes with $A$ leaves $E_{+}$and $E_{-}$invariant, so, as linear transformations on $\mathbb{R}^{7}$, can be expressed as the direct sum of a linear transformation on $E_{+}$with a linear transformation on $E_{-}$. Moreover, any matrix that can be so expressed commutes with $A$. Hence, the space of matrices that commute with $A$ is isomorphic to $M_{4 \times 4} \oplus M_{3 \times 3}$, and so has dimension $16+9=25$. It follows that dim range $T=49-25=24$.

Solution to 7.2.21: $m>n:$ We write $T=T_{1} T_{2}$, where $T_{2}: M_{n \times m} \rightarrow M_{n \times n}$ is defined by $T_{2}(X)=X B$ and $T_{1}: M_{n \times n} \rightarrow M_{m \times n}$ is defined by $T_{1}(Y)=A Y$. Since $\operatorname{dim} M_{n \times m}=n m>n^{2}=\operatorname{dim} M_{n \times n}$, the transformation $T_{2}$ has a nontrivial kernel, by the Rank-Nullity Theorem [HK61, p. 71]. Hence, $T$ also has a nontrivial kernel and is not invertible.

$m<n:$ We write $T=T_{2} T_{1}$, where $T_{1}: M_{n \times m} \rightarrow M_{m \times m}$ is defined by $T_{1}(X)=A X$ and $T_{2}: M_{m \times m} \rightarrow M_{m \times n}$ is defined by $T_{2}(Y)=Y B$. Now we have $\operatorname{dim} M_{n \times m}=n m>m^{2}=\operatorname{dim} M_{m \times m}$, so $T_{1}$ has a nontrivial kernel, and we conclude as before that $T$ is not invertible.

\subsection{Systems of Equations}

Solution to 7.3.1: Subtracting the first equation from the third we get $x_{1}=x_{7}$, continuing in this fashion subtracting the $k^{t h}$ equation from the $(k+2)^{t h}$ equation we obtain

$$
x_{k}=x_{k+6} \text { for every } k
$$

and we are left with a maximum of six independent parameters in the system. Solving the first and second equations, separatedly, we get

$$
\begin{aligned}
&x_{5}=-\left(x_{1}+x_{3}\right) \\
&x_{6}=-\left(x_{2}+x_{4}\right)
\end{aligned}
$$

so four free parameters is all that is required.

Solution to 7.3.2: 1 . Through linear combinations of rows, reduce the system of equations to a row-reduced echelon form, that is, a system where:

- the first nonzero entry in each nonzero row is equal to 1 ;

- each column which contains the leading nonzero entry of some row has all its other entries 0 ;

- every row which has all entries 0 occurs below every row that has a nonzero entry;

- if rows $1, \ldots, r$ are the nonzero rows and if the leading nonzero entry of row $i$ occurs in column $k_{i}, i=1, \ldots, r$, then $k_{i}<k_{2}<\cdots<k_{r}$.

This new system has a number of nonzero rows $r \leqslant m<n$ and it is easy to see that it has nonzero solution.

Since the original system is equivalent to the row-reduced one, they have exactly the same solutions.

2. Let $V$ be a vector space spanned by $m$ vectors $\beta_{1}, \ldots, \beta_{m}$. We will show that every subset $S=\left\{\alpha_{1}, \ldots, \alpha_{n}\right\}$ of $V$ with $n>m$ vectors is linear dependent.

Since $\beta_{1}, \ldots, \beta_{m}$ span $V$, there are scalars $A_{i j}$ in the field $\mathbf{F}$ such that

$$
\alpha_{i}=\sum_{i=1}^{m} A_{i j} \beta_{i} .
$$

For any set of scalar $x_{1}, \ldots, x_{n}$ in $\mathbf{F}$, we have

$$
\begin{aligned}
x_{1} \alpha_{1}+\cdots+x_{n} \alpha_{n} &=\sum_{j=1}^{n} x_{j} \alpha_{j} \\
&=\sum_{j=1}^{n} x_{j} \sum_{i=1}^{m} A_{i j} \beta_{i} \\
&=\sum_{j=1}^{n} \sum_{i=1}^{m}\left(A_{i j} x_{j}\right) \beta_{i} \\
&=\sum_{i=1}^{m}\left(\sum_{j=1}^{n} A_{i j} x_{j}\right) \beta_{i} .
\end{aligned}
$$

Since $n>m$, the linear systems of equations

$$
\sum_{j=1}^{n} A_{i j} x_{j}=0, \quad 1 \leqslant i \leqslant m
$$

has a nontrivial solution so the set is linear dependent, proving the assertion.

Solution to 7.3.3: The answer is yes. Writing the system of linear equations in matrix form, we have $A x=0$, where $A$ is an $m \times n$ matrix with rational entries. Let the column vector $x=\left(x_{1}, \ldots, x_{n}\right)^{l}$ be a complex solution to this system, and let $V$ be the $\mathbb{Q}$-vector space spanned by the $x_{i}$ 's. Then $\operatorname{dim} V=p \leqslant n$. If $y_{1}, \ldots, y_{p} \in \mathbb{C}$ is a basis of $V$, then there is a rational $n \times p$ matrix $B$ with $B y=x$ (where $y$ is the column vector $\left(y_{1}, \ldots, y_{p}\right)^{t}$ ). Substituting this into the original equation, we get $A B y=0$. Since $y$ is composed of basis vectors, this is possible only if $A B=0$. In particular, every column of $B$ is a rational solution of the equation $A x=0$.

\subsection{Linear Transformations}

Solution to 7.4.1: 1. We need to show that vector addition and scalar multiplication are closed in $S(E)$, but this is a trivial verification because if $v=S(x)$ and $w=S(y)$ are vectors in $S(E)$, then

$$
v+w=S(x+y) \text { and } c v=S(c x)
$$

are also in $S(E)$.

2. If $S$ is not injective, then two different vectors $x$ and $y$ have the same image $S(x)=S(y)=v$, so

$$
S(x-y)=S(x)-S(y)=v-v=0
$$

that is, $x-y \neq 0$ is a vector in the kernel of $S$. On the other hand, if $S$ is injective, it only takes $0 \in E$ into $0 \in F$, showing the result.

3. Assuming that $S$ is injective, the application $S^{-1}: S(E) \rightarrow E$ is well defined. Given $a v+b w \in S(E)$ with $v=S(x)$ and $w=S(y)$, we have

$$
\begin{aligned}
S^{-1}(a v+w) &=S^{-1}(a S(x)+b S(y)) \\
&=S^{-1}(S(a x+b y)) \\
&=a x+b y \\
&=a S^{-1}(v)+b S^{-1}(w)
\end{aligned}
$$

therefore, $S^{-1}$ is linear.

Solution to 7.4.2: Let $\left\{\alpha_{1}, \ldots, \alpha_{k}\right\}$ be a basis for ker $T$ and extend it to $\left\{\alpha_{1}, \ldots, \alpha_{k}, \ldots, \alpha_{n}\right\}$, a basis of $V$. We will show that $\left\{T \alpha_{k+1}, \ldots, T \alpha_{n}\right\}$ is a basis for the range of $T$. It is obvious they span the range since $T \alpha_{j}=0$ for $j \leqslant k$. Assume

$$
\sum_{i=k+1}^{n} c_{i} T \alpha_{i}=0
$$

which is equivalent to

$$
T\left(\sum_{i=k+1}^{n} c_{i} \alpha_{i}\right)=0,
$$

that is, $\alpha=\sum_{i=k+1}^{n} c_{i} \alpha_{i}$ is in the kernel of $T$. We can then write $\alpha$ as $\alpha=\sum_{i=1}^{k} b_{i} \alpha_{i}$ and have

$$
\sum_{i=1}^{k} b_{i} \alpha_{i}-\sum_{i=k+1}^{n} c_{i} T \alpha_{i}=0
$$

which implies all $c_{i}=0$, and the vectors $T \alpha_{k+1}, \ldots, T \alpha_{n}$ form a basis for the range of $T$.

Solution to 7.4.3: Applying the Rank-Nullity Theorem [HK61, p. 71] (see Solution to Problem 7.4.2) to the map $\left.T\right|_{T^{-1}(X)}: T^{-1}(X) \rightarrow X$, we get

$$
\operatorname{dim} T^{-1}(X)=\left.\operatorname{dim} \operatorname{ker} T\right|_{T^{-1}(X)}+\operatorname{dim} X,
$$

but $\left.\operatorname{ker} T\right|_{T^{-1}(X)}=\operatorname{ker} T$, so, by the same theorem, we have

$$
\left.\operatorname{dim} \operatorname{ker} T\right|_{T^{-1}(X)}=\operatorname{dim} V-\operatorname{dim} X \leqslant \operatorname{dim} V-\operatorname{dim} W .
$$

Therefore

$$
\operatorname{dim} T^{-1}(X) \geqslant \operatorname{dim} V-\operatorname{dim} W+\operatorname{dim} X .
$$

Solution 2. Let $Q$ be the projection from $W$ onto the quotient space $Z=W / X$. The linear map $Q T: V \rightarrow Z$ has null space $T^{-1}(X)$, therefore

$$
\operatorname{dim} V-\operatorname{dim} T^{-1}(X)=\operatorname{rank}(Q T) \leqslant \operatorname{dim} Z=\operatorname{dim} W-\operatorname{dim} X .
$$

Solution to 7.4.4: We have $A(B(\operatorname{ker}(A B)))=\{0\}$ implying that $B(\operatorname{ker}(A B)) \subset$ $\operatorname{ker} A, \operatorname{sodim} B(\operatorname{ker}(A B) \leqslant \operatorname{dim} \operatorname{ker} A$. Using the Rank-Nullity Theorem [HK61, p. 71], we have

$$
\begin{aligned}
\operatorname{dim} \operatorname{ker}(A B) &=\operatorname{dim} B(\operatorname{ker}(A B))+\operatorname{dim} \operatorname{ker}\left(\left.B\right|_{\operatorname{ker}(A B)}\right) \\
& \leqslant \operatorname{dim} \operatorname{ker} A+\operatorname{dim} \operatorname{ker} B .
\end{aligned}
$$

Solution to 7.4.5: Let $v_{0}$ be a vector in $V$ such that $A v_{0} \neq 0$. By the second condition there is a scalar $\alpha$ such that $B v_{0}=\alpha A v_{0}$. Suppose $v$ is any vector in $V$ such that $A v_{0}$ and $A v$ are linearly independent. By the second condition there are scalars $\beta$ and $\gamma$ such that $B v=\beta A v$ and $B\left(v_{0}+v\right)=\gamma A\left(v_{0}+v\right)$. Then

$$
\alpha A v_{0}+\beta A v_{0}=\gamma A v_{0}+\gamma A v,
$$

which by the linear independence of $A v_{0}$ and $A v$ implies that $\alpha=\beta=\gamma$. Hence $B v=\alpha A v$.

Fix a vector $v_{1}$ in $V$ such that $A v_{0}$ and $A v_{1}$ are linearly independent. Suppose $v$ is a vector in $V$ such that $A v_{0}$ and $A v$ are linearly dependent, but $A v \neq 0$. Then $A v_{1}$ and $A v$ are linearly independent, and the reasoning above shows that $B v=\alpha A v$.

Suppose finally that $v$ is a nonzero vector in $V$ such that $A v=0$. Then $A(v+$ $\left.v_{0}\right)$ and $A v_{1}$ are linearly independent, so the reasoning above gives $B\left(v+v_{0}\right)=$ $\alpha A\left(v+v_{0}\right)=\alpha A v_{0}$, implying that $B v=0$ (since $B v_{0}=\alpha A v_{0}$ ). Thus $B v=$ $\alpha A v$ for all $v$ in $V$, i.e., $B=\alpha A$.

Solution to 7.4.6: Let $v_{1}, \ldots, v_{n}$ be a basis for $V$ such that $v_{1}, \ldots, v_{k}$ is a basis for $W$. Then the matrix for $L$ in terms of this basis has the form $\left(\begin{array}{cc}M & M \\ 0 & 0\end{array}\right)$, where $M$ is a $k \times k$ matrix and $N$ is $k \times(n-k)$. It follows that $M$ is the matrix of $L_{W}$ with respect to the basis $v_{1}, \ldots, v_{k}$. As the matrix of $1-t L$ is $\left(\begin{array}{cc}1-t M-t N \\ 0 & 1\end{array}\right)$, it follows that $\operatorname{det}(1-t L)=\operatorname{det}(1-t M)=\operatorname{det}\left(1-t L_{W}\right)$.

Solution to 7.4.7: The trace of $f$ equals the trace of the restriction $\left.f\right|_{W}$ plus the trace of the induced endomorphism $g$ of $V / W$. Since $f$ maps $V$ into $W, g$ is zero and has trace zero.

Solution to 7.4.8: Suppose $W$ is a $k$-dimensional invariant subspace for $T$. Any basis $\left\{e_{1}, \ldots, e_{k}\right\}$ of $W$ extends to a basis of $V,\left\{e_{1}, \ldots, e_{n}\right\}$. The matrix of $T$ with respect to this basis has the form $\left(\begin{array}{cc}B & C \\ 0 & D\end{array}\right)$, where $B$ is the matrix of $\left.T\right|_{w}$. It follows that the characteristic polynomial of $\left.T\right|_{W}$ is a factor of the characteristic polynomial of $T$, of degree $k$.

Conversely, suppose $\operatorname{det}(a-\lambda I)=f(t) g(t)$ where $f, g \in \mathbf{F}[t]$ are both nonconstant. Then $f(A) g(A)=0$, by the Cayley-Hamilton Theorem [HK61, p. 194], so at least one of $f(A), g(A)$, is singular. Assume $f(A)$ is singular, and choose a nonzero vector $w \in \operatorname{ker} f(A)$. Then the subspace $W$ spanned by

$$
w, T w, \ldots, T^{k-1} w,
$$

where $k=\operatorname{deg} f$, is invariant under $T$, nonzero, and has dimension at most $k<n$

Solution to 7.4.9: Let $f(x) \in \mathbb{R}[x]$ denote the characteristic polynomial of $T$. If $f(x)$ has a nonreal root $\lambda$, let $v \in \mathbb{C}^{n}$ be a nonzero eigenvector with eigenvalue $\lambda$; then the 2-dimensional subspace $M$ spanned by the real vectors $v+\bar{v}$ and $(\bar{v}-v) / i$ will be mapped into itself by $T$. Otherwise all roots of the degree-n polynomial $f(x)$ are real. If $f(x)$ has two distinct real roots $r_{1}, r_{2}$, then there exist nonzero eigenvectors $v_{1}$ and $v_{2}$ with these eigenvalues, and we may take $M$ to be the span of $v_{1}$ and $v_{2}$.

We are left with the case in which $f(x)=(x-r)^{n}$ for some $r \in \mathbb{R}$. Let $v_{1}$ be a nonzero eigenvector. Then the characteristic polynomial of the transformation $T^{\prime}$ of $\mathbb{R}^{n} /\left(\mathbb{R} v_{1}\right)$ induced by $T$ is $(x-r)^{n-1}$, and there exists a nonzero eigenvector $\tilde{v}_{2} \in \mathbb{R}^{n} /\left(\mathbb{R} v_{1}\right)$ of $T^{\prime}$, since $n>1$. Choose $v_{2} \in \mathbb{R}^{n}$ representing $\tilde{v}_{2}$. Then $T$ maps the span $M$ of $v_{1}$ and $v_{2}$ into itself.

Solution to 7.4.10: Let $V_{i}=\left\{v \in V \mid \chi_{i}(L)(v)=0\right\}$, for $i=1$, 2. Clearly, each $V_{i}$ is a subspace of $V$ with $\chi_{i}(L) V_{i}=0$. To show that $V$ is the direct sum of $V_{1}$ and $V_{2}$, choose polynomials $a$ and $b$ over $\mathrm{F}$ for which $a \chi_{1}+b \chi_{2}=1$. Then $a(L) \chi_{1}(L)+b(L) \chi_{2}(L)=1$. If $v \in V_{1} \cap V_{2}$, then $v=1 \cdot v=a(L) \chi_{1}(L)+$ $b(L) \chi_{2}(L) v=a(L) 0+b(L) 0=0$, so $V_{1} \cap V_{2}=\{0\}$. If $v \in V$, then by the Cayley-Hamilton Theorem [HK61, p. 194], we have $\chi(L) v=0$. Hence, $v_{1}=a(L) \chi_{1}(L) v$ is annihilated by $\chi_{2}(L)$ and, therefore, belongs to $V_{2}$. Likewise, $v_{2}=b(L) \chi_{2}(L) v$ belongs to $V_{1}$. Since $v=v_{1}+v_{2}$, this shows that $V=V_{1}+V_{2}$.

Solution to 7.4.11: It is sufficient to prove that $y \notin\langle x\rangle$ and $z \notin\langle x, y\rangle$. Suppose that $y=\lambda x$ for some $\lambda \in \mathbb{Q}$. Then $z=T y=\lambda y=\lambda^{2} x$, and $x+y=T z=$ $\lambda^{2} y=\lambda^{3} x$. Hence $x+\lambda x=\lambda^{3} x,\left(\lambda^{3}-\lambda-1\right) x=0$ and $\lambda^{3}-\lambda-1=0$ since $x \neq 0$. This is a contradiction, since $\lambda^{3}-\lambda-1=0$ has no rational solutions. Therefore $y \notin\langle x\rangle$.

Suppose that $z=\alpha x+\beta y$ for some $\alpha, \beta \in \mathbb{Q}$. Then

$x+y=T z=\alpha T x+\beta T y=\alpha y+\beta z=\alpha y+\beta(\alpha x+\beta y)=\alpha \beta x+\left(\alpha+\beta^{2}\right) y$.

By the independence of $x, y$ we obtain

$$
1=\alpha \beta, \quad 1=\alpha+\beta^{2} .
$$

Eliminating $\beta$ yields $\alpha^{3}-\alpha^{2}+1=0$. This is a contradiction since $\alpha^{3}-\alpha^{2}+1=0$ has no rational solutions.

Hence $x \neq 0, y \notin\langle x\rangle$ and $z \notin\langle x, y\rangle$, so $x, y_{1} z$ are linearly independent.

Solution to 7.4.14: Since the linear transformation $f$ has rank $n-1$, we know that $f\left(\mathbb{R}^{m}\right)$ is an $n-1$-dimensional subspace of $\mathbb{R}^{n}$. Hence, there exist real constants $\lambda_{1}, \ldots, \lambda_{n}$, not all zero, such that

$$
\sum_{i=1}^{n} \lambda_{i} f_{i}(v)=0
$$

for all $v \in \mathbb{R}^{m}$. The $\lambda_{i}$ 's are unique up to constant multiples. Further, this equation determines the subspace: If $w \in \mathbb{R}^{n}$ satisfies it, then $w \in f\left(\mathbb{R}^{m}\right)$.

Now suppose that the $\lambda_{i}$ 's all have the same sign, or, without loss of generality, that they are all nonnegative. Then if there existed $v \in \mathbb{R}^{m}$ with $f_{i}(v)>0$ for all $i$, we would have $\sum \lambda_{i} f_{i}(v)>0$, a contradiction. Hence, there can be no such $v$. Conversely, suppose that two of the $\lambda_{i}$ 's, say $\lambda_{1}$ and $\lambda_{2}$, have different signs. Let $x_{3}=x_{4}=\cdots=x_{n}=1$, and choose $x_{1}>0$ sufficiently large so that

$$
\sum_{i \neq 2} \lambda_{i} x_{i}>0 .
$$

Then there is a real number $x_{2}>0$ such that

$$
\sum_{i=1}^{n} \lambda_{i} x_{i}=0 .
$$

But then we know that there exists $v \in f\left(\mathbb{R}^{m}\right)$ such that $f(v)=\left(x_{1}, \ldots, x_{n}\right)$. Since each of the $x_{i}$ 's is positive, we have found the desired point $v$.

Solution to 7.4.15: Let $($,$\rangle denote the ordinary inner product. From d(s, t)^{2}=$ $d(s, 0)^{2}+d(t, 0)^{2}-2\langle s, t\rangle$ and the hypothesis, it follows that

$$
\langle\varphi(s), \varphi(t)\rangle=\langle s, t\rangle \quad \text { for all } \quad s, t \in S .
$$

Let $V \subset \mathbb{R}^{n}$ denote the subspace spanned by $S$, and choose a subset $T \subset S$ that is a basis of $V$. Clearly, there is a unique linear map $f: V \rightarrow V$ that agrees with $\varphi$ on $T$. Then one has $\left\langle f(t), f\left(t^{\prime}\right)\right\rangle=\left\langle t, t^{\prime}\right\rangle$ for all $t$ and $t^{\prime} \in T$. By bilinearity, it follows that $\left\langle f(v), f\left(v^{\prime}\right)\right\rangle=\left\langle v, v^{\prime}\right\rangle$ for all $v$ and $v^{\prime} \in V$. Taking $v=v^{\prime}$, one finds that $f(v) \neq 0$ for $v \neq 0$, so $f$ is injective, and $f(V)=V$. Taking $v=s \in S$ and $v^{\prime}=t \in T$, one finds that

$$
\langle f(s), f(t)\rangle=\langle s, t\rangle=\langle\varphi(s), \varphi(t)\rangle=\langle\varphi(s), f(t)\rangle,
$$

so $f(s)-\varphi(s)$ is orthogonal to $f(t)$ for all $t \in T$, and hence to all of $f(V)=V$. That is, for all $s \in S$, one has $f(s)-\varphi(s) \in V^{\perp}$; but also $f(s)-\varphi(s) \in V$, so $f(s)-\varphi(s)=0$. This shows that $f$ agrees with $\varphi$ on $S$. It now suffices to extend $f$ to a linear map $\mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$, which one can do by supplementing $T$ to a basis for $\mathbb{R}^{n}$ and defining $f$ arbitrarily on the new basis vectors.

Solution to 7.4.16: Define $U(x)=T(x)-T(0)$, then $\|U(x)-U(y)\|=\|x-y\|$ for all $x$ and $y$ and, in particular, since 0 is a fixed point for $U,\|U(x)\|=\|x\|$. Squaring the first equality yields

$$
\langle U(x)-U(y), U(x)-U(y)\rangle=\langle x-y, x-y\rangle,
$$

or

$$
\|U(x)\|^{2}-2\langle U(x), U(y)\rangle+\|U(y)\|^{2}=\|x\|^{2}-2\langle x, y\rangle+\|y\|^{2} .
$$

Canceling equal terms gives $\langle U(x), U(y)\rangle=\langle x, y\rangle$. Expanding the inner product $\langle U(x+y)-U(x)-U(y), U(x+y)-U(x)-U(y)\rangle$ and applying this equality immediately gives that $U(x+y)-U(x)-U(y)=0$. A similar calculation shows that $U(\alpha x)-\alpha U(x)=0$. Hence, $U$ is linear, and $\langle U(x), U(y)\rangle=\langle x, y\rangle$, so $U$ is in fact orthogonal. Letting $a=T(0)$ gives the desired decomposition of $T$.

Solution 2. An isometry $T$ of $\mathbb{R}^{2}$ is a bijection $\mathbb{R}^{2} \rightarrow \mathbb{R}^{2}$ which preserves distance. Such a map also preserves the inner product, and hence preserves angles. Euclidean geometry shows that a point in the plane is specified by its distances from any three non-collinear points. Thus an isometry is determined by its images of any three non-collinear points.

The orthogonal linear maps of $\mathbb{R}^{2}$ correspond to rotations about 0 , and reflections in lines through 0 . The rotation of $\mathbb{R}^{2}$ through the angle $\theta$ is the map $R: z \mapsto z e^{i \theta}$. The reflection on a line with an angle $\alpha$ with the $x$-axis is given by composing three maps: rotation through $-\alpha$, reflection in the $x$-axis, rotation through $\alpha$. These are the maps $z \mapsto z e^{-i \alpha}, z \mapsto \bar{z}$, and $z \mapsto z e^{i \alpha}$, respectively. Thus reflection in that line is the map $S: z \mapsto \bar{z} e^{2 i \alpha}$.

Let $T(0)=a$. The translation $W(x)=x-a$ is an isometry, and $U=W T$ is an isometry that satisfies $U(0)=0$. Now $d(U(1), 0)=d(U(1), U(0))=d(1,0)=$ 1 and also $d(U(i), 0)=1$. Further if $U(i)=e^{i \theta}$, then, by preservation of angles, we see that $U(i)=\pm i e^{i \theta}$.

First the case $U(i)=i U(1)$. As $R: z \mapsto z e^{i \theta}$ has the same effect as $U$ on the non-collinear points $0,1, i$, we deduce that $U$ is that rotation. Next the case $U(i)=-i U(1)$. Let $S: z \mapsto \bar{z} e^{i \theta}$ be the reflection in line of slope $\alpha=\theta / 2$. We see that that $U$ agrees with $S$ at $0,1, i$. Thus $U$ is reflection in the line with slope $\theta / 2$. In either case, $U$ is an orthogonal map.

Finally, as $U=W T$ we have $T(x)=W^{-1} U(x)=a+U(x)$, where $U$ is an orthogonal linear transformation.

Solution to 7.4.17: We use Complete Induction [MH93, p. 32] on the dimension of $V$. If $\operatorname{dim} V=1$, then $V$ has a single basis vector $f_{1} \neq 0$, so there is $x_{1} \in X$ such that $f_{1}\left(x_{1}\right) \neq 0$. Hence, the map $f \mapsto f\left(x_{1}\right)$ is the desired isomorphism.

Now suppose the result is true for dimensions less than $n$ and let $\operatorname{dim} V=n$. Fix a basis $\left\{f_{1}, f_{2}, \ldots, f_{n}\right\}$ of $V$. Then, by the induction hypothesis, there are points $x_{1}, x_{2}, \ldots, x_{n-1}$ such that the map $f \mapsto\left(f\left(x_{1}\right), \ldots, f\left(x_{n-1}\right), 0\right)$ is an isomorphism of the subspace of $V$ spanned by $\left\{f_{1}, \ldots, f_{n-1}\right\}$ onto $\mathbb{R}^{n-1} \subset \mathbb{R}^{n}$. In particular, the vector $\left(f_{n}\left(x_{1}\right), \ldots, f_{n}\left(x_{n-1}\right), 0\right)$ is a linear combination of the basis vectors $\left\{\left(f_{i}\left(x_{1}\right), \ldots, f_{i}\left(x_{n-1}\right), 0\right), 1 \leqslant i \leqslant n-1\right\}$, so there exists a unique set of $\lambda_{i}$ 's, $1 \leqslant i \leqslant n$, such that

$$
\sum_{i=1}^{n} \lambda_{i} f_{i}\left(x_{j}\right)=0, \quad 1 \leqslant j \leqslant n-1 .
$$

Suppose there is no point $x \in X$ such that the given map is an isomorphism from $V$ onto $\mathbb{R}^{n}$. This implies that the set $\left\{\left(f_{i}\left(x_{1}\right), \ldots, f_{i}\left(x_{n-1}\right), f_{i}(x)\right), 1 \leqslant i \leqslant n\right\}$ is linearly dependent for all $x$. But because of the uniqueness of the $\lambda_{i}$ 's, this, in turn, implies that for all $x$,

$$
\sum_{i=1}^{n} \lambda_{i} f_{i}(x)=0 .
$$

Hence, the $f_{i}$ 's are linearly dependent in $V$, a contradiction. Therefore, such an $x$ exists and we are done.

\section{Solution to 7.4.18: We argue by induction on the dimension of $V$.}

If $\operatorname{dim} V=1$ let $\left\{g_{1}\right\}$ be a basis of $V . g$ is not the zero function, therefore we have $g\left(x_{1}\right) \neq 0$ for some $x_{1} \in X$. Thus $f_{1}(x)=g_{1}(x) / g_{1}\left(x_{1}\right)$ satisfies the given condition.

Suppose now that for any given subspace $W$ of the vector space of continuous real valued functions on $X$ with $\operatorname{dim} W=n$ there exists a basis $\left\{f_{1}, \ldots, f_{n}\right\}$ for $W$ and points $x_{1}, \ldots, x_{n} \in X$ such that $f_{i}\left(x_{j}\right)=\delta_{i j}$.

Let $V$ be such a subspace with $\operatorname{dim} V=n+1$. Let $\left\{g_{1}, \ldots, g_{n+1}\right\}$ be a basis for $V$, and let $W$ be the subspace spanned by $\left\{g_{1}, \ldots, g_{n}\right\}$. Then there exists a basis for $W,\left\{h_{1}, \ldots, h_{n}\right\}$, and points $x_{1}, \ldots, x_{n} \in X$ with $h_{i}\left(x_{j}\right)=\delta_{i j}$.

As each $h_{i}$ is a linear combination of elements of the basis $\left\{g_{1}, \ldots, g_{n}\right\}$, the set $\left\{h_{1}, \ldots, h_{n}, g_{n+1}\right\}$ is linerly independent, therefore a basis for $V$. Let

$$
h_{n+1}=g_{n+1}-g_{n+1}\left(x_{1}\right) h_{1}-\cdots-g_{n+1}\left(x_{n}\right) h_{n},
$$

then $\left\{h_{1}, \ldots, h_{n}, h_{n+1}\right\}$ is linearly independent and, for each $1 \leqslant j \leqslant n$, we have

$$
h_{n+1}\left(x_{j}\right)=g_{n+1}\left(x_{j}\right)-\sum_{i=1}^{n} g_{n+1}\left(x_{i}\right) \delta_{i j}=g_{n+1}\left(x_{j}\right)-g_{n+1}\left(x_{j}\right)=0 .
$$

As $h_{n+1}$ is nonzero, there is $x_{n+1} \in X$ with $h_{n+1}\left(x_{n+1}\right) \neq 0$. We have $x_{n+1} \neq x_{i}$ for $1 \leqslant i \leqslant n$ since $h_{n+1}\left(x_{i}\right)=0$. Let $f_{n+1}=h / h_{n+1}\left(x_{n+1}\right)$. For $1 \leqslant i \leqslant n$ let $f_{i}=h_{i}-h_{i}\left(x_{n+1}\right) f_{n+1}$ so that $f_{i}\left(x_{n+1}\right)=0$. The set $\left\{f_{1}, \ldots, f_{n+1}\right\}$ is linearly independent and satisfies $f_{i}\left(x_{j}\right)=\delta_{i j}$.

Solution to 7.4.19: Since the formula holds, irrespective of the values of $c_{k}$, for the polynomials $x^{2 n+1}$, it suffices, by linearity, to restrict to the vector space $P_{2 n}$ of polynomials of degree, at most, $2 n$. This vector space has dimension $2 n+1$ and the map $P_{2 n} \rightarrow \mathbb{R}^{2 n+1}$ given by $p \mapsto(p(-n), p(-n+1), \ldots, p(n))$ is an isomorphism. As the integral is a linear function on $\mathbb{R}^{2 n+1}$, there exist unique real numbers $c_{-n}, c_{-n+1}, \ldots, c_{n}$ such that

$$
\int_{-1}^{1} p(x) d x=\sum_{k=-n}^{n} c_{k} p(k) \quad \text { for all } p \in P_{2 n} \text {. }
$$

We have

$$
\int_{-1}^{1} p(x) d x=\int_{-1}^{1} p(-x) d x=\sum_{k=-n}^{n} c_{k} p(k) \quad \text { for all } p \in P_{2 n},
$$

so $c_{k}=c_{-k}$ by uniqueness of the $c_{k}$, and, therefore,

$$
\int_{-1}^{1} p(x) d x=c_{0} p(0)+\sum_{k=1}^{n} c_{k}(p(k)+p(-k)) \quad \text { for all } p \in P_{2 n} .
$$

Setting $p=1$, we find that

$$
2=c_{0}+\sum_{k=1}^{n} 2 c_{k}
$$

so, upon eliminating $c_{0}$,

$$
\int_{-1}^{1} p(x) d x=2 p(0)+\sum_{k=1}^{n} c_{k}(p(k)+p(-k)-2 p(0)) .
$$

Solution to 7.4.20: Let $\left\{v_{1}, v_{2}, \ldots, v_{n}\right\}$ be a basis for $\mathbb{R}^{n}$ consisting of eigenvectors of $T$, say $T v_{n}=\lambda_{n} v_{n}$. Let $\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ be the orthonormal basis one obtains from $\left\{v_{1}, v_{2}, \ldots, v_{n}\right\}$ by the Gram-Schmidt Procedure [HK61, p. 280]. Then, for each index $k$, the vector $u_{k}$ is a linear combination of $v_{1}, \ldots, v_{k}$, say

$$
u_{k}=c_{k 1} v_{1}+c_{k 2} v_{2}+\cdots+c_{k k} v_{k} .
$$

Also, each $v_{k}$ is a linear combination of $u_{1}, \ldots, u_{k}$. (This is guaranteed by the Gram-Schmidt Procedure; in fact, $\left\{u_{1}, \ldots, u_{k}\right\}$ is an orthonormal basis for the subspace generated by $\left\{v_{1}, \ldots, v_{k}\right\}$.) We have

$$
\begin{aligned}
T u_{k} &=c_{k 1} T v_{1}+c_{k 2} T v_{2}+\cdots+c_{k k} T v_{k} \\
&=c_{k 1} \lambda_{1} v_{1}+c_{k 2} \lambda_{2} v_{2}+\cdots+c_{k k} \lambda_{k} v_{k} .
\end{aligned}
$$

In view of the preceding remark, it follows that $T u_{k}$ is a linear combination of $u_{1}, \ldots, u_{k}$, and, thus, $T$ has upper-triangular matrix in the basis $\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$.

Solution to 7.4.21: Let $\operatorname{dim} V=k$. Relative to a basis of $V$, each linear operator $f$ on $V$ is represented by a matrix $A$ over $\mathbf{F}$. The correspondence $f \mapsto A$ is a ring isomorphism from $L(V, V)$ onto $M_{k}(\mathbf{F})$. Further, $f$ is invertible if, and only if, $A$ is invertible. As $\mathbf{F}$ is finite, the multiplicative group of invertible matrices $G L_{k}(F)$ is a finite group, so each element in $G L_{k}(F)$ has finite order. Thus if $P$ is invertible, there is $n$ such that $P^{n}=I$.

Solution to 7.4.23: 1. The characteristic polynomial of $T$ has degree 3 so it has at least one real root. The space generated by the eigenvector associated with this eigenvalue is invariant under $T$.

2. The linear transformation $T-\lambda I$ has rank 0,1 , or 2 . If the rank is 0 then $T=\lambda I$ and all subspaces are invariant, if it is 1 then $\operatorname{ker}(T-\lambda I)$ will do and if it is 2 the image of $(T-\lambda I)$ is the desired subspace. This is equivalent to the Jordan Canonical Form [HK61, p. 247] of $T$ being either a diagonal matrix with three $1 \times 1$ blocks or with one $1 \times 1$ and one $2 \times 2$ block, in both cases there is a 2-dimensional invariant subspace.

Solution to 7.4.24: Since $A$ is symmetric and orthogonal, we have $A^{2}=I$, therefore its minimal polynomial, $\mu_{A}$, divides $x^{2}-1$, which leaves three possibilities: - $\mu_{A}=x-1$. In this case $A=I$.

- $\mu_{A}=x+1$. We have $A=-I$.

- $\mu_{A}=x^{2}-1$. Here we consider two cases. If $\operatorname{dim} \operatorname{ker}(A-I)=2$, there is a 2-dimensional subspace where every vector remains fixed $(\operatorname{ker}(A-I))$, and the complement of $\operatorname{ker}(A-I), \operatorname{ker}(A+I)$, has dimension 1 , is perpendicular to $\operatorname{ker}(A-I)$, since $A$ is orthogonal. In this case we have a reflection about $\operatorname{ker}(A-I)$.

If $\operatorname{dim} \operatorname{ker}(A-I)=1$, there is an axis $(\operatorname{ker}(A-I))$ that remains fixed and $\operatorname{ker}(A+I)$ has dimension 2. Moreover, since $A$ is orthogonal, $\operatorname{ker}(A+I) \perp$ $\operatorname{ker}(A-I)$, hence for $x \in \operatorname{ker}(A+I)$ we have $A(x)=-x$, that is, $A$ is a rotation by $180^{\circ}$ about the axis $\operatorname{ker}(A-I)$.

Solution to 7.4.25: Clearly, both $R$ and $S$ are rotations and so have rank 3. Therefore, $T$, their composition, is a rank 3 operator. In particular, it must have trivial kernel. Since $T$ is an operator on $\mathbb{R}^{3}$, its characteristic polynomial is of degree 3 , and so it has a real root. This root is an eigenvalue, which must be nontrivial since $T$ has trivial kernel. Hence, the associated eigenspace must contain a line which is fixed by $T$.

Solution to 7.4.26: Let $x=\left(x_{1}, x_{2}, x_{3}\right)$ in the standard basis of $\mathbb{R}^{3}$. The line joining the points $x$ and $T x$ intersects the line containing $e$ at the point $f=\langle e, x\rangle e$ and is perpendicular to it. We then have $T x=2(f-x)+x=2 f-x$, or, in the standard basis, $T x=\left(2\langle e, x\rangle a-x_{1}, 2\langle e, x\rangle b-x_{2}, 2\langle e, x\rangle c-x_{3}\right)$. With respect to the standard basis for $\mathbb{R}^{3}$, the columns of the matrix of $T$ are $T e_{1}, T e_{2}$, and $T e_{3}$. Applying our formula and noting that $\left\langle e, e_{1}\right\rangle=a,\left\langle e, e_{2}\right\rangle=b$, and $\left\langle e, e_{3}\right\rangle=c$, we get that the matrix for $T$ is

$$
\left(\begin{array}{ccc}
2 a^{2}-1 & 2 a b & 2 a c \\
2 a b & 2 b^{2}-1 & 2 b c \\
2 a c & 2 b c & 2 c^{2}-1
\end{array}\right)
$$

Solution to 7.4.27: Since the minimal polynomial divides the characteristic polynomial and this last one has degree 3 , it follows that the characteristic polynomial of $T$ is $\left(t^{2}+1\right)(t-10)$ and the eigenvalues $\pm i$ and 10 .

Now $T(1,1,1)=\lambda(1,1,1)$ implies that $\lambda=10$ because 10 is the unique real eigenvalue of $T$.

The plane perpendicular to $(1,1,1)$ is generated by $(1,-1,0)$ and $\left(\frac{1}{2}, \frac{1}{2},-1\right)$ since these are perpendicular to each other and to $(1,1,1)$.

Let

$$
\begin{aligned}
&f_{1}=(1,1,1) \\
&f_{2}=(1,-1,0) / \sqrt{2} \\
&f_{3}=\left(\frac{1}{2}, \frac{1}{2},-1\right) / \sqrt{\frac{1}{4}+\frac{1}{4}+1}=\left(\frac{1}{2}, \frac{1}{2},-1\right) / \sqrt{\frac{3}{2}}
\end{aligned}
$$

we have $T f_{1}=10 f_{1}$ and, for $\pm i$ to be the other eigenvalues of $T, T f_{2}=f_{3}$, and $T f_{3}=-f_{2}$.

The matrix of $T$ in the basis $\left\{f_{1}, f_{2}, f_{3}\right\}=\beta$ is then

$$
[T]_{\beta}=\left(\begin{array}{ccc}
10 & 0 & 0 \\
0 & 0 & 1 \\
0 & -1 & 0
\end{array}\right)
$$

The matrix that transforms the coordinates relative to the basis $\beta$ into the coordinates relative to the canonical basis is

$$
P=\left(\begin{array}{ccc}
1 & 1 / \sqrt{2} & \sqrt{6} / 4 \\
1 & -1 / \sqrt{2} & \sqrt{6} / 4 \\
1 & 0 & -\sqrt{6} / 2
\end{array}\right)
$$

and a calculation gives

$$
P^{-1}=\left(\begin{array}{ccc}
1 / 3 & 1 / 3 & 1 / 3 \\
\sqrt{2} / 2 & -\sqrt{2} / 2 & 0 \\
2 / 3 \sqrt{6} & 2 / 3 \sqrt{6} & -4 / 3 \sqrt{6}
\end{array}\right) .
$$

Therefore, the matrix of $T$ in the canonical basis is

$$
[T]=P[T]_{\beta} P^{-1}=\left(\begin{array}{ccc}
\frac{10}{3}+\frac{1}{3 \sqrt{3}} & \frac{10}{3}+\frac{13}{36} \sqrt{3} & \frac{10}{3}-\frac{2}{3 \sqrt{3}} \\
\frac{10}{3}-\frac{1}{3 \sqrt{3}} & \frac{10}{3}+\frac{5 \sqrt{3}}{36} & \frac{10}{3}+\frac{2}{3 \sqrt{3}} \\
\frac{10}{3}+\frac{\sqrt{3}}{2} & \frac{10}{3}-\frac{\sqrt{3}}{2} & \frac{10}{3}
\end{array}\right) .
$$

Solution to 7.4.28: $A A^{t}=I$ so $|A|^{2}=1$ and indeed $|A|=1$. Thus also $\left|A^{t}\right|=1$. We have

$$
\begin{aligned}
|A-I| &=|(A-I)|\left|A^{t}\right|=\left|(A-I) A^{t}\right| \\
&=\left|I-A^{t}\right|=\left|\left(I-A^{t}\right)^{t}\right| \\
&=|I-A|=(-1)^{3}|A-I| ;
\end{aligned}
$$

hence $|A-I|=0$ and there is a nonzero $v$ such that $A v=v$.

Solution to 7.4.29: For $n=1,2, \ldots$, let $P_{n}$ be the space of polynomials whose degrees are, at most, $n$. The subspaces $P_{n}$ are invariant under $E$, they increase with $n$, and their union is $P$. To prove $E$ is invertible (i.e., one-to-one and onto), it will suffice to prove that each restriction $\left.E\right|_{P_{n}}$ is invertible. The subspace $P_{n}$ is of dimension $n+1$, it has the basis $1, x, x^{2}, \ldots, x^{n}$, with respect to which the matrix of $\left.E\right|_{P_{n}}$ is

$$
\left(\begin{array}{cccccc}
1 & 1 & 0 & 0 & \cdots & 0 \\
0 & 1 & 2 & 0 & \cdots & 0 \\
0 & 0 & 1 & 3 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \ddots & \vdots \\
0 & 0 & 0 & 0 & 1 & n \\
0 & 0 & 0 & 0 & 0 & 1
\end{array}\right) .
$$

In particular, the matrix is upper-triangular, with 1 at every diagonal entry, so its determinant is 1 . Thus, $\left.E\right|_{P_{n}}$ is invertible, as desired. Alternatively, since deg $E f=\operatorname{deg} f$, the kernel of $E$ is trivial, so its restriction to any finite dimensional invariant subspace is invertible.

Solution 2. We can describe $E$ to be $I+D$, where $I$ is the identity operator and $D$ is the derivative operator, on the vector space of all real polynomials $P$. For any element $f$ of $P$, there exists $n$ such that $D^{n}(f)=0$; namely $n=\operatorname{deg} p+1$. Thus, the inverse of $E$ can be described as $I-D+D^{2}-D^{3}+\cdots$

Specifically, writing elements of $P$ as polynomials in $x$, we have $E^{-1}(1)=1$, $E^{-1}(x)=x-1, E^{-1}\left(x^{2}\right)=x^{2}-2 x+2$, etc.

Solution to 7.4.30: Given the polynomial $\pi(x)$, there are constants $a$ and $r>0$ and a polynomial $\varphi(x)$ such that $\pi(x)=x^{r} \varphi(x)+a$. If $\varphi(x) \equiv 0$, then $\pi(D)=a I$, it follows that the minimal polynomial of the operator $\pi(D)$ is $x-a$. If $\varphi(x)$ is not zero, then for any polynomial $f \in P_{n}$, by the definition of $D,(\pi(D)-a I)(f(x))=g(x)$, where $g(x)$ is some polynomial such that $\operatorname{deg} g=\max (\operatorname{deg} f-r, 0)$. Hence, letting $E=\pi(D)-a l$, we have $e^{\lfloor n / r\rfloor+1}(f)=0$ for all $f \in P_{n} .(\lfloor n / r\rfloor$ denotes the greatest integer less than or equal to $n / r_{\text {. }}$ ) The polynomial $f(x)=x^{n}$ shows that $\lfloor n / r\rfloor+1$ is the minimal degree such that this is true. It follows from this that the minimal polynomial of $\pi(D)$ is $(x-a)^{\lfloor n / r\rfloor+1}$.

Solution to 7.4.31: 1 . Consider the basis $\left\{1, x, x^{2}, \ldots, x^{10}\right\}$ for this space of polynomials of dimension 11 . On this basis the operator takes each element into a multiple of a previous one, of one degree less, so the diagonal on this basis consist of zeroes and $\operatorname{tr} D=0$.

2. Since $D$ decreases the degree of the polynomial by one, the equation

$$
D p(x)=\alpha p(x)
$$

has no non-trivial solutions for $\alpha$, and $D$ has only 0 as eigenvalue, the eigenvectors being the constant polynomials.

Now since the $11^{\text {th }}$-derivative of any of the polynomials in this space is zero, the expression for the exponential of $D$ becomes a finite sum

$$
e^{D}=I+D+\frac{D^{2}}{2 !}+\cdots+\frac{D^{10}}{10 !}
$$

then any eigenvector $p(x)$ of degree $k$ of $e^{D}$ would have to satisfy

$$
p+p^{\prime}+\frac{p^{\prime \prime}}{2 !}+\cdots+\frac{p^{(10)}}{10 !}=\alpha p
$$

since $p$ is the only polynomial of degree $k$ in this sum, the eigenvalue $\alpha=1$ and the summation can be reduced to

$$
p^{\prime}+\frac{p^{\prime \prime}}{2 !}+\cdots+\frac{p^{(10)}}{10 !}=0
$$

now $p^{\prime}$ is the only polynomial of degree $k-1$ in the sum so its leading coefficient $k . a_{k}$ is zero and the same argument repeated over the lower degrees shows that all of them are zero except for the constant one, making it again the only eigenvector.

Solution to 7.4.32: Let $\mathcal{P}_{n}$ be the vector space of polynomials with real coefficients and degrees at most $n$. It is a real vector space of dimension $n+1$. Define the map $V: \mathcal{P}_{n} \rightarrow \mathbb{R}^{n+1}$ by

$$
V p=\left(p\left(x_{0}\right), p\left(x_{1}\right), \ldots, p\left(x_{n}\right)\right) .
$$

This map is linear, and its null space is trivial because a nonzero polynomial in $\mathcal{P}_{n}$ cannot vanish at $n+1$ distinct points. Hence $V$ is invertible. Define the homomorphism $\varphi: \mathbb{R}^{n+1} \rightarrow \mathbb{R}$ by

$$
\varphi(w)=\int_{0}^{1}\left(V^{-1} w\right)(t) d t .
$$

Being a linear functional $\varphi$ is induced by a unique vector $a=\left(a_{0}, a_{1}, \ldots, a_{n}\right)$ in $\mathbb{R}^{n+1}$ :

$$
\varphi(w)=\sum_{j=0}^{n} a_{j} w_{j}
$$

The numbers $a_{0}, a_{1}, \ldots, a_{n}$ have the required property.

Solution to 7.4.33: 1. If $p$ satisfies the equations then it does not have a constant sign on $I_{j}, j=1, \ldots, n$, so $p$ has at least $n$ zeros. If $\operatorname{deg} p<n$ this implies that $p$ is the zero polynomial.

2. Let $P_{n}$ denote the vector space of real polynomials of degree at most $n$. Its dimension is $n+1$. Each $I_{j}$ defines a functional $\varphi_{j}$ in the dual space of $P_{n}$ by

$$
\varphi_{j}(p)=\int_{l_{j}} p(x) d x .
$$

The dual space of $P_{n}$ has dimension $n+1$, so the functionals $\varphi_{1}, \ldots, \varphi_{n}$ do not span it. Hence there is a nonzero $p$ in $P_{n}$ such that $\varphi_{j}(p)=0$ for each $j$, in other words, all equations hold.

\subsection{Eigenvalues and Eigenvectors}

Solution to 7.5.1: 1 . The minimal polynomial of $M$ divides

$$
x^{3}-1=(x-1)\left(x^{2}+x+1\right) .
$$

Since $M \neq I$, the minimal polynomial (and characteristic as well) is

$$
(x-1)\left(x^{2}+x+1\right)
$$

and the only possible real eigenvalue is 1.

2.

$$
\left(\begin{array}{ccc}
1 & 0 & 0 \\
0 & \cos \frac{2 \pi}{3} & \sin \frac{2 \pi}{3} \\
0 & -\sin \frac{2 \pi}{3} & \cos \frac{2 \pi}{3}
\end{array}\right)
$$

Solution to 7.5.2: Suppose such $X$ exists; then the characteristic polynomial for $X$ is $\chi_{X}(t)=t^{n}$, but this is a contradiction since $X^{2 n-1} \neq 0$ and $2 n-1>n$.

Solution to 7.5.3: Since $A^{m}=0$ for some $m, A$ is a root of the polynomial $p(x)=x^{m}$. By the definition of the minimal polynomial, $\mu_{A}(t) \mid p(t)$, so $\mu_{A}(t)=t^{k}$ for some $k \leqslant n$, then $A^{n}=A^{k}=0$.

Solution to 7.5.4: Let $\{\alpha, \beta\}$ be a basis for the kernel of $B$, and let $\gamma$ be another vector so that $\{\alpha, \beta, \gamma\}$ is.a basis for the three dimensional space. The representation of $B$ with respect to this basis is

$$
\left(\begin{array}{lll}
0 & 0 & a \\
0 & 0 & b \\
0 & 0 & c
\end{array}\right)
$$

and the characteristic polynomial is: $\chi(\lambda)=\lambda^{2}(c-\lambda)$, from which the two first assertions follow.

The third is false since if we take $a=1, b=c=0$ above we get a matrix all of whose eigenvalues vanish, but which is not diagonalizable since it is not similar to the zero matrix.

Solution to 7.5.5: The condition on the minimal polynomial implies that $T$ is not the zero operator and that its square, $T^{2}$, is. Therefore, $\operatorname{dim}$ ker $T$ is at most six, and dim $\operatorname{ker} T^{2}$ is 7 . Since the nullity of the square of any linear operator is at most twice the nullity of the operator, the nullity of $T$ is at least 4.

Each of the three remaining integers, $4,5,6$, is possible, as is illustrated by the examples below, where $\left\{e_{1}, e_{2}, \ldots e_{7}\right\}$ is a basis for the space.

- $\operatorname{dim} \operatorname{ker} T=6$. Define $T$ by $T e_{7}=e_{6}$, and $T e_{i}=0$ for all $i<7$

- $\operatorname{dim} \operatorname{ker} T=5$. Let $T e_{7}=e_{5}, T e_{6}=e_{4}$, and $T e_{i}=0$ for all $i<6$.

- $\operatorname{dim} \operatorname{ker} T=4$. Define $T$ by $T e_{7}=e_{4}, T e_{6}=e_{3}, T e_{5}=e_{2}$, and $T e_{i}=0$ for all $i<5$.

Solution to 7.5.6: 1. Being real and symmetric, $L$ has an orthonormal basis of eigenvectors $e_{1}, \ldots, e_{n}$. Let $\lambda_{1}, \ldots, \lambda_{n}$ be the associated eigenvalues. We can assume that $\lambda_{1}=0$ and $\lambda_{i} \neq 0$ for $i>1$. Write the two vectors $v=\sum_{i=1}^{n} v_{i}$ and $x=\sum_{i=1}^{n} x_{i}$ in terms of its coordinates, then the equation $L x+\varepsilon=v$ becomes $\lambda_{i} x_{i}+\varepsilon x_{i}=v_{i}$ for each $i$, which has the unique solution $x_{i}=v_{i} /\left(\lambda_{i}+\varepsilon\right)$, provided that $0<\varepsilon<\min _{i \neq 1}\left|\lambda_{i}\right|$.

2. Writing $\varepsilon x$ in $e_{i}$ coordinates

$$
\varepsilon x=\sum_{i=1}^{n} \varepsilon x_{i} e_{i}=\sum_{i=1}^{n} \frac{\varepsilon}{\lambda_{i}+\varepsilon} v_{i} e_{i}
$$

as $\varepsilon \rightarrow 0$, all terms in the summation on the right tend to 0 , except the first which approaches $v_{1} e_{1}=\left\langle v, e_{1}\right\rangle e_{1}$.

Solution to 7.5.7: As $M^{p}=I$, the minimal polynomial of $M$ divides $t^{p}-1=$ $(t-1)\left(t^{p-1}+t^{p-2}+\cdots+t+1\right)$. Since $M$ fixes no nontrivial vector, 1 is not an eigenvalue of $M$, so it cannot be a root of the minimal polynomial. Therefore, $\mu_{M}(t) \mid\left(t^{p-1}+t^{p-2}+\cdots+t+1\right)$. Since $p$ is prime, the polynomial on the right is irreducible, so $\mu_{M}(t)$ must equal it. The minimal and characteristic polynomials of $M$ have the same irreducible factors, so $\chi_{M}(t)=\mu_{M}(t)^{k}$ for some $k \geqslant 1$. Therefore,

$$
\operatorname{dim} V=\operatorname{deg} \chi_{M}(t)=k(p-1)
$$

and we are done.

Solution to 7.5.8: 1 . Let $d=\operatorname{deg} \mu$. Since $\mu(T) v=0$, the vector $T^{d} v$ is linearly dependent on the vectors $v, T v, \ldots, T^{d-1} v$. Hence, $T^{d+n} v$ is linearly dependent on $T^{n} v, T^{n+1} v, \ldots, T^{n+d-1} v$ and so, by the Induction Principle [MH93, p. 7], on $v, T v, \ldots, T^{d-1} v$ ( $\left.n=1,2, \ldots\right)$. Thus, $v, T v, \ldots, T^{d-1} v$ span $V_{1}$, so dim $V_{1} \leqslant d$

On the other hand, the minimal polynomial of $\left.T\right|_{V_{1}}$ must divide $\mu$ (since $\mu\left(T \mid V_{1}\right)=0$ ), so it equals $\mu$ because $\mu$ is irreducible. Thus, $\operatorname{dim} V_{1} \geqslant d$. The desired equality, dim $V_{1}=d$, now follows.

2. In the case $V_{1} \neq V$, let $T_{1}$ be the linear transformation on the quotient space $V / V_{1}$ induced by $T$. (It is well defined because $V_{1}$ is $T$-invariant.) Clearly, $\mu\left(T_{1}\right)=0$, so the minimal polynomial of $T_{1}$ divides $\mu$, hence equals $\mu$. Therefore, by Part $1, V / V_{1}$ has a $T_{1}$-invariant subspace of dimension $d$, whose inverse image under the quotient map is a $T$-invariant subspace $V_{2}$ of $V$ of dimension $2 d$. In the case $V_{2} \neq V$, we can repeat the argument to show that $V$ has a $T$-invariant subspace of dimension $3 d$, and so on. After finitely many repetitions, we find dim $V=k d$ for some integers $k$.

Solution to 7.5.9: Let $v$ be an eigenvector in $\mathbb{C}^{n}$ for $M$ with nonzero eigenvalue $\alpha$. Then,

$$
M A^{-1} v=A^{-1} M^{2} v=\alpha^{2} A^{-1} v
$$

so $\alpha^{2}$ is also an eigenvalue. Thus $\alpha^{2^{k}}$ is an eigenvalue for all nonnegative integers $k$. Since the set of eigenvalues is finite there exist $0 \leqslant k<m \in \mathbb{Z}$ such that $\alpha^{2^{k}}=a^{2^{m}}$. Thus $\alpha$ is a root of unity. Solution to 7.5.10: Since the matrix is real and symmetric, its eigenvalues are real. As the trace of the matrix is 0 , and equal to the sum of its eigenvalues, it has at least one positive and one negative eigenvalue.

The matrix is invertible because the span of its columns has dimension 4. In fact, the space of the first and last columns contains all columns of the form

$$
\left(\begin{array}{l}
0 \\
* \\
* \\
0
\end{array}\right) \text {. }
$$

The span of all four columns thus contains

$$
\left(\begin{array}{l}
5 \\
0 \\
0 \\
1
\end{array}\right) \text { and }\left(\begin{array}{l}
1 \\
0 \\
0 \\
5
\end{array}\right) \text {, }
$$

which together span all columns of the form

$$
\left(\begin{array}{l}
* \\
0 \\
0 \\
*
\end{array}\right) \text {. }
$$

Since the matrix is invertible it does not have 0 as an eigenvalue. There are now only three possibilities:

- three positive and one negative eigenvalues;

- two positive and two negative eigenvalues;

- one positive and three negative eigenvalues.

A calculation shows that the determinant is positive. Since it equals the product of the eigenvalues, we can only have two positives and two negatives, completing the proof.

Solution to 7.5.11: A calculation shows that the characteristic polynomial of the given matrix is

$$
-x\left(x^{2}-3 x-2\left(1.00001^{2}-1\right)\right)
$$

therefore one of the eigenvalues is 0 and the product of the other two is

$$
\left.-2\left(1.00001^{2}-1\right)\right)<0
$$

so one is negative and the other is positive.

Solution to 7.5.12: Denote the matrix by $A$. A calculation shows that $A$ is a root of the polynomial $p(t)=t^{3}-c t^{2}-b t-a$. In fact, this is the minimal polynomial of $A$. To prove this, it suffices to find a vector $x \in \mathbf{F}^{3}$ such that $A^{2} x, A x$, and $x$ are linearly independent. Let $x=(1,0,0)$. Then $A x=(0,1,0)$ and $A^{2} x=(0,0,1)$; these three vectors are linearly independent, so we are done.

Solution to 7.5.13: The matrix $A+I$ is diagonalizable, being unitary, so $A$ also is diagonalizable. It will therefore suffice to show that $A$ has no nonzero eigenvalues.

Suppose $\lambda$ is a nonzero eigenvalue of $A$. Then $\lambda^{j}+1$ is an eigenvalue of $A^{j}+I$ $(j=1,2,3)$ and so has unit modulus. Thus $\lambda, \lambda^{2}, \lambda^{3}$ all lie on the circle $|z+1|=$ 1 , so each has an argument strictly between $\frac{\pi}{2}$ and $\frac{3 \pi}{2}$. But if $\frac{3 \pi}{4} \leqslant \arg \lambda \leqslant \pi$ then $\frac{3 \pi}{2} \leqslant \arg \lambda^{2} \leqslant 2 \pi$, a contradiction. And if $\frac{\pi}{2}<\arg \lambda<\frac{3 \pi}{4}$ then $\frac{3 \pi}{2}<$ $\arg \lambda^{3}<\frac{9 \pi}{4} \equiv \frac{\pi}{4}(\bmod 2 \pi)$, again a contradiction. The case where $\lambda$ is in the third quadrant can be handled similarly.

Solution to 7.5.14: The first direction is obvious, lets assume now that every nonzero vector is an eigenvector of $A$, then in particular, the vectors of the standard basis, $e_{i}$ are are eigenvectors and $A$ is diagonal. Lets assume the diagonal entries are $A_{i i}=\lambda_{i}$. If $\lambda_{i} \neq \lambda_{j}$, then $A\left(e_{i}+e_{j}\right)=\lambda_{i} e_{i}+\lambda_{j} e_{j}$ is not a multiple scalar of $e_{i}+e_{j}$, contradicting the hypothesis that $e_{i}+e_{j}$ is an eigenvector of $A$. Then all diagonal entries are equal, finishing the proof.

![](https://cdn.mathpix.com/cropped/2022_10_26_a807eeb357d0f35abe4eg-062.jpg?height=76&width=1382&top_left_y=1157&top_left_x=369)
$A B=\left(\begin{array}{ll}1 & 2 \\ 1 & 2\end{array}\right)$ and has $(1,1)$ as an eigenvector, which is clearly not an eigenvector of $B A=\left(\begin{array}{ll}2 & 2 \\ 1 & 1\end{array}\right)$. The condition that $A B$ and $B A$ have a common eigenvector is algebraic in the entries of $A$ and $B$; therefore it is satisfied either by all of $A$ and $B$ or by a subset of codimension at least one, so in dimensions two and higher almost every pair of matrices would be a counterexample.

2. This is true. Let $x$ be an eigenvector associated with the eigenvalue $\lambda$ of $A B$. We have

$$
B A(B x)=B(A B x)=B(\lambda x)=\lambda B x
$$

so $\lambda$ is an eigenvalue of $B A$.

Solution to 7.5.16: Regard $A$ and $B$ as linear transformations on $\mathbb{C}^{n}$. Then $A$ has an eigenvalue $\lambda$. Let $S_{\lambda}$ be the corresponding eigenspace. For $v$ in $S_{\lambda}$, we have

$$
A(B v)=B(A v)=B(\lambda v)=\lambda B v,
$$

showing that $S_{\lambda}$ is invariant under $B$. The linear transformation $\left.B\right|_{S_{\lambda}}$ has an eigenvalue $\mu$. If $v$ is a corresponding eigenvector we have $A v=\lambda v$ and $B v=\mu v$.

Solution to 7.5.17: We use the Induction Principle [MH93, p. 7]. As the space $M_{n}(\mathbb{C})$ is finite dimensional, we may assume that $S$ is finite. If $S$ has one element, the result is trivial. Now suppose any commuting set of $n$ elements has a common eigenvector, and let $S$ have $n+1$ elements $A_{1}, \ldots, A_{n+1}$. By induction hypothesis, the matrices $A_{1}, \ldots, A_{n}$ have a common eigenvector $v$. Let $E$ be the vector space spanned by the common eigenvectors of $A_{1}, \ldots, A_{n}$. If $v \in E, A_{i} A_{n+1} v=$ $A_{n+1} A_{i} v=\lambda_{i} A_{n+1} v$ for all $i$, so $A_{n+1} v \in E$. Hence, $A_{n+1}$ fixes $E$. Let $B$ be the restriction of $A_{n+1}$ to $E$. The minimal polynomial of $B$ splits into linear factors (since we are dealing with complex matrices), so $B$ has an eigenvector in $E$, which must be an eigenvector of $A_{n+1}$ by the definition of $B$, and an eigenvector for each of the other $A_{i}$ 's by the definition of $E$.

Solution to 7.5.20: 1. For $\left(a_{1}, a_{2}, a_{3}, \ldots\right)$ to be an eigenvector associated with the eigenvalue $\lambda$, we must have

$$
S\left(\left(a_{1}, a_{2}, a_{3}, \ldots\right)\right)=\lambda\left(a_{2}, a_{3}, a_{4}, \ldots\right)
$$

which is equivalent to

$$
a_{2}=\lambda a_{1}, \quad a_{3}=\lambda a_{2}, \ldots, \quad a_{n}=\lambda a_{n-1}, \ldots
$$

so the eigenvectors are of the form $a_{1}\left(1, \lambda, \lambda^{2}, \ldots\right)$.

2. Let $x=\left(x_{1}, x_{2}, \ldots\right) \in W$. Then $x$ is completely determined by the first two components $x_{1}$ and $x_{2}$. Therefore, the dimension of $W$ is, at most, two. If an element of $W$ is an eigenvector, it must be associated with an eigenvalue satisfying $\lambda^{2}=\lambda+1$, which gives the two possible eigenvalues

$$
\varphi=\frac{1+\sqrt{5}}{2} \text { and }-\varphi^{-1}=\frac{1-\sqrt{5}}{2} \text {. }
$$

A basis for $W$ is then

$$
\left\{\left(\varphi, \varphi^{2}, \varphi^{3}, \ldots\right),\left(-\varphi^{-1}, \varphi^{-2},-\varphi^{-3}, \ldots\right)\right\}
$$

which is clearly invariant under $S$.

3. To express the Fibonacci sequence in the basis above, we have just to find the constants $k_{1}$ and $k_{2}$ that satisfy

$$
\left\{\begin{array}{l}
1=k_{1} \varphi-k_{2} \varphi^{-1} \\
1=k_{1} \varphi^{2}+k_{2} \varphi^{-2}
\end{array}\right.
$$

which give $k_{1}=\frac{1}{\sqrt{5}}=-k_{2}$. We then have, for the Fibonacci numbers,

$$
f_{n}=\frac{1}{\sqrt{5}}\left(\left(\frac{1+\sqrt{5}}{2}\right)^{n}-\left(\frac{1-\sqrt{5}}{2}\right)^{n}\right) .
$$

Solution to 7.5.21: If $T(v)=\lambda v$ then $T^{n}(v)=\lambda^{n} v$ for all $n \geqslant 1$, by induction. If $f=a_{0}+a_{1} x+\cdots+a_{n} x^{n}$ then $f(T)=a_{0} I+a_{1} T+\cdots+a_{n} T^{n}$, and $f(T)(v)=a_{0} v+a_{1} T(v)+\cdots+a_{n} T^{n}(v)=f(\lambda) v$ so that $f(\lambda)$ is an eigenvalue of $f(T)$.

If $\lambda$ is an eigenvalue of $f(T)$ then $|f(T)-\lambda I|=0$. Factorize $f(x)-\lambda=$ $\alpha_{0}\left(x-\alpha_{1}\right) \cdots\left(x-\alpha_{n}\right)$. (The case of constant $f(x)$ is trivial.) Then $f(T)-\lambda I=$ $\alpha_{0}\left(T-\alpha_{1} I\right) \cdots\left(T-\alpha_{n} I\right)$. Taking determinants we see that there is $i$ such that $\left|T-\alpha_{i} I\right|=0$. Therefore $\alpha_{i}$ is an eigenvalue of $T$ with $f\left(\alpha_{i}\right)=\lambda$. 

\section{Solution to 7.5.22:}

$$
A=u u^{t}-I \quad \text { where } \quad u=\left(\begin{array}{c}
1 \\
\vdots \\
1
\end{array}\right)
$$

and $I$ is the identity matrix. If $A x=\lambda x$, where $x \neq 0$, then

$$
u u^{t} x-x=\left(u^{t} x\right) u-x=\lambda x
$$

so $x$ is either perpendicular or parallel to $u$. In the latter case, we can suppose without loss of generality that $x=u$, so $u^{t} u u-u=\lambda u$ and $\lambda=n-1$. This gives a 1-dimensional eigenspace spanned by $u$ with eigenvalue $n-1$. In the former case $x$ lies in a $(n-1)$-dimensional eigenspace which is the null space of the rank one matrix $u u^{t}$, so

$$
A x=\left(u u^{t}-I\right) x=-I x=-x
$$

and the eigenvalue associated with this eigenspace is $-1$, with multiplicity $n-1$. Since the determinant is the product of the eigenvalues, we have $\operatorname{det}(A)=(-1)^{n-1}(n-1)$.

Solution to 7.5.23: Since $A$ is positive definite, there is an invertible Hermitian matrix $C$ such that $C^{2}=A$. Thus, we have $C^{-1}(A B) C=C^{-1} C^{2} B C=C B C$. By taking adjoints, we see that $C B C$ is Hermitian, so it has real eigenvalues. Since similar matrices have the same eigenvalues, $A B$ has real eigenvalues.

Solution to 7.5.24: Let $v$ be the column vector defined by $\left(\begin{array}{llll}a & b & c & d\end{array}\right)^{t}$, then the operator given, that we will call $T$, is literally the product of the two matrices $v v^{t}$. and the operator $T$ applied to a vector $x$ is:

$$
T x=\left(v v^{t}\right) x=v\left(v^{t} x\right)=v\langle v, x\rangle
$$

where $\langle$,$\rangle is the standard euclidean inner product in \mathbb{R}^{4}$.

Let $u$ be the vector of norm 1 in the direction of $v$, that is, $u=v /|v|, V$ the uni-dimensional space generated by $v$, and $V^{\perp}$ its orthogonal complement in $\mathbb{R}^{4}$. Then any vector in $x \in \mathbb{R}^{4}$ can be written as $x=x_{\|}+x_{\perp}$ where

$$
\begin{aligned}
x_{\|} &=\langle u, x\rangle u \\
x_{\perp} &=x-\langle u, x\rangle u
\end{aligned}
$$

In this decomposition $T u=|v|^{2} u|u|^{2}=|v|^{2} u$ and $u$ is an eigenvector of $T$ with eigenvalue $|v|^{2}$ and multiplicity 1 . For any vector in the $V^{\perp}$ space

$$
T x_{\perp}=|v|^{2} u\left\langle u, x_{\perp}\right\rangle=0
$$

so taking any basis for this 3-dimensional space, the matrix representing $T$ will be

$$
\left(\begin{array}{cccc}
a^{2}+b^{2}+c^{2}+d^{2} & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{array}\right)
$$

Solution to 7.5.25: $1 \Rightarrow 2:$ As $A$ is symmetric, it is similar to a diagonal matrix, $\operatorname{diag}\left(\lambda_{1}, \lambda_{2}, \lambda_{3}\right)$. We have $\operatorname{tr} A=\lambda_{1}+\lambda_{2}+\lambda_{3}$. The eigenvalues of $A$ are the zeros of the polynomial $\left(\lambda-\lambda_{1}\right)\left(\lambda-\lambda_{2}\right)\left(\lambda-\lambda_{3}\right)$ and the conclusion follows.

$2 \Rightarrow 3$ : Let $\left\{c_{1}, c_{2}, c_{3}\right\}$ be the ordered basis of $\mathbb{R}^{3}$ with respect to which the representation of $A$ is $\operatorname{diag}\left(\lambda_{1}, \lambda_{2}, \lambda_{3}\right)$. For $W \in S$ we have $W^{t}=-W$, therefore, using the standard inner product in $\mathbb{R}^{3}$, we obtain, for $i=1,2,3$,

$$
\left\langle W c_{i}, c_{i}\right\rangle=\left\langle c_{i}, W^{t} c_{i}\right\rangle=-\left\langle c_{i}, W c_{i}\right\rangle=-\left\langle W c_{i}, c_{i}\right\rangle
$$

therefore $W c_{i} \perp c_{i}$

$L$ is clearly a linear self map of $S$, so it suffices to show that it is injective.

Suppose $L(W)=0$. Then $A W=-W A$, so we get, for $i \neq j$,

$$
\begin{aligned}
\left\langle A W c_{i}, c_{j}\right\rangle &=-\left\langle W A c_{i}, c_{j}\right\rangle \\
&=\left\langle A c_{i}, W c_{j}\right\rangle \\
&=\lambda_{i}\left\langle c_{i}, W c_{j}\right\rangle
\end{aligned}
$$

and

$$
\begin{aligned}
\left\langle-W A c_{i}, c_{j}\right\rangle &=\left\langle A W c_{i}, c_{j}\right\rangle \\
&=\lambda_{j}\left\langle W c_{i}, c_{j}\right\rangle \\
&=-\lambda_{j}\left\langle c_{i}, W c_{j}\right\rangle
\end{aligned}
$$

as $\lambda_{i}+\lambda_{j} \neq 0$, we must have $W c_{j} \perp c_{i}$. We conclude then that $W c_{i}=0$ for $i=1,2,3$, therefore $W=0$.

$3 \Rightarrow 1:$ Suppose $\operatorname{tr} A$ is an eigenvalue. According to what we saw previously in the Part $1 \Rightarrow 2$, there is no loss in generality in assuming $\lambda_{1}+\lambda_{2}=0$. Let $W$ be defined on the vectors of the basis $\left\{c_{1}, c_{2}, c_{3}\right\}$ by $W c_{1}=-c_{2}, W c_{2}=-c_{1}$, $W c_{3}=0$. Clearly $W \in S$, and $W \neq 0$. We have,

$$
\begin{aligned}
(A W+W A) c_{1} &=A W c_{1}+W A c_{1} \\
&=-A c_{2}+\lambda_{1} W c_{1} \\
&=-\lambda_{2} c_{2}-\lambda_{1} c_{2} \\
&=0
\end{aligned}
$$

so $L$ is not an isomorphism.

Solution to 7.5.26: The characteristic polynomial of $A$ is

$$
\chi_{A}(t)=t^{2}-(a+d) t+(a d-b c)
$$

which has roots

$$
t=\frac{1}{2}(a+d) \pm \frac{1}{2} \sqrt{(a-d)^{2}+4 b c}=\frac{1}{2}(a+d \pm \sqrt{\Delta})
$$

$\Delta$ is positive, so $A$ has real eigenvalues. Let $\lambda=\frac{1}{2}(a+d+\sqrt{\Delta})$ and let $v=(x, y)$ be an eigenvector associated with this eigenvalue with $x>0$. Expanding the first entry of $A v$, we get

$$
a x+b y=\frac{1}{2}(a+d+\sqrt{\Delta}) x
$$

or

$$
2 b y=(d-a+\sqrt{\Delta}) x
$$

Since $b>0$, to see that $y>0$ it suffices to show that $d-a+\sqrt{\Delta}>0$, or $\sqrt{\Delta}>a-d$. But this is immediate from the definition of $\sqrt{\Delta}$ and we are done.

Solution to 7.5.27: It suffices to show that $A$ is positive definite. Let $x=\left(x_{1}, \ldots, x_{n}\right)$, we have

$$
\begin{aligned}
\langle A x, x\rangle &=2 x_{1}^{2}-x_{1} x_{2}-x_{1} x_{2}+2 x_{2}^{2}-x_{2} x_{3}-\cdots-x_{n-1} x_{n}+2 x_{n}^{2} \\
&=x_{1}^{2}+\left(x_{1}-x_{2}\right)^{2}+\left(x_{2}-x_{3}\right)^{2}+\cdots+\left(x_{n-1}-x_{n}\right)^{2}+x_{n}^{2}
\end{aligned}
$$

Thus, for all nonzero $x,\langle A x, x\rangle \geqslant 0$. In fact, it is strictly positive, since one of the center terms is greater than 0 , or $x_{1}=x_{2}=\cdots=x_{n}$ and all the $x_{i}$ 's are nonzero, so $x_{1}^{2}>0$. Hence, $A$ is positive definite and we are done.

Solution 2. Since $A$ is symmetric, all eigenvalues are real. Let $x=\left(x_{i}\right)_{1}^{n}$ be an eigenvector with eigenvalue $\lambda$. Since $x \neq 0$, we have $\max _{i}\left|x_{i}\right|>0$. Let $k$ be the least $i$ with $\left|x_{i}\right|$ maximum. Replacing $x$ by $-x$, if necessary, we may assume $x_{k}>0$. We have

$$
\lambda x_{k}=-x_{k-1}+2 x_{k}-x_{k+1}
$$

where nonexistent terms are taken to be zero. By the choice of $x_{k}$, we have $x_{k-1}<x_{k}$ and $x_{k+1} \leqslant x_{k}$, so we get $\lambda x_{k}>0$ and $\lambda>0$.

Solution to 7.5.28: Let $\lambda_{0}$ be the largest eigenvalue of $A$. We have

$$
\lambda_{0}=\max \{\langle A x, x\rangle \mid x \in \mathbb{R},\|x\|=1\} \text {, }
$$

and the maximum it attained precisely when $x$ is an eigenvector of $A$ with eigenvalue $\lambda_{0}$. Suppose $v$ is a unit vector for which the maximum is attained, and let $u$ be the vector whose coordinates are the absolute values of the coordinates of $v$. Since the entries of $A$ are nonnegative, we have

$$
\langle A u, u\rangle \geqslant\langle A v, v\rangle=\lambda_{0}
$$

implying that $\langle A u, u\rangle=\lambda_{0}$ and so that $u$ is an eigenvector of $A$ for the eigenvalue $\lambda_{0}$ Solution to 7.5.29: Let $\xi_{n}$ be the characteristic polynomial of $A_{n}$. We have $\xi_{1}=-z, \xi_{2}=z^{2}-1$, and, for $n \geqslant 2, \xi_{n}=-z \xi_{n-1}(z)-\xi_{n-2}(z)$. Thus, by induction, $\xi_{n}$ contains only even powers of $z$ for even $n$, and only odd powers of $z$ for odd $n$. It follows that the zeros of $\xi_{n}$ are symmetric with respect to the origin. Solution 2. Let $S_{n}$ be the diagonal $n \times n$ matrix with alternating l's and $-1$ 's on the main diagonal. Then $S^{-1} A S=-A$, from which the desired conclusion follows.

Solution to 7.5.30: Let $\lambda$ be an eigenvalue of $A$ and $x=\left(x_{1}, \ldots, x_{n}\right)^{t}$ a corresponding eigenvector. Let $x_{i}$ be the entry of $x$ whose absolute value is greatest. We have

$$
\lambda x_{i}=\sum_{j=1}^{n} a_{i j} x_{j}
$$

so

$$
|\lambda|\left|x_{i}\right| \leqslant \sum_{j=1}^{n} a_{i j}\left|x_{j}\right| \leqslant\left|x_{j}\right| \sum_{j=1}^{n} a_{i j}=\left|x_{i}\right| \text {. }
$$

Hence, $|\lambda| \leqslant 1$.

Solution to 7.5.31: 1. Regard $S$ as a linear transformation on $\mathbb{C}^{n}$. Since $S$ is orthogonal it preserves norms, $\|S x\|=\|x\|$ for all $x$. Hence all eigenvalues have modulo 1. The characteristic polynomial of $S$ has real coefficients, so the nonreal eigenvalues occur in conjugate pairs. There is thus an even number of nonreal eigenvalues, counting multiplicities, and the product of all of them is 1 . Assuming $n$ is odd, there must be an odd number of real eigenvalues, which can only equal 1 or $-1$. The product of all the eigenvalues is $\operatorname{det}(S)=1$. Hence $-1$ must occur as an eigenvalue with even multiplicity, so that 1 occurs with an odd, hence nonzero, multiplicity.

2. If $n$ is even then $-I_{n}$ is a special orthogonal matrix, yet it does not have 1 as an eigenvalue.

Solution to 7.5.32: Since $A$ is Hermitian, by Rayleigh's Theorem [ND88, p. 418], we have

$$
\lambda_{\min } \leqslant \frac{\langle x, A x\rangle}{\langle x, x\rangle} \leqslant \lambda_{\max }
$$

for $x \in \mathbb{C}^{m}, x \neq 0$, where $\lambda_{\min }$ and $\lambda_{\max }$ are its smallest and largest eigenvalues, respectively. Therefore,

$$
a \leqslant \frac{\left\langle x, A_{x}\right\rangle}{\langle x, x\rangle} \leqslant a^{\prime}
$$

Similarly for $B$ :

$$
b \leqslant \frac{\langle x, B x\rangle}{\langle x, x\rangle} \leqslant b^{\prime}
$$

Hence,

$$
a+b \leqslant \frac{\langle x,(A+B) x\rangle}{\langle x, x\rangle} \leqslant a^{\prime}+b^{\prime} .
$$

However, $A+B$ is Hermitian, since $A$ and $B$ are, so the middle term above is bounded above and below by the largest and smallest eigenvalues of $A+B$. But, again by Rayleigh's Theorem, we know these bounds are sharp, so all the eigenvalues of $A+B$ must lie in $\left[a+b, a^{\prime}+b^{\prime}\right]$.

Solution to 7.5.33: Let $v=(1,1,0, \ldots, 0)$. A calculation shows that $A v=(k+1, k+1,1,0, \ldots, 0)$, so

$$
\frac{\langle A v, v\rangle}{\langle v, v\rangle}=k+1 \text {. }
$$

Similarly, for $u=(1,-1,0, \ldots, 0)$, we have $A u=(k-1,1-k,-1,0, \ldots, 0)$ and so

$$
\frac{\langle A u, u\rangle}{\langle u, u\rangle}=k-1 \text {. }
$$

By Rayleigh's Theorem [ND88, p. 418], we know that

$$
\lambda_{\min } \leqslant \frac{\langle A v, v\rangle}{\langle v, v\rangle} \leqslant \lambda_{\max } .
$$

for all nonzero vectors $v$, and the desired conclusion follows.

Solution to 7.5.34: As $B$ is positive definite, there is an invertible matrix $C$ such that $B=C^{t} C$, so

$$
\frac{\langle A x, x\rangle}{\langle B x, x\rangle}=\frac{\langle A x, x\rangle}{\left\langle C^{t} C x, x\right\rangle}=\frac{\langle A x, x\rangle}{\langle C x, C x\rangle} .
$$

Let $C x=y$. The right-hand side equals

$$
\frac{\left\langle A C^{-1} y, C^{-1} y\right\rangle}{\langle y, y\rangle}=\frac{\left\langle\left(C^{-1}\right)^{t} A C^{-1} y, y\right\rangle}{\langle y, y\rangle} .
$$

Since the matrix $\left(C^{-1}\right)^{t} A C^{-1}$ is symmetric, by Rayleigh's Theorem [ND88, p. 418], the right-hand side is bounded by $\lambda$, where $\lambda$ is the largest eigenvalue of $\left(C^{-1}\right)^{t} A C^{-1}$. Further, the maximum is attained at the associated eigenvector. Let $y_{0}$ be such an eigenvector. Then $G(x)$ attains its maximum at $x=C^{-1} y 0$, which is an eigenvector of the matrix $\left(C^{-1}\right)^{t} A$.

Solution to 7.5.35: Let $y \neq 0$ in $\mathbb{R}^{n}$. $A$ is real symmetric, so there is an orthogonal matrix, $P$, such that $B=P^{t} A P$ is diagonal. Since $P$ is invertible, there is a nonzero vector $z$ such that $y=P z$. Therefore,

$$
\frac{\left\langle A^{m+1} y, y\right\rangle}{\left\langle A^{m} y, y\right\rangle}=\frac{\left\langle A^{m+1} P z, P z\right\rangle}{\left\langle A^{m} P z, P z\right\rangle}=\frac{\left\langle P^{t} A^{m+1} P z, z\right\rangle}{\left\langle P^{t} A^{m} P z, z\right\rangle}=\frac{\left\langle B^{m+1} z, z\right\rangle}{\left\langle B^{m} z, z\right\rangle} .
$$

Since $A$ is positive definite, we may assume without loss of generality that $B$ has the form

$$
\left(\begin{array}{cccc}
\lambda_{1} & 0 & \cdots & 0 \\
0 & \lambda_{2} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \lambda_{n}
\end{array}\right)
$$

where $\lambda_{1} \geqslant \lambda_{2} \geqslant \cdots \geqslant \lambda_{n}>0$. Let $z=\left(z_{1}, \ldots, z_{n}\right) \neq 0$, and $i \leqslant n$ be such that $z_{i}$ is the first nonzero coordinate of $z$. Then

$$
\begin{aligned}
\frac{\left\langle B^{m+1} z, z\right\rangle}{\left\langle B^{m} z, z\right\rangle} &=\frac{\lambda_{i}^{m+1} z_{i}^{2}+\cdots+\lambda_{n}^{m+1} z_{n}^{2}}{\lambda_{i}^{m} z_{i}^{2}+\cdots+\lambda_{n}^{m} z_{n}^{2}} \\
&=\lambda_{i}\left(\frac{z_{i}^{2}+\left(\lambda_{i+1} / \lambda_{i}\right)^{m+1} z_{i+1}^{2}+\cdots+\left(\lambda_{n} / \lambda_{i}\right)^{m+1} z_{n}^{2}}{z_{i}^{2}+\left(\lambda_{i+1} / \lambda_{i}\right)^{m} z_{i+1}^{2}+\cdots+\left(\lambda_{n} / \lambda_{1}\right)^{m} z_{n}^{2}}\right) \\
& \sim \lambda_{i} \quad(m \rightarrow \infty) .
\end{aligned}
$$

\subsection{Canonical Forms}

Solution to 7.6.1: The minimal polynomial of $A$ divides $x^{k}-1$ so it has no multiple roots, which implies that $A$ is diagonalizable.

Solution to 7.6.2: Assume $A^{m}$ is diagonalizable. Then its minimal polynomial, $\mu_{A^{m}}(x)$, has no repeated roots, that is,

$$
\mu_{A^{m}}(x)=\left(x-a_{1}\right) \cdots\left(x-a_{k}\right)
$$

where $a_{i} \neq a_{j}$ for $i \neq j$.

The matrix $A^{m}$ satisfies the equation

$$
\left(A^{m}-a_{1} I\right) \cdots\left(A^{m}-a_{k} I\right)=0
$$

so $A$ is a root of the polynomial $\left(x^{m}-a_{1}\right) \cdots\left(x^{m}-a_{k}\right)$, therefore, $\mu_{A}(x)$ divides this polynomial. To show that $A$ is diagonalizable, it is enough to show this polynomial has no repeated roots, which is clear, because the roots of the factors $x^{m}-a_{i}$ are different, and different factors have different roots.

This proves more than what was asked; it shows that if $A$ is an invertible linear transformation on a finite dimensional vector space over a field $\mathbf{F}$ of characteristic not dividing $n$, the characteristic polynomial of $A$ factors completely over $F$, and if $A^{n}$ is diagonalizable, then $A$ is diagonalizable.

On this footing, we can rewrite the above proof as follows: We may suppose that the vector space $V$ has positive dimension $m$. Let $\lambda$ be an eigenvalue of $A$. Then $\lambda \neq 0$. We may replace $V$ by the largest subspace of $V$ on which $A-\lambda I$ is nilpotent, so that we may suppose the characteristic polynomial of $A$ is $(x-\lambda)^{m}$. Since $A^{n}$ is diagonalizable, we must have $A^{n}=\lambda^{n} I$ since $\lambda^{n}$ is the only eigenvalue of $A^{n}$. Thus, $A$ satisfies the equation $x^{n}-\lambda^{n}=0$. Since the only common factor of $x^{n}-\lambda^{n}$ and $(x-\lambda)^{n}$ is $x-\lambda$, and as the characteristic of $\mathbf{F}$ does not divide $n, A=\lambda I$ and, hence, is diagonal.

Solution to 7.6.3: The characteristic polynomial of $A$ is $\chi_{A}(x)=x^{2}-3$, so $A^{2}=31$, and multiplying both sides by $A^{-1}$, we have

$$
A^{-1}=\frac{1}{3} A \text {. }
$$

Solution to 7.6.4: 1. Subtracting the second line from the other three and expanding along the first line, we have

$$
\begin{aligned}
\operatorname{det} A_{x} &=(x-1)\left|\begin{array}{ccc}
x & 1 & 1 \\
1-x & x-1 & 0 \\
1-x & 0 & x-1
\end{array}\right|+(x-1)\left|\begin{array}{ccc}
1 & 1 & 1 \\
0 & x-1 & 0 \\
0 & 0 & x-1
\end{array}\right| \\
&=(x-1)^{3}(x+3) .
\end{aligned}
$$

2. Suppose now that $x \neq 1$ and $-3$. Then $A_{x}$ is invertible and the characteristic polynomial is given by:

$$
\begin{aligned}
\chi_{A_{x}}(t) &=\left|\begin{array}{cccc}
t-x & -1 & -1 & -1 \\
-1 & t-x & -1 & -1 \\
-1 & -1 & t-x & -1 \\
-1 & -1 & -1 & t-x
\end{array}\right|=\left|\begin{array}{cccc}
x-t & 1 & 1 & 1 \\
1 & x-t & 1 & 1 \\
1 & 1 & x-t & 1 \\
1 & 1 & 1 & x-t
\end{array}\right| \\
&=(x-t-1)^{3}(x-t+3)
\end{aligned}
$$

Now an easy substitution shows that the minimal polynomial is

$$
\mu_{A_{x}}(t)=(x-t-1)(x-t-3)
$$

so substituting $t$ by $A_{x}$, we have

$$
\begin{aligned}
\left((x-1) I_{4}-A_{x}\right)\left((x+3) I_{4}-A_{x}\right) &=0 \\
(x-1)(x+3) I_{4}-2(x+1) A_{x}-A_{x}^{2} &=0
\end{aligned}
$$

multiplying both sides by $A_{x}^{-1}$,

$$
\begin{aligned}
(x-1)(x+3) A_{x}^{-1} &=2(x+1) I_{4}-A_{x} \\
&=-A_{-x-2}
\end{aligned}
$$

so

$$
A_{x}^{-1}=-(x-1)^{-1}(x+3)^{-1} A_{-x-2} .
$$

Solution to 7.6.5: The characteristic polynomial of the matrix $A$ is $\chi_{A}(t)=t^{3}-8 t^{2}+20 t-16=(t-4)(t-2)^{2}$ and the minimal polynomial is $\mu_{A}(t)=(t-2)(t-4)$. By the Euclidean Algorithm [Her75, p. 155], there is a polynomial $p(t)$ and constants $a$ and $b$ such that

$$
t^{10}=p(t) \mu_{A}(t)+a t+b .
$$

Substituting $t=2$ and $t=4$ and solving for $a$ and $b$ yields $a=2^{9}\left(2^{10}-1\right)$ and $b=-2^{11}\left(2^{9}-1\right)$. Therefore, since $A$ is a root of its minimal polynomial,

$$
A^{10}=a A+b I=\left(\begin{array}{ccc}
3 a+b & a & a \\
2 a & 4 a+b & 2 a \\
-a & -a & a+b
\end{array}\right) \text {. }
$$

Solution to 7.6.6: The characteristic polynomial of $A$ is $\chi_{A}(t)=t^{2}-2 t+1=$ $(t-1)^{2}$. By the Euclidean Algorithm [Her75, p. 155], there is a polynomial $q(t)$ and constants $a$ and $b$ such that $t^{100}=q(t)(t-1)^{2}+a t+b$. Differentiating both sides of this equation, we get $100 t^{99}=q^{\prime}(t)(t-1)^{2}+2 q(t)(t-1)+a$. Substituting $t=1$ into each equation and solving for $a$ and $b$, we get $a=100$ and $b=-99$. Therefore, since $A$ satisfies its characteristic equation, substituting it into the first equation yields $A^{100}=100 A-991$, or

$$
A^{100}=\left(\begin{array}{cc}
51 & 50 \\
-50 & -49
\end{array}\right) \text {. }
$$

An identical calculation shows that $A^{7}=7 A-6 I$, so

$$
A^{7}=\left(\begin{array}{cc}
9 / 2 & 7 / 2 \\
-7 / 2 & -5 / 2
\end{array}\right) \text {. }
$$

From this it follows immediately that

$$
A^{-7}=\left(\begin{array}{rr}
-5 / 2 & -7 / 2 \\
7 / 2 & 9 / 2
\end{array}\right) .
$$

Solution to 7.6.7: Counterexample: Let

$$
A=\left(\begin{array}{ll}
0 & 1 \\
0 & 0
\end{array}\right)=B^{2}=\left(\begin{array}{ll}
a & b \\
c & d
\end{array}\right)\left(\begin{array}{ll}
a & b \\
c & d
\end{array}\right)=\left(\begin{array}{ll}
a^{2}+b c & a b+b d \\
c a+d c & c b+d^{2}
\end{array}\right) .
$$

Equating entries, we find that $c(a+d)=0$ and $b(a+d)=1$, so $b \neq 0$ and $a+d \neq 0$. Thus, $c=0$. The vanishing of the diagonal entries of $B^{2}$ then implies that $a^{2}=d^{2}=0$ and, thus, $a+d=0$. This contradiction proves that no such $B$ can exist, so $A$ has no square root.

Solution 2. Let

$$
A=\left(\begin{array}{ll}
0 & 1 \\
0 & 0
\end{array}\right) .
$$

Any square root $B$ of $A$ must have zero eigenvalues, and since it cannot be the zero matrix, it must have Jordan Canonical Form [HK61, p. 247] $J B J^{-1}=A$. But then $B^{2}=J^{-1} A^{2} J=0$ since $A^{2}=0$, so no such $B$ can exist.

Solution to 7.6.8: 1. Let

$$
A=\left(\begin{array}{ll}
a & b \\
c & d
\end{array}\right)
$$

then

$$
A^{2}=\left(\begin{array}{cc}
a^{2}+b c & (a+d) b \\
(a+d) c & b c+d^{2}
\end{array}\right) .
$$

Therefore, $A^{2}=-I$ is equivalent to the system

$$
\left\{\begin{array}{lrr}
a^{2}+b c & = & -1 \\
(a+d) b & = & 0 \\
(a+d) c & = & 0 \\
b c+d^{2} & = & -1
\end{array}\right.
$$

if $a+d \neq 0$, the second equation above gives $b=0$, and from the fourth, we obtain $d^{2}=-1$, which is absurd. We must then have $a=-d$ and the result follows.

2. The system that we get in this case is

![](https://cdn.mathpix.com/cropped/2022_10_26_a807eeb357d0f35abe4eg-072.jpg?height=237&width=473&top_left_y=1345&top_left_x=810)

As above, we cannot have $a \neq-d$. But combining $a=-d$ with the first and fourth equations of the system, we get $\varepsilon=0$, a contradiction. Therefore, no such matrix exists.

Solution to 7.6.9: Suppose such a matrix $A$ exists. One of the eingenvalues of $A$ would be $w$ and the other $(1+\varepsilon)^{1 / 20} w$ where $w$ is a twentieth root of $-1$. From the fact that $A$ is real we can see that both eigenvalues are real or form a complex conjugate pair, but neither can occur because none the twentieth root of $-1$ are real and the fact that

$$
|w|=1 \neq(1+\varepsilon)^{1 / 20}
$$

make it impossible for them to be a conjugate pair, so no such a matrix exist.

Solution to 7.6.10: $A^{n}=I$ implies that the minimal polynomial of $A, \mu(x) \in$ $\mathbb{Z}[x]$, satisfies $\mu(x) \mid\left(x^{n}-1\right)$. Let $\zeta_{1}, \ldots, \zeta_{n}$ be the distinct roots of $x^{n}-1$ in $\mathbb{C}$. We will separate the two possible cases for the degree of $\mu$ :

- $\operatorname{deg} \mu=1$. We have $\mu(x)=x-1$ and $A=I$, or $\mu(x)=x+1$ and $A=-I, A^{2}=I$. - $\operatorname{deg} \mu=2$. In this case, $\zeta_{i}$ and $\zeta_{j}$ are roots of $\mu$ for some $i \neq j$, in which case we have $\zeta_{j}=\zeta_{i}=\zeta$ say, since $\mu$ has real coefficients. Thus, $\mu(x)=(x-\zeta)(x-\zeta)=x^{2}-2 \mathfrak{R}(\zeta) x+1$. In particular, $2 \mathfrak{R}(\zeta) \in \mathbb{Z}$, so the possibilities are $\Re(\zeta)=0, \pm 1 / 2$, and $\pm 1$. We cannot have $\mathfrak{R}(\zeta)=\pm 1$ because the corresponding polynomials, $(x-1)^{2}$ and $(x+1)^{2}$, have repeated roots, so they are not divisors of $x^{n}-1$.

$$
\begin{aligned}
&\mathfrak{R}(\zeta)=0 \text {. We have } \mu(x)=x^{2}+1 \text { and } A^{2}=-I, A^{4}=I \text {. } \\
&\Re(\zeta)=1 / 2 \text {. In this case } \mu(x)=x^{2}-x+1 . \zeta \text { is a primitive sixth } \\
&\text { root of unity, so } A^{6}=I \text {. } \\
&\mathfrak{M}(\zeta)=-1 / 2 \text {. We have } \mu(x)=x^{2}+x+1 . \zeta \text { is a primitive third root } \\
&\text { of unity, so } A^{3}=I \text {. }
\end{aligned}
$$

From the above, we see that if $A^{n}=I$ for some $n \in \mathbb{Z}_{+}$, then one of the following holds:

$$
A=I, A^{2}=I, A^{3}=I, A^{4}=I, A^{6}=I .
$$

Further, for each $n=2,3,4$, and 6 there is a matrix $A$ such that $A^{n}=I$ but $A^{k} \neq I$ for $0<k<n$ :

$$
\begin{aligned}
& \text { - } n=2 \text {. } \\
& \left(\begin{array}{cc}-1 & 0 \\0 & -1\end{array}\right) \\
& \text { - } n=3 \text {. } \\
& \left(\begin{array}{cc}0 & 1 \\-1 & -1\end{array}\right) \\
& \text { - } n=4 \text {. } \\
& \left(\begin{array}{cc}0 & 1 \\-1 & 0\end{array}\right) \\
& \text { - } n=6 \text {. } \\
& \left(\begin{array}{cc}0 & 1 \\-1 & 1\end{array}\right)
\end{aligned}
$$

Solution to 7.6.11: Since $A$ is upper-triangular, its eigenvalues are its diagonal entries, that is, 1,4 , and 9 . It can, thus, be diagonalized, and in, fact, we will have

$$
S^{-1} A S=\left(\begin{array}{ccc}
1 & 0 & 0 \\
0 & 4 & 0 \\
0 & 0 & 9
\end{array}\right)
$$

where $S$ is a matrix whose columns are eigenvectors of $A$ for the respective eigenvalues 1,4 , and 9. The matrix

$$
B=S\left(\begin{array}{lll}
1 & 0 & 0 \\
0 & 2 & 0 \\
0 & 0 & 3
\end{array}\right) S^{-1}
$$

will then be a square root of $A$.

Carrying out the computations, one obtains

$$
S=\left(\begin{array}{lll}
1 & 1 & 1 \\
0 & 1 & 1 \\
0 & 0 & 1
\end{array}\right) \quad \text { and } \quad S^{-1}=\left(\begin{array}{ccc}
1 & -1 & 0 \\
0 & 1 & -1 \\
0 & 0 & 1
\end{array}\right)
$$

giving

$$
B=\left(\begin{array}{ccc}
1 & 1 & -1 \\
0 & 2 & 1 \\
0 & 0 & 3
\end{array}\right) \text {. }
$$

The number of square roots of $A$ is the same as the number of square roots of its diagonalization, $D=S^{-1} A S$. Any matrix commuting with $D$ preserves its eigenspaces and so is diagonal. In particular, any square root of $D$ is diagonal. Hence, $D$ has exactly eight square roots, namely

$$
\sqrt{\left(\begin{array}{lll}
1 & 0 & 0 \\
0 & 4 & 0 \\
0 & 0 & 9
\end{array}\right)}=\left(\begin{array}{ccc}
\pm 1 & 0 & 0 \\
0 & \pm 2 & 0 \\
0 & 0 & \pm 3
\end{array}\right) .
$$

Solution to 7.6.12: $n=1$. There is the solution $X=A$. $n=2 . A$ is similar to the matrix

$$
\left(\begin{array}{llll}
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{array}\right)
$$

under the transformation that interchanges the third and fourth basis vectors and leaves the first and second basis vectors fixed. The latter matrix is the square of

$$
\left(\begin{array}{llll}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{array}\right) \text {. }
$$

Hence, $A$ is the square of

$$
\left(\begin{array}{llll}
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{array}\right) .
$$

$n=3$. The Jordan matrix [HK61, p. 247]

$$
X=\left(\begin{array}{llll}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0
\end{array}\right)
$$

is a solution.

$n \geqslant 4$. If $X^{k}=A$, then $X$ is nilpotent since $A$ is. Then the characteristic polynomial of $X$ divides $x^{4}$, so that $X^{4}=0$, and, a fortiori, $X^{n}=0$ for $n \geqslant 4$. There is, thus, no solution for $n \geqslant 4$.

Solution to 7.6.13: Suppose such a matrix $A$ exists. Its minimal polynomial must divide $t^{2}+2 t+5$. However, this polynomial is irreducible over $\mathbb{R}$, so $\mu_{A}(t)=t^{2}+2 t+5$. Since the characteristic and minimal polynomials have the same irreducible factors, $\chi_{A}(t)=\mu_{A}(t)^{k}$. Therefore, $\operatorname{deg} \chi_{A}(t) \doteq n$ must be even.

Conversely, a calculation shows that the $2 \times 2$ real matrix

$$
A_{0}=\left(\begin{array}{ll}
0 & -5 \\
1 & -2
\end{array}\right)
$$

is a root of this polynomial. Therefore, any $2 n \times 2 n$ block diagonal matrix which has $n$ copies of $A_{0}$ on the diagonal will satisfy this equation as well.

Solution to 7.6.14: Let $p(t)=t^{5}+t^{3}+t-3$. As $p(A)=0$, we have $\mu_{A}(t) \mid p(t)$. However, since $A$ is Hermitian, its minimal polynomial has only real roots. Taking the derivative of $p$, we see that $p^{\prime}(t)=5 t^{4}+3 t^{2}+1>0$ for all $t$, so $p(t)$ has exactly one real root. A calculation shows that $p(1)=0$, but $p^{\prime}(1) \neq 0$. Therefore, $p(t)=(t-1) q(t)$, where $q(t)$ has only nonreal complex roots. It follows that $\mu_{A}(t) \mid(t-1)$. Since $t-1$ is irreducible, $\mu_{A}(t)=t-1$ and $A=I$.

Solution to 7.6.16: Note that

$$
A=\left(\begin{array}{ccc}
2 & 0 & 0 \\
0 & 2 & 0 \\
0 & -1 & 1
\end{array}\right)
$$

can be decomposed into the two blocks $(2)$ and $\left(\begin{array}{cc}2 & 0 \\ -1 & 1\end{array}\right)$, since the space spanned by $(100)^{t}$ is invariant. We will find a $2 \times 2$ matrix $C$ such that $C^{4}=\left(\begin{array}{cc}2 & 0 \\ -1 & 1\end{array}\right)=D$, say.

The eigenvalues of the matrix $D$ are 2 and 1 , and the corresponding Lagrange Polynomials [MH93, p. 286] are $p_{1}(x)=(x-2) /(1-2)=2-x$ and $p_{2}(x)=(x-1) /(2-1)=x-1$. Therefore, the spectral projection of $D$ can be given by

$$
\begin{aligned}
&P_{1}=-\left(\begin{array}{cc}
2 & 0 \\
-1 & 1
\end{array}\right)+2\left(\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right)=\left(\begin{array}{cc}
0 & 0 \\
1 & 1
\end{array}\right) \\
&P_{2}=-\left(\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right)+\left(\begin{array}{cc}
2 & 0 \\
-1 & 1
\end{array}\right)=\left(\begin{array}{cc}
1 & 0 \\
-1 & 0
\end{array}\right)
\end{aligned}
$$

We have

$$
D=\left(\begin{array}{ll}
0 & 0 \\
1 & 1
\end{array}\right)+2\left(\begin{array}{cc}
1 & 0 \\
-1 & 0
\end{array}\right) .
$$

As $P_{1} \cdot P_{2}=P_{2} \cdot P_{1}=0$ and $P_{1}^{2}=P_{1}, P_{2}^{2}=P_{2}$, letting $C=\left(\begin{array}{cc}0 & 0 \\ 1 & 1\end{array}\right)+2^{1 / 4}\left(\begin{array}{cc}1 & 0 \\ -1 & 0\end{array}\right)=$ $1 P_{1}+2^{1 / 4} P_{2}$, we get

$$
C^{4}=P_{1}^{4}+\underbrace{\ldots}_{0}+\left(2^{1 / 4} P_{2}\right)^{4}=P_{1}+2 P_{2}=D .
$$

Then

$$
C=\left(\begin{array}{cc}
2^{1 / 4} & 0 \\
1-2^{1 / 4} & 1
\end{array}\right)
$$

and $B$ is

$$
\left(\begin{array}{ccc}
2^{1 / 4} & 0 & 0 \\
0 & 2^{1 / 4} & 0 \\
0 & 1-2^{1 / 4} & 1
\end{array}\right)
$$

Solution to 7.6.17: It suffices to show that every element $w \in W$ is a sum of eigenvectors of $T$ in $W$. Let $a_{1}, \ldots, a_{n}$ be the distinct eigenvalues of $T$. We may write

$$
w=v_{1}+\cdots+v_{n}
$$

where each $v_{i}$ is in $V$ and is an eigenvector of $T$ with eigenvalue $a_{i}$. Then

$$
\prod_{i \neq j}\left(T-a_{j}\right) w=\prod_{i \neq j}\left(a_{i}-a_{j}\right) v_{i} .
$$

This element lies in $W$ since $W$ is $T$ invariant. Hence, $v_{i} \in W$ for all $i$ and the result follows.

Solution 2. To see this in a matrix form, take an ordered basis of $W$ and extend it to a basis of $V$; on this basis, a matrix representing $T$ will have the block form

$$
[T]_{\mathcal{B}}=\left(\begin{array}{cc}
A & C \\
0 & B
\end{array}\right)
$$

because of the invariance of the subspace $W$ with respect to $T$.

Using the block structure of $T$, we can see that the characteristic and minimal polynomials of $A$ divide the ones for $T$. For the characteristic polynomial, it is immediate from the fact that

$$
\operatorname{det}\left(x I-[T]_{\mathcal{B}}\right)=\operatorname{det}(x I-A) \operatorname{det}(x I-B)
$$

For the minimal polynomial, observe that

$$
[T]_{\mathcal{B}}^{k}=\left(\begin{array}{cc}
A^{k} & C_{k} \\
0 & B^{k}
\end{array}\right)
$$

where $C_{k}$ is some $r \times(n-r)$ matrix. Therefore, any polynomial that annihilates [T] also annihilates $A$ and $B$; so the minimal polynomial of $A$ divides the one for $[T]$ Now, since $T$ is diagonalizable, the minimal polynomial factors out in different linear terms and so does the one for $A$, proving the result.

Solution to 7.6.19: Let $\lambda$ be an eigenvalue of $A$ and $v$ a vector in the associated eigenspace, $A_{\lambda}$. Then $A(B v)=B A v=B(\lambda v)=\lambda(B v)$, so $B v \in A_{\lambda}$. Now fix an eigenvalue $\lambda$ and let $C$ be the linear transformation obtained by restricting $B$ to $A_{\lambda}$. Take any $v \in A_{\lambda}$. Then, since $C$ is the restriction of $B$,

$$
\mu_{B}(C) v=\mu_{B}(B) v=0,
$$

so $C$ is a root of $\mu_{B}(t)$. It follows from this that $\mu_{C}(t) \mid \mu_{B}(t)$. But $B$ was diagonalizable, so $\mu_{B}(t)$ splits into distinct linear factors. Therefore, $\mu_{C}(t)$ must split into distinct linear factors as well and so $A_{\lambda}$ has a basis of eigenvectors of $C$. As $A$ is diagonalizable, $V$ can be written as the direct sum of the eigenspaces of $A$. However, each of these eigenspaces has a basis which consists of vectors which are simultaneously eigenvectors of $A$ and of $B$. Therefore, $V$ itself must have such a basis, and this is the basis which simultaneously diagonalizes $A$ and $B$.

Solution 2. (This one, in fact, shows much more; it proves that a set of $n \times n$ diagonalizable matrices over a field $\mathbf{F}$ which commute with each other are all simultaneously diagonalizable.) Let $S$ be a set of $n \times n$ diagonalizable matrices over a field $\mathbf{F}$ which commute with each other. Let $V=\mathbf{F}^{n}$. Suppose $T$ is a maximal subset of $S$ such that there exists a decomposition of

$$
V=\oplus_{i} V_{i}
$$

where $V_{i}$ is a nonzero eigenspace for each element of $T$ such that for $i \neq j$, there exists an element of $T$ with distinct eigenvalues on $V_{i}$ and $V_{j}$. We claim that $T=S$. If not, there exists an $N \in S-T$. Since $N$ commutes with all the elements of $T, N V_{i} \subset V_{i}$. Indeed, there exists a function $a_{i}: T \rightarrow \mathbf{F}$ such that $v \in V_{i}$, if and only if $M v=a_{i}(M) v$ for all $M \in T$. Now if $v \in V_{i}$ and $M \in T$,

$$
M N v=N M v=N a_{i}(M)=a_{i}(M) N v
$$

so $N v \in V_{i}$. Since $N$ is diagonalizable on $V$, it is diagonalizable on $V_{i}$. (See Problem 7.6.17; it satisfies a polynomial with distinct roots in $\mathbf{K}$.) This means we can decompose each $V_{i}$ into eigenspaces $V_{i, j}$ for $N$ with distinct eigenvalues. Hence, we have a decomposition of the right sort for $T \cup N$,

$$
V=\oplus_{i} \oplus_{j} V_{i, j} .
$$

Hence, $T=S$. We may now make a basis for $V$ by choosing a basis for $V_{i}$ and taking the union. Then $A$ will be the change of basis matrix.

Solution to 7.6.20: 1. Take

$$
A=\left(\begin{array}{cc}
1 & 0 \\
-1 & -1
\end{array}\right), \quad B=\left(\begin{array}{cc}
-1 & 1 \\
0 & 1
\end{array}\right)
$$

they are both diagonalizable since the characteristic polynomial factors in linear terms, but $A+B=\left(\begin{array}{cc}0 & 1 \\ -1 & 0\end{array}\right)$ is not diagonalizable.

![](https://cdn.mathpix.com/cropped/2022_10_26_a807eeb357d0f35abe4eg-078.jpg?height=82&width=1387&top_left_y=325&top_left_x=369)
diagonalizable. However $A B=\left(\begin{array}{ll}1 & 1 \\ 0 & 1\end{array}\right)$ is not diagonalizable.

3. If $A^{2}=A$ then $\mu_{A}$ divides $x^{2}-x$, whence $\mu_{A}=x$ or $x-1$ or $x(x-1)$. It follows that $A$ is diagonalizable, as $\mu_{A}$ is a product of distinct linear factors.

4. If $A^{2}$ is diagonalizable its minimal polynomial is the product of distinct linear factors: $\mu_{A^{2}}=\left(x-\lambda_{1}\right) \cdots\left(x-\lambda_{k}\right)$, where $\lambda_{i} \in \mathbb{C}$. Therefore

$$
\left(A^{2}-\lambda_{1} I\right) \cdots\left(A^{2}-\lambda_{k} I\right)=0 \text {. }
$$

It follows that $\left(A-\sqrt{\lambda_{1}} I\right)\left(A+\sqrt{\lambda_{1}} I\right) \cdots\left(A-\sqrt{\lambda_{k}} I\right)\left(A+\sqrt{\lambda_{k}} I\right)=0$. The minimum polynomial of $A$ divides the polynomial

$$
f=\left(x-\sqrt{\lambda_{1}}\right)\left(x+\sqrt{\lambda_{1}}\right) \cdots\left(x-\sqrt{\lambda_{k}}\right)\left(x+\sqrt{\lambda_{k}}\right) .
$$

If $A$ is invertible then none of the $\lambda_{i}$ is zero, and thus the factors of $f$ are distinct. It follows that $\mu_{A}$ is a product of distinct linear factors and $A$ is diagonalizable.

Solution to 7.6.21: The characteristic polynomial of $A$ is

$$
\chi_{A}(x)=\left|\begin{array}{cc}
x-7 & -15 \\
2 & x+4
\end{array}\right|=(x-1)(x-2)
$$

so $A$ is diagonalizable and a short calculation shows that eigenvectors associated with the eigenvalues 1 and 2 are $(5,-2)^{t}$ and $(3,-1)^{t}$, so the matrix $B$ is $\left(\begin{array}{cc}5 & 3 \\ -2 & -1\end{array}\right)$. Indeed, in this case, $B^{-1} A B=\left(\begin{array}{ll}1 & 0 \\ 0 & 2\end{array}\right)$.

Solution to 7.6.23: The characteristic polynomial is $\chi_{A}(t)=(t-1)(t-4)^{2}$. Since the minimal polynomial and the characteristic polynomial share the same irreducible factors, another calculation shows that $\mu_{A}(t)=(t-1)(t-4)^{2}$. Therefore, the Jordan Canonical Form [HK61, p. 247] of $A$ must have one Jordan block of order 2 associated with 4 and one Jordan block of order 1 associated with 1. Hence, the Jordan form of $A$ is

$$
\left(\begin{array}{lll}
1 & 0 & 0 \\
0 & 4 & 1 \\
0 & 0 & 4
\end{array}\right) \text {. }
$$

Solution to 7.6.24: We have

$$
\begin{aligned}
|A-\lambda I| &=\left|\begin{array}{ccc}
2-\lambda & 1 & 1 \\
1 & 2-\lambda & 1 \\
1 & 1 & 2-\lambda
\end{array}\right|=\left|\begin{array}{ccc}
1-\lambda & 1 & 0 \\
-1+\lambda & 2-\lambda & -1+\lambda \\
0 & 1 & 1-\lambda
\end{array}\right| \\
&=(\lambda-1)^{2}\left|\begin{array}{ccc}
-1 & 1 & 0 \\
1 & 2-\lambda & 1 \\
0 & 1 & -1
\end{array}\right|=-(\lambda-1)^{3},
\end{aligned}
$$

using column operations $C_{1}-C_{2}$ and $C_{3}-C_{2}$. The single eigenvalue of $A$ is $\lambda=1$ with algebraic multiplicity 3 .

The eigenvectors of $A$ are the solutions to the equation $(A-I) x=0$. That is, $x_{1}+x_{2}+x_{3}=0$, or $x=\left(-x_{2}-x_{3}, x_{2}, x_{3}\right)$. The eigenvectors corresponding to the eigenvalue $\lambda=1$ are $x_{2}(-1,1,0)+x_{3}(-1,0,1), \quad\left(x_{2}, x_{3} \in \mathbf{F}_{3}\right)$ (including the zero vector).

The characteristic polynomial of $A$ is $(x-1)^{3}$. Now

$$
A-I=\left(\begin{array}{lll}
1 & 1 & 1 \\
1 & 1 & 1 \\
1 & 1 & 1
\end{array}\right), \quad(A-I)^{2}=0 .
$$

The minimal polynomial of $A$ is $(x-1)^{2}$. The Jordan form has an elementary Jordan block of size 2 . The Jordan form of $A$ is therefore

$$
\left(\begin{array}{lll}
1 & 1 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right) .
$$

Solution to 7.6.25: Combining the equations, we get $\mu(x)^{2}=\mu(x)(x-i)\left(x^{2}+1\right)$ and, thus, $\mu(x)=(x-i)^{2}(x+i)$. So the Jordan blocks of the Jordan Canonical Form [HK61, p. 247] $J_{A}$, correspond to the eigenvalues $\pm i$. There is at least one block of size 2 corresponding to the eigenvalue $i$ and no larger block corresponding to $i$. Similarly, there is at least one block of size 1 corresponding to $-i$. We have $\chi(x)=(x-i)^{3}(x+i)$, so $n=\operatorname{deg} \chi=4$, and the remaining block is a block of size 1 corresponding to $i$, since the total dimension of the eigenspace is the degree with which the factor appears in the characteristic polynomial. Therefore,

$$
J_{A}=\left(\begin{array}{cccc}
i & 1 & 0 & 0 \\
0 & i & 0 & 0 \\
0 & 0 & i & 0 \\
0 & 0 & 0 & -i
\end{array}\right) .
$$

Solution to 7.6.26: 1 . As all the rows of $M$ are equal, $M$ must have rank 1 , so its nullity is $n-1$. It is easy to see that $M^{2}=n M$, or $M(M-n I)=0$, so the minimal polynomial is $\mu_{M}=x(x-n)$, since the null space associated with the characteristic value 0 is $n-1$, then the characteristic polynomial is $\chi_{M}=x^{n-1}(x-n)$

2. If char $\mathbf{F}=0$ or if $\operatorname{char} \mathbf{F}=p$ and $p$ does not divide $n$, then 0 and $n$ are the two distinct eigenvalues, and since the minimal polynomial does not have repeated roots, $M$ is diagonalizable.

If char $\mathbf{F}=p, p \mid n$, then $n$ is identified with 0 in $\mathbf{F}$. Therefore, the minimal polynomial of $M$ is $\mu_{M}(x)=x^{2}$ and $M$ is not diagonalizable. 3. In the first case, since the null space has dimension $n-1$, the Jordan form [HK61, p. 247] is

$$
\left(\begin{array}{cccc}
n & 0 & \cdots & 0 \\
0 & 0 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 0
\end{array}\right) .
$$

If char $\mathbf{F}=p, p \mid n$, then all the eigenvalues of $M$ are 0 , and there is one 2-block and $n-1$ 1-blocks in the Jordan form:

$$
\left(\begin{array}{cccc}
0 & 1 & \cdots & 0 \\
0 & 0 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 0
\end{array}\right) .
$$

\section{Solution to 7.6.27: A computation gives}

$$
T\left(\begin{array}{ll}
x_{11} & x_{12} \\
x_{21} & x_{22}
\end{array}\right)=\left(\begin{array}{cc}
-x_{21} & x_{11}-x_{22} \\
0 & x_{21}
\end{array}\right) .
$$

In particular, for the basis elements

$$
E_{1}=\left(\begin{array}{ll}
1 & 0 \\
0 & 0
\end{array}\right), \quad E_{2}=\left(\begin{array}{ll}
0 & 1 \\
0 & 0
\end{array}\right), \quad E_{3}=\left(\begin{array}{ll}
0 & 0 \\
1 & 0
\end{array}\right), \quad E_{4}=\left(\begin{array}{ll}
0 & 0 \\
0 & 1
\end{array}\right)
$$

we have

$$
\begin{gathered}
T E_{1}=\left(\begin{array}{ll}
0 & 1 \\
0 & 0
\end{array}\right)=E_{2}, \quad T E_{2}=0 \\
T E_{3}=\left(\begin{array}{cc}
-1 & 0 \\
0 & 1
\end{array}\right)=-E_{1}+E_{4}, \quad T E_{4}=\left(\begin{array}{cc}
0 & -1 \\
0 & 0
\end{array}\right)=-E_{2} .
\end{gathered}
$$

The matrix for $T$ with respect to the basis $\left\{E_{1}, E_{2}, E_{3}, E_{4}\right\}$ is then

$$
S=\left(\begin{array}{cccc}
0 & 0 & -1 & 0 \\
1 & 0 & 0 & -1 \\
0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0
\end{array}\right) .
$$

A calculation shows that the characteristic polynomial of $S$ is $\lambda^{4}$. Thus, $S$ is nilpotent. Moreover, the index of nilpotency is 3 , since we have

$$
T^{2} E_{1}=T^{2} E_{2}=T^{2} E_{4}=0, \quad T^{2} E_{3}=-2 E_{2} .
$$

The only $4 \times 4$ nilpotent Jordan matrix [HK61, p. 247] with index of nilpotency 3 is

$$
\left(\begin{array}{llll}
0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0
\end{array}\right)
$$

which is, therefore, the Jordan Canonical Form of $T$. A basis in which $T$ is represented by the preceding matrix is

$$
\left\{E_{1}+E_{4}, E_{2}, \frac{E_{1}-E_{4}}{2},-\frac{E_{3}}{2}\right\} .
$$

Solution to 7.6.28: It will suffice to prove that $\operatorname{ker}\left(T_{A}\right)$, the subspace of matrices that commute with $A$, has dimension at least $n$. We can assume without loss of generality that $A$ is in Jordan form. A $k \times k$ Jordan block commutes with the $k \times k$ identity and with its own powers up to the $(k-1)$, hence with $k$ linearly independent matrices. Thus, the commutant of each Jordan block has a dimension at least as large as the size of the block, in fact, equality holds. By taking direct sums of matrices commuting with the separate Jordan blocks of $A$, we therefore obtain a subspace of matrices commuting with $A$ of dimension at least $n$ and, in fact, equal to $n$.

Solution to 7.6.29: The answer is 0 and 1. If $A$ is diagonal with diagonal entries $1,2, \ldots, n$ and $g(t)=(t-1)(t-2) \ldots(t-(n-1))$, then $g(A)$ has $(n-1) !$ in the lower right corner and zeros elsewhere, so $g(A)$ has rank 1 . If $A$ is the zero matrix and $g(t)=t^{n-1}$, then $g(A)$ has rank 0 . We will show that these are the only possibilities for the rank of $g(A)$, even if $A$ is allowed to have complex entries.

Since the ground field is now algebraically closed, we may conjugate $A$ to put it in Jordan canonical form, without affecting the rank of $g(A)$. Let $A_{1}, \ldots, A_{r}$ be the Jordan blocks of $A$, and let $f_{i}(t) \in \mathbb{C}[t]$ be the characteristic polynomial of $A_{i}$ for each $i$. Since $g(t)$ divides $\prod \xi_{i}(t)$ and deg $g=n-1$, we can factor $g(t)$ as $\prod g_{i}(t)$ where $g_{i}=\xi_{i}$ for all $i$ except one. Without loss of generality the exception is $i=1$. Then for some $k \geqslant 1$ and $\lambda \in \mathbb{C}, A_{1}$ is a $k \times k$ block

$$
A_{1}=\left(\begin{array}{ccccc}
\lambda & 1 & 0 & \ldots & 0 \\
0 & \lambda & 1 & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \ldots & 1 \\
0 & 0 & 0 & \ldots & \lambda
\end{array}\right),
$$

$\xi_{1}(t)=(t-\lambda)^{k}$, and $g_{1}(t)=c(t-\lambda)^{k-1}$, where $c \neq 0$. Then $g(A)$ is formed of the blocks $g\left(A_{1}\right), \ldots, g\left(A_{r}\right)$ (not necessarily full Jordan blocks), and $g\left(A_{i}\right)=0$ for $i \geqslant 2$, since the characteristic polynomial of $A_{i}$ divides $g$. On the other hand $g\left(A_{1}\right)$ is some matrix times

$$
g_{1}\left(A_{1}\right)=c\left(\begin{array}{ccccc}
0 & 1 & 0 & \ldots & 0 \\
0 & 0 & 1 & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \ldots & 1 \\
0 & 0 & 0 & \ldots & 0
\end{array}\right)^{k-1}=\left(\begin{array}{ccccc}
0 & 0 & 0 & \ldots & c \\
0 & 0 & 0 & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \ldots & 0 \\
0 & 0 & 0 & \ldots & 0
\end{array}\right),
$$

which is of rank 1, so the rank of $g\left(A_{1}\right)$ is at most 1 , and the rank of $g(A)$ is at most 1.

Solution to 7.6.30: A direct calculation shows that $(A-I)^{3}=0$ and this is the least positive exponent for which this is true. Hence, the minimal polynomial of $A$ is $\mu_{A}(t)=(t-1)^{3}$. Thus, its characteristic polynomial must be $\chi_{A}(t)=(t-1)^{6}$. Therefore, the Jordan Canonical Form [HK61, p. 247] of $A$ must contain one $3 \times 3$ Jordan block associated with 1 . The number of blocks is the dimension of the eigenspace associated with 1 . Letting $x=\left(x_{1}, \ldots, x_{6}\right)^{t}$ and solving $A x=x$, we get the two equations $x_{1}=0$ and $x_{2}+x_{3}+x_{4}+x_{5}=0$. Since $x_{6}$ is not determined, these give four degrees of freedom, so the eigenspace has dimension 4. Therefore, the Jordan Canonical Form of $A$ must contain four Jordan blocks and so it must be

$$
\left(\begin{array}{llllll}
1 & 1 & 0 & 0 & 0 & 0 \\
0 & 1 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 1
\end{array}\right) .
$$

Solution to 7.6.31: Since $A B$ has rank 3, both $A$ and $B$ must also have rank 3, from which it follows that $B A$ has rank 3. Hence the null space of $B A$ has dimension 2; let $v_{1}, v_{2}$ form a basis for it.

Let $e_{1}, e_{2}, e_{3}$ be the standard basis vectors for $\mathbb{C}^{3}$. We have

$$
A B e_{1}=e_{1}, \quad A B e_{2}=e_{1}+e_{2}, \quad A B e_{3}=-e_{3},
$$

so

$$
B A B e_{1}=B e_{1}, \quad B A B e_{2}=B e_{1}+B e_{2}, \quad B A B e_{3}=-B e_{3} .
$$

Since $B$ has rank 3, no nontrivial linear combination of $B e_{1}, B e_{1}+B e_{2},-B e_{3}$ can vanish, implying that $B e_{1}, B e_{2}, B e_{3}$ are linearly independent modulo the null space of $B A$. Hence $v_{1}, v_{2}, B e_{1}, B e_{2}, B e_{3}$ form a basis for $\mathbb{C}^{5}$. Relative to his basis, the transformation induced by $B A$ has the matrix

$$
\left(\begin{array}{rrrrr}
0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 1 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & -1
\end{array}\right)
$$

which is the Jordan form of $B A$.

Solution to 7.6.33: The $i^{t h}$ diagonal element of $A A^{t}$ is $\sum_{j=i}^{n} a_{i j}^{2}$ and the same element on $A^{t} A$ is $\sum_{j=1}^{i} a_{j i}^{2}$. Comparing these expressions successively for $i=n, n-1, \ldots 1$, we conclude that all $a_{i j}$ with $j>i$ are zero, that is, $\mathrm{A}$ is diagonal.

Solution to 7.6.38: Since $A$ is nonsingular, $A^{t} A$ is positive definite. Let $B=\sqrt{A^{t} A}$. Consider $P=B A^{-1}$. Then $P A=B$, so it suffices to show that $P$ is orthogonal, for in that case, $Q=P^{-1}=P^{*}$ will be orthogonal and $A=Q B$. We have

$$
P^{t} P=\left(A^{t}\right)^{-1} B^{t} B A^{-1}=\left(A^{t}\right)^{-1} B^{2} A^{-1}=\left(A^{t}\right)^{-1} A^{t} A A^{-1}=1 .
$$

Suppose that we had a second factorization $A=Q_{1} B_{1}$. Then

$$
B^{2}=A^{t} A=B_{1}^{t} Q_{1}^{t} Q_{1} B_{1}=B_{1}^{2} .
$$

Since a positive matrix has a unique positive square root, it follows that $B=B_{1}$. As $A$ is invertible, $B$ is invertible, and canceling gives $Q=Q_{1}$.

Solution to 7.6.39: Conjugating $A$ changes neither the convergence nor the eigenvalues, so we may assume $A$ in Jordan canonical form, $A=\left(\begin{array}{ll}a & 0 \\ 0 & b\end{array}\right)$ or $A=\left(\begin{array}{ll}a & 1 \\ 0 & a\end{array}\right)$. In the first case $A^{n}=\left(\begin{array}{cc}a^{n} & 0 \\ 0 & b^{n}\end{array}\right)$ and $\sum A^{n}$ converges when the eigenvalues, $a$ and $b$, have absolute value less than 1 , since the entries of the sum are geometric series. In the second case write $A=a I+N$, with $N$ the nilpotent matrix $N=\left(\begin{array}{ll}0 & 1 \\ 0 & b\end{array}\right)$. In this case $N^{2}=0$, and $A^{n}=a^{n} I+n a^{n-1} N$. If $I+A+A^{2}+\cdots$ converges, then the diagonal entries, $a^{n}$, of the terms $A^{n}$ must converge to 0 , so $|a|<1$. Conversely, if $|a|<1$, then $\sum a^{n}$ and $\sum n a^{n-1}$ converge, by the Ratio Test [Rud87, p. 66], therefore, $\sum A^{n}$ converges.

Solution to 7.6.40: An easy calculation shows that $A$ has eigenvalues 0,1 , and 3 , so $A$ is similar to the diagonal matrix with entries 0,1 , and 3 . Since clearly the problem does not change when $A$ is replaced by a similar matrix, we may replace $A$ by that diagonal matrix. Then the condition on $a$ is that each of the sequences $\left(0^{n}\right),\left(a^{n}\right)$, and $\left((3 a)^{n}\right)$ has a limit, and that at least one of these limits is nonzero. This occurs if and only if $a=1 / 3$.

Solution to 7.6.41: Let $g$ be an element of the group. Consider the Jordan Canonical Form [HK61, p. 247] of the matrix $g$ in $\mathbf{F}_{p^{2}}^{*}$ a quadratic extension of $\mathbf{F}_{p}$. The characteristic polynomial has degree 2 and is either irreducible in $\mathbf{F}_{p}$ and the canonical form is diagonal with two conjugate entries in the extension or reducible with the Jordan Canonical Form having the same diagonal elements and a 1 in the upper right-hand corner. In the first case, we can see that $g^{p^{2}-1}=I$ and in the second $g^{p(p-1)}=I$. 

\subsection{Similarity}

Solution to 7.7.1: A simple calculation shows that $A$ and $B$ have the same characteristic polynomial, namely $(x-1)^{2}(x-2)$. However,

$$
A-I=\left(\begin{array}{ccc}
0 & 0 & 0 \\
-1 & 0 & 1 \\
-1 & 0 & 1
\end{array}\right), \quad B-I=\left(\begin{array}{lll}
0 & 1 & 0 \\
0 & 0 & 0 \\
0 & 0 & 1
\end{array}\right) \text {. }
$$

Since $A-I$ has rank 1 and $B-I$ has rank 2 , these two matrices are not similar, and therefore, neither are $A$ and $B$.

\section{Solution to 7.7.3: A calculation gives}

$$
\operatorname{det}(B-z I)=z^{2}(z-1)(z+1)=\operatorname{det}(A-z I) .
$$

The matrix $A$ is in Jordan Canonical Form, and there are only two matrices with the same characteristic polynomial as $A$, namely $A$ and the diagonal matrix with diagonal entries $1,-1,0,0$. Since $B$ obviously has rank 3 , its Jordan form must be $A$, i.e., $B$ and $A$ are similar.

Solution to 7.7.5: The eigenvalues of $A$ an $B$ are either $\pm 1$ and neither is $I$ or $-I$, since the equation $A B+B A=0$ would force the other matrix to be zero. Therefore, $A$ and $B$ have distinct eigenvalues and are both diagonalizable. Let $S$ be such that $S A S^{-1}=\left(\begin{array}{cc}1 & 0 \\ 0 & -1\end{array}\right)$. Multiplying on the left by $S$ and on the right by $S^{-1}$ the relations above we see that $C=S B S^{-1}$ satisfies $C^{2}=I$ and $\left(S A S^{-1}\right)\left(S B S^{-1}\right)+\left(S B S^{-1}\right)\left(S A S^{-1}\right)=0$. We get

$$
C=\left(\begin{array}{cc}
0 & 1 / c \\
c & 0
\end{array}\right) \text { for } c \neq 0
$$

and taking $D=\left(\begin{array}{cc}c k & 0 \\ 0 & k\end{array}\right)$ we can easily see that $T=D S$ satisfies

$$
T A T^{-1}=\left(\begin{array}{cc}
1 & 0 \\
0 & -1
\end{array}\right) \quad T B T^{-1}=\left(\begin{array}{ll}
0 & 1 \\
1 & 0
\end{array}\right) .
$$

Solution to 7.7.7: Let $\chi(x)=x^{2}-b x+d$ be the characteristic polynomial of $A$. Since $d$ is the determinant of $A, d^{n}=1$, but the only roots of unity in $\mathbb{Q}$ are $\pm 1$ so $d=\pm 1$. Let $\alpha$ and $\beta$ be the two complex roots of $\chi$. Over $\mathbb{C} A$ is similar to a matrix $M$ of the form

$$
M=\left(\begin{array}{ll}
\alpha & 0 \\
0 & \beta
\end{array}\right) .
$$

Since the roots of the characteristic polynomial of $A^{n}$ are the the $n$th powers of the roots of $\chi$, it follows that $\alpha^{n}=\beta^{n}=1$. Since $\pm 1$ are the only real roots of unity, if $\alpha$ or $\beta$ is real, we get that $\alpha=\pm \beta=\pm 1$ so in this case, since $A$ is similar to $M, A^{2}=I$. Now suppose $\alpha$ is not real. This means $b^{2}-4 d<0$ and $|\alpha|=d$. So $d=1$ and $b=0$ or $b=\pm 1$. If $b=0, \alpha$ and $\beta$ are fourth roots of unity so $M^{4}=I$ and thus $A^{4}=I$. If $b=\pm 1, \alpha$ and $\beta$ are sixth roots of unity so $M^{6}$ and $A^{6}$ equal $I$. Thus in all cases $A^{12}=I$.

Solution 2. Alternatively, we can analyze the characteristic polynomial in the following way: $\chi(x)=x^{2}-b x+d$ where $d=\operatorname{det} A=\pm 1$ and $b=\operatorname{trace} A$ is an integer. Since $A$ has finite order, its eigenvalues are roots of unity, so have modulus 1 . Since $b$ is the sum of the eigenvalues, $|b| \leqslant 2$.

Therefore the characteristic polynomial of $A$ is one of $x^{2} \pm 1, x^{2} \pm x+1$, $x^{2} \pm x-1, x^{2} \pm 2 x+1, x^{2} \pm 2 x-1$. Note that $x^{2} \pm 1$ divides $x^{4}-1$, and $x^{2} \pm x+1$ divides $x^{6}-1$, so any matrix with one of these polynomials has order dividing 12. The zeros of $x^{2} \pm x-1$ and $x^{2} \pm 2 x-1$ do not have modulus 1 , so these polynomials cannot occur.

A matrix $A$ with polynomial $x^{2} \pm 2 x+1$ has repeated eigenvalues $\pm 1$. Recalling that $A^{n}=1$ for some $n>0, A$ is diagonalizable (for example, since it satisfies a polynomial equation with distinct linear factors), so $A=\pm I$.

Solution 3. Since $A^{n}-I=0$ the minimal polynomial $\mu_{A}$ of $A$ is a divisor of $x^{n}-1$. The irreducible factorization of $x^{n}-1$ is given by

$$
x^{n}-1=\prod_{m \mid n} \Phi_{m},
$$

where $\Phi_{m}=\prod_{\omega_{i} \in \Omega_{m}}\left(x-\omega_{i}\right)$, is the m-th cyclotomical polynomial, product of all the factors $\left(x-\omega_{i}\right)$ where $w_{i}$ are the primitive $m$-th roots of unity. $\Omega_{m}=\left\{e^{2 \pi i r / m} \mid 1 \leqslant r \leqslant m,(r, m)=1\right\}$ and the numbers of elements is given by $\varphi(m)$, the Euler's totient function. Thus the irreducible factorizations of $\mu_{A}$ and $\chi_{A}$ over $\mathbb{Q}$ have the form

$$
m_{A}=\Phi_{m_{1}} \cdots \Phi_{m_{r}}, \quad \chi_{A}=\Phi_{m_{1}}^{d_{1}} \cdots \Phi_{m_{r}}^{d_{r}}
$$

where $r \geqslant 1, m_{1}, \ldots, m_{r}$ are distinct and divide $n$, and $d_{i} \geqslant 1$. It follows that $d_{1} \varphi\left(m_{1}\right)+\cdots+d_{r} \varphi\left(m_{r}\right)=2$.

There are two cases: solving $\varphi\left(m_{1}\right)+\varphi\left(m_{2}\right)=2$ gives $\left(m_{1}, m_{2}\right)=(1,1)$, $(1,2),(2,2)$ and solving $\varphi(m)=2$ gives $m=3,4,6$.

Take the case $(1,1)$, that is the minimal and characteristic polynomial of $A$ are given by

$$
\mu_{A}=\Phi_{1}, \quad \chi_{A}=\Phi_{1}^{2} .
$$

By the Cyclic Decomposition Theorem $A \sim C\left(\Phi_{1}\right) \oplus C\left(\Phi_{1}\right)$, a direct sum of companion matrices.

The other cases are similar, that is, $A$ is similar over $\mathbb{Q}$ to one of the six matrices

$C\left(\Phi_{1}\right) \oplus C\left(\Phi_{1}\right), C\left(\Phi_{1}\right) \oplus C\left(\Phi_{2}\right), C\left(\Phi_{2}\right) \oplus C\left(\Phi_{2}\right), C\left(\Phi_{3}\right), C\left(\Phi_{4}\right), C\left(\Phi_{6}\right)$

which are

$$
\left(\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right),\left(\begin{array}{cc}
1 & 0 \\
0 & -1
\end{array}\right),\left(\begin{array}{cc}
-1 & 0 \\
0 & -1
\end{array}\right),
$$



$$
\left(\begin{array}{cc}
0 & -1 \\
1 & -1
\end{array}\right),\left(\begin{array}{cc}
0 & -1 \\
1 & 0
\end{array}\right),\left(\begin{array}{cc}
0 & -1 \\
1 & 1
\end{array}\right) \text {, }
$$

and these have orders $1,2,2,3,4,6$ respectively. (the order of $C\left(\Phi_{m}\right)$ is $m$ ). In all cases $A^{12}=I$.

Solution to 7.7.8: 1. Let $A$ be any element of the group:

(i) Every element in a finite group has finite order, so there is an $n>0$ such that $A^{n}=I$. Therefore, $(\operatorname{det} A)^{n}=\operatorname{det}\left(A^{n}\right)=1$. But $A$ is an integer matrix, so $\operatorname{det} A$ must be $\pm 1$.

(ii) If $\lambda$ is an eigenvalue of $A$, then $\lambda^{n}=1$, so each eigenvalue has modulo 1 , and at the same time, $\lambda$ is a root of a second degree monic characteristic polynomial $\chi_{A}(x)=x^{2}+a x+b$ for $A$. If $|\lambda|=1$ then $b=\pm 1$ and $a=0, \pm 1$, and $\pm 2$ since all roots are in the unit circle. Writing out all 10 polynomials and eliminating the ones whose roots are not in the unit circle, we are left with $x^{2} \pm 1, x^{2} \pm x+1$, and $x^{2} \pm 2 x+1$, and the possible roots are $\lambda=\pm 1, \pm i$, and $\frac{1 \pm \sqrt{3} i}{2}$ and $\frac{-1 \pm \sqrt{3} i}{2}$, the sixth roots of unity.

(iii) The Jordan Canonical Form [HK61, p. 247] of $A, J_{A}$, must be diagonal, otherwise it would be of the form $J_{A}=\left(\begin{array}{ll}x & 1 \\ 0 & x\end{array}\right)$, and the subsequent powers $\left(J_{A}\right)^{k}=\left(\begin{array}{cc}x^{k} & k x^{k-1} \\ 0 & x^{k}\end{array}\right)$, which is never the identity matrix since $k x^{k-1} \neq 0$ (remember $|x|=1$ ). So the Jordan Canonical Form of $A$ is diagonal, with the root above and the complex roots occurring in conjugate pairs only.

The Rational Canonical Form [HK61, p. 238] can be read off from the possible polynomials.

(iv) $A$ can only have order $1,2,3,4$, or 6 , depending on $\lambda$.

Solution to 7.7.10: Let $R_{A}$ and $R_{B}$ be the Rational Canonical Forms [HK61, p. 238] of $A$ and $B$, respectively, over $\mathbb{R}$; that is, there are real invertible matrices $K$ and $L$ such that

$$
\begin{aligned}
&R_{A}=K A K^{-1} \\
&R_{B}=L B L^{-1} .
\end{aligned}
$$

Observe now that $R_{A}$ and $R_{B}$ are also the Rational Canonical Forms over $\mathbb{C}$ as well, and by the uniqueness of the canonical form, they must be the same matrices. If $K A K^{-1}=L B L^{-1}$ then $A=K^{-1} L B\left(K^{-1} L\right)^{-1}$, so $K^{-1} L$ is a real matrix defining the similarity over $\mathbb{R}$. Observe that the proof works for any subfield; in particular, two rational matrices that are similar over $\mathbb{R}$ are similar over $\mathbb{Q}$.

Solution 2. Let $U=K+i L$ where $K$ and $L$ are real and $L \neq 0$ (otherwise we are done). Take real and imaginary parts of

$$
A(K+i L)=A U=U B=(K+i L) B
$$

and add them together after multiplying the imaginary part by $z$ to get

$$
A(K+z L)=(K+z L) B
$$

for any complex $z$. Let $p(z)=\operatorname{det}(K+z L)$. Since $p$ is a polynomial of degree $n$, not identically zero $(p(i) \neq 0)$, it has, at most, $n$ roots. For real $z_{0}$ not one of the roots of $p, V=K+z_{0} L$ is real and invertible and $A=V B V^{-1}$.

Solution to 7.7.11: The minimal polynomial of $A$ divides $(x-1)^{n}$, so $I-A$ is nilpotent, say of order $r$. Thus, $A$ is invertible with

$$
A^{-1}=(I-(I-A))^{-1}=\sum_{j=0}^{r-1}(I-A)^{j} .
$$

Suppose first that $A$ is just a single Jordan block [HK61, p. 247], say with matrix

$$
\left(\begin{array}{ccccc}
1 & 1 & 0 & \cdots & 0 \\
0 & 1 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 1
\end{array}\right)
$$

relative to the basis $\left\{v_{1}, v_{2}, \ldots, v_{n}\right\}$. Then $A^{-1}$ has the same matrix relative to the basis $\left\{v_{1}, v_{1}+v_{2}, \cdots, v_{1}+v_{2}+\cdots+v_{n}\right\}$, so $A$ and $A^{-1}$ are similar.

In the general case, by the theory of Jordan Canonical Form, the vector space can be written as a direct sum of $A$-invariant subspaces on each of which $A$ acts as a single Jordan block. By the formula above for $A^{-1}$, each subspace in the decomposition is $A^{-1}$-invariant, so the direct sum decomposition of $A$ is also one of $A^{-1}$. The general case thus reduces to the case where $A$ is a single Jordan block.

Solution to 7.7.12: The statement is true. First of all, $A$ is similar to a Jordan matrix [HK61, p. 247], $A=S^{-1} J S$, where $S$ is invertible and $J$ is a direct sum of Jordan blocks. Then $A^{t}=S^{t} J^{t}\left(S^{t}\right)^{-1}$ (since $\left(S^{-1}\right)^{t}=\left(S^{t}\right)^{-1}$ ); that is, $A^{t}$ is similar to $J^{t}$. Moreover, $J^{t}$ is the direct sum of the transposes of the Jordan blocks whose direct sum is $J$. It will, thus, suffice to prove that each of these Jordan blocks is similar to its transpose. In other words, it will suffice to prove the statement for the case where $A$ is a Jordan block.

Let $A$ be an $n \times n$ Jordan block:

$$
A=\left(\begin{array}{ccccc}
\lambda & 1 & 0 & \cdots & 0 \\
0 & \lambda & 1 & \cdots & \vdots \\
\vdots & \vdots & \ddots & \ddots & 0 \\
0 & 0 & \cdots & \lambda & 1
\end{array}\right)
$$

Let $e_{1}, \ldots, e_{n}$ be the standard basis vectors for $\mathbb{C}^{n}$, so that $A e_{j}=\lambda e_{j}+e_{j-1}$ for $j>1$ and $A e_{1}=\lambda e_{1}$. Let the matrix $S$ be defined by $S e_{j}=e_{n-j+1}$. Then $S=S^{-1}$, and

$$
\begin{aligned}
S^{-1} A S e_{j} &=S A e_{n-j} \\
&= \begin{cases}S\left(\lambda e_{n-j+1}+e_{n-j}\right), & j<n \\
S\left(\lambda e_{n-j+1}\right), & j=n\end{cases} \\
&= \begin{cases}\lambda e_{j}+e_{j+1}, & j<n \\
\lambda e_{j}, & j=n\end{cases}
\end{aligned}
$$

which shows that $S^{-1} A S=A^{t}$.

Solution to 7.7.14: Using the first condition the Jordan Canonical Form [HK61, p. 247] of this matrix is a $6 \times 6$ matrix with five l's and one $-1$ on the diagonal. The blocks corresponding to the eigenvalue 1 are either $1 \times 1$ or $2 \times 2$, by the second condition, with at least one of them having dimension 2 . Thus, there could be three 1-blocks and one 2-block (for the eigenvalue 1), or one 1-block and two 2-blocks. In this way, we get the following two possibilities for the Jordan Form of the matrix:

$$
\left(\begin{array}{rrrrrr}
1 & 0 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 1 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & -1
\end{array}\right), \quad\left(\begin{array}{rrrrrr}
1 & 0 & 0 & 0 & 0 & 0 \\
0 & 1 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 1 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & -1
\end{array}\right) .
$$

Solution to 7.7.15: Since $A$ and $B$ have the same characteristic polynomial, they have the same $n$ distinct eigenvalues $l_{1}, \ldots, l_{n}$. Let $\chi(x)=\left(x-l_{1}\right)^{c_{1}} \cdots\left(x-l_{n}\right)^{c_{n}}$ be the characteristic polynomial and let $\mu(x)=\left(x-l_{1}\right)^{m_{1}} \cdots\left(x-l_{n}\right)^{m_{n}}$ be the minimal polynomial. Since a nondiagonal Jordan block [HK61, p. 247] must be at least $2 \times 2$, there can be, at most, one nondiagonal Jordan block for $N \leqslant 3$. Hence, the Jordan Canonical Form is completely determined by $\mu(x)$ and $\chi(x)$ for $N \leqslant 3$. If $\mu(x)=\chi(x)$, then each distinct eigenvalue corresponds to a single Jordan block of size equal to the multiplicity of the eigenvalue as a root of $\chi(x)$, so the Jordan Canonical Form is completely determined by $\chi(x)$, and $A$ and $B$ must then be similar.

Solution to 7.7.16: It will suffice to show that $A$ is similar to a matrix of the form $\left(\begin{array}{l}0 \\ * B\end{array}\right)$, where $B$ is $(n-1) \times(n-1)$. For in that case $B$ has the same trace as $A$, hence trace 0 , and the argument can be iterated.

Assume, without loss of generality, that $A$ is not the zero matrix. Since $A$ has trace 0 , it is not a scalar multiple of the identity matrix. Hence, there is a nonzero vector $v$ in $\mathbb{C}^{n}$ that is not an eigenvector of $A$. Take a basis for $\mathbb{C}^{n}$ in which the first basis vector is $v$ and the second one is $A v$. The matrix with respect to such a basis of the linear transformation induced by $A$ is similar to $A$ and has the required form. Solution to 7.7.17: Since $M^{3}=t I, N^{3}=0$, so it suffices to show that $N^{2} \neq 0$. If $N^{2}=0$, then $M^{2}=t P$ for some $3 \times 3$ matrix $P$ with entries in $\mathbb{R}[t]$. Therefore $t P M=M^{3}=t I$, hence $P M=I$, implying that $t I=M^{3}$ has an inverse $P^{3}$ with entries in $\mathbb{R}[t]$, a contradiction.

Solution to 7.7.19: Since the $\operatorname{map} X \mapsto X^{2}$ on $M_{n}(\mathbb{C})$ is continuous, we have

$$
B^{2}=\left(\lim _{n \rightarrow \infty} A^{n}\right)^{2}=\lim _{n \rightarrow \infty} A^{2 n}=B .
$$

The minimal polynomial of $B$ divides $x^{2}-x$, so the eigenvalues of $B$ are zeros and ones. Since the minimal polynomial is squarefree, the Jordan blocks of $B$ are of size 1, i.e., $B$ is similar to a diagonal matrix with its eigenvalues (zeros and ones) along the main diagonal.

\subsection{Bilinear, Quadratic Forms, and Inner Product Spaces}

Solution to 7.8.1: For all $x \in \mathbb{R}^{n}, g(x, x)=f(2 x)-2 f(x)=4 f(x)-2 f(x)=$ $2 f(x)$. Therefore $f(x)=g(x, x) / 2$ is the quadratic form associated with the bilinear form $g$. For each $x \in \mathbb{R}^{n}, y \mapsto g(x, y)$ is linear, so there is exactly one vector $A(x) \in \mathbb{R}^{n}$ such that $g(x, y)=\langle A(x), y\rangle$ for all $y \in \mathbb{R}^{n}$. Since $g$ is linear in the first variable, $A$ is a linear map.

Solution to 7.8.2: Let $H$ be the Hermitian matrix that induces the quadratic form

$$
H=\left(\begin{array}{lll}
A & B & D \\
B & C & E \\
D & E & F
\end{array}\right),
$$

and let

$$
G=\left(\begin{array}{ll}
A & B \\
B & C
\end{array}\right) .
$$

$H$ is positive definite if and only if all of its eigenvalues are positive. Since the product of those eigenvalues is det $H$, the positivity of det $H$ is a necessary condition for $H$ to be positive definite. Since the positive definiteness of $H$ implies the positive definiteness of $G$, whose associated quadratic form is $A x^{2}+2 B x y+C y^{2}$, the positivity of det $G$ is also a necessary condition. The positivity of $A$ is similarly a necessary condition.

Assume that $A>0$, det $G>0$, and det $H>0$, but that $H$ is not positive definite. Then the product of the eigenvalues of $H(=\operatorname{det} H)$ is positive, but not all of the eigenvalues are positive. Hence $H$ must have one positive eigenvalue and two negative ones, or a single negative one of multiplicity two. It follows that $H$ is negative definite on the two dimensional subspace spanned by the eigenvectors corresponding to the negative eigenvalues. That subspace must have a nontrivial intersection with the subspace spanned by the first two basis vectors. Hence there is a nonzero vector $v$ in $\mathbb{R}^{3}$ with its last coordinate 0 such that $v^{t} H v<0$. It follows that $G$ is not positive definite. But since $A>0$ neither is $G$ negative definite. Thus $G$ must have one positive and one negative eigenvalue, in contradiction to the assumption det $G>0$.

Solution to 7.8.3: We have

$$
2 x_{1}^{2}+x_{2}^{2}+3 x_{3}^{2}+2 t x_{1} x_{2}+2 x_{1} x_{3}=\left(x_{1}, x_{2}, x_{3}\right)\left(\begin{array}{lll}
2 & t & 1 \\
t & 1 & 0 \\
1 & 0 & 3
\end{array}\right)\left(\begin{array}{l}
x_{1} \\
x_{2} \\
x_{3}
\end{array}\right)
$$

By the Solution to Problem 7.8.2 above, the form is positive definite if and only if the determinants

$$
\left|\begin{array}{lll}
2 & t & 1 \\
t & 1 & 0 \\
1 & 0 & 3
\end{array}\right|>0 \quad \text { and } \quad\left|\begin{array}{ll}
2 & t \\
t & 1
\end{array}\right|>0
$$

that is, when $-1+3\left(2-t^{2}\right)=5-3 t^{2}>0$ and $2-t^{2}>0$.

Both conditions hold iff $|t|<\sqrt{\frac{5}{3}}$. For these values of $t$ the form is positive definite.

Solution to 7.8.4: Every vector in $W$ is orthogonal to $v=(a, b, c)$. Let $Q$ be the orthogonal projection of $\mathbb{R}^{3}$ onto the space spanned by $v$, identified with its matrix. The columns of $Q$ are $Q e_{j}, 1 \leqslant j \leqslant 3$, where the $e_{j}$ 's are the standard basis vectors in $\mathbb{R}^{3}$. But

$$
\begin{aligned}
&Q e_{1}=\left\langle v, e_{1}\right\rangle v=\left(a^{2}, a b, a c\right) \\
&Q e_{2}=\left\langle v, e_{2}\right\rangle v=\left(a b, b^{2}, b c\right) \\
&Q e_{3}=\left\langle v, e_{3}\right\rangle v=\left(a c, b c, c^{2}\right) .
\end{aligned}
$$

Therefore, the orthogonal projection onto $W$ is given by

$$
P=I-Q=\left(\begin{array}{ccc}
1-a^{2} & -a b & -a c \\
-a b & 1-b^{2} & -b c \\
-a c & -b c & 1-c^{2}
\end{array}\right) \text {. }
$$

Solution to 7.8.5: 1 . The monomials $1, t, t^{2}, \ldots, t^{n}$ form a basis for $P_{n}$. Applying the Gram-Schmidt Procedure [HK61, p. 280] to this basis gives us an orthonormal basis $p_{0}, p_{1}, \ldots, p_{n}$. The $(k+1)^{t h}$ vector in the latter basis, $p_{k}$, is a linear combination of $1, t, \ldots, t^{k}$, the first $k+1$ vectors in the former basis, with $t^{k}$ having a nonzero coefficient. (This is built into the Gram-Schmidt Procedure.) Hence, $\operatorname{deg} p_{k}=k$. 2. Since $p_{k}^{\prime}$ has degree $k-1$, it is a linear combination of $p_{0}, p_{1}, \ldots, p_{k-1}$, for those functions form an orthonormal basis for $P_{k-1}$. Since $p_{k}$ is orthogonal to $p_{0}, p_{1}, \ldots, p_{k-1}$, it is orthogonal to $p_{k}^{\prime}$.

Solution to 7.8.6: Since $p$ is even, it is orthogonal on $[-1,1]$ to all odd polynomials. Hence $a$ and $b$ are determined by the two conditions

$$
\int_{-1}^{1} p(x) d x=0, \quad \int_{-1}^{1} x^{2} p(x) d x=0 .
$$

Carrying out the integrations, one obtains the equations

$$
2 a+\frac{2 b}{3}-\frac{2}{5}=0, \quad \frac{2 a}{3}+\frac{2 b}{5}-\frac{2}{7}=0 \text {. }
$$

Solving these for $a$ and $b$, one gets $a=-\frac{3}{35}, b=\frac{6}{7}$, therefore

$$
p(x)=-\frac{3}{35}+\frac{6 x^{2}}{7}-x^{4} \text {. }
$$

Solution to 7.8.7: Let $n=\operatorname{dim} E$, and choose a basis $v_{1}, \ldots, v_{n}$ for $E$. Define the $n \times n$ matrix $A=\left(a_{j k}\right)$ by $a_{j k}=B\left(v_{k}, v_{j}\right)$. The linear transformation $T_{A}$ on $E$ induced by $A$ is determined by the relations

$$
T_{A} v_{k}=\sum_{j} a_{j k} v_{j}=\sum_{j} B\left(v_{k}, v_{j}\right) v_{j}, \quad k=1, \ldots, n,
$$

implying that $T_{A} v=\sum_{j} B\left(v, v_{j}\right) v_{j}(v \in E)$. It follows that $E_{1}=\operatorname{ker} T_{A}$. By similar reasoning, $E_{2}=\operatorname{ker} T_{A^{t}}$, where $A^{l}$ is the transpose of $A$. By the RankNullity Theorem [HK61, p. 71], $\operatorname{dim} E_{1}$ equals $n$ minus the dimension of the column space of $A$, and $\operatorname{dim} E_{2}$ equals $n$ minus the dimension of the row space of A. Since the row space and the column space of a matrix have the same dimension, the desired equality follows.

Solution to 7.8.8: We have, for any $x \in \mathbb{R}^{n}$,

$$
\langle A x, x\rangle=\langle x, A x\rangle=\left\langle A^{t} x, x\right\rangle \geqslant 0,
$$

hence, $\left\langle\left(A+A^{t}\right) x, x\right\rangle \geqslant 0$ for all $x \in \mathbb{R}^{n}$. As $A+A^{t}$ is symmetric, therefore diagonalizable, there exist an integer $k \leqslant n$, positive numbers $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{k}$, and a basis of $\mathbb{R}^{n},\left\{v_{1}, \ldots, v_{n}\right\}$, such that $\left(A+A^{T}\right) v_{i}=\lambda_{i} v_{i}$ for $1 \leqslant i \leqslant k$, and $\left(A+A^{T}\right) v_{i}=0$ for $k+1 \leqslant i \leqslant n$. The matrix representing $A$ in this basis has the form

$$
A^{\prime}=\left(\begin{array}{cc}
B & C \\
-C & D
\end{array}\right)
$$

where $D$ is antisymmetric and

$$
B=\frac{1}{2}\left(\begin{array}{ccc}
\lambda_{1} & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & \lambda_{k}
\end{array}\right)+B^{\prime},
$$

where $B^{\prime}$ is antisymmetric. We have

$$
x^{t} A^{\prime} x=\lambda_{1} x_{1}^{2}+\cdots+\lambda_{k} x_{k}^{2},
$$

so $x \in \operatorname{ker} A^{\prime}$ exactly when the first $k$ components of $x$ are zero and the last $n-k$ form a vector in $\operatorname{ker} C \cap \operatorname{ker} D$. As

$$
\left(A^{\prime}\right)^{t}=\left(\begin{array}{cc}
B^{t} & C \\
-C & D
\end{array}\right),
$$

we conclude that a vector is in the kernel of $A^{\prime}$ iff it is in the kernel of $\left(A^{\prime}\right)^{t}$.

Solution to 7.8.9: 1. Since $A$ is positive definite, one can define a new inner product $\langle,\rangle_{A}$ on $\mathbb{R}^{n}$ by

$$
\langle x, y\rangle_{A}=\langle A x, y\rangle .
$$

The linear operator $A^{-1} B$ is symmetric with respect to this inner product, that is,

$$
\begin{aligned}
\left\langle A^{-1} B x, y\right\rangle_{A} &=\langle B x, y\rangle=\left\langle x, B^{t} y\right\rangle=\langle x, B y\rangle \\
&=\left\langle A^{-1} A x, B y\right\rangle=\left\langle A x, A^{-1} B y\right\rangle=\left\langle x, A^{-1} B y\right\rangle_{A} .
\end{aligned}
$$

So there is a basis $\left\{v_{1}, \ldots v_{n}\right\}$ of $\mathbb{R}^{n}$, orthonormal with respect to $\langle,\rangle_{A}$, in which the matrix for $A^{-1} B$ is diagonal. This is the basis we are looking for; in particular, $v_{i}$ is an eigenvector for $A^{-1} B$, with eigenvalue $\lambda_{i}$ and

$$
\begin{aligned}
\left\langle v_{i}, v_{j}\right\rangle_{A} &=\delta_{i j} \\
\left\langle B v_{i}, v_{j}\right\rangle=\left\langle A^{-1} B v_{i}, v_{j}\right\rangle_{A} &=\left\langle\lambda_{i} v_{i}, v_{j}\right\rangle_{A}=\lambda_{i} \delta_{i j} .
\end{aligned}
$$

2. Let $U$ be the matrix which takes the standard basis to $\left\{v_{1}, \ldots v_{n}\right\}$ above, that is, $U e_{i}=v_{i}$. Since the $e_{i}$ form an orthonormal basis, for any matrix $M$, we have

$$
M x=\sum_{j=1}^{n}\left\langle M x, e_{j}\right) e_{j},
$$

in particular,

$$
\begin{aligned}
U^{t} A U e_{i} &=\sum_{j=1}^{n}\left\langle U^{t} A U e_{i}, e_{j}\right\rangle e_{j} \\
&=\sum_{j=1}^{n}\left\langle A U e_{i}, U e_{j}\right\rangle e_{j}
\end{aligned}
$$



$$
\begin{aligned}
&=\sum_{j=1}^{n}\left\langle A v_{i}, v_{j}\right\rangle e_{j} \\
&=\sum_{j=1}^{n} \delta_{i j} e_{j}=e_{i},
\end{aligned}
$$

showing that $U^{t} A U=I$.

Using the same decomposition for $U^{t} B U$, we have

$$
\begin{aligned}
U^{t} B U e_{i} &=\sum_{j=1}^{n}\left\langle U^{t} B U e_{i}, e_{j}\right\rangle e_{j} \\
&=\sum_{j=1}^{n}\left\langle B v_{i}, v_{j}\right\rangle e_{j} \\
&=\sum_{j=1}^{n} \lambda_{i} \delta_{i j} e_{i}=\lambda_{i} e_{i},
\end{aligned}
$$

so $U^{t} B U$ is diagonal.

Solution 2. Since $A$ is positive definite $A=W^{t} W$ for some invertible matrix $W$. Setting $N=W^{-1}$ we get $N^{t} A N=W^{t-1} W^{t} W W^{-1}=I$ and $N^{t} B N$ is still positive definite since $\left\langle x, N^{t} B N x\right\rangle=\langle N x, B(N x)\rangle \geqslant 0$. As $N^{t} B N$ is positive definite there is an orthogonal matrix $O$ such that $O^{t}\left(N^{t} B N\right) O=D$, where $D$ is a diagonal matrix. The entries of $D$ are positive, since $N^{t} B N$ is positive definite. Let $M=N O$. Then $M^{t} B M=O^{t} N^{t} B N O=D$ and $M^{t} A M=O^{t} N^{t} A N O=$ $O^{t} I O=I$, and we can easily see that $M$ is invertible, since $N$ and $O$ are.

Solution to 7.8.11: From the Criteria proved in the Solution to Problem 7.8.2 we see that the matrix is positive definite and $v^{t} A v=N(v)$ defines a norm in $\mathbb{R}^{3}$. Now all norms in $\mathbb{R}^{3}$ are equivalent, see the Solution to Problem 2.1.6, so

$$
\alpha N(v) \leqslant\|v\| \leqslant \beta N(v)
$$

where $\alpha$ and $\beta$ are the minimum and the maximum of $\|v\|$ on the set $v^{t} A v=1$.

We can use the Method of Lagrange Multipliers [MH93, p. 414] to find the maximum of the function

$$
f(x, y, z)=x^{2}+y^{2}+z^{2}
$$

over the surface defined by $\psi(x, y, z)=6$, where

$$
\psi=\frac{1}{6}\left(13 x^{2}+13 y^{2}+10 z^{2}-10 x y-4 x z-4 y z\right)
$$

is the quadratic form defined by the matrix $A$. Setting up the equations we have:

$$
\frac{\partial \psi}{\partial x}=26 x-10 y-4 z=\lambda \frac{\partial f}{\partial x}=2 \lambda x
$$



$$
\begin{aligned}
&\frac{\partial \psi}{\partial y}=26 y-10 x-4 z=\lambda \frac{\partial f}{\partial y}=2 \lambda y \\
&\frac{\partial \psi}{\partial z}=20 z-4 x-4 y=\lambda \frac{\partial f}{\partial z}=2 \lambda z \\
&\psi(x, y, z)=6
\end{aligned}
$$

which is equivalent to the linear system of equations

$$
\left(\begin{array}{ccc}
13-\lambda & -5 & -2 \\
-5 & 13-\lambda & -2 \\
-2 & -2 & 10-\lambda
\end{array}\right)\left(\begin{array}{l}
x \\
y \\
z
\end{array}\right)=0
$$

together with the equation $\psi(x, y, z)=6$. Now if the determinant of the system is nonzero, there is only one solution to the system, the trivial one $x=y=z=0$ and that point is not in the surface $\psi(x, y, z)=6$, so let's consider the case where the determinant is zero. One can easily compute it and it is:

$$
-\lambda^{3}+36 \lambda^{2}-396 \lambda+1296
$$

and one can easily see that $\lambda=18$ is a root of the polynomial, because it renders the first two rows of the matrix the same, factoring it completely we get:

$$
(18-\lambda)(\lambda-6)(\lambda-12)
$$

Considering each one of the roots that will correspond to non-trivial solutions of the system of linear equations

- $\lambda=18$. In this case the system becomes:

$$
\left(\begin{array}{lll}
-5 & -5 & -2 \\
-5 & -5 & -2 \\
-2 & -2 & -8
\end{array}\right)\left(\begin{array}{l}
x \\
y \\
z
\end{array}\right)=0
$$

which reduces to $\left\{\begin{array}{l}5 x+5 y+2 z=0 \\ 2 x+2 y+8 z=0\end{array}\right.$. Therefore $z=0$ and $y=-x$. Substituting this back in the equation of the ellipsoid, we find $x=\pm \frac{1}{\sqrt{6}}$,
and the solution points are:

$$
\pm \frac{1}{\sqrt{6}}(1,-1,0)
$$

- $\lambda=6$. The system now is

$$
\left(\begin{array}{rrr}
7 & -5 & -2 \\
-5 & 7 & -2 \\
-2 & -2 & 4
\end{array}\right)\left(\begin{array}{l}
x \\
y \\
z
\end{array}\right)=0
$$

and the solutions are: $x=y=z$ and the only points in this line on the ellipsoid are:

$$
\pm \frac{1}{\sqrt{3}}(1,1,1)
$$

$-\lambda=12$. The system now is

$$
\left(\begin{array}{rrr}
1 & -5 & -2 \\
-5 & 1 & -2 \\
-2 & -2 & -2
\end{array}\right)\left(\begin{array}{l}
x \\
y \\
z
\end{array}\right)=0
$$

and the solutions are: $y=x$ and $z=-2 x$ the points in this line on the ellipsoid are:

$$
\pm \frac{1}{2 \sqrt{3}}(1,1,-2) \text {. }
$$

Computing the sizes of each (pair) of the vectors we obtain $1 / \sqrt{3}, 1$ and $1 / \sqrt{2}$, respectively; so the least upper bound is 1 .

Solution 2. A, being a symmetric matrix, can be diagonalized by an orthogonal matrix $S$, so

$$
D=S^{t} A S
$$

where the matrix $S$ is given by the eigenvalues of $A$. Following the computation of the previous solution, we get

$$
S=\left(\begin{array}{ccc}
1 / \sqrt{2} & 1 / \sqrt{3} & 1 / \sqrt{6} \\
-1 / \sqrt{2} & 1 / \sqrt{3} & 1 / \sqrt{6} \\
0 & 1 / \sqrt{3} & -2 / \sqrt{6}
\end{array}\right)
$$

but in our case we only need the matrix $D$ which is the diagonal matrix with the eigenvalues of $A$, that is,

$$
D=\left(\begin{array}{ccc}
18 & 0 & 0 \\
0 & 6 & 0 \\
0 & 0 & 12
\end{array}\right)
$$

so after the change of basis given by the matrix $S$ above, the form $\psi$ becomes $\psi(r, s, t)=18 r^{2}+6 s^{2}+12 t^{2}$ and on these variables the ellipsoid

$$
\frac{r^{2}}{1 / 3}+s^{2}+\frac{t^{2}}{1 / 2}=1
$$

has semi-axis $1 / \sqrt{3}, 1$ and $1 / \sqrt{2}$, and the least upper bound is 1 .

Solution 3. Rotating around the $z$-axis by $45^{\circ}$ on the positive direction is equivalent to the change of variables

$$
\begin{aligned}
&x=\frac{1}{\sqrt{2}}(r+s) \\
&y=\frac{1}{\sqrt{2}}(-r+s)
\end{aligned}
$$

which substituted in the equation of the ellipsoid

$$
13 x^{2}+13 y^{2}+10 z^{2}-10 x y-4 x z-4 y z=6
$$

eliminates the mixed terms in $x y$ and $x z$. After the substitution we end up with

$$
18 r^{2}+8 s^{2}+10 z^{2}-4 \sqrt{2} s z=6 .
$$

Now we rotate around the $r$-axis by an angle $\alpha$ where $\cot 2 \alpha=\frac{8-10}{-4 \sqrt{2}}=\frac{1}{2 \sqrt{2}}$ in order to eliminate the mixed term in $s z$. Using trigonometric identities we find $\cos 2 \alpha=\frac{1}{3}$, and $\cos \alpha=\sqrt{\frac{2}{3}}, \sin \alpha=\frac{1}{\sqrt{3}}$, so the change of coordinates can now be written as

$$
\begin{aligned}
&s=w \cos \alpha-t \sin \alpha=\frac{1}{\sqrt{3}}(w \sqrt{2}-t) \\
&z=w \sin \alpha+t \cos \alpha=\frac{1}{\sqrt{3}}(w+t \sqrt{2})
\end{aligned}
$$

and the final substitution renders the equation of the ellipsoid

$$
18 r^{2}+6 w^{2}+12 t^{2}=6
$$

whose largest semi-axis is 1.

Solution to 7.8.12: Suppose that $A$ has a diagonalization with $p$ strictly positive and $q$ strictly negative entries, and that $B$ has one with $p^{\prime}$ strictly positive and $q^{\prime}$ strictly negative entries. Then $\mathbb{R}^{n}$ contains a subspace $V$ of dimension $p$ such that $x^{t} A x>0$ for all nonzero $x \in V$, and a subspace $W$ of dimension $n-p^{\prime}$ such that $x^{t} B x \leqslant 0$ for all $x \in W$. Since $x^{t} A x \leqslant x^{t} B x$ for all $x \in \mathbb{R}^{n}, x^{t} A x \leqslant 0$ for all $x \in W$. Therefore $V \cap W=\{0\}$, so $\operatorname{dim} V+\operatorname{dim} W \leqslant n$, i.e., $p+\left(n-p^{\prime}\right) \leqslant n$ giving $p \leqslant p^{\prime}$. Similarly $q^{\prime} \leqslant q$, so $p-q \leqslant p^{\prime}-q^{\prime}$.

Solution to 7.8.13: Let $A=T^{*} T$. Then $\langle x, y\rangle=0$ implies $\langle A x, y\rangle=0$. For any $x \in \mathbb{C}^{n}$ let $x^{\perp}=\left\{y \in \mathbb{C}^{n} \mid\langle x, y\rangle=0\right\}$. Thus, $\left\langle x, x^{\perp}\right\rangle=0$ so $\left\langle A x, x^{\perp}\right\rangle=0$, implying that $A x \in\left(x^{\perp}\right)^{\perp}$, so $A x=\lambda x$ for some $\lambda \in \mathbb{C}$. Since every vector is an eigenvector of $A$, it follows that $r I$ for some scalar $r$. The constant $r$ is a nonnegative real number since $A$ is positive semidefinite. If $r=0$ then $A=0$, hence $T=0$ and we may take $k=0, U=I$. If $r>0$ we take $k=\sqrt{r}$ and set $U=\frac{1}{\sqrt{r}} T$. Clearly $k$ is real, and $U$ is unitary because

$$
U^{*} U=\frac{1}{\sqrt{k}} T^{*} T \frac{1}{\sqrt{k}}=\frac{1}{k} A=I .
$$

As $T=k U$ we are done. Solution to 7.8.15: Assume that such $u, v$ exist. Then there is a $3 \times 3$ orthogonal matrix $Q$ whose three rows are $\left(u_{1}, u_{2}, a\right),\left(v_{1}, v_{2}, b\right),\left(w_{1}, w_{2}, c\right)$. Every column of $Q$ is a unit vector. This implies that $a^{2}+b^{2} \leqslant 1$.

Conversely, assume that $a^{2}+b^{2} \leqslant 1$. Let $c$ be a real number such that $a^{2}+b^{2}+c^{2}=1$. The vector $(a, b, c)$ can be extended to a basis of $\mathbb{R}^{3}$ and by the Gram-Schmidt Procedure there exist vectors $\left(u_{1}, v_{1}, w_{1}\right),\left(u_{2}, v_{2}, w_{2}\right)$ that together with $(a, b, c)$ form an orthonormal basis. Thus the matrix $M$ given by

$$
\left(\begin{array}{lll}
u_{1} & u_{2} & a \\
v_{1} & v_{2} & b \\
w_{1} & w_{2} & c
\end{array}\right)
$$

is an orthogonal, that is, $M^{t} M=I$. Since a square matrix is a left-inverse if and only if is a right-inverse we also have that $M M^{t}=I$, and the first two rows of this matrix are the desired vectors.

Solution 2. Suppose we have such $u$ and $v$. By Cauchy-Schwarz Inequality [MH93, p. 69], we have

$$
\left(u_{1} v_{1}+u_{2} v_{2}\right)^{2} \leqslant\left(u_{1}^{2}+u_{2}^{2}\right)\left(v_{1}^{2}+v_{2}^{2}\right) \text {. }
$$

Since $u \cdot v=0,\left(u_{1} v_{1}+u_{2} v_{2}\right)^{2}=(a b)^{2} ;$ since $\|u\|=\|v\|=1,1-a^{2}=u_{1}^{2}+u_{2}^{2}$, and $1-b^{2}=v_{1}^{2}+v_{2}^{2}$. Combining these, we get

$$
(a b)^{2} \leqslant\left(1-a^{2}\right)\left(1-b^{2}\right)=1-a^{2}-b^{2}+(a b)^{2},
$$

which implies $a^{2}+b^{2} \leqslant 1$.

Conversely, suppose that $a^{2}+b^{2} \leqslant 1$. Let $u=\left(0, \sqrt{1-a^{2}}, a\right)$. $\|u\|=1$, and we now find $v_{1}$ and $v_{2}$ such that $v_{1}^{2}+v_{2}^{2}+b^{2}=1$ and $u_{2} v_{2}+a b=0$. If $a=1$, then $b=0$, so we can take $v=(0,1,0)$. If $a \neq 1$, solving the second equation for $v_{2}$, we get

$$
v_{2}=\frac{-a b}{\sqrt{1-a^{2}}} \text {. }
$$

Using this to solve for $v_{1}$, we get

$$
v_{1}=\frac{\sqrt{1-a^{2}-b^{2}}}{\sqrt{1-a^{2}}} .
$$

By our condition on $a$ and $b$, both of these are real, so $u$ and $v=\left(v_{1}, v_{2}, b\right)$ are the desired vectors.

\subsection{General Theory of Matrices}

Solution to 7.9.1: Let $A$ be such an algebra, and assume $A$ does not consist of just the scalar multiples of the identity. Take a matrix $M$ in $A$ that is not a scalar multiple of the identity. We can assume without loss of generality that $M$ is in Jordan form, so there are two possibilities:

![](https://cdn.mathpix.com/cropped/2022_10_26_a807eeb357d0f35abe4eg-098.jpg?height=142&width=1126&top_left_y=344&top_left_x=505)

In the first case the matrices commuting with $M$ are the $2 \times 2$ diagonal matrices, which form an algebra of dimension 2 . Thus $\operatorname{dim} A \leqslant 2$.

In the second case $M$ has a one-dimensional eigenspace spanned by the vector $\left(\begin{array}{l}1 \\ 0\end{array}\right)$, so any matrix commuting with $M$ has $\left(\begin{array}{l}1 \\ 0\end{array}\right)$ as an eigenvector, i.e., it is upper triangular. Also, a matrix commuting with $M$ cannot have two distinct eigenvalues, because then the corresponding eigenvectors would have to be eigenvectors of $M$ (and the only eigenvectors of $M$ are the multiples of $\left(\begin{array}{l}1 \\ 0\end{array}\right)$ ). Thus, the matrices commuting with $M$ are the matrices

$$
\left(\begin{array}{cc}
\mu & \nu \\
0 & \mu
\end{array}\right) \text {. }
$$

The algebra of all such matrices has dimension 2 , so $\operatorname{dim} A \leqslant 2$ in this case also.

Solution to 7.9.2: In what follows we will use frequently the facts that $I+A$ is invertible, which can easily be concluded from the fact that $-1$ is not an eigenvalue of $A$, as we have $\operatorname{ker}(A+I)=0$, as well as that $A$ commutes with $(I+A)^{-1}$, which can be seen by factoring $A+A^{2}$ in two different ways $(I+A) A=A(I+A)$ and now multiplying on the right and left by $(I+A)^{-1}$.

1.

$$
\begin{aligned}
\left((I-A)(I+A)^{-1}\right)^{t} &=\left((I+A)^{-1}\right)^{t}(I-A)^{t} \\
&=\left((I+A)^{t}\right)^{-1}\left(I-A^{t}\right) \\
&=\left(I+A^{t}\right)^{-1}\left(I-A^{t}\right) \quad \text { since } A^{t}=A^{-1} \\
&=\left(I+A^{-1}\right)^{-1}\left(I-A^{-1}\right) \\
&=\left(A^{-1}(A+I)\right)^{-1}\left(A^{-1}(A-I)\right) \\
&=(A+I)^{-1} A A^{-1}(A-I) \\
&=(A+I)^{-1}(A-I) \\
&=-(I+A)^{-1}(I-A) \\
&=-(I-A)(I+A)^{-1}
\end{aligned}
$$

on the last equality we used the fact that $A$ commutes with $(I+A)^{-1}$ and this shows that the product is skew-symmetric.

2. Now let's suppose $S$ is skew-symmetric, then

$$
\left((I-S)(I+S)^{-1}\right)^{t}=\left((I+S)^{-1}\right)^{t}(I-S)^{t}
$$



$$
\begin{aligned}
&=\left(I+S^{t}\right)^{-1}\left(I-S^{t}\right) \\
&=(I-S)^{-1}(I+S) \\
&=(I+S)(I-S)^{-1} \\
&=\left((I-S)(I+S)^{-1}\right)^{-1}
\end{aligned}
$$

that is the product is orthogonal. Now to see that it doesn't have an eigenvalue 1 , observe that

$$
\begin{aligned}
\operatorname{det}\left((I-S)(I+S)^{-1}+I\right) &=\operatorname{det}\left((I-S+I+S)(I+S)^{-1}\right) \\
&=\operatorname{det}\left(2 I(I+S)^{-1}\right) \\
&=2^{n} \operatorname{det}\left((I+S)^{-1}\right) \\
& \neq 0
\end{aligned}
$$

3. Suppose the correspondence is not one-to-one, then

$$
\begin{aligned}
&(I-A)(I+A)^{-1}=(I-B)(I+B)^{-1} \\
&(I+A)^{-1}(I-A)=(I-B)(I+B)^{-1} \\
&(I-A)=(I+A)(I-B)(I+B)^{-1} \\
&(I-A)(I+B)=(I+A)(I-B)
\end{aligned}
$$

simplifying we see that $A=B$.

Solution to 7.9.4: If $x, y \in \mathbb{R}^{n}$ then $x^{t} P y$ is a scalar, so $\left(x^{t} P y\right)^{t}=x^{t} P y$, that is, $y^{t} P^{t} x=x^{t} P y$. We conclude, from the hypothesis, that $y^{t} P^{t} x=-y^{t} P x$ for all $x, y$. Therefore $y^{t}\left(P+P^{t}\right) x=0$. Write $A=P+P^{t}=\left(a_{i j}\right)$. Then, $a_{i j}=e_{i}^{t} A e_{j}=0$, where the $e_{i}$ 's are the standard basis vectors, proving that $A=0$, that is, $P$ is skew-symmetric.

Solution to 7.9.5: We will use a powerful result on the structure of real normal operators, not commonly found in the literature. We provide also a second solution, not using the normal form, but which is inspired on it. Lemma (Structure of Real Normal Operators): Given a normal operator $A$ on $\mathbb{R}^{n}$, there exists an orthonormal basis in which the matrix of $A$ has the form

![](https://cdn.mathpix.com/cropped/2022_10_26_a807eeb357d0f35abe4eg-100.jpg?height=605&width=1269&top_left_y=337&top_left_x=428)

where the numbers $\lambda_{j}=\sigma_{j}+i \tau_{j}, j=1, \ldots, k$ and $\lambda_{2 k+1}, \ldots, \lambda_{n}$ are the eigenvalues of $A$.

The proof is obtained by embedding each component of $\mathbb{R}^{n}$ as the real slice of each component of $\mathbb{C}^{n}$, extending A to a normal operator on $\mathbb{C}^{n}$, and noticing that the new operator has the same real matrix (on the same basis) and over $\mathbb{C}^{n}$ has basis of eigenvectors. A change of basis, picking the new vectors as the real and imaginary parts of the eigenvectors associated with the imaginary eigenvalues, reduces it to the desired form. For details on the proof we refer the reader to [Shi77, p. 265-271] or [HS74, p. 117].

The matrix of an anti-symmetric operator A has the property

$$
a_{i j}=\left\langle A e_{i}, e_{j}\right\rangle=\left\langle e_{i}, A^{*} e_{j}\right\rangle=\left\langle e_{i},-A e_{j}\right\rangle=-\overline{\left\langle A e_{j}, e_{i}\right\rangle}=-\overline{a_{j i}} .
$$

Since anti-symmetric operators are normal, they have a basis of eigenvalues, these satisfy the above equality and are all pure imaginary.

Thus, in the standard decomposition described above all eigenvalues are pure imaginary, i.e., $\sigma_{1}=\cdots=\sigma_{k}=\lambda_{2 k+1}=\cdots=\lambda_{n}=0$ and the decomposition in this case is

![](https://cdn.mathpix.com/cropped/2022_10_26_a807eeb357d0f35abe4eg-100.jpg?height=610&width=1150&top_left_y=1836&top_left_x=482)

which obviously has even rank. Solution 2. Consider $\hat{A}$ the complexification of $A$, that is, the linear operator from $\mathbb{C}^{n}$ to $\mathbb{C}^{n}$ with the same matrix as $A$ with respect to the standard basis. Since $A$ is skew-symmetric, all its eigenvalues are pure imaginary and from the fact that the characteristic polynomial has real coefficients, the non-real eigenvalues show up in conjugate pairs, therefore, the polynomial has the form

$$
\chi_{A}(t)=t^{k} p_{1}(t)^{n_{1}} \cdots p_{r}(t)^{n_{r}},
$$

where the $p_{i}$ 's are real, irreducible quadratics.

From the diagonal form of $\hat{A}$ over $\mathbb{C}$ we can see that the minimal polynomial has the factor in $t$ with power 1 , that is, of the form

$$
\mu_{A}(t)=\mu_{\hat{A}}(t)=t p_{1}(t)^{m_{1}} \cdots p_{r}(t)^{m_{r}} .
$$

Now consider the Rational Canonical Form [HK61, p. 238] of $A$. It is a block diagonal matrix composed of blocks of even size and full rank, together with a block of a zero matrix corresponding to the zero eigenvalues, showing that $A$ has even rank.

Solution to 7.9.6: Since $A$ is real and symmetric, there is an orthogonal matrix $U$ such that $D=U^{-1} A U$ is diagonal, say with entries $\lambda_{1}, \ldots, \lambda_{n}$. Let $E=U^{-1} B U$, then $\operatorname{tr} A B=\operatorname{tr} D E=\sum_{i=1}^{n} \lambda_{i} e_{i i}$. If $A$ and $B$ are both positive semi-definite, then so are $D$ and $E$, therefore $\lambda_{i} \geqslant 0$ and $e_{i i} \geqslant 0$, giving $\operatorname{tr} A B \geqslant 0$.

If $A$ is not positive definite then $\lambda_{i}<0$ for some $i$. Let $E$ be a diagonal matrix with $e_{i i}=1$ and all other entries zero. Then $B=U^{-1} E U$ is positive semidefinite and $\operatorname{tr} A B<0$.

Solution to 7.9.7: Since $A$ is symmetric it can be diagonalized: Let

$$
A=Q D Q^{-1}
$$

where $D=\operatorname{diag}\left(d_{1}, \ldots, d_{n}\right)$ and each $d_{i}$ is nonnegative. Then

$$
0=Q^{-1}(A B+B A) Q=D C+C D
$$

where $C=Q^{-1} B Q$. Individual entries of this equation read

$$
0=\left(d_{i}+d_{j}\right) c_{i j}
$$

so for each $i$ and $j$ we must have either $c_{i j}=0$ or $d_{i}=d_{j}=0$. In either case,

$$
d_{i} c_{i j}=d_{j} c_{i j}=0
$$

which is the same as

$$
D C=C D=0 .
$$

Hence, $A B=B A=0$. 

\section{Example:}

$$
A=\left(\begin{array}{ll}
1 & 0 \\
0 & 0
\end{array}\right) \quad B=\left(\begin{array}{ll}
0 & 0 \\
0 & 1
\end{array}\right) .
$$

Solution 2. Since $A$ is symmetric, it is diagonalizable. Let $v$ be an eigenvector of $A$ with $A v=\lambda v$, then

$$
A(B v)=-B A v=-\lambda B v
$$

that is, $B v$ is an eigenvector of $A$ with eigenvalue $-\lambda$.

Using one of the conditions we get $\langle A B v, B v\rangle \geqslant 0$ but on the other hand $\langle A B v, B v\rangle=-\lambda\langle B v, B v\rangle \leqslant 0$, so either $\lambda=0$ or $B v=0$. Writing $A$ and $B$ on this basis, that diagonalizes $A$, ordered with the zero eigenvalues in a first block we have

![](https://cdn.mathpix.com/cropped/2022_10_26_a807eeb357d0f35abe4eg-102.jpg?height=370&width=1248&top_left_y=875&top_left_x=428)

which implies that $A B=0$ and similarly that $B A=0$.

Solution to 7.9.8: Assuming first that $A$ are $B$ are invertible, we have

$$
\begin{aligned}
A(A+B)^{-1} B &=\left(A^{-1}\right)^{-1}(A+B)^{-1}\left(B^{-1}\right)^{-1} \\
&=\left(B^{-1}(A+B) A^{-1}\right)^{-1} \\
&=\left(B^{-1}+A^{-1}\right)^{-1} .
\end{aligned}
$$

and the same reasoning shows that $B(A+B)^{-1} A=\left(B^{-1}+A^{-1}\right)$.

In the general case, for some $\delta>0$ the matrices $A+\lambda I$ and $B-\lambda I$ will be invertible for $0<|\lambda|<\delta$. Then by what has already been established,

$$
(A+\lambda I)(A+B)^{-1}(B-\lambda I)=(B-\lambda I)(A+B)^{-1}(A+\lambda I) .
$$

Taking the limit as $\lambda \rightarrow 0$, we get the desired equality.

Solution to 7.9.9: 1 . No, for example take $A=\left(\begin{array}{ll}1 & 0 \\ 1 & 1\end{array}\right)$.

$$
A A^{t}=\left(\begin{array}{ll}
1 & 1 \\
1 & 2
\end{array}\right) \neq\left(\begin{array}{ll}
2 & 1 \\
1 & 1
\end{array}\right)=A^{t} A .
$$

2. True. If the columns of $A$ form an orthonormal set then $A^{t} A=I$, transposing the product $A A^{t}=\left(A^{t} A\right)^{t}=I$ and now the rows form an orthonormal set.

Solution to 7.9.10: The characteristic polynomial of $M_{1}$ is $z^{2}-7 z+10$, with roots 5, 2. Hence $M_{1}$ is similar to $\left(\begin{array}{l}50 \\ 0\end{array}\right)$, so $\left(M_{1}^{n}\right)_{n=1}^{\infty}$ is bounded away from 0 but is not bounded. The characteristic polynomial of $M_{2}$ is $z^{2}-z+1$, with roots $(1 \pm i \sqrt{3}) / 2$, both of unit modulus. Hence $M_{2}$ is similar to a unitary matrix, and $\left(M_{2}^{n}\right)_{n=1}^{\infty}$ is both bounded and bounded away from 0 .

The characteristic polynomial of $M_{3}$ is $z^{2}-z+0.7$, with roots $(1 \pm i \sqrt{1.8}) / 2$, each of modulus less than 1 . Hence $M_{2}$ is similar to a diagonal matrix with each diagonal entry of modulus less than 1, implying that $\left(M_{3}^{n}\right)_{n=1}^{\infty}$ is bounded but not bounded away from 0 .

Solution to 7.9.13: 1 . The matrix $A-I$ has the property that the sum of the entries in each column is equal to 0 . The row operation $R_{1} \rightarrow R_{1}+\cdots+R_{n}$ applied to $A-I$ results in a matrix whose first row is zero. Hence $|A-I|=0$. Thus 1 is an eigenvalue of $A$ and there is an eigenvector $x \neq 0$ such that $A x=x$.

2. Let $A=\left(\begin{array}{ll}a & b \\ c & d\end{array}\right)$ with $a, b, c, d>0$. The characteristic polynomial of $A$ is $\lambda^{2}-(a+d) \lambda+(a d-b c)$. The discriminant is $(a+d)^{2}-4(a d-b c)=$ $(a-d)^{2}+4 b c>0$. Therefore, $A$ has two distinct real eigenvalues $\lambda_{1}, \lambda_{2}$. Now $\lambda_{1}+\lambda_{2}=a+d>0$. Thus $A$ has at least one positive eigenvalue $\lambda$, and there is an eigenvector $y \neq 0$ such that $A y=\lambda y$.

Solution to 7.9.14: Let $Y=A D-B C$. We have

$$
\left(\begin{array}{ll}
A & B \\
C & D
\end{array}\right)\left(\begin{array}{cc}
D & -B \\
-C & A
\end{array}\right)=\left(\begin{array}{ll}
A D-B C & -A B+B A \\
C D-D C & -C B+D A
\end{array}\right)=\left(\begin{array}{ll}
Y & 0 \\
0 & Y
\end{array}\right) .
$$

If $Y$ is invertible, then so are $\left(\begin{array}{ll}Y & 0 \\ 0 & Y\end{array}\right)$ and $X$.

Assume now that $X$ is invertible, and let $v$ be vector in the kernel of $Y:(A D-B C) v=0$. Then

$$
\left(\begin{array}{ll}
A & B \\
C & D
\end{array}\right)\left(\begin{array}{c}
D v \\
-C v
\end{array}\right)=0=\left(\begin{array}{ll}
A & B \\
C & D
\end{array}\right)\left(\begin{array}{c}
-B v \\
A v
\end{array}\right)
$$

implying that $D v=C v=B v=A v=0$. But then $X\left(\begin{array}{l}v \\ v\end{array}\right)=0$, so, by the invertibility of $X, v=0$, proving that $Y$ is invertible.

Solution to 7.9.15: Let $A \in G L_{2}(\mathbb{C})$ be a matrix representing $a$. Then $A^{n}=\lambda I$ for some $\lambda \in \mathbb{C}^{*}$. We may assume, without loss of generality, that $A^{n}=I$. Since the polynomial $x^{n}-1$ has distinct roots, $A$ is diagonalizable, and its eigenvalues must be the $n$-roots of 1 . Now conjugating and dividing by the first root of 1 , we may assume that $A=\left(\begin{array}{l}10 \\ 0\end{array}\right)$. If for some $m \geqslant 1, A^{m}=s I$ with $s \in \mathbb{C}^{*}$, then, comparing upper left hand corners we see that $s=1$. Since the order of $a$ is exactly $n$, the previous sentence implies that $A$ has order exactly $n$, so $\zeta$ is a primitive $n$-root of 1 .

In the same way we may represent $b$ by a matrix that is conjugate to $B=\left(\begin{array}{ll}1 & 0 \\ 0 & \zeta^{\prime}\end{array}\right)$ for some primitive $n$-root of $1, \zeta^{\prime}$. Then $\zeta^{\prime}$ is a power of $\zeta$, so $B$ is a power $A$, and consequently $b$ is conjugate to a power of $a$.

Solution to 7.9.16: If $\operatorname{det} B=0$, then $\operatorname{det} B \equiv 0 \quad(\bmod 2)$. Hence, if we can show that det $B \neq 0$ over the field $\mathbb{Z}_{2}$, we are done. In the field $\mathbb{Z}_{2}, 1=-1$, so $B$ is equal to the matrix with zeros along the diagonal and 1's everywhere else. Since adding one row of a matrix to another row does not change the determinant, we can replace $B$ by the matrix obtained by adding the first nineteen rows of $B$ to the last row. Since each column of $B$ contains exactly nineteen 1's, $B$ becomes

$$
\left(\begin{array}{cccccc}
0 & 1 & 1 & \cdots & 1 & 1 \\
1 & 0 & 1 & \cdots & 1 & 1 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
1 & 1 & 1 & \cdots & 0 & 1 \\
1 & 1 & 1 & \cdots & 1 & 1
\end{array}\right) .
$$

By adding the last row of $B$ to each of the other rows, $B$ becomes

$$
\left(\begin{array}{cccccc}
1 & 0 & 0 & \cdots & 0 & 0 \\
0 & 1 & 0 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \cdots & 1 & 0 \\
1 & 1 & 1 & \cdots & 1 & 1
\end{array}\right) .
$$

This is a lower-triangular matrix, so its determinant is the product of its diagonal elements. Hence, the determinant of $B$ is equal to 1 over $\mathbb{Z}_{2}$, and we are done.

Solution 2. In the matrix modulo 2 , the sum of all columns except column $i$ is the $i^{t h}$ standard basis vector, so the span of the columns has dimension 20 , and the matrix is nonsingular.

Solution to 7.9.17: Let $\mathcal{C}$ be the set of real matrices that commute with $A$. It is clearly a vector space of dimension, at most, 4 . The set $\{s I+t A \mid s, t \in \mathbb{R}\}$ is a two-dimensional subspace of $\mathcal{C}$, so it suffices to show that there are two linearly independent matrices which do not commute with $A$. A calculation show that the matrices $\left(\begin{array}{ll}0 & 1 \\ 1 & 0\end{array}\right)$ and $\left(\begin{array}{cc}0 & 1 \\ -1 & 0\end{array}\right)$ are such matrices.

Solution to 7.9.18: Let

$$
A=\left(\begin{array}{ll}
a & b \\
c & d
\end{array}\right) \quad \text { and } \quad X=\left(\begin{array}{cc}
x & y \\
z & w
\end{array}\right) .
$$

If $A X=X A$, we have the three equations: $b z=y c, a y+b w=x b+y d$, and $c x+d z=z a+w c$.

- $b=c=0$. Since $A$ is not a multiple of the identity, $a \neq d$. The above equations reduce to $a y=d y$ and $d z=a z$, which, in turn, imply that $y=z=0$. Hence,

$$
X=\left(\begin{array}{cc}
x & 0 \\
0 & w
\end{array}\right)=\left(x-\frac{a}{a-d}(x-w)\right) \mathbf{I}+\left(\frac{x-w}{a-d}\right) A .
$$

- $b \neq 0$ or $c \neq 0$. We can assume, without loss of generality, that $b \neq 0$, as the other case is identical. Then $z=c y / b$ and $w=x-y(a-d) / b$. Hence,

$$
X=\frac{1}{b}\left(\begin{array}{cc}
b x-a y+a y & b y \\
c y & b x-a y+d y
\end{array}\right)=\left(\frac{b x-a y}{b}\right) \mathrm{I}+\frac{y}{b} A .
$$

Solution to 7.9.19: 1. Assume without loss of generality that $\lambda=0$ (otherwise replace $J$ by $J-\lambda I)$. Let $e_{1}, \ldots, e_{n}$ be the standard basis vectors for $\mathbb{C}^{n}$, so that $J e_{k}=e_{k-1}$ for $k=2, \ldots, n, J e_{1}=0$. Suppose $A J=J A$. Then, for $k=1, \ldots, n-1$,

$$
A e_{k}=A J^{n-k} e_{n}=J^{n-k} A e_{n},
$$

so $A$ is completely determined by $A e_{n}$. If $A e_{n}=\left(\begin{array}{c}c_{n} \\ \vdots \\ c_{1}\end{array}\right)$, then

$$
A=\left(\begin{array}{cccccc}
c_{1} & c_{2} & c_{3} & \ldots & c_{n-1} & c_{n} \\
0 & c_{1} & c_{2} & \ldots & c_{n-2} & c_{n-1} \\
0 & 0 & c_{1} & \ldots & c_{n-3} & c_{n-2} \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \ldots & c_{1} & c_{2} \\
0 & 0 & 0 & \ldots & 0 & c_{1}
\end{array}\right) .
$$

In other words, $A$ has zero entries below the main diagonal, the entry $c_{1}$ at each position on the main diagonal, the entry $c_{2}$ at each position immediately above the main diagonal, the entry $c_{3}$ at each position two slots above the main diagonal, etc. From (*) it follows that every matrix of this form commutes with $J$. The commutant of $J$ thus has dimension $n$.

2. Consider a $2 n \times 2 n$ matrix in block form $\left(\begin{array}{ll}A & B \\ C & D\end{array}\right)$ where $A, B, C, D$ are $n \times n$. A simple computation shows that it commutes with $J \oplus J$ if and only if $A, B, C, D$ all commute with $J$. The commutant of $J$, as a vector space, is thus the direct sum of 4 vector spaces of dimension $n$, so it has dimension $4 n$.

Solution to 7.9.20: The characteristic polynomial

$$
\chi_{A}(x)=(2-x)\left((2-x)^{2}-2\right)
$$

has distinct roots $2-\sqrt{2}, 2,2+\sqrt{2}$, therefore $A$ is diagonalizable, and, as $B$ commutes with $A, B$ is diagonalizable over the same basis. Let $T$ be the transformation that realizes these diagonalizations,

$$
T A T^{-1}=\operatorname{diag}(2,2+\sqrt{2}, 2-\sqrt{2}), \quad T B T^{-1}=\operatorname{diag}(\alpha, \beta, \gamma) .
$$

The linear system

$$
a I+b \operatorname{diag}(2,2+\sqrt{2}, 2-\sqrt{2})+c \operatorname{diag}^{2}(2,2+\sqrt{2}, 2-\sqrt{2})=\operatorname{diag}(\alpha, \beta, \gamma)
$$

always has a solution, as it is equivalent to the Vandermonde system

$$
\left(\begin{array}{ccc}
1 & 2 & 2^{2} \\
1 & 2+\sqrt{2} & (2+\sqrt{2})^{2} \\
1 & 2-\sqrt{2} & (2-\sqrt{2})^{2}
\end{array}\right)\left(\begin{array}{l}
a \\
b \\
c
\end{array}\right)=\left(\begin{array}{l}
\alpha \\
\beta \\
\gamma
\end{array}\right)
$$

with nonzero determinant, see the Solution to Problem 7.2.11 or [HK61, p. 125]. Applying $T^{-1}$ on the left and $T$ on the right to the above equation we get

$$
B=a I+b A+c A^{2} \text {. }
$$

Solution 2. The characteristic polynomial

$$
\chi_{A}(x)=\left(2^{\prime}-x\right)\left((2-x)^{2}-2\right)
$$

has distinct roots $2-\sqrt{2}, 2,2+\sqrt{2}$, that we will call $\lambda_{1}, \lambda_{2}$ and $\lambda_{3}$, and let $\Lambda=\operatorname{diag}\left(\lambda_{1}, \lambda_{2}, \lambda_{3}\right)$, then if $A B=B A$ and $A=T \Lambda T^{-1}$ then $\Lambda\left(T^{-1} B T\right)=\left(T^{-1} B T\right) \Lambda$ and since the $\lambda_{i}$ are distinct, $T^{-1} B T$ must be diagonal. Say $T^{-1} B T=\operatorname{diag}\left(b_{1}, b_{2}, b_{3}\right)$, choose a quadratic polynomial $p$ such that $p\left(\lambda_{i}\right)=b_{i}$, for $i=1,2,3$, for example

$$
p(\lambda)=b_{1} \frac{\left(\lambda-\lambda_{2}\right)\left(\lambda-\lambda_{3}\right)}{\left(\lambda_{1}-\lambda_{2}\right)\left(\lambda_{1}-\lambda_{3}\right)}+b_{2} \frac{\left(\lambda-\lambda_{1}\right)\left(\lambda-\lambda_{3}\right)}{\left(\lambda_{2}-\lambda_{1}\right)\left(\lambda_{2}-\lambda_{3}\right)}+b_{3} \frac{\left(\lambda-\lambda_{1}\right)\left(\lambda-\lambda_{2}\right)}{\left(\lambda_{3}-\lambda_{1}\right)\left(\lambda_{3}-\lambda_{2}\right)}
$$

Then $p(A)=T p(\Lambda) T^{-1}=T \operatorname{diag}\left(b_{1}, b_{2}, b_{3}\right) T^{-1}=B$.

Solution 3. Let

$$
B=\left(\begin{array}{ccc}
\alpha & \beta & \gamma \\
\delta & \varepsilon & \zeta \\
\eta & \theta & \imath
\end{array}\right) .
$$

Solving for $A B=B A$ leads to a system of 9 equations in 9 unknows with a solution of the form

Solving the system

$$
\left(\begin{array}{ccc}
\alpha & \beta & \gamma \\
\beta & \alpha+\gamma & \beta \\
\gamma & \beta & \alpha
\end{array}\right) \text {. }
$$

we obtain

$$
\begin{gathered}
\left(\begin{array}{ccc}
\alpha & \beta & \gamma \\
\beta & \alpha+\gamma & \beta \\
\gamma & \beta & \alpha
\end{array}\right)= \\
a\left(\begin{array}{lll}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right)+b\left(\begin{array}{ccc}
2 & -1 & 0 \\
-1 & 2 & -1 \\
0 & -1 & 2
\end{array}\right)+c\left(\begin{array}{ccc}
5 & -4 & 1 \\
-4 & 6 & -4 \\
1 & -4 & 5
\end{array}\right)
\end{gathered}
$$

$$
\begin{aligned}
&a=\alpha+2 \beta+3 \gamma \\
&b=-\beta-4 \gamma \\
&c=\gamma
\end{aligned}
$$

thus,

$$
B=(\alpha+2 \beta+3 \gamma) I+(-\beta-4 \gamma) A+\gamma A^{2} .
$$

![](https://cdn.mathpix.com/cropped/2022_10_26_a807eeb357d0f35abe4eg-107.jpg?height=73&width=1387&top_left_y=403&top_left_x=369)
$A^{2}=B^{2}=0$ but $(A+B)^{2}=I$, so $A+B$ is not nilpotent.

2. If $A$ is nilpotent then neither $\pm 1$ are eigenvalues of $A$, otherwise 1 would be eigenvalue of $A^{2 k}$ for all $k$, which is a contradiction, so $\operatorname{ker}(I \pm A)=0$, that is, $I \pm A$ are invertible.

3. Let $k_{A}$ and $k_{B}$ be exponents that take $A$ and $B$ into zero, respectively. Since A commutes with $\mathbf{B}$

$$
(A+B)^{k_{A}+k_{B}}=\sum_{i=0}^{k_{A}+k_{B}}\left(\begin{array}{c}
k_{A}+k_{B} \\
i
\end{array}\right) A^{i} B^{k_{A}+k_{B}-i}
$$

and one of the powers on the right is always zero, and so is the whole expression.

Solution to 7.9.23: Define the norm of a matrix $X=\left(x_{i j}\right)$ by $\|X\|=\sum_{i, j}\left|x_{i j}\right|$. Notice that if $B_{k}, k=0,1, \ldots$, are matrices such that $\sum\left\|B_{k}\right\|<\infty$, then $\sum B_{k}$ converges, because the convergence of the norms clearly implies the absolute entrywise convergence.

In our case, we have $B_{k}=A^{k}$. The desired result follows from the fact that $\sum\|A\|^{k} / k !$ converges for any matrix $A$.

Solution to 7.9.24: Note that, if $A$ is an invertible matrix, we have

$$
A e^{M} A^{-1}=e^{A M A^{-1}}
$$

so we may assume that $M$ is upper-triangular. Under this assumption $e^{M}$ is also upper-triangular, and if $a_{1}, \ldots, a_{n}$ are $M$ 's diagonal entries, then the diagonal entries of $e^{M}$ are

$$
e^{a_{1}}, \ldots, e^{a_{n}}
$$

and we get

$$
\operatorname{det}\left(e^{M}\right)=\prod_{i=1}^{n} e^{a_{i}}=e^{\sum_{i=1}^{n} a_{i}}=e^{\operatorname{tr}(M)} .
$$

Solution to 7.9.26: It is easy to see that

$$
\langle X, Y\rangle=\operatorname{tr}\left(X Y^{*}\right)
$$

defines an inner product on the space of $n \times n$ complex matrices, in fact, it is the standard inner product under the identification with $\mathbb{C}^{n^{2}}$. The inequality is then the Cauchy-Schwarz Inequality [MH93, p. 69] for the norm. Solution to 7.9.27: 1. Let $A$ and $B$ be $n \times n$ real matrices and $k$ a positive integer. We have

$$
(A+t B)^{k}=A^{k}+t \sum_{i=0}^{k-1} A^{i} B A^{k-1-i}+O\left(t^{2}\right) \quad(t \rightarrow 0)
$$

where $O\left(t^{2}\right)$ is composed of terms with a power of $t$ higher than 1. Hence

$$
\lim _{t \rightarrow 0} \frac{1}{t}\left((A+t B)^{k}-A^{k}\right)=\sum_{i=0}^{k-1} A^{i} B A^{k-1-i} .
$$

2. We have

$$
\left.\frac{d}{d t} \operatorname{tr}(A+t B)^{k}\right|_{t=0}=\left.\operatorname{tr} \frac{d}{d t}(A+t B)^{k}\right|_{t=0}
$$

By definition,

$$
\left.\frac{d}{d t}(A+t B)^{k}\right|_{t=0}=\lim _{t \rightarrow 0} \frac{1}{t}\left((A+t B)^{k}-A^{k}\right) .
$$

Using the previous part of the problem we get

$$
\left.\frac{d}{d t} \operatorname{tr}(A+t B)^{k}\right|_{t=0}=\operatorname{tr} \sum_{j=0}^{k-1} A^{j} B A^{k-j-1}=\sum_{j=0}^{k-1} \operatorname{tr}\left(A^{j} B A^{k-j-1}\right) .
$$

Solution to 7.9.28: 1. Let $\alpha=\frac{1}{n} \operatorname{tr}(M)$, so that $\operatorname{tr}(M-\alpha I)=0$. Let

$$
A=\frac{1}{2}\left(M-M^{t}\right), \quad S=\frac{1}{2}\left(M+M^{t}\right)-\alpha I .
$$

The desired conditions are then satisfied.

2. We have

$$
M^{2}=A^{2}+S^{2}+\alpha^{2} I+2 \alpha A+2 \alpha S+A S+S A .
$$

The trace of $M$ is the sum of the traces of the seven terms on the right. We have $\operatorname{tr}(A)=\operatorname{tr}(B)=0$. Also,

$$
\operatorname{tr}(A S)=\operatorname{tr}\left((A S)^{l}\right)=\operatorname{tr}\left(S^{t} A^{t}\right)=\operatorname{tr}(-S A)=-\operatorname{tr}(S A),
$$

so $\operatorname{tr}(A S+S A)=0$ (in fact, $\operatorname{tr}(A S)=0$ since $\operatorname{tr}(A S)=\operatorname{tr}(S A)$ ). The desired equality now follows.

Solution to 7.9.29: We must show that 0 is the only eigenvalue of $A$. Suppose the contrary. We may assume that $A$ is in Jordan form, since the trace is preserved by similarity, in particular that $A$ is upper-triangular. Let $\lambda_{1}, \ldots, \lambda_{p}$ be the distinct nonzero eigenvalues of $A$, each $\lambda_{j}$ repeated $m_{j}$ times in the diagonal of $A$. Then the nonzero entries in the diagonal of $A^{k}$ are $\lambda_{1}^{k}, \ldots, \lambda_{p}^{k}$, repeated $m_{1}, \ldots, m_{p}$ times, respectively. We have

$$
0=\operatorname{tr} A^{k}=\sum_{j=1}^{p} m_{j} \lambda_{j}^{k}, \quad \text { for } k=1, \ldots, n .
$$

Therefore $M\left(m_{1} m_{2} \cdots m_{p}\right)^{t}=0$ where the matrix $M$ is

$$
\left(\begin{array}{cccc}
\lambda_{1} & \lambda_{2} & \ldots & \lambda_{p} \\
\lambda_{1}^{2} & \lambda_{2}^{2} & \ldots & \lambda_{p}^{2} \\
\lambda_{1}^{3} & \lambda_{2}^{3} & \ldots & \lambda_{p}^{3} \\
\vdots & \vdots & \ddots & \vdots \\
\lambda_{1}^{k} & \lambda_{2}^{k} & \ldots & \lambda_{p}^{k}
\end{array}\right) .
$$

But

$$
\operatorname{det} M=\lambda_{1} \lambda_{2} \cdots \lambda_{p} \operatorname{det}\left(\begin{array}{cccc}
1 & 1 & \ldots & 1 \\
\lambda_{1}^{2} & \lambda_{2}^{2} & \ldots & \lambda_{p}^{2} \\
\lambda_{1}^{3} & \lambda_{2}^{3} & \ldots & \lambda_{p}^{3} \\
\vdots & \vdots & \ddots & \vdots \\
\lambda_{1}^{k} & \lambda_{2}^{k} & \ldots & \lambda_{p}^{k}
\end{array}\right)
$$

which is nonzero since the $\lambda$ 's are nonzero and the other factor is a Vandermonde determinant. We got a contradiction, therefore $A$ has no nonzero eigenvalues.

Solution to 7.9.30: Consider the function $f$ defined on the unit disc by $f(z)=(1+z)^{1 / r}$, and its Maclaurin expansion [MH87, p. 234] $f(z)=\sum_{k=0}^{\infty} c_{k} z^{k}$. We have $f(z)^{r}=1+z$. Let the matrix $A$ be defined by

$$
A=\sum_{k=0}^{\infty} c_{k} N^{k},
$$

a finite sum, because $A$ is nilpotent. The computation of $A^{r}$ involves the same formal manipulations with power series as does the computation of $f(z)^{r}$. If we replace $z$ by $N$ everywhere in the latter computation, we arrive in the end at the equality $A^{r}=I+N$.

Solution to 7.9.31: We will prove the equivalent result that the kernel of $A$ is trivial. Let $x=\left(x_{1}, \ldots, x_{n}\right)^{t}$ be a nonzero vector in $\mathbb{R}^{n}$. We have

$$
\begin{aligned}
A x \cdot x &=\sum_{i, j=1}^{n} a_{i j} x_{i} x_{j} \\
&=\sum_{i=1}^{n} a_{i i} x_{i}^{2}+\sum_{i \neq j} a_{i j} x_{i} x_{j}
\end{aligned}
$$



$$
\begin{aligned}
&\geqslant \sum_{i=1}^{n} x_{i}^{2}-\sum_{i \neq j}\left|a_{i j} x_{i} x_{j}\right| \\
&\geqslant \sum_{i=1}^{n} x_{i}^{2}-\sqrt{\sum_{i \neq j} a_{i j}^{2} \sum_{i \neq j} x_{i}^{2} x_{j}^{2}} \\
&>\sum_{i=1}^{n} x_{i}^{2}-\sqrt{\sum_{i \neq j} x_{i}^{2} x_{j}^{2}} \\
&\geqslant \sum_{i=1}^{n} x_{i}^{2}-\sqrt{\sum_{i=1}^{n} x_{i}^{2} \sum_{j=1}^{n} x_{j}^{2}} \\
&\geqslant 0 .
\end{aligned}
$$

So $\langle A x, x\rangle \neq 0$ and $A x \neq 0$, therefore, the kernel of $A$ is trivial.

Solution to 7.9.32: Suppose $x$ is in the kernel. Then

$$
a_{i j} x_{i}=-\sum_{j \neq i} a_{i j} x_{j}
$$

for each $i$. Let $i$ be such that $\left|x_{i}\right|=\max _{k}\left|x_{k}\right|=M$, say. Then

$$
\left|a_{i i}\right| M \leqslant \sum_{j \neq i}\left|a_{i j}\right|\left|x_{j}\right| \leqslant \sum_{j \neq i}\left|a_{i j}\right| M
$$

so

$$
\left(\left|a_{i i}\right| \sum_{\boldsymbol{j} \neq i}\left|a_{i j}\right|\right) M \leqslant 0 .
$$

Since the therm inside the parenthesis is strictly positive by assumption, we must have $M=0$, so $x=0$ and $A$ is invertible.

Solution to 7.9.33: It will suffice to prove that $\operatorname{ker}(I-A)$ is trivial. Let $x=\left(x_{1}, x_{2}, \cdots, x_{n}\right)^{t}$ be a nonzero vector in $\mathbb{R}^{n}$, and let $y=(I-A) x$. Pick $k$ such that $\left|x_{k}\right|=\max \left\{\left|x_{1}\right|, \ldots,\left|x_{n}\right|\right\}$. Then

$$
\begin{aligned}
\left|y_{k}\right| &=\left|x_{k}-\sum_{j=1}^{n} a_{k j} x_{j}\right| \\
& \geqslant\left|x_{k}\right|-\sum_{j=1}^{n}\left|a_{k j}\right|\left|x_{j}\right| \\
& \geqslant\left|x_{k}\right|-\sum_{j=1}^{n}\left|a_{k j}\right|\left|x_{k}\right|
\end{aligned}
$$



$$
=\left|x_{k}\right|\left(1-\sum_{j=1}^{n}\left|a_{k j}\right|\right)>0 .
$$

Hence, $y \neq 0$, as desired.

Solution 2. Let $\alpha<1$ be a positive number such that, for all values of $i$, we have $\sum_{j=1}^{n}\left|a_{i j}\right| \leqslant \alpha$. Then,

$$
\sum_{j, k}\left|a_{i j} a_{j k}\right|=\sum_{j}\left(\left|a_{i j}\right| \sum_{k}\left|a_{j k}\right|\right) \leqslant \alpha \sum_{j}\left|a_{i j}\right| \leqslant \alpha^{2} .
$$

And so, inductively, the sum of the absolute values of the terms in one row of $A^{n}$ is bounded by $\alpha^{n}$. Thus, the entries in the infinite sum

$$
I+A+A^{2}+A^{3}+\cdots
$$

are all bounded by the geometric series $1+\alpha+\alpha^{2}+\cdots$, and so are absolutely convergent; thus, this sum exists and the product

$$
(I-A)\left(I+A+A^{2}+\cdots\right)=I
$$

is valid, so that the inverse of $I-A$ is this infinite sum.

Solution to 7.9.34: 1 . If $A$ is symmetric then, by the Spectral Theorem [HK61, p. 335], [Str93, p. 235], there is an orthonormal basis $\left\{e_{1}, e_{2}, \ldots, e_{n}\right\}$ for $\mathbb{R}^{n}$ with respect to which $A$ is diagonal: $A e_{j}=\lambda_{j} e_{j}, j=1, \ldots, n$. Let $x$ be any vector in $\mathbb{R}^{n}$. We can write $x=c_{1} e_{1}+\cdots+c_{n} e_{n}$ for some scalars $c_{1}, \ldots, c_{n}$, and have $A x=\lambda_{1} c_{1} e_{1}+\cdots+\lambda_{n} c_{n} e_{n}$. Moreover,

$$
\begin{aligned}
\|x\|^{2} &=c_{1}^{2}+\cdots+c_{n}^{2} \\
\|A x\|^{2} &=\lambda_{1}^{2} c_{1}^{2}+\cdots+\lambda_{n}^{2} c_{n}^{2} \\
& \leqslant \max \left\{\lambda_{1}^{2}, \ldots, \lambda_{n}^{2}\right\}\left(c_{1}^{2}+\cdots+c_{n}^{2}\right)=M^{2}\|x\|^{2},
\end{aligned}
$$

which is the desired inequality.

![](https://cdn.mathpix.com/cropped/2022_10_26_a807eeb357d0f35abe4eg-111.jpg?height=74&width=1385&top_left_y=1947&top_left_x=370)
yet it is not the zero matrix.

Solution to 7.9.35: 1. Suppose $\lambda$ s not an eigenvalue of $A$, so $\mu(A) \neq 0$. Write

$$
\mu(z)=\mu(\lambda)+(z-\lambda) g(z)
$$

with $g$ a polynomial of degree $k-1$. Then

$$
0=\mu(A)=\mu(\lambda) I+(A-\lambda I) g(A),
$$

so the polynomial $\mu_{\lambda}(z)=-g(z) / \mu(\lambda)$ has the required property. 2. It will suffice to show that the polynomials $\mu_{\lambda_{1}}, \ldots, \mu_{\lambda_{k}}$ are linearly independent, for then they will form a basis for the vector space of polynomials of degree $\leqslant k-1$, so some linear combination of them will equal the constant polynomial 1. Suppose $a_{1}, \ldots, a_{k}$ are complex numbers such that $\sum a_{j} \mu_{\lambda_{j}}=0$. Then $\sum a_{j}\left(A-\lambda_{j} I\right)^{-1}=0$. Multiplying the last equality by $\prod\left(A-\lambda_{i} I\right)$, we find that $q(A)=0$, where

$$
q(z)=\sum_{j=1}^{k} a_{j} \prod_{i \neq j}\left(z-\lambda_{i}\right),
$$

a polynomial of degree less than $k$. Therefore $q=0$. Hence, for each $j$,

$$
q\left(\lambda_{j}\right)=a_{j} \prod_{i \neq j}\left(\lambda_{j}-\lambda_{i}\right)=0,
$$

so $a_{j}=0$. This proves the desired linear independence.