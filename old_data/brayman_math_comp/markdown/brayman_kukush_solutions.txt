\title{
1995
}

1 The function $f(t)=(t-1) \ln t$ is continuous and strictly increasing on the interval $[1,+\infty)$, moreover $f(1)=0$ and $f(t) \rightarrow+\infty$, as $t \rightarrow+\infty$. By the intermediate value theorem, $f$ attains all positive integers, i.e., for each $n \in \mathbb{N}$ there exists $t(n)>1$ such that $f(t)=n$, and $t(n)$ is unique due to the monotonicity of $f$. Since $\frac{\ln n}{n} \rightarrow 0$, as $n \rightarrow \infty$, we have

$$
\begin{aligned}
\lim _{n \rightarrow \infty} t(n) \frac{\ln n}{n} &=\lim _{n \rightarrow \infty}(t(n)-1) \frac{\ln n}{n}=\lim _{n \rightarrow \infty} \frac{\ln n}{\ln t(n)}=\\
&=\lim _{t \rightarrow+\infty} \frac{\ln f(t)}{\ln t}=\lim _{t \rightarrow+\infty} \frac{\ln (t-1)+\ln \ln t}{\ln t}=1,
\end{aligned}
$$

because $n=f(t(n)), n \in \mathbb{N}$ and $t(n) \rightarrow+\infty$, as $n \rightarrow \infty$

2 We will show that if $\left\{a_{n}, n \geq 1\right\}$ is bounded then $B$ is either a point or a segment. Then the equality $A=B$ will imply the statement of the problem.

Assume that $B$ is not a point. Then $\alpha=\liminf _{n \rightarrow \infty} b_{n}<\beta=\limsup _{n \rightarrow \infty} b_{n}$. If additionally $B$ is not a segment then there exists a number $\gamma$, for which $\alpha<\gamma<\beta$ and $\gamma \notin B$. Then there exists $\varepsilon>0$ such that $(\gamma-\varepsilon, \gamma+\varepsilon) \subset[\alpha, \beta]$, and the interval $(\gamma-\varepsilon, \gamma+\varepsilon)$ contains only finite number of elements of the sequence $\left\{b_{n}, n \geq 1\right\}$. Let $\left\{a_{n}, n \geq 1\right\} \subset[-c, c]$. Choose $n_{0}$ such that for each $n \geq n_{0}$ the number $b_{n}$ does not belong to $(\gamma-\varepsilon, \gamma+\varepsilon)$ and the inequalities $\frac{n(\gamma-\varepsilon)+c}{n+1}<\gamma, \frac{n(\gamma+\varepsilon)-c}{n+1}>\gamma$ hold (such $n_{0}$ exists because $\frac{n(\gamma-\varepsilon)+c}{n+1} \rightarrow \gamma-\varepsilon<\gamma$ and $\frac{n(\gamma+\varepsilon)-c}{n+1} \rightarrow \gamma+\varepsilon>\gamma$, as $n \rightarrow \infty$ ). Assume that $n \geq n_{0}$ and $b_{n} \leq \gamma-\varepsilon$. Then

$$
b_{n+1}=\frac{n b_{n}+a_{n+1}}{n+1} \leq \frac{n(\gamma-\varepsilon)+c}{n+1}<\gamma .
$$

Therefore, $b_{n+1} \leq \gamma-\varepsilon$, because $b_{n+1} \notin(\gamma-\varepsilon, \gamma+\varepsilon)$. Similarly $b_{n} \geq \gamma+\varepsilon$ implies that $b_{n+1} \geq \gamma+\varepsilon$. Thus, either $b_{n} \leq \gamma-\varepsilon$ for all $n \geq n_{0}$, or $b_{n} \geq \gamma+\varepsilon$ for all $n \geq n_{0}$, hence $\alpha$ and $\beta$ cannot both be limit points of $\left\{b_{n}, n \geq 1\right\}$, a contradiction. So, $B=[\alpha, \beta]$.

Let $\left\{a_{1}, a_{2}, a_{3}, a_{4}, \ldots\right\}=\left\{0, q_{1}, 0, q_{2}, \ldots\right\}$, where $q_{1}, q_{2}, \ldots$ are all the rational numbers of the segment $[0,1]$ enumerated in some order. Then it is clear that $A=$ $[0,1]$, and because of $0 \leq b_{n} \leq \frac{1}{2}$ for all $n \geq 1$, we have $B \subset\left[0, \frac{1}{2}\right]$, in particular $B \neq A$

3 We have $2 x F(x)-F^{\prime}(x)=0$,

$$
e^{-x^{2}}\left(2 x F(x)-F^{\prime}(x)\right)=-\left(e^{-x^{2}} F(x)\right)^{\prime}=0 .
$$

Thus, $e^{-x^{2}} F(x)=$ const, $F(x)=c e^{x^{2}}, x \in \mathbb{R}$, where $c \in \mathbb{R}$ is some constant. Hence $f(x)=2 c x e^{x^{2}}=c_{1} x e^{x^{2}}$.

Answer: $f(x)=c x e^{x^{2}}$, where $c \in \mathbb{R}$ is an arbitrary constant.

4 Introduce a function $g(x)=(1-x) \int_{0}^{x} f(t) d t, \quad x \in[0,1]$. Since $g \in C^{(1)}$ $([0,1]), g(0)=g(1)=0$, by Rolle's Theorem

$$
\exists c \in(0,1): \quad g^{\prime}(c)=-\int_{0}^{c} f(t) d t+(1-c) f(c)=0 .
$$

5 The matrix $A_{0}=A$ is symmetric and positive definite, therefore, it has an eigenbasis and all its eigenvalues $\lambda_{i}, 1 \leq i \leq m$, are positive. We have $\sum_{i=1}^{m} \lambda_{i}=\operatorname{tr} A<1$, so $0<\lambda_{i}<1,1 \leq i \leq m$. It holds $A_{n+1}-\frac{1}{2} I=\left(A_{n}-\frac{1}{2} I\right)^{2}=\left(A-\frac{1}{2} I\right)^{2^{n+1}}$. Therefore, all the matrices have common eigenbasis, and moreover the eigenvalues of matrix $A_{n}$ are equal to $\frac{1}{2}+\left(\lambda_{i}-\frac{1}{2}\right)^{2^{n}} \rightarrow \frac{1}{2}$, as $n \rightarrow \infty$, because $\left|\lambda_{i}-\frac{1}{2}\right|<\frac{1}{2}$, $1 \leq i \leq m$. Thus, $A_{n} \rightarrow \frac{1}{2} I$, as $n \rightarrow \infty$.

6 Notice that $|\sin x-\sin a| \leq|x-a|, x \in \mathbb{R}$, and use the QM-AM inequality to get

$$
\begin{aligned}
&\left|\frac{1}{n} \sum_{k=1}^{n} \sin x_{k}-\sin a\right| \leq \frac{1}{n} \sum_{k=1}^{n}\left|\sin x_{k}-\sin a\right| \leq \frac{1}{n} \sum_{k=1}^{n}\left|x_{k}-a\right| \leq \\
&\leq\left(\frac{1}{n} \sum_{k=1}^{n}\left|x_{k}-a\right|^{2}\right)^{1 / 2}=\left(\frac{1}{n} \sum_{k=1}^{n} x_{k}^{2}-\frac{2}{n} \sum_{k=1}^{n} x_{k} \cdot a+a^{2}\right)^{1 / 2} \rightarrow 0, \text { as } n \rightarrow \infty .
\end{aligned}
$$

7 It is evident that a plane can be tiled with equal quadrangles, which are similar to $F$ and have disjoint interiors (see Fig. 1).

We tile a plane with such quadrangles of area $\frac{1}{n}$. The diameters of the quadrangles equal to $\frac{1}{\sqrt{n}} d(F)$, where $d(F)$ stands for the diameter of $F$. Therefore, the quadrangles which lie within the disc $G$ completely cover the concentric to $G$ disc of radius $\frac{1}{\sqrt{\pi}}-\frac{1}{\sqrt{n}} d(F)$. Thus, their total area is not less than

$$
\pi\left(\frac{1}{\sqrt{\pi}}-\frac{1}{\sqrt{n}} d(F)\right)^{2}=1-2 \sqrt{\frac{\pi}{n}} d(F)+\frac{\pi}{n} d^{2}(F) \rightarrow 1, \text { as } n \rightarrow \infty .
$$

![](https://cdn.mathpix.com/cropped/2022_10_28_1f3b279542cbcf365e2cg-003.jpg?height=414&width=1008&top_left_y=617&top_left_x=556)

Fig. 1 A plane tiled with equal quadrangles

On the other hand, one cannot place more than $n$ quadrangles of area $\frac{1}{n}$ with disjoint interiors in a disc of unit area. Thus, $\frac{a(n)}{n} \leq 1, \lim _{n \rightarrow \infty} \frac{a(n)}{n}=1$.

Consider $b(n)$ discs of area $\frac{1}{n}$ which lie inside $F$ and have disjoint interiors. Without loss of generality we may assume that each circle touches at least one another circle (we can shift some circle otherwise). For each circle $\omega$, consider equilateral triangle shown in Fig. $2\left(\omega^{\prime}\right.$ is one of the circles which touch $\left.\omega\right)$.

Fig. 2 The shaded figure is a union of a disc and an equilateral triangle

![](https://cdn.mathpix.com/cropped/2022_10_28_1f3b279542cbcf365e2cg-003.jpg?height=398&width=507&top_left_y=1498&top_left_x=1183)

It is evident that the figures of area $\frac{1}{n}\left(1+\left(\frac{1}{\sqrt{3} \pi}-\frac{1}{6}\right)\right)$, each of which is a union of a disc and the corresponding triangle, have disjoint interiors. Their total area $\frac{b(n)}{n}\left(1+\frac{1}{\sqrt{3} \pi}-\frac{1}{6}\right)$ does not exceed the area of $F$, that is 1 , whence

$$
\limsup _{n \rightarrow \infty} \frac{b(n)}{n} \leq\left(1+\frac{1}{\sqrt{3} \pi}-\frac{1}{6}\right)^{-1}<1 .
$$

8 The length of circle with diameter $d$ equals $\pi d$. Prove that it is the maximum. Assume that some convex piecewise-smooth (hence rectifiable) contour with diameter $d$ has perimeter larger than $\pi d$. Inscribe into the contour a convex polygon $\mathscr{A}=A_{1} A_{2} \ldots A_{n}$ with diameter at most $d$ and perimeter $P>\pi d$. Construct a centrally symmetric polygon $\mathscr{B}=B_{1} B_{2} \ldots B_{2 n}$ which also has diameter at most $d$ and perimeter $P$. To this purpose we draw the vectors $\pm \overrightarrow{A_{1} A_{2}}, \pm \overrightarrow{A_{2} A_{3}}, \ldots, \pm \overrightarrow{A_{n} A_{1}}$ with a common starting point and number them clockwise as $\overrightarrow{b_{1}}, \overrightarrow{b_{2}}, \ldots, \overrightarrow{b_{2 n}}$ (if some of these vectors are codirectional, we number them in any order, but in such a way that $\overrightarrow{b_{n+k}}=-\overrightarrow{b_{k}}, 1 \leq k \leq n$ ). Consider a polygon $\mathscr{B}$ such that $\overrightarrow{B_{k} B_{k+1}}=\frac{1}{2} \overrightarrow{b_{k}}$, $1 \leq k \leq 2 n, B_{2 n+1}=B_{1}$ (if some angles of $\mathscr{B}$ are equal to $180^{\circ}$, then we can unify some sides by removing unnecessary vertices).

The polygon $\mathscr{B}$ has perimeter $P$, is convex and centrally symmetric by construction (see Fig. 3). Therefore, its diameter is a distance between some opposite vertices $B_{k}$ and $B_{n+k}$. Hence, the diameter equals the sum of absolute values of projections of vectors $\overrightarrow{b_{k}}, \ldots, \overrightarrow{b_{n+k-1}}$ onto $\overrightarrow{B_{k} B_{n+k}}$, i.e., it equals a half of the sum of absolute values of projections of vectors $\overrightarrow{A_{1} A_{2}}, \ldots, \overrightarrow{A_{n} A_{1}}$ onto $\overrightarrow{B_{k} B_{n+k}}$. So it equals the projection of $\mathscr{A}$ on $\overrightarrow{B_{k} B_{n+k}}$ and does not exceed the diameter of $\mathscr{A}$. Thus, the diameter of $\mathscr{B}$ does not exceed $d$. Denote by $O$ the center of the polygon $\mathscr{B}$. Then $\mathscr{B}$ is contained in a disc with center $O$ and radius $\frac{d}{2}$. Consider a polygon $\mathscr{C}=C_{1} C_{2} \ldots C_{2 n}$ such that $B_{k}$ lies on $O C_{k}$ and $O C_{k}=\frac{d}{2}, 1 \leq k \leq 2 n$. The perimeter of $\mathscr{C}$ is less than $\pi d$, because each side of $\mathscr{C}$ is less than the corresponding arc. On the other hand, the perimeter of $\mathscr{C}$ is not less then the perimeter of $\mathscr{B}$ (to prove this one can consecutively cut off the parts of polygon $\mathscr{C}$, which lie outside $\mathscr{B}$, along the lines $B_{k} B_{k+1}$ and apply the polygon inequality), hence the perimeter of $\mathscr{C}$ is greater than $\pi d$. We get a contradiction, so indeed $\pi d$ is the maximum length of a contour with diameter $d$.

![](https://cdn.mathpix.com/cropped/2022_10_28_1f3b279542cbcf365e2cg-004.jpg?height=395&width=1087&top_left_y=1594&top_left_x=519)

Fig. 3 From a convex polygon $\mathscr{A}$ to a centrally symmetric convex polygon $\mathscr{B}$

Remark 1 Maximum is attained not only for a circle, but also for other constant width curves, i.e., convex contours, for which the length of projection on any straight line equals $d$. An example of constant width curve is Reuleaux triangle, which consists of three arcs of a circle with radius $d$ subtending an angle $60^{\circ}$. 9 By the formula of solution to linear differential equation we have

$$
y(x)=\left(c+\int_{0}^{x}(\arctan t) e^{-2 t-\sin t} d t\right) \cdot e^{2 x+\sin x} .
$$

Improper integral $\int_{0}^{\infty}(\arctan t) e^{-2 t-\sin t} d t$ converges, because

$$
(\arctan t) e^{-2 t-\sin t} \leq \frac{\pi}{2} e^{-2 t+1}
$$

It holds $e^{2 x+\sin x} \rightarrow+\infty$, as $x \rightarrow+\infty$. Therefore, for each

$$
c \neq-\int_{0}^{\infty}(\arctan t) e^{-2 t-\sin t} d t
$$

the solution is unbounded. It suffices to show that the solution

$$
y(x)=-e^{2 x+\sin x} \int_{x}^{\infty}(\arctan t) e^{-2 t-\sin t} d t
$$

is bounded on $\mathbb{R}$. We have

$$
|y(x)| \leq e^{2 x+1} \int_{x}^{\infty} \frac{\pi}{2} e^{-2 t+1} d t=e^{2 x+1} \cdot \frac{\pi e}{4} e^{-2 x}=\frac{\pi e^{2}}{4}, \quad x \in \mathbb{R} .
$$

10 It is easy to see that $y(x)=x$ is a solution to the Cauchy problem. Prove that the solution is unique. Suppose that $z(x)$ is another solution. Then

$$
\begin{aligned}
\left|y^{\prime}(x)-z^{\prime}(x)\right| &=\left|\int_{0}^{x}(\sin (y(u))-\sin (z(u))) d u\right| \leq \\
& \leq \int_{0}^{x}|\sin (y(u))-\sin (z(u))| d u \leq \int_{0}^{x}|y(u)-z(u)| d u .
\end{aligned}
$$

If $t=\inf \{x>0: y(x) \neq z(x)\}<\infty$, then $0<c=\sup _{x \leq t+1}|y(x)-z(x)|<\infty$. But for $x \leq t$ it holds $y(x)=z(x)$, and for $t \leq x \leq t+1$ we have

$$
\begin{aligned}
|y(x)-z(x)| &=\left|\int_{t}^{x}\left(y^{\prime}(s)-z^{\prime}(s)\right) d s\right| \leq \int_{t}^{x} \int_{0}^{s}|y(u)-z(u)| d u d s=\\
&=\int_{t}^{x} \int_{t}^{s}|y(u)-z(u)| d u d s \leq \frac{c(x-t)^{2}}{2} \leq \frac{c}{2} .
\end{aligned}
$$

Thus, $c \leq \frac{c}{2}$, a contradiction. So $y(x)=x$ is the unique solution.

11 Set $z_{1}=e^{\frac{2 \pi i}{k}}, z_{j}=z_{1}^{j}, j=0, \ldots, k-1$. We have 

$$
\sum_{j=0}^{k-1} z_{j}^{k-l} f\left(z \cdot z_{j}\right)=\sum_{n=0}^{\infty} c_{n} z^{n} \cdot \sum_{j=0}^{k-1} z_{j}^{n+k-l}=\sum_{m=0}^{\infty} c_{k m+l} z^{k m+l} \cdot k=0
$$

because

$$
\sum_{j=0}^{k-1} z_{j}^{n+k-l}=\left\{\begin{array}{c}
k, \text { if } n=k m+l \\
0 \text { otherwise }
\end{array}\right.
$$

Thus, $\sum_{j=0}^{k-1} z_{j}^{k-l} f\left(z \cdot z_{j}\right)=0$ for all $z$ such that $|z|<1$, hence for $|z|=1$. Since the radius of convergence of the series equals 1 , then there exists at least one singular point of $f$ on the unit circle (otherwise $f$ has an analytic extension on a neighborhood of each point of the unit circle, therefore, the radius of convergence exceeds 1).

Suppose that $z_{*}$ is a unique singular point of $f$ on the unit circle. Then $z_{*}$ is a singular point of the function $\sum_{j=0}^{k-1} z_{j}^{k-l} f\left(z \cdot z_{j}\right)=0$ as well, because it is a singular point of the function $z_{j}^{k-l} f\left(z \cdot z_{j}\right)$ if and only if $j=0$, a contradiction. Thus, there exists a number $j \neq 0$ such that $z_{*} z_{j}$ also is a singular point of $f$.

12 Introduce polar coordinates $(r, \varphi)$ and let

$$
\bar{u}(r, \varphi)=u(r \cos \varphi, r \sin \varphi) .
$$

The set $W$ corresponds to a set $\bar{W}$ of functions $\bar{u}(r, \varphi)$ such that $r^{2} \frac{\partial^{2} \bar{u}}{\partial r^{2}}+\frac{\partial^{2} \bar{u}}{\partial \varphi^{2}}=0$ in a ring $\bar{K}=\{(r, \varphi) \mid 1 \leq r \leq 2,0 \leq \varphi<2 \pi\}$ and $\int_{0}^{2 \pi} j \frac{\partial \bar{u}}{\partial r}(j, \varphi) d \varphi=(-1)^{j+1} 2 \pi$, $j=1,2$. In polar coordinates we have

$$
\bar{D}(\bar{u})=\int_{1}^{2} \int_{0}^{2 \pi}\left(r\left(\frac{\partial \bar{u}}{\partial r}(r, \varphi)\right)^{2}+\frac{1}{r}\left(\frac{\partial \bar{u}}{\partial \varphi}(r, \varphi)\right)^{2}\right) d \varphi d r
$$

For each function $\bar{u} \in \bar{W}$, it is easy to check that a function $\bar{v}(r, \psi)=\frac{1}{2 \pi} \int_{0}^{2 \pi} \bar{u}(r, \varphi)$ $d \varphi$ belongs to $\bar{W}$ as well. Since

$$
\frac{\partial \bar{v}}{\partial r}(r, \psi)=\frac{1}{2 \pi} \int_{0}^{2 \pi} \frac{\partial \bar{u}}{\partial r}(r, \varphi) d \varphi, \quad \frac{\partial \bar{v}}{\partial \psi}(r, \psi)=0,
$$

by the Cauchy-Schwarz inequality it holds

$$
\begin{aligned}
&\int_{0}^{2 \pi}\left(\frac{\partial \bar{\nu}}{\partial r}(r, \psi)\right)^{2} d \psi=\int_{0}^{2 \pi}\left(\frac{1}{2 \pi} \int_{0}^{2 \pi} \frac{\partial \bar{u}}{\partial r}(r, \varphi) d \varphi\right)^{2} d \psi= \\
&\quad=2 \pi\left(\frac{1}{2 \pi} \int_{0}^{2 \pi} \frac{\partial \bar{u}}{\partial r}(r, \varphi) d \varphi\right)^{2} \leq \int_{0}^{2 \pi}\left(\frac{\partial \bar{u}}{\partial r}(r, \varphi)\right)^{2} d \varphi \text { for each } r \in[1,2] .
\end{aligned}
$$

Therefore, $\bar{D}(\bar{v}) \leq \bar{D}(\bar{u})$, and in the case $\int_{1}^{2} \int_{0}^{2 \pi} \frac{1}{r}\left(\frac{\partial \bar{u}}{\partial \varphi}(r, \varphi)\right)^{2} d \varphi d r \neq 0$ the inequality is strict. Thus, if $\bar{D}\left(\bar{u}^{*}\right)=\min _{\bar{u} \in \bar{W}} \bar{D}(\bar{u})$, then the function $\bar{u}^{*}$ satisfies the condition $\frac{\partial \bar{u}^{*}}{\partial \varphi}(r, \varphi)=0,(r, \varphi) \in \bar{K}$, that is $\bar{u}^{*}$ does not depend on $\varphi$. In particular the corresponding function $u^{*}$ is constant on both $S_{1}$ and $S_{2}$.

13 We assume that the hare continues jumping after it has been trapped. The sizes of the jumps are jointly independent, and the size of a jump is positive with probability $\frac{2}{3}$. Therefore, by the Borel-Cantelli lemma the hare will almost surely make infinitely many positive jumps. Thus, with probability 1 it will visit points with arbitrary large coordinates. Maximum length of jump equals 2 . Hence, the hare will almost surely visit all the sets

$$
\{1,2\},\{3,4\}, \ldots,\{2 n-1,2 n\}, \ldots
$$

Both numbers in couple are traps with probability $0.16$, so with probability 1 there exists a couple $\{2 n-1,2 n\}$ which contains two traps. Therefore, the hare will be trapped almost surely.

14 Since

$$
\forall x \in H \exists c_{x}>0 \forall n \geq 1\left\|A_{n} x\right\| \leq c_{x},
$$

by Banach-Steinhaus Theorem there exists $C>0$ such that $\left\|A_{n}\right\| \leq C$ for all $n \geq 1$. We have

$$
\left\|A_{n} K\right\|=\sup _{\|x\|=1}\left\|A_{n} K x\right\|=\sup \left\{\left\|A_{n} y\right\| \mid y=K x,\|x\|=1\right\} .
$$

A set $\{y=K x,\|x\|=1\}$ is compact, thus, for every $\varepsilon>0$ there exists a finite $\varepsilon$-net $\left\{y_{1}, \ldots, y_{m}\right\}$. Hence

$$
\left\|A_{n} y\right\| \leq\left\|A_{n} y_{k}\right\|+\left\|A_{n}\left(y-y_{k}\right)\right\| \leq\left\|A_{n} y_{k}\right\|+C\left\|y-y_{k}\right\| \leq\left\|A_{n} y_{k}\right\|+C \varepsilon,
$$

where $y_{k}$ is an element of the net, for which $\left\|y-y_{k}\right\|<\varepsilon$. Therefore,

$$
\left\|A_{n} K\right\| \leq \max _{1 \leq k \leq m}\left\|A_{n} y_{k}\right\|+C \varepsilon \rightarrow C \varepsilon, \text { as } n \rightarrow \infty
$$

Since $\varepsilon>0$ is arbitrary, it holds $\left\|A_{n} K\right\| \rightarrow 0$, as $n \rightarrow \infty$. 1 Let $a=a_{0} e^{i \alpha}, b=b_{0} e^{i \beta}$, and $c=c_{0} e^{i \gamma}$, where $a_{0}, b_{0}, c_{0} \geq 0$ and $\alpha, \beta, \gamma \in$ $[0,2 \pi)$. Without loss of generality we may assume that $a_{0} \geq b_{0} \geq c_{0}$. Then

$$
\limsup _{n \rightarrow \infty}\left|a^{n}+b^{n}+c^{n}\right|^{\frac{1}{n}} \leq \limsup _{n \rightarrow \infty}\left(|a|^{n}+|b|^{n}+|c|^{n}\right)^{\frac{1}{n}} \leq \limsup _{n \rightarrow \infty}\left(3 a_{0}^{n}\right)^{\frac{1}{n}}=a_{0} .
$$

Construct an increasing sequence of numbers $\left\{n_{k}, k \geq 1\right\}$ such that

$$
\left|a^{n_{k}}+b^{n_{k}}+c^{n_{k}}\right|^{\frac{1}{n_{k}}} \geq a_{0}, k \geq 1 .
$$

By the pigeonhole principle, there exist infinitely many numbers $m_{k}$ such that all the numbers $e^{i m_{k}(\beta-\alpha)}$ lie in the same coordinate quadrant and all the numbers $e^{i m_{k}(\gamma-\alpha)}$ lie in the same (possibly other) coordinate quadrant as well. Therefore, for $n_{k}=$ $m_{k+1}-m_{1}, k \geq 1$, it holds $\operatorname{Re} e^{i n_{k}(\beta-\alpha)} \geq 0$ and $\operatorname{Re} e^{i n_{k}(\gamma-\alpha)} \geq 0$. Hence

$$
\begin{aligned}
\left|a^{n_{k}}+b^{n_{k}}+c^{n_{k}}\right| &=\left|a_{0}^{n_{k}}+b_{0}^{n_{k}} e^{i n_{k}(\beta-\alpha)}+c_{0}^{n_{k}} e^{i n_{k}(\gamma-\alpha)}\right| \geq \\
& \geq a_{0}^{n_{k}}+b_{0}^{n_{k}} \operatorname{Re} e^{i n_{k}(\beta-\alpha)}+c_{0}^{n_{k}} \operatorname{Re} e^{i n_{k}(\gamma-\alpha)} \geq a_{0}^{n_{k}} .
\end{aligned}
$$

Thus, $\limsup _{n \rightarrow \infty}\left|a^{n}+b^{n}+c^{n}\right|^{\frac{1}{n}} \geq a_{0}$, so

$$
\limsup _{n \rightarrow \infty}\left|a^{n}+b^{n}+c^{n}\right|^{\frac{1}{n}}=a_{0}=\max \{|a|,|b|,|c|\} .
$$

Answer: $\limsup _{n \rightarrow \infty}\left|a^{n}+b^{n}+c^{n}\right|^{\frac{1}{n}}=\max \{|a|,|b|,|c|\}$. 2 For $x, y \geq 1$, it holds

$$
\begin{aligned}
\varphi(x y)-\varphi(x) &=\lim _{A \rightarrow \infty}\left(\int_{A}^{A x y} f(u) d u-\int_{A}^{A x} f(u) d u\right)=\\
&=\lim _{A \rightarrow \infty} \int_{A x}^{A x y} f(u) d u=\lim _{A x \rightarrow \infty} \int_{A x}^{A x y} f(u) d u=\varphi(y),
\end{aligned}
$$

that is $\varphi(x y)=\varphi(x)+\varphi(y)$. Notice that for each $c \in \mathbb{R}$ the function $\varphi(x)=c \ln x$ satisfies the functional equation

$$
\varphi(x y)=\varphi(x)+\varphi(y), x, y \geq 1 .
$$

Show that this equation has no other solution which is continuous at point 1 . Let $\varphi(x)$ be any such solution, $c=\varphi(e)$, and $\psi(x)=\varphi(x)-c \ln x, x \geq 1$. Then $\psi$ is other solution, and $\psi(1)=\psi(e)=0$. It is easy to prove by induction on $n$ that $\psi\left(x^{n}\right)=n \psi(x)$, thus,

$$
\forall n, m \geq 1 \quad \psi\left(x^{\frac{n}{m}}\right)=n \psi\left(x^{\frac{1}{m}}\right)=\frac{n}{m} \psi(x),
$$

hence $\psi\left(e^{r}\right)=0, r \in \mathbb{Q}, r \geq 0$. Assume that $\psi(a) \neq 0$ for some $a>1$. Since the set of numbers $e^{r}, r \in \mathbb{Q}$, is dense in $[1,+\infty)$, for each $n \geq 1$ there exists $r_{n} \in \mathbb{Q}$ such that $1<\frac{a}{e^{r_{n}}}<1+\frac{1}{n}$, so $\frac{a}{e^{r_{n}}} \rightarrow 1$, as $n \rightarrow \infty$. But

$$
\psi(a)=\psi\left(e^{r_{n}}\right)+\psi\left(\frac{a}{e^{r_{n}}}\right)=\psi\left(\frac{a}{e^{r_{n}}}\right) \nrightarrow 0=\psi(1), \text { as } n \rightarrow \infty,
$$

a contradiction with continuity of $\psi$ at point 1 . Therefore, $\psi(x) \equiv 0, x \geq 1$, i.e. $\varphi(x)=c \ln x, x \geq 1$. For $f(x)=\frac{c}{x}, x \geq 1$ and $A \geq 1$ we have an identity

$$
\int_{A}^{A x} f(u) d u=c \ln x .
$$

Thus, the function $\varphi(x)=c \ln x$ satisfies the conditions of the problem for each $c \in \mathbb{R}$.

Answer: $\varphi(x)=c \ln x, c \in \mathbb{R}$.

3 Introduce a function $F(x)=\int_{0}^{x} f^{2}(u) d u$. It holds $F^{\prime}(x)=f^{2}(x)$, and condition of the problem takes a form $f(x) F(x) \rightarrow 1$, as $x \rightarrow+\infty$, or $F^{\prime}(x) F^{2}(x) \rightarrow 1$, as $x \rightarrow+\infty$. For every $\varepsilon>0$ there exists $N>0$ such that $\left|F^{\prime}(x) F^{2}(x)-1\right|<\varepsilon$, $x \geq N$. Then for all $x \geq N$ it holds

$$
\begin{aligned}
F^{3}(N) &+3(x-N)(1-\varepsilon) \leq F^{3}(x)=\\
&=F^{3}(N)+\int_{N}^{x} 3 F^{\prime}(x) F^{2}(x) d x \leq F^{3}(N)+3(x-N)(1+\varepsilon),
\end{aligned}
$$

hence $\frac{F^{3}(x)}{3 x} \rightarrow 1$, as $x \rightarrow+\infty$, or $\frac{F(x)}{(3 x)^{\frac{1}{3}}} \rightarrow 1$, as $x \rightarrow+\infty$. Finally,

$$
f(x) \sim \frac{1}{F(x)} \sim \frac{1}{(3 x)^{\frac{1}{3}}}, \text { as } x \rightarrow+\infty
$$

4 Consider an arbitrary partition $\lambda=\left\{0=x_{0}<x_{1}<\ldots<x_{n-1}<x_{n}=1\right\}$. Since

$$
\sum_{k=0}^{n-1} \int_{x_{k}}^{x_{k+1}} \sin 2 \pi t d t=\int_{0}^{1} \sin 2 \pi t d t=0,
$$

it holds

$$
\begin{aligned}
&\sum_{k=0}^{n-1}\left(x_{k+1}-x_{k}\right) \sin 2 \pi x_{k}=\sum_{k=0}^{n-1}\left(\left(x_{k+1}-x_{k}\right) \sin 2 \pi x_{k}-\int_{x_{k}}^{x_{k+1}} \sin 2 \pi t d t\right)= \\
&=\sum_{k=0}^{n-1} \int_{x_{k}}^{x_{k+1}}\left(\sin 2 \pi x_{k}-\sin 2 \pi t\right) d t=-2 \pi \sum_{k=0}^{n-1} \int_{x_{k}}^{x_{k+1}}\left(x_{k+1}-t\right) \cos 2 \pi t d t= \\
&=-2 \pi \sum_{k=0}^{n-1} \int_{x_{k}}^{x_{k+1}}\left(x_{k+1}-t\right) d t \cdot \cos 2 \pi \theta_{k}=-\pi \sum_{k=0}^{n-1}\left(x_{k+1}-x_{k}\right)^{2} \cdot \cos 2 \pi \theta_{k}
\end{aligned}
$$

for some numbers $\theta_{k} \in\left[x_{k}, x_{k+1}\right]$ (we applied integration by parts and the intermediate value theorem).

Hence for every partition $\lambda$ it holds

$$
\sum_{k=0}^{n-1}\left(x_{k+1}-x_{k}\right) \sin 2 \pi x_{k} \leq \pi\left(\sum_{k=0}^{n-1}\left(x_{k+1}-x_{k}\right)^{2}\right) .
$$

On the other hand, consider the partition

$$
\lambda_{N}=\left\{0<\frac{1}{2 N^{3}}<\ldots<\frac{N^{3}}{2 N^{3}}<\frac{N^{3}+N^{2}}{2 N^{3}}<\ldots<\frac{2 N^{3}-1}{2 N^{3}}<1\right\},
$$

which has $2 N^{3}-N^{2}$ sub-intervals of length $\frac{1}{2 N^{3}}$ and sub-interval $\left[\frac{1}{2}, \frac{1}{2}+\frac{1}{2 N}\right]$. For this partition, we have

$$
\frac{\sum_{k=0}^{n-1}\left(x_{k+1}-x_{k}\right) \sin 2 \pi x_{k}}{\sum_{k=0}^{n-1}\left(x_{k+1}-x_{k}\right)^{2}}=\frac{-\pi \sum_{k=0}^{n-1}\left(x_{k+1}-x_{k}\right)^{2} \cdot \cos 2 \pi \theta_{k}}{\sum_{k=0}^{n-1}\left(x_{k+1}-x_{k}\right)^{2}}>
$$



$$
>\frac{\pi\left(\cos \frac{\pi}{N} \cdot \frac{1}{4 N^{2}}-\frac{2 N^{3}-N^{2}}{4 N^{6}}\right)}{\frac{1}{4 N^{2}}+\frac{2 N^{3}-N^{2}}{4 N^{6}}} \rightarrow \pi \text {, as } N \rightarrow \infty .
$$

Thus, the supremum in question equals $\pi$.

Answer: $\pi$.

5 By Riemann-Schwarz symmetry principle, the function

$$
F(z)= \begin{cases}f(z), & \text { if } \operatorname{Im} z \geq 0, \\ \overline{f(\bar{z})}, & \text { if } \operatorname{Im} z<0\end{cases}
$$

is analytic on $\mathbb{C} \backslash\{\pm i\}$. Let $\lim _{z \rightarrow i} f(z)(z-i)=c$. Then the function

$$
G(z)=F(z)-\frac{c}{z-i}-\frac{\bar{c}}{z+i}
$$

is analytic on $\mathbb{C}$, and $\lim _{z \rightarrow \infty} G(z)=A$. Hence, by Liouville's theorem $G(z) \equiv A$, $z \in \mathbb{C}$, that is $f(z)=A+\frac{c}{z-i}+\frac{\bar{c}}{z+i}, \operatorname{Im} z \geq 0$.

6 Since $\operatorname{Im}\left(\frac{f(z)}{F(z)}+1\right)=\operatorname{Im} \frac{f(z)+F(z)}{F(z)} \neq 0$ for $z \in \partial D$, we have $f(z)+F(z) \neq 0$, $z \in \partial D$. Therefore, the number of roots of $F(z)$ in $D$ equals $\frac{1}{2 \pi i} \int_{\partial D} \frac{F^{\prime}(z)}{F(z)} d z$, and the number of roots of $f(z)+F(z)$ in $D$ equals $\frac{1}{2 \pi i} \int_{\partial D} \frac{f^{\prime}(z)+F^{\prime}(z)}{f(z)+F(z)} d z$, respectively. We have

$$
\begin{gathered}
\frac{1}{2 \pi i} \int_{\partial D} \frac{f^{\prime}(z)+F^{\prime}(z)}{f(z)+F(z)} d z=\frac{1}{2 \pi i} \int_{\partial D} \frac{d}{d z} \ln (f(z)+F(z)) d z= \\
=\frac{1}{2 \pi i} \int_{\partial D} \frac{d}{d z} \ln F(z) d z+\frac{1}{2 \pi i} \int_{\partial D} \frac{d}{d z} \ln \left(1+\frac{f(z)}{F(z)}\right) d z, \\
\frac{1}{2 \pi i} \int_{\partial D} \frac{F^{\prime}(z)}{F(z)} d z=\frac{1}{2 \pi i} \int_{\partial D} \frac{d}{d z} \ln F(z) d z .
\end{gathered}
$$

It remains to prove that

$$
\frac{1}{2 \pi i} \int_{\partial D} \frac{d}{d z} \ln \left(1+\frac{f(z)}{F(z)}\right) d z=0
$$

Let $\omega(z)=1+\frac{f(z)}{F(z)}$ traverse the contour $\gamma$, as $z$ traverses $\partial D$. Then it holds either $\gamma \subset\{\operatorname{Im} \omega>0\}$ or $\gamma \subset\{\operatorname{Im} \omega<0\}$, because $\operatorname{Im}\left(1+\frac{f(z)}{F(z)}\right) \neq 0$. Hence 

$$
\int_{\partial D} \frac{d}{d z} \ln \left(1+\frac{f(z)}{F(z)}\right) d z=\int_{\gamma} \frac{d \omega}{\omega}=0 .
$$

7 The eigenvalues of the matrix $A$ are the roots of the polynomial

$$
P_{A}(\lambda)=\lambda^{1996}+\lambda^{998}+1996 .
$$

Since

$$
P_{A}^{\prime}(\lambda)=1996 \lambda^{1995}+998 \lambda^{997}=998 \lambda^{997}\left(2 \lambda^{998}+1\right),
$$

the polynomials $P_{A}^{\prime}$ and $P_{A}$ do not have common roots. Therefore, all the roots of $P_{A}(\lambda)$ are distinct, thus, $A$ has an eigenbasis.

8 First columns of the matrices $A_{1}, \ldots, A_{n+1}$ are linearly dependent. Therefore, for some numbers $a_{1}, \ldots, a_{n+1}$, not all of which are zeros, the first column of the matrix $a_{1} A_{1}+\ldots+a_{n+1} A_{n+1}$ is zero, so the latter matrix is singular.

9 Call the matrix $A=\left(a_{i j}\right)_{i, j=1}^{n}$ good if it holds $a_{k k}=a_{k l}=-a_{l k}=-a_{l l} \neq 0$ for some $1 \leq k \neq l \leq n$, and all other entries are zeros. It is easy to check that the square of each good matrix is the zero matrix. Clearly, every matrix with zero trace can be written as a sum of several good matrices and a matrix with zero diagonal. It remains to represent the latter matrix as a sum of matrices with exactly one nonzero entry, which is off-diagonal. 1 Each decomposition $n=a_{1}+a_{2}+\ldots+a_{m}$ corresponds to collection of partial sums:

$$
b_{0}=0, b_{1}=a_{1}, b_{2}=a_{1}+a_{2}, \ldots, b_{m-1}=a_{1}+\ldots+a_{m-1}, b_{m}=n .
$$

Each summand $k$ corresponds to adjacent numbers of the collection which differ by $k$. Fix the numbers $c$ and $c+k$, and find the number of collections in which the numbers $c$ and $c+k$ are adjacent. It is evident that $c$ can take values from 0 to $n-k$. If $c=0$ then the collection contains the numbers $0, k, n$ and some subset of the set $\{k+1 \leq i \leq n-1\}$, which can be selected in $2^{n-k-1}$ ways. If $c=n-k$ then the collection contains the numbers $0, n-k, n$ and some subset of the set $\{1 \leq i \leq n-k-1\}$, which can be selected in $2^{n-k-1}$ ways. Finally for $0<c<n-k$ the collection contains $0, c, c+k, n$ and some subsets of both sets

$$
\{1 \leq i \leq c-1\} \text { and }\{c+k+1 \leq i \leq n-1\} .
$$

For each of $(n-k-1)$ values of $c$, we have $2^{n-k-2}$ ways to select a couple of subsets. Thus, a total number of summands $k$ in the decompositions is

$$
2 \cdot 2^{n-k-1}+(n-k-1) \cdot 2^{n-k-2}=(n-k+3) \cdot 2^{n-k-2} .
$$

2 Let $K \subset \mathbb{Q}(x)$ be a field of all even rational functions over $\mathbb{Q}$, and $F \subset \mathbb{Q}(x)$ be a field of all rational functions $f(x)$ over $\mathbb{Q}$ such that $f(x+1)$ is an even function, i.e. $f(x+1)=f(-x+1)$. It is evident that $K$ and $F$ are fields different from $\mathbb{Q}(x)$, because $x \notin K$ and $x \notin F$. Show that $[\mathbb{Q}(x): K]=2$. Indeed, each function $f \in \mathbb{Q}(x)$ can be expressed as

$$
\frac{1}{2}(f(x)+f(-x))+x \cdot \frac{1}{2 x} \cdot(f(x)-f(-x))=f_{1}(x)+x f_{2}(x),
$$

where $f_{1}(x)=\frac{1}{2}(f(x)+f(-x)) \in K$ and $f_{2}(x)=\frac{1}{2 x}(f(x)-f(-x)) \in K$. Similarly $[\mathbb{Q}(x): F]=2$. Consider $K \cap F$. If $f \in K \cap F$ then for each $x$ it holds

$$
f(x+2)=f(1+(x+1))=f(1-(x+1))=f(-x)=f(x),
$$

i.e., the function $f$ has period 2 . But the only periodic rational functions are constants, thus, $K \cap F=\mathbb{Q}$. It remains to notice that $[\mathbb{Q}(x): \mathbb{Q}]=\infty$, because $\mathbb{Q}(x)$ contains the functions $1, x, x^{2}, \ldots, x^{n}, \ldots$, which are linearly independent over $\mathbb{Q}$.

3 Consider $J_{A}=S^{-1} A S$, the Jordan normal form of the matrix $A$. Notice that matrices $A$ and $B$ commute if and only if the matrices $S^{-1} A S$ and $S^{-1} B S$ commute, for each polynomial $P(x)$ it holds $P\left(J_{A}\right)=S^{-1} P(A) S$, and the rank of a matrix does not depend on the choice of a basis. Therefore, it suffices to prove the statement for the matrix $J_{A}$.

Let the rank of $J_{A}-a I$ be less than $n-1$. Then $J_{A}$ consists of several Jordan cells, and each polynomial of $J_{A}$ is a direct sum of polynomials of the Jordan cells. The matrix $B=\left(b_{i j}\right)$, where $b_{1 n}=1$ and all other entries are zeros, commutes with $J_{A}$ (in fact, $J_{A} B=B J_{A}=a \cdot B$ ), but it is not a polynomial of $J_{A}$.

Let the rank of $J_{A}-a I$ be equal to $n-1$. Then $J_{A}$ is a single Jordan cell of size $n$ and $J_{0}=J_{A}-a I$ is the nilpotent Jordan cell of size $n$. If $J_{A} B=B J_{A}$ then $\left(J_{A}-a I\right) B=B\left(J_{A}-a I\right)$, i.e., matrices which commute with $J_{A}$ also commute with $J_{0}$. For $B=\left(b_{i j}\right)$ the condition $J_{0} B=B J_{0}$ implies that $b_{i, j-1}=b_{i+1, j}$, $1 \leq i, j \leq n$, where $b_{i 0}=b_{n+1, j}=0$. Hence $b_{i j}=0$ for $i>j$, and $b_{i j}=$ $\tilde{b}_{j-i}, j \geq i$. For each polynomial $P(x)$ we have $P\left(J_{A}\right)=\left(p_{i j}\right)$, where

$$
p_{i j}=\left\{\begin{array}{cc}
0, & \text { if } i>j, \\
\frac{P^{(j-i)}(a)}{(j-i) !}, & \text { if } j \geq i .
\end{array}\right.
$$

Thus, if matrix $B$ commutes with $J_{A}$ then $B=P\left(J_{A}\right)$, where $P(x)=\sum_{k=0}^{n} \tilde{b}_{k}(x-a)^{k}$. Hence $J_{A}$ commutes only with polynomials of $J_{A}$.

4 Numbers $x=0, x=1$, and $x=3$ satisfy the equation. Show that there are no other solutions. Indeed, suppose that for the function

$$
f(x)=2^{x}-\frac{2}{3} x^{2}-\frac{1}{3} x-1,
$$

there exist points $x_{1}<x_{2}<x_{3}<x_{4}$ such that $f\left(x_{i}\right)=0, i=1,2,3,4$. Then by Rolle's theorem there exist points $x_{1}<y_{1}<x_{2}<y_{2}<x_{3}<y_{3}<x_{4}$ such that $f^{\prime}\left(y_{i}\right)=0, i=1,2,3$, points $y_{1}<z_{1}<y_{2}<z_{2}<y_{3}$ such that $f^{\prime \prime}\left(z_{1}\right)=f^{\prime \prime}\left(z_{2}\right)=0$, and a point $z_{1}<t<z_{2}$, such that $f^{\prime \prime \prime}(t)=0$. But $f^{\prime \prime \prime}(x)=(\ln 2)^{3} \cdot 2^{x}>0$ for all $x \in \mathbb{R}$, a contradiction.

Answer: $0,1,3$. 5 Fix an arbitrary $\varepsilon>0$. Since the function $y=e^{t}$ is convex, for $t \in[0, \varepsilon]$ its graph lies above the tangent $y=1+t$ but under the secant $y=1+a_{\varepsilon} t$, where $a_{\varepsilon}=\frac{1}{\varepsilon}\left(e^{\varepsilon}-1\right)$. Hence for $\frac{1}{n} \leq \varepsilon$ it holds

$$
\left(\int_{0}^{1} e^{x^{2} / n} d x\right)^{n} \geq\left(\int_{0}^{1}\left(1+\frac{x^{2}}{n}\right) d x\right)^{n}=\left(1+\frac{1}{3 n}\right)^{n} \rightarrow e^{1 / 3}, \text { as } n \rightarrow \infty,
$$

and

$$
\left(\int_{0}^{1} e^{x^{2} / n} d x\right)^{n} \leq\left(\int_{0}^{1}\left(1+\frac{a_{\varepsilon} x^{2}}{n}\right) d x\right)^{n}=\left(1+\frac{a_{\varepsilon}}{3 n}\right)^{n} \rightarrow e^{a_{\varepsilon} / 3}, \text { as } n \rightarrow \infty
$$

Since $a_{\varepsilon}=\frac{1}{\varepsilon}\left(e^{\varepsilon}-1\right) \rightarrow 1$, as $\varepsilon \rightarrow 0+$, we get $\lim _{n \rightarrow \infty}\left(\int_{0}^{1} e^{x^{2} / n} d x\right)^{n}=e^{1 / 3}$.

Answer: $e^{1 / 3}$.

6 The matrix $I+a a^{\mathrm{T}}$ is positive definite, therefore, there exists the inverse matrix $\left(I+a a^{\mathrm{T}}\right)^{-1}$. Notice that the expressions $1-a^{\mathrm{T}}\left(I+a a^{\mathrm{T}}\right)^{-1} a$ and $1+a^{\mathrm{T}} a$ take real values. We have

$$
\begin{gathered}
\left(1+a^{\mathrm{T}} a\right)\left(1-a^{\mathrm{T}}\left(I+a a^{\mathrm{T}}\right)^{-1} a\right)= \\
=1+a^{\mathrm{T}} a-a^{\mathrm{T}}\left(I+a a^{\mathrm{T}}\right)^{-1} a-a^{\mathrm{T}}\left(a a^{\mathrm{T}}+I-I\right)\left(I+a a^{\mathrm{T}}\right)^{-1} a= \\
=1+a^{\mathrm{T}} a-a^{\mathrm{T}}\left(I+a a^{\mathrm{T}}\right)^{-1} a-a^{\mathrm{T}}\left(I+a a^{\mathrm{T}}\right)\left(I+a a^{\mathrm{T}}\right)^{-1} a+a^{\mathrm{T}}\left(I+a a^{\mathrm{T}}\right)^{-1} a=1,
\end{gathered}
$$

thus, $\left(1-a^{\mathrm{T}}\left(I+a a^{\mathrm{T}}\right)^{-1} a\right)^{-1}=1+a^{\mathrm{T}} a$.

7 Taking into account the periodicity and signs of the functions $\sin x$ and $\cos x$, it suffices to consider $f(x)$ for $x \in\left[0, \frac{\pi}{2}\right]$ only. A necessary condition for a maximum is

$$
f^{\prime}(x)=e^{\sin x} \cos x-e^{\cos x} \sin x=0,
$$

hence $x \in\left(0, \frac{\pi}{2}\right)$ and $\frac{e^{\sin x}}{\sin x}=\frac{e^{\cos x} x}{\cos x}$. The function $g(t)=\frac{e^{t}}{t}$ decreases on $(0,1)$, because $g^{\prime}(t)=\frac{e^{t}(t-1)}{t^{2}}<1$. Therefore, the latter equality is equivalent to $\sin x=$ $\cos x$, so $x=\frac{\pi}{4}$. Thus, $x=\frac{\pi}{4}$ is a unique extreme point in $\left[0, \frac{\pi}{2}\right]$ and

$$
\max _{x \in \mathbb{R}} f(x)=\max _{x \in\left[0, \frac{\pi}{2}\right]} f(x)=f\left(\frac{\pi}{4}\right)=2 e^{\frac{1}{\sqrt{2}}} .
$$

Answer: $2 e^{\frac{1}{\sqrt{2}}}$.

8 Denote $I_{k}=\left[k \pi-\frac{\pi}{2}, k \pi+\frac{\pi}{2}\right], k \geq 1$. Estimate the integral $\int_{I_{k}} \frac{f(x)}{|\sin x|^{1-\frac{1}{x}}} d x$. The function $|\sin x|$ is concave on both segments $\left[k \pi-\frac{\pi}{2}, k \pi\right]$ and $\left[k \pi, k \pi+\frac{\pi}{2}\right]$, hence $|\sin x| \geq \frac{2}{\pi}|x-k \pi|, x \in I_{k}$. For $x \in I_{k}$, it also holds $1-\frac{1}{x}<1-\frac{1}{(k+1) \pi}$ and $f(x) \leq f\left(k \pi-\frac{\pi}{2}\right)$. Thus,

$$
|\sin x|^{1-\frac{1}{x}} \geq|\sin x|^{1-\frac{1}{(k+1) \pi}} \geq\left(\frac{2}{\pi}|x-k \pi|\right)^{1-\frac{1}{(k+1) \pi}} \geq \frac{2}{\pi}|x-k \pi|^{1-\frac{1}{(k+1) \pi}}, x \in I_{k} .
$$

Therefore,

$$
\begin{aligned}
&\int_{I_{k}} \frac{f(x)}{|\sin x|^{1-\frac{1}{x}}} d x \leq \frac{\pi}{2} f\left(k \pi-\frac{\pi}{2}\right) \cdot \int_{I_{k}} \frac{d x}{|x-k \pi|^{1-\frac{1}{(k+1) \pi}}}= \\
&=\frac{\pi}{2} f\left(k \pi-\frac{\pi}{2}\right) \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}|x|^{\frac{1}{(k+1) \pi}-1} d x= \\
&=\pi f\left(k \pi-\frac{\pi}{2}\right) \cdot(k+1) \pi \cdot\left(\frac{\pi}{2}\right)^{\frac{1}{(k+1) \pi}} \leq \frac{\pi^{3}}{2} f\left(k \pi-\frac{\pi}{2}\right)(k+1) .
\end{aligned}
$$

Thus,

$$
\int_{1}^{\infty} \frac{f(x) d x}{|\sin x|^{1-\frac{1}{x}}} \leq \int_{1}^{\frac{\pi}{2}} \frac{f(x) d x}{|\sin x|^{1-\frac{1}{x}}}+\frac{\pi^{3}}{2} \sum_{k=1}^{\infty} f\left(k \pi-\frac{\pi}{2}\right)(k+1) .
$$

It remains to prove convergence of the series $\sum_{k=1}^{\infty} f\left(k \pi-\frac{\pi}{2}\right)(k+1)$. For every $k \geq 4$ it holds

$$
f\left(k \pi-\frac{\pi}{2}\right)(k+1) \pi \leq 2 f\left(k \pi-\frac{\pi}{2}\right)\left(k \pi-\frac{3 \pi}{2}\right) \leq \int_{I_{k-1}} x f(x) d x .
$$

Therefore, convergence of the integral $\int_{1}^{\infty} x f(x) d x$ implies convergence of the series.

9 Rewrite the expression as an integral sum:

$$
\sum_{j=1}^{n^{2}} \frac{n}{n^{2}+j^{2}}=\sum_{j=1}^{n^{2}} \frac{1}{n} \cdot \frac{1}{1+\left(\frac{j}{n}\right)^{2}} .
$$

Since for $x \in\left[\frac{j}{n}, \frac{j+1}{n}\right]$ the inequality

$$
\frac{1}{1+\left(\frac{j+1}{n}\right)^{2}} \leq \frac{1}{1+x^{2}} \leq \frac{1}{1+\left(\frac{j}{n}\right)^{2}}
$$

holds, we have $\int_{\frac{1}{n}}^{n+\frac{1}{n}} \frac{d x}{1+x^{2}} \leq \sum_{j=1}^{n^{2}} \frac{n}{n^{2}+j^{2}} \leq \int_{0}^{n} \frac{d x}{1+x^{2}}$. Hence

$$
\lim _{n \rightarrow \infty} \sum_{j=1}^{n^{2}} \frac{n}{n^{2}+j^{2}}=\int_{0}^{\infty} \frac{d x}{1+x^{2}}=\frac{\pi}{2}
$$

Answer: $\frac{\pi}{2}$.

10 Assume that $\left|f^{\prime}\left(x_{0}\right)\right|>2$ for some point $x_{0} \in I=(a, b)$. Without loss of generality $f^{\prime}\left(x_{0}\right) \geq 2+\varepsilon$, where $0<\varepsilon<1$. Then for every $x \in(a, b)$ it holds

$$
f^{\prime}(x)=f^{\prime}\left(x_{0}\right)+\int_{x_{0}}^{x} f^{\prime \prime}(t) d t \geq 2+\varepsilon-\left|x-x_{0}\right| .
$$

Select points $a<a_{1}<x_{0}$ and $x_{0}<b_{1}<b$ such that $2-\varepsilon<b_{1}-a_{1} \leq 2$. Then

$$
\begin{aligned}
f\left(b_{1}\right)-f\left(a_{1}\right) &=\int_{a_{1}}^{b_{1}} f^{\prime}(t) d t \geq \int_{a_{1}}^{b_{1}}\left(2+\varepsilon-\left|t-x_{0}\right|\right) d t=\\
&=(2+\varepsilon)\left(b_{1}-a_{1}\right)-\frac{\left(a_{1}-x_{0}\right)^{2}}{2}-\frac{\left(b_{1}-x_{0}\right)^{2}}{2}
\end{aligned}
$$

Find the minimum of the latter expression for $x_{0} \in\left(a_{1}, b_{1}\right)$ and obtain

$$
\begin{aligned}
f\left(b_{1}\right)-f\left(a_{1}\right) & \geq(2+\varepsilon)\left(b_{1}-a_{1}\right)-\frac{\left(b_{1}-a_{1}\right)^{2}}{2}>\\
&>(2+\varepsilon-1)(2-\varepsilon)=2+\varepsilon-\varepsilon^{2}>2,
\end{aligned}
$$

because $\varepsilon<1$. On the other hand, $\left|f\left(b_{1}\right)\right| \leq 1$ and $\left|f\left(a_{1}\right)\right| \leq 1$, so $f\left(b_{1}\right)-f\left(a_{1}\right) \leq$ 2 , and we get a contradiction. Thus, $\left|f^{\prime}(x)\right| \leq 2$ on $I$.

11 Let $\operatorname{deg} P=n$ and $\operatorname{deg} Q=k, n \geq k$. Then the degree of the polynomial $P-Q$ does not exceed $n$. Find the number of roots of $P-Q$. Let $x_{1}, \ldots, x_{m}$ be the roots of the polynomial $P$ of multiplicities $s_{1}, \ldots, s_{m}$, and $y_{1}, \ldots, y_{l}$ be the roots of the polynomial $P+1$ of multiplicities $t_{1}, \ldots, t_{l}$, respectively, where $s_{1}+\ldots+s_{m}=t_{1}+\ldots+t_{l}=n$. Then the polynomial $P-Q$ has at least $m+l$ distinct roots, namely $x_{1}, \ldots, x_{m}, y_{1}, \ldots, y_{l}$. On the other hand, $x_{1}, \ldots, x_{m}$ and $y_{1}, \ldots, y_{l}$ are the roots of $P^{\prime}$ of multiplicities $s_{1}-1, \ldots, s_{m}-1$ and $t_{1}-1, \ldots, t_{l}-1$, respectively, thus,

$$
s_{1}-1+\ldots+s_{m}-1+t_{1}-1+\ldots+t_{l}-1=n-m+n-l \leq n-1 .
$$

Hence $m+l \geq n+1$, so the polynomial $P-Q$ of degree at most $n$ has at least $n+1$ roots. Thus, $P-Q \equiv 0$. 

\section{8}

1 Denote by $\mathbb{N}_{n}$ the set $\{1,2, \ldots, n\}$ and by $f_{n}$ the number of minimal selfish subsets of $\mathbb{N}_{n}$. Then the number of minimal selfish subsets of $\mathbb{N}_{n}$, which do not contain $n$, equals $f_{n-1}$. On the other hand, each minimal selfish subset of $\mathbb{N}_{n}$, which contains $n$, allows to obtain a minimal selfish subset of $\mathbb{N}_{n-2}$ by subtracting 1 from each elements and then removing the element $n-1$. It is possible, because minimal selfish subset except $\{1\}$ cannot contain 1 . The inverse procedure maps each minimal selfish subset of $\mathbb{N}_{n-2}$ to a minimal selfish subset of $\mathbb{N}_{n}$, which contains $n$. Hence, $f_{n}=f_{n-1}+f_{n-2}$. Since $f_{1}=f_{2}=1$, we get $f_{n}=F_{n}$, where $F_{n}$ is the $n$th Fibonacci number.

Answer: the number of subsets in question equals to the $n$th Fibonacci number.

2 Let binary expansion of $\alpha$ be $0 . a_{1} a_{2} \ldots a_{n} \ldots$ We will toss the coin till the first head appears. If it appears first at the $n$th try, then we regard that the first player wins if $a_{n}=1$ and loses if $a_{n}=0$. The probability that the game ends exactly after $n$ tosses equals $\frac{1}{2^{n}}$. Therefore, the game ends after finite number of moves with probability $\sum_{n=1}^{\infty} \frac{1}{2^{n}}=1$, and moreover the first player wins with probability

$$
\sum_{n=1}^{\infty} \frac{a_{n}}{2^{n}}=0 . a_{1} a_{2} \ldots a_{n} \ldots=\alpha .
$$

Answer: yes.

3 Place the triangle on the Cartesian plane so that its vertices have coordinates $C(0,0), A(4,0)$, and $B(0,3)$. Also consider points $D\left(\frac{27}{13}, 0\right)$ and $E\left(\frac{20}{13}, \frac{24}{13}\right)$. It is easy to verify that $D$ lies on $A C, E$ lies on $A B$, and $B E=D E=A D=\frac{25}{13}$ (see Fig. 1). Since some part of the dissection contains at least two of the points $A, B, C, D, E$, and all the distances between these points are not less than $\frac{25}{13}$, the diameter of the dissection is not less than $\frac{25}{13}$. Give an example of dissection with diameter $\frac{25}{13}$. Consider points $F\left(\frac{32}{13}, \frac{15}{13}\right)$, $G(1,0), H\left(\frac{7}{13}, \frac{15}{13}\right), I\left(0, \frac{20}{13}\right)$, and divide the triangle $A B C$ into parts $A F D, E F D G H$, $B E H I$, and $C G H I$. Then $B E D H$ and $A D H F$ are rhombuses with the side $\frac{25}{13}$. It is easy to verify that all the distances between other vertices of polygons of the dissection not exceed $\frac{25}{13}$, so the diameter of the dissection equals $\frac{25}{13}$.

Answer: $\frac{25}{13}$.

Fig. 1 The dissection of the triangle

![](https://cdn.mathpix.com/cropped/2022_10_28_1f3b279542cbcf365e2cg-019.jpg?height=490&width=610&top_left_y=739&top_left_x=1077)

4 One can take $B=\frac{1}{1-q} \cdot A^{-1}$.

5 First show that $A^{2}=\pm I$ for all $A \in M$. Indeed, for arbitrary $B \in M$ it holds $B A=$ $k A B$, where $k^{2}=1$, thus, $B A^{2}=k A B A=k^{2} A^{2} B=A^{2} B$. Therefore, $A^{2}$ commutes with all $B \in M$ and $-A^{2}$ has the same property. By condition (ii) either $A^{2}$ or $-A^{2}$ belongs to $M$, so either $A^{2}=\pm I$ or we get a contradiction with condition (iv).

If $M$ contains more than $n^{2}$ matrices, then the matrices are linearly dependent, i.e., $\sum_{A \in M} \lambda_{A} A=O$ for some real numbers $\lambda_{A}$ not all of which are zeros. Consider a nontrivial collection $\lambda_{A}, A \in M$, which contains the least possible number of nonzero coefficients. Let $\lambda_{A_{0}} \neq 0$. Then $\sum_{A \in M} \lambda_{A} A A_{0}=O, A_{0}^{2}=\pm I$, and for each matrix $A \in M$, either $A A_{0} \in M$ or $-A A_{0} \in M$. Thus, we found a new linear combination with the least possible number of nonzero coefficients, moreover now $\lambda_{I} \neq 0$. There exists at least one matrix $A_{1} \in M$, for which $A_{1} \neq I$ and $\lambda_{A_{1}} \neq 0$. Select a matrix $B \in M$ from condition (iv) such that $A_{1} B=-B A_{1}$. We have $\sum_{A \in M} \lambda_{A}(A B+B A)=O$. For each $A \in M$ it holds either $A B+B A=0$, or $\frac{1}{2}(A B+B A) \in M$, or $-\frac{1}{2}(A B+B A) \in M$. In particular if $A=A_{1}$ then $A B+B A=0$, and if $A=I$ then $\frac{1}{2}(A B+B A)=B \in M$. Therefore, the left-hand side of the equality $\sum_{A \in M} \lambda_{A}(A B+B A)=O$ can be rewritten as a nontrivial linear combination of matrices from $M$ with less number of nonzero coefficients. We get a contradiction. 6 In the field with two elements, $2=0$ and $a_{n}^{2}=a_{n}$. Therefore,

$$
\alpha^{2}=1+\sum_{n \geq 1} a_{n}^{2} x^{2 n}=1+\sum_{n \geq 1} a_{n} x^{2 n}, \quad \alpha^{4}=1+\sum_{n \geq 1} a_{n} x^{4 n}
$$

Find $\alpha^{4}+x \alpha^{2}+\alpha$. Consider separately the coefficients at $x^{4 n}, x^{4 n+2}$, and $x^{2 n+1}$. The coefficient at $x^{4 n}$ equals $a_{n}+a_{4 n}$ if $n \geq 1$, and equals $1+1=0$ if $n=0$. Binary expansion of the number $4 n$ is obtained by appending 00 to the expansion of $n$. Therefore, $a_{4 n}=a_{n}$, so $a_{4 n}+a_{n}=0$. The coefficient at $x^{4 n+2}$ equals $a_{4 n+2}=0$, because binary expansion of $4 n+2$ ends with 10 . At last, the coefficient at $x^{2 n+1}$ equals $a_{2 n+1}+a_{n}$. The expansion of $2 n+1$ is obtained from the expansion of $n$ by appending 1 , hence $a_{2 n+1}=a_{n}$, so $a_{2 n+1}+a_{n}=0$.

Thus, $\alpha^{4}+x \alpha^{2}+\alpha=0$. Since $\alpha \neq 0$, we have $\alpha^{3}+x \alpha+1=0$.

7 Introduce a function $h(x)=(f(x))^{2}+\left(f^{\prime}(x)\right)^{2}$. It holds

$$
h^{\prime}(x)=2 f(x) f^{\prime}(x)+2 f^{\prime}(x) f^{\prime \prime}(x)=-x g(x)\left(f^{\prime}(x)\right)^{2} .
$$

Hence, the function $h(x)$ is increasing for $x<0$ and decreasing for $x>0$. Therefore, $(f(x))^{2} \leq h(x) \leq h(0)$ for all $x \in \mathbb{R}$.

8 Construct a function $F \in C^{(1)}(\mathbb{R})$ such that

$$
\forall t \in \mathbb{R} \quad F(t+1)=F(t)+\arctan t .
$$

Since $\int_{0}^{1} f(x+t) d x=\int_{t}^{t+1} f(x) d x$, the function $f(t)=F^{\prime}(t)$ will satisfy the condition of the problem. First define $F$ on the interval $[0,1)$, and then define $F$ on the intervals $[1,2),[2,3), \ldots$ and $[-1,0),[-2,-1), \ldots$ in such a way that the condition

$$
F(t+1)=F(t)+\arctan t
$$

holds. It suffices to ensure that $F \in C^{(1)}(\mathbb{R})$. Continuity of $F$ and $F^{\prime}$ at point $t=1$ implies that $F(1)=F(1-), F^{\prime}(1-)=F^{\prime}(1+)$, i.e.,

$$
\begin{gathered}
F(1-)=F(0)+\arctan 0=F(0), \\
F^{\prime}(1-)=F^{\prime}(0+)+\left.(\arctan t)^{\prime}\right|_{t=0}=F^{\prime}(0+)+1 .
\end{gathered}
$$

These conditions hold if, e.g., $F(t)=\frac{1}{2}\left(t^{2}-t\right)$ for $t \in[0,1)$. Then $F \in C^{(1)}((0,2))$ and by the construction $F$ and $F^{\prime}$ are continuous at all integer points. Therefore, $F \in C^{(1)}(\mathbb{R})$

9 Since $\varphi(g) \varphi(e) \varphi\left(g^{-1}\right)=\varphi(e) \varphi(g) \varphi\left(g^{-1}\right)$, elements $\varphi(g)$ and $\varphi(e)$ commute for all $g \in G$. Hence, $\varphi(g)$ and $\varphi(e)^{-1}$ commute. Also we have

$$
\varphi(x) \varphi(y) \varphi\left(y^{-1} x^{-1}\right)=\varphi(e) \varphi(x y) \varphi\left(y^{-1} x^{-1}\right),
$$

whence $\varphi(x) \varphi(y)=\varphi(e) \varphi(x y)$. Therefore,

$$
\varphi(e)^{-1} \varphi(x) \varphi(e)^{-1} \varphi(y)=\varphi(e)^{-1} \varphi(e)^{-1} \varphi(x) \varphi(y)=\varphi(e)^{-1} \varphi(x y),
$$

i.e., for $\psi(x)=\varphi(e)^{-1} \varphi(x)$ it holds $\psi(x) \psi(y)=\psi(x y), x, y \in G$

10 Show that the limit in question does not change if we replace $\left\{x_{n}\right\}$ with a new sequence $\left\{y_{n}\right\}$, defined as

$$
y_{1}=1, \quad y_{n+1}=\frac{1}{2+y_{n}}+\xi_{n}, n \geq 1,
$$

where $\xi_{n} \in[0,1], n \geq 1$, are arbitrary numbers such that $\{\sqrt{n}\}-\xi_{n} \rightarrow 0$, as $n \rightarrow \infty$. Indeed,

$$
\begin{aligned}
& \left|y_{n+1}-x_{n+1}\right| \leq\left|\frac{1}{2+x_{n}}-\frac{1}{2+y_{n}}\right|+\left|\{\sqrt{n}\}-\xi_{n}\right| \leq \\
& \leq \frac{1}{4}\left|y_{n}-x_{n}\right|+\left|\{\sqrt{n}\}-\xi_{n}\right|, n \geq 1,
\end{aligned}
$$

thus,

$$
\left|y_{n}-x_{n}\right| \leq \sum_{k=1}^{n-1} \frac{\left|\{\sqrt{n}\}-\xi_{n}\right|}{4^{n-k-1}} \rightarrow 0, \text { as } n \rightarrow \infty .
$$

It is easy to verify that $x_{n}, y_{n} \in[0,3 / 2], n \geq 1$. Therefore,

$$
\left|y_{n}^{2}-x_{n}^{2}\right| \leq 3\left|y_{n}-x_{n}\right| \rightarrow 0, \text { as } n \rightarrow \infty,
$$

and by Toeplitz's theorem $\lim _{N \rightarrow \infty} \frac{1}{N} \sum_{k=1}^{N}\left|x_{k}^{2}-y_{k}^{2}\right|=0$.

For $M \geq 1, M^{2} \leq N \leq(M+1)^{2}-1$ and $n=N^{2}+a, 0 \leq a \leq 2 N$, set $\xi_{n}=$ $\frac{1}{M}\left[\frac{a M}{2 N}\right]$. Then $\left\{\xi_{n}, n \geq 1\right\} \subset[0,1]$. Since

$$
N+\frac{a-1}{2 N}<\sqrt{N^{2}+a}<N+\frac{a}{2 N},
$$

we have $\left|\frac{a}{2 N}-\left\{\sqrt{N^{2}+a}\right\}\right|<\frac{1}{2 N}$. Also we have $\left|\frac{a}{2 N}-\frac{1}{M}\left[\frac{a M}{2 N}\right]\right|<\frac{1}{M}$, whence

$$
\left|\{\sqrt{n}\}-\xi_{n}\right| \leq \frac{1}{2 N}+\frac{1}{M} \rightarrow 0, \text { as } n \rightarrow \infty,
$$

because $N, M \rightarrow \infty$, as $n \rightarrow \infty$ Notice that the finite sequence $\xi_{N^{2}}, \xi_{N^{2}+1}, \ldots, \xi_{N^{2}+2 N}$ consists of numbers $0, \frac{1}{M}$, $\ldots, \frac{M-1}{M}$, each of which repeats $\left[\frac{2 N+1}{M}\right]$ times or $\left[\frac{2 N+1}{M}\right]+1$ times in a row, i.e., the sequence $\left\{\xi_{n}, n \geq 1\right\}$ consists of constant parts, lengths of which tend to infinity.

For each fixed $\xi \in[0,1]$, an equation $x(\xi)=\frac{1}{2+x(\xi)}+\xi$ has a unique solution in $[0,3 / 2]$, namely $x(\xi)=\frac{\xi-2+\sqrt{\xi^{2}+4 \xi+8}}{2}$. For arbitrary $z_{1} \in[0,3 / 2]$, define a sequence $\left\{z_{n}\right\}$ as

$$
z_{n+1}=\frac{1}{2+z_{n}}+\xi, n \geq 1 .
$$

Then

$$
\begin{aligned}
\left|z_{n}-x(\xi)\right| &=\left|\frac{1}{2+z_{n-1}}-\frac{1}{2+x(\xi)}\right| \leq \\
& \leq \frac{1}{4}\left|z_{n-1}-x(\xi)\right| \leq \ldots \leq \frac{3}{2 \cdot 4^{n-1}}, n \geq 2
\end{aligned}
$$

Thus, $\left|z_{n}^{2}-x^{2}(\xi)\right| \leq \frac{9}{2 \cdot 4^{n-1}}, n \geq 1$, and $\sum_{k=1}^{n}\left|z_{k}^{2}-x^{2}(\xi)\right| \leq \sum_{k=1}^{\infty} \frac{9}{2 \cdot 4^{k-1}}=6$.

Fix $M \geq 1$ and $M^{2} \leq N \leq(M+1)^{2}-1$. Consider an average $\frac{1}{2 N+1} \sum_{k=N^{2}+1}^{(N+1)^{2}} y_{k}^{2}$. The numbers $y_{N^{2}+1}, \ldots, y_{(N+1)^{2}}$ form $M$ finite sequences of length either $\left[\frac{2 N+1}{M}\right]$ or $\left[\frac{2 N+1}{M}\right]+1$, each of which is constructed by the same rule as $z_{n}$, where $\xi$ takes values $0, \frac{1}{M}, \ldots, \frac{M-1}{M}$. Therefore,

$$
\begin{aligned}
\frac{1}{2 N+1} \sum_{k=N^{2}+1}^{(N+1)^{2}} y_{k}^{2} &-\frac{1}{M} \sum_{j=0}^{M-1} x^{2}\left(\frac{j}{M}\right)\left|\leq \frac{1}{2 N+1} \sum_{k=N^{2}+1}^{(N+1)^{2}}\right| y_{k}^{2}-x^{2}\left(\xi_{k-1}\right) \mid+\\
&+\left|\frac{1}{2 N+1} \sum_{k=N^{2}+1}^{(N+1)^{2}} x^{2}\left(\xi_{k-1}\right)-\frac{1}{M} \sum_{j=0}^{M-1} x^{2}\left(\frac{j}{M}\right)\right| \leq \\
& \leq \frac{6 M}{2 N+1}+\frac{9}{4} \cdot \frac{M}{2 N+1}=\frac{33 M}{4(2 N+1)}
\end{aligned}
$$

It holds

$$
M=[\sqrt{N}] \rightarrow \infty, \frac{M}{2 N+1} \leq \frac{\sqrt{N}}{N} \rightarrow 0, \frac{1}{M} \sum_{j=0}^{M-1} x^{2}\left(\frac{j}{M}\right) \rightarrow \int_{0}^{1} x^{2}(\xi) d \xi, \text { as } N \rightarrow \infty .
$$

Thus, $\left|\frac{1}{2 N+1} \sum_{k=N^{2}+1}^{(N+1)^{2}} y_{k}^{2}-\int_{0}^{1} x^{2}(\xi) d \xi\right| \rightarrow 0$, as $N \rightarrow \infty$.

By Toeplitz's theorem, 

$$
\begin{aligned}
\mid \frac{1}{L^{2}} \sum_{k=1}^{L^{2}} y_{k}^{2} &-\int_{0}^{1} x^{2}(\xi) d \xi \mid \leq \frac{1}{L^{2}} \sum_{N=0}^{L-1}(2 N+1) \times \\
& \times\left|\frac{1}{2 N+1} \sum_{k=N^{2}+1}^{(N+1)^{2}} y_{k}^{2}-\int_{0}^{1} x^{2}(\xi) d \xi\right| \rightarrow 0, \text { as } L \rightarrow \infty .
\end{aligned}
$$

At last for all integers $n, L^{2} \leq n<(L+1)^{2}$, it holds

$$
\begin{aligned}
\left|\frac{1}{n} \sum_{k=1}^{n} y_{k}^{2}-\int_{0}^{1} x^{2}(\xi) d \xi\right| & \leq \frac{L^{2}}{n}\left|\frac{1}{L^{2}} \sum_{k=1}^{L^{2}} y_{k}^{2}-\int_{0}^{1} x^{2}(\xi) d \xi\right|+\\
&+\frac{n-L^{2}}{n}\left(\frac{9}{4}+\int_{0}^{1} x^{2}(\xi) d \xi\right) \rightarrow 0, \text { as } n \rightarrow \infty,
\end{aligned}
$$

because $\frac{L^{2}}{n} \rightarrow 1, \frac{n-L^{2}}{n} \rightarrow 0$, as $n \rightarrow \infty$. Therefore,

$$
\begin{aligned}
\lim _{n \rightarrow \infty} \frac{1}{n} \sum_{k=1}^{n} x_{k}^{2} &=\lim _{n \rightarrow \infty} \frac{1}{n} \sum_{k=1}^{n} y_{k}^{2}=\\
&=\int_{0}^{1} x^{2}(\xi) d \xi=\int_{0}^{1}\left(\frac{\xi-2+\sqrt{\xi^{2}+4 \xi+8}}{2}\right)^{2} d \xi .
\end{aligned}
$$

It remains to compute the integral

$$
\begin{aligned}
\int_{0}^{1}\left(\frac{t-2+\sqrt{t^{2}+4 t+8}}{2}\right)^{2} d t=\frac{1}{4} \int_{0}^{1}\left(t-2+\sqrt{t^{2}+4 t+8}\right)^{2} d t=\\
&=\frac{1}{4} \int_{0}^{1}\left(2 t^{2}+12+2(t-2) \sqrt{t^{2}+4 t+8}\right) d t=\\
&=3 \frac{1}{6}+\frac{1}{2} \int_{2}^{3}(s-4) \sqrt{s^{2}+4} d s=\\
&=3 \frac{1}{6}+\frac{1}{2} \int_{2}^{3} s \sqrt{s^{2}+4} d s-2 \int_{2}^{3} \sqrt{s^{2}+4} d s=\\
&=3 \frac{1}{6}+\left.\frac{1}{6}\left(s^{2}+4\right)^{3 / 2}\right|_{2} ^{3}-\left.2\left(2 \ln \left(s+\sqrt{s^{2}+4}\right)+\frac{s}{2} \sqrt{s^{2}+4}\right)\right|_{2} ^{3}=\\
&=3 \frac{1}{6}-\frac{5}{6} \sqrt{13}+\frac{4}{3} \sqrt{2}-4 \ln \frac{3+\sqrt{13}}{2+2 \sqrt{2}} .
\end{aligned}
$$

Answer: $\lim _{n \rightarrow \infty} \frac{1}{n} \sum_{k=1}^{n} x_{k}^{2}=3 \frac{1}{6}-\frac{5}{6} \sqrt{13}+\frac{4}{3} \sqrt{2}-4 \ln \frac{3+\sqrt{13}}{2+2 \sqrt{2}}$. 11 Consider a matrix $A^{\mathrm{T}}$ of size $n \times n$, where $A=\left(a_{i j}\right)$. Since $\operatorname{tr} A^{\mathrm{T}}>0$, there exists $\lambda$, an eigenvalue of $A^{\mathrm{T}}$ such that $\operatorname{Re} \lambda>0$. Consider the corresponding eigenvector $v=\left(v_{i}\right)$ and a linear combination $y=\sum_{i=1}^{n} v_{i} x_{i}$. We have

$$
y^{\prime}=\sum_{i=1}^{n} v_{i} \cdot \sum_{j=1}^{n} a_{i j} x_{j}=\sum_{j=1}^{n}\left(\sum_{i=1}^{n} a_{i j} v_{i}\right) x_{j}=\lambda \sum_{j=1}^{n} v_{j} x_{j}=\lambda y
$$

thus, $y(t)=k e^{\lambda t}$ for some constant $k$. But $\left|e^{\lambda t}\right|$ does not tend to zero, as $t \rightarrow \infty$, because $\operatorname{Re} \lambda>0$. Since $y(t)=\sum_{i=1}^{n} v_{i} x_{i}(t) \rightarrow 0$, as $t \rightarrow \infty$, we have $k=0$. Thus, $\sum_{i=1}^{n} v_{i} x_{i}(t)=0$, and the functions $x_{1}, x_{2}, \ldots, x_{n}$ are linearly dependent.

12 The statement of the problem is a corollary of the following more general statement: for arbitrary operators $A_{1}, A_{2} \in \mathscr{L}(B)$

$$
\sigma\left(A_{1} A_{2}\right) \backslash\{0\}=\sigma\left(A_{2} A_{1}\right) \backslash\{0\} .
$$

Prove this statement.

Let $\lambda \in \mathbb{C} \backslash\{0\}$ be such a number that there exists $\left(A_{1} A_{2}-\lambda I\right)^{-1} \in \mathscr{L}(B)$, where $I$ is the identity operator. It holds

$$
\begin{gathered}
\left(A_{2} A_{1}-\lambda I\right) A_{2}=A_{2}\left(A_{1} A_{2}-\lambda I\right), \\
\left(A_{2} A_{1}-\lambda I\right) A_{2}\left(A_{1} A_{2}-\lambda I\right)^{-1}=A_{2}, \\
\left(A_{2} A_{1}-\lambda I\right) A_{2}\left(A_{1} A_{2}-\lambda I\right)^{-1} A_{1}=A_{2} A_{1}-\lambda I+\lambda I, \\
\left(A_{2} A_{1}-\lambda I\right)\left(A_{2}\left(A_{1} A_{2}-\lambda I\right)^{-1} A_{1}-I\right)=\lambda I, \\
\left(A_{2} A_{1}-\lambda I\right)\left(\frac{1}{\lambda}\left(A_{2}\left(A_{1} A_{2}-\lambda I\right)^{-1} A_{1}-I\right)\right)=I .
\end{gathered}
$$

In a similar way $\frac{1}{\lambda}\left(A_{2}\left(A_{1} A_{2}-\lambda I\right)^{-1} A_{1}-I\right)\left(A_{2} A_{1}-\lambda I\right)=I$. Thus, there exists

$$
\left(A_{2} A_{1}-\lambda I\right)^{-1}=\frac{1}{\lambda}\left(A_{2}\left(A_{1} A_{2}-\lambda I\right)^{-1} A_{1}-E\right) \in \mathscr{L}(B) .
$$

Hence, for resolvent sets it holds $\rho\left(A_{1} A_{2}\right) \backslash\{0\}=\rho\left(A_{2} A_{1}\right) \backslash\{0\}$. Therefore,

$$
\sigma\left(A_{1} A_{2}\right) \backslash\{0\}=\sigma\left(A_{2} A_{1}\right) \backslash\{0\} .
$$

2 A solution is similar to the solution of Problem 7, 1997. Answer: $2 \cdot 2^{\frac{1}{\sqrt{2}}}$.

3 If either $f$ or one of the derivatives changes sign then the expression

$$
f(a) \cdot f^{\prime}(a) \cdot f^{\prime \prime}(a) \cdot f^{\prime \prime \prime}(a)
$$

attains zero value. Assume that each of the functions $f, f^{\prime}, f^{\prime \prime}, f^{\prime \prime \prime}$ is either strictly positive or strictly negative. Show that $f(x) \cdot f^{\prime \prime}(x)>0$ for all $x \in \mathbb{R}$. Indeed, let $f^{\prime \prime}(x)>0, x \in \mathbb{R}$. Then $f$ is convex, and

$$
f(x+y) \geq f(x)+y f^{\prime}(x), x, y \in \mathbb{R} .
$$

For $y$ of the same sign as $f^{\prime}(x)$, with absolute value large enough, it holds $f(x+y)>$ $f(x)+y f^{\prime}(x)>0$. Hence $f(x)>0$ for all $x \in \mathbb{R}$. If $f^{\prime \prime}(x)<0$ then in a similar way we induce that the function $f$ is negative, so $f(x) \cdot f^{\prime \prime}(x)>0$ as well. Similarly $f^{\prime}(x) \cdot f^{\prime \prime \prime}(x)>0, x \in \mathbb{R}$. Thus, $f(x) \cdot f^{\prime}(x) \cdot f^{\prime \prime}(x) \cdot f^{\prime \prime \prime}(x)>0$.

4 Denote by $\Lambda$ the sum of corresponding eigenvalues. Consider one of the eigenvectors with some eigenvalue $\lambda$. Other $n$ eigenvectors are linearly independent, so they form a basis, and the trace of the matrix in this basis equals $\Lambda-\lambda$. But the trace of a matrix does not depend on the choice of a basis. Therefore, all the eigenvalues are equal and the transformation has a form $\lambda I$.

5 For $m=999$, it holds $N=\frac{1}{9}\left(10^{2 m}-1\right)$, and $\sqrt{N}=\frac{10^{m}}{3} \sqrt{1-10^{-2 m}}$. It is easy to verify that

$$
1-\frac{2}{3} \cdot 10^{-2 m}<\sqrt{1-10^{-2 m}}<1-\frac{1}{2} \cdot 10^{-2 m} .
$$

Therefore,

$$
3 \ldots 3.3 \ldots 311 \ldots<\sqrt{N}<3 \ldots 3.3 \ldots 316 \ldots
$$

(there are 999 threes before and 999 threes after decimal point). Thus, the thousandth digit after decimal point equals 1.

Answer: 1.

6 If $-1 \in S$ then $(-1)(-1)=1 \in S$, and for $r=1$ we get a contradiction. Thus, $1 \in S, 1+1=2 \in S, 1+2=3 \in S$, and by induction on $n \geq 1$ we get that $n \in S$ for all positive integers $n$. Therefore, $S$ contains all positive integers. It is evident that $S$ does not contain zero, otherwise for $r=0$ all three statements are true. Suppose that $S$ contains some negative rational number $-\frac{k}{m}$, where $k, m \in \mathbb{N}$. Again by induction on $n \geq 1$ we obtain that the set $S$ contains all the numbers of the form $-\frac{k n}{m}, n \in \mathbb{N}$. Then simultaneously $-k \in S$ and $k \in S$, a contradiction. Therefore, $S \subset(0,+\infty)$, so $S=(0,+\infty) \cap \mathbb{Q}$, otherwise there exists such $r$ that neither of three statements from the condition of the problem is true.

8 Show that the set $S$ contains the circle which bounds the disc $D$. Since each interior point of the disc is the middle of the chord orthogonal to the diameter passing through the point, the statement of the problem will follow.

Fig. 1 A disc $D^{\prime}$ such that $D \backslash B_{\varepsilon}(P) \subset D^{\prime}$ but $D \not \subset D^{\prime}$

![](https://cdn.mathpix.com/cropped/2022_10_28_1f3b279542cbcf365e2cg-026.jpg?height=483&width=567&top_left_y=1260&top_left_x=1126)

Take an arbitrary point $P$ of the circle, and show that the point $P$ belongs to $S$. If it is not true then some $\operatorname{disc} B_{\varepsilon}(P)$ with the center $P$ does not intersect $S$, because the set $\mathbb{R}^{2} \backslash S$ is open. Then one can find a disc $D^{\prime}$ such that $S \subset\left(D \backslash B_{\varepsilon}(P)\right) \subset D^{\prime}$ but $D \not \subset D^{\prime}$, in particular $P \notin D^{\prime}$ (see Fig. 1). We get a contradiction.

9 By the polar decomposition theorem there exist self-adjoint positive semidefinite matrices $\left\{P_{n}, n \geq 1\right\}$ and orthogonal matrices $\left\{U_{n}, n \geq 1\right\}$ such that $S_{n}=P_{n} U_{n}$, $n \geq 1$. We have

$$
S_{n} S_{n}^{\mathrm{T}}=P_{n} U_{n} U_{n}^{\mathrm{T}} P_{n}^{\mathrm{T}}=P_{n}^{2} \rightarrow I, \text { as } n \rightarrow \infty
$$

Therefore, all the eigenvalues of $P_{n}$ tend to 1 , and $P_{n} \rightarrow I$, as $n \rightarrow \infty$. Hence $S_{n}-U_{n}=\left(P_{n}-I\right) U_{n} \rightarrow O$, as $n \rightarrow \infty$ 10 Denote by $F_{\xi}$ and $F_{\eta}$ the cumulative distribution functions of $\xi$ and $\eta$. Assume that for all $a \in \mathbb{R}$ it holds $\mathrm{P}(\xi=a) \cdot \mathrm{P}(\eta=a)=0$, that is $F_{\xi}$ and $F_{\eta}$ satisfy the equality $\left(F_{\xi}(a)-F_{\xi}(a-)\right)\left(F_{\eta}(a)-F_{\eta}(a-)\right)=0$. Then the independence of $\xi$ and $\eta$ implies that

$$
\mathrm{P}(\xi=\eta)=\int_{-\infty}^{\infty}\left(F_{\xi}(a)-F_{\xi}(a-)\right) d F_{\eta}(a)=0,
$$

a contradiction with the condition $\mathrm{P}(\xi=\eta)>0$.

11 Let $\left\{e_{i}, i \geq 2\right\}$ be an orthobasis in $H$ and $e_{1}=\sum_{i=2}^{\infty} \frac{e_{i}}{i}$. Then $e_{1}$ is well-defined because for $n>m \geq 2$ it holds

$$
\left\|\sum_{i=2}^{n} \frac{e_{i}}{i}-\sum_{i=2}^{m} \frac{e_{i}}{i}\right\|_{H}^{2}=\sum_{i=m+1}^{n} \frac{1}{i^{2}} \rightarrow 0, \text { as } m, n \rightarrow \infty,
$$

and therefore, the series converges in $H$. It is evident that every nontrivial linear combination of a finite number of elements $e_{i_{1}}, e_{i_{2}}, \ldots, e_{i_{k}}$ does not equal zero, i.e., the elements of the set $M=\left\{e_{i}, i \geq 1\right\}$ are linearly independent. It is clear that $H$ is the closed linear hull of the set $\left\{e_{i}, i \geq 2\right\}$. Thus, it suffices to show that $e_{i}$ belongs to the closed linear hull of the set $M \backslash\left\{e_{i}\right\}$, for each $i \geq 2$. It is true, because for $n>i$ we have

$$
\left\|e_{i}-i\left(e_{1}-\sum_{\substack{j=2 \\ j \neq i}}^{n} \frac{e_{j}}{j}\right)\right\|_{H}^{2}=i^{2} \sum_{j=n+1}^{\infty} \frac{1}{j^{2}} \rightarrow 0, \text { as } n \rightarrow \infty
$$

1 If the series $\sum_{n=1}^{\infty} a_{n}$ converges to $S$ then $b_{n} \leq S n$. Hence $\frac{1}{b_{n}} \geq \frac{1}{S n}$, and the series $\sum_{n=1}^{\infty} \frac{1}{b_{n}}$ diverges.

2 Consider $M_{\alpha}^{\prime}=[0, \alpha] \cap \mathbb{Q}, \alpha \in \mathscr{A}=[0,1]$. Take an arbitrary bijection of $\mathbb{Q}$ onto $\mathbb{N}$. For each $\alpha$ let $M_{\alpha}$ be the image of $M_{\alpha}^{\prime}$ under this bijection.

Answer: Yes, it is possible.

3 For $y=0$, we have that $f(1)=\frac{f(x)+f(0)}{f(x)-f(0)}$ does not depend on $x$. Therefore, $f(0)=0$ and $f(1)=1$. For $x=t y, t>1$, we have

$$
f\left(\frac{t+1}{t-1}\right)=\frac{f(t y)+f(y)}{f(t y)-f(y)},
$$

whence

$$
\frac{f(t y)}{f(y)}=\frac{f\left(\frac{t+1}{t-1}\right)+1}{f\left(\frac{t+1}{t-1}\right)-1}
$$

does not depend on $y$. Therefore,

$$
f(t y)=f(y) \cdot \frac{f(t)}{f(1)}=f(y) f(t) .
$$

By induction on $n$ we get $f\left(x^{n}\right)=f(x)^{n}, n \in \mathbb{N}$. Since $f\left(x^{n}\right) f\left(x^{-n}\right)=f(1)=1$, we have $f\left(x^{n}\right)=f(x)^{n}, n \in \mathbb{Z}$. Let $f(2)=2^{a}$, where $a>0$ because $f(2)>$ $f(1)=1$. Then $f\left(2^{1 / m}\right)^{m}=f(2)=2^{a}$, thus, $f\left(2^{1 / m}\right)=2^{a / m}$, so $f\left(2^{n / m}\right)=$ $2^{\text {na/m}}$. Therefore, $f\left(2^{q}\right)=2^{q a}, q \in \mathbb{Q}$. Since the numbers $2^{q}, q \in \mathbb{Q}$, are dense in $(0, \infty)$, due to the monotonicity of $f$ we get $f(x)=x^{a}$ for all $x>0$. Verification shows that this function satisfies the initial equation only for $a=1$.

Answer: $f(x)=x$.

4 Let $f(x)=x^{3}-3 x$. Assume that the sequence converges to $\ell$. Then $f(\ell)=\ell$. Therefore, $\ell \in\{0, \pm 2\}$. In a neighborhood of the point $\ell$ it holds $\left|f^{\prime}(x)\right|>1$. There exists $N \in \mathbb{N}$ such that $x_{n}$ lies in this neighborhood for every $n \geq N$. If $x_{n} \neq \ell$ then by the mean value theorem we have

$$
\left|x_{n+1}-\ell\right|=\left|f\left(x_{n}\right)-f(\ell)\right|=\left|f^{\prime}(\theta)\right| \cdot\left|x_{n}-\ell\right|>\left|x_{n}-\ell\right| .
$$

Thus, the convergence holds only in the case $x_{n}=\ell$ for some $n$. Find for which values of $a$ it is possible. If $|a|>2$ then it is easy to verify that $\left|x_{n+1}\right|>\left|x_{n}\right|>2, n \geq 1$. Therefore, $|a| \leq 2$, so one can set $x_{1}=2 \cos \varphi$. Then $x_{n}=2 \cos 3^{n-1} \varphi, n \geq 1$. To find the answer it remains to solve the equation $2 \cos 3^{n-1} \varphi=\ell$ for $\ell=0, \pm 2$.

Answer: $a=2 \cos \frac{\pi k}{2 \cdot 3^{n-1}}, k, n \in \mathbb{N}$.

5 It is easy to check that

$$
\sum_{n=1}^{\infty} \frac{d(n)}{n^{2}}=\left(\sum_{k=1}^{\infty} \frac{1}{k^{2}}\right)^{2}<\left(1+\sum_{k=2}^{\infty}\left(\frac{1}{k-1}-\frac{1}{k}\right)\right)^{2}=2^{2}=4
$$

6 Show that the wolves can catch the hare. Consider circles which are the parallels of the torus

$$
K_{\varphi}=\left\{(x, y, z) \mid \sqrt{x^{2}+y^{2}}=2000-\sqrt{2000} \cos \varphi, z=\sqrt{2000} \sin \varphi\right\}, \varphi \in(-\pi, \pi] .
$$

Notice that a movement along the parallel $K_{\varphi}$ at speed 1 corresponds to the angular velocity (of movement along the parallel)

$$
\omega_{\varphi}=\frac{1}{2000-\sqrt{2000} \cos \varphi} .
$$

At the beginning both wolves should get to the least parallel of the torus $K_{0}$. Then the wolves move along this circle in opposite directions till one of them reaches a common meridian with the hare. Then it shares a meridian with the hare for the rest of the time. The second wolf also should reach a common meridian with the hare.

Show that this is possible. Indeed, the hare cannot approach the first wolf closer than at distance 1, therefore, the hare permanently occupies the parallels with $|\varphi| \geq$ $\varphi_{0}=\frac{1}{\sqrt{2000}}$ (because a circle which is the meridian has radius $\sqrt{2000}$ ) and moves at the angular velocity at most $\omega_{\varphi_{0}}$, which is less than the angular velocity of the wolves $\omega_{0}$

After both wolves get to the same meridian, they should permanently occupy a common meridian with the hare and move along the meridian in opposite directions. At the moment when the wolves occupy a common meridian with the hare at the parallels $K_{\pm \varphi}, 0 \leq \varphi \leq \pi-\varphi_{0}$, the hare surely occupies a parallel $K_{\psi}$, where $|\psi| \geq$ $\varphi+\varphi_{0}$ (if it has not been caught yet). Hence the wolves can move along the meridian at a speed at least $v_{\varphi}=\sqrt{1-\left(\frac{\omega_{\varphi}+\varphi_{0}}{\omega_{\varphi}}\right)^{2}}$. Since $v_{\varphi}$ is a continuous nonnegative function on a segment, it holds $\min _{0 \leq \varphi \leq \pi-\varphi_{0}} v_{\varphi}>0$. Therefore, the wolves will catch the hare in finite time.

Answer: yes, they can.

7 It is evident that $\mathrm{rk} A_{n} \leq 2$ and $\mathrm{rk} B_{n} \leq 1$. Thus, det $A_{n}=\overline{0}$ and det $B_{n}=\overline{0}$ for every $n>2$.

Answer: $\operatorname{det} A_{2}=\overline{1}, \operatorname{det} B_{2}=\overline{1}$, $\operatorname{det} A_{n}=\operatorname{det} B_{n}=\overline{0}, n>2$.

8 Necessity:

$$
\frac{1}{2} \geq|z|-\operatorname{Re} z=\sqrt{z \bar{z}}-\frac{z+\bar{z}}{2}=\frac{1}{2}(\overline{\sqrt{z}}-\sqrt{z})^{2} .
$$

One can set $u=v=\sqrt{z}$.

Sufficiency:

$$
\begin{aligned}
|z|-\operatorname{Re} z=\sqrt{z \bar{z}}-\frac{z+\bar{z}}{2}=\sqrt{u \bar{u} v \bar{v}}-\frac{u v+\bar{u} \bar{v}}{2} & \leq \\
\leq \frac{u \bar{u}+v \bar{v}}{2}-\frac{u v+\bar{u} \bar{v}}{2}=\frac{1}{2}(u-\bar{v})(\bar{u}-v)=\frac{1}{2}|\bar{u}-v|^{2} \leq \frac{1}{2} .
\end{aligned}
$$

9 Let us select sets $A_{1}$ and $A_{2}$ element-wise. For each element $k$ we decide whether it belongs to $A_{1}$ and/or to $A_{2}$, which gives us 4 options, so we have $4^{n}$ possible pairs of sets. Sets $A_{1}$ and $A_{2}$ will be disjoint if and only if for each element $k$ we choose one of three options (except $k \in A_{1}$ and $k \in A_{2}$ ), so we have $3^{n}$ possible pairs of disjoint sets.

Answer: $(3 / 4)^{n}$.

10 For the sake of convenience add a chair on the left end of the row. Denote by 1 each person together with a free chair on the left of him/her and by 0 each other free chair. Each arrangement which satisfies the condition of the problem corresponds to a row of $n$ ones and $N+1-n$ zeros, and there are $\left(\begin{array}{c}N+1-n \\ n\end{array}\right)$ such rows. Total number of arrangements in question is $\left(\begin{array}{l}N \\ n\end{array}\right)$.

Answer: $\left(\begin{array}{c}N+1-n \\ n\end{array}\right) /\left(\begin{array}{l}N \\ n\end{array}\right)$.

11 Introduce new variables $t=x y, y=y$ in the second integral, and get

$$
\int_{0}^{1} \int_{0}^{1}(x y)^{x y} d x d y=\int_{0}^{1}\left(\int_{t}^{1} t^{t} \frac{1}{y} d y\right) d t=\int_{0}^{1} t^{t}(-\ln t) d t=\int_{0}^{1} t^{t} d t
$$

The latter equality holds because $\int_{0}^{1} t^{t}(1+\ln t) d t=\left.t^{t}\right|_{0} ^{1}=0$.

Answer: the integrals are equal. 12 A solution is similar to the solution of Problem 4.

Answer: $a=2 \sin \frac{\pi k}{3^{n-1}}, a=2 \sin \frac{\pi(1+2 k)}{4 \cdot 3^{n-1}}, k \in \mathbb{Z}, n \in \mathbb{N}$.

13 Unit element is self-double because for each $u \neq e$ it holds $u u^{-1}=u^{-1} u=e$. If $x$ has order $n \geq 3$ then $x^{n-1} x^{2}=x^{2} x^{n-1}=x$, so $x$ is self-double.

Let $x$ be not self-double, hence $x^{2}=e$, and $x^{-1}=x$. Consider orbits of the mapping $u \mapsto x u^{-1}, G \rightarrow G$. We have

$$
u \mapsto x u^{-1} \mapsto x u x \mapsto u^{-1} x \mapsto u .
$$

If $u \neq e$ and $u \neq x$ then $u \neq x u x$, otherwise for $v=x u^{-1} \neq e$ we get $v u=x$ and $u v=x u x x u^{-1}=x$, so $x$ is self-double. Therefore, all the orbits except $e \mapsto x \mapsto e$ have length 4 . Hence the number of elements in the group equals $4 k-2$, where $k$ is the number of orbits.

14 Let $\varphi$ be a required homomorphism, $A=\left(\begin{array}{ll}0 & 1 \\ 0 & 0\end{array}\right), B=\left(\begin{array}{ll}0 & 0 \\ 1 & 0\end{array}\right), O_{2}, I_{2}, O_{3}$, and $I_{3}$ be zero and identity matrices of size $2 \times 2$ and $3 \times 3$, respectively. Since $A^{2}=$ $B^{2}=O_{2},(A+B)^{2}=I_{2}$, we have $(\varphi(A))^{2}=(\varphi(B))^{2}=O_{3},(\varphi(A)+\varphi(B))^{2}=$ $I_{3}$. Then it is easy to verify that $\operatorname{rk} \varphi(A) \leq 1$ and $\operatorname{rk} \varphi(B) \leq 1$ (to do this, it suffices to consider the Jordan normal forms of the matrices). Hence rk $(\varphi(A)+\varphi(B)) \leq 2$, and the equality $(\varphi(A)+\varphi(B))^{2}=I_{3}$ is impossible, a contradiction.

Answer: there is no such homomorphism.

15 Since $d\left(\frac{x^{4}}{4}+\frac{y^{2}}{2}\right)=x^{3} d x+y d y=0$, for every solution it holds $\frac{x^{4}}{4}+\frac{y^{2}}{2}=$ const. Stationary solutions have a form $x=c, y=c$, where $c \in \mathbb{R}$ is fixed. Each other solution starts from the point $(-c,-c)$ and comes to the point $(c, c)$, where $c$ is such that $c \geq 0$ and $\frac{x^{4}}{4}+\frac{y^{2}}{2}=\frac{c^{4}}{4}+\frac{c^{2}}{2}$. Therefore, this solution is not periodic. Hence there is no periodic solution, which differs from constant function.

16

(a) To be specific let us assume that the null solution $x(t, 0, \overrightarrow{0}), t \geq 0$, is stable in the sense of Lyapunov. Consider an arbitrary $\varepsilon>0$ and $\delta=\delta(\varepsilon, 0)$ from the definition of stability. Since the system is autonomous, for every $t_{0}$ it holds

$$
x\left(t, t_{0}, \overrightarrow{0}\right)=x\left(t-t_{0}, 0, \overrightarrow{0}\right), t \geq t_{0},
$$

therefore $\left\|x_{0}\right\|<\delta$ implies that

$$
\left\|x\left(t, t_{0}, x_{0}\right)\right\|=\left\|x\left(t-t_{0}, 0, x_{0}\right)\right\|<\varepsilon, t \geq t_{0} .
$$

(b) Choose $R>0$ such that $f$ is a Lipschitz function on the set

$$
B_{R}=\left\{x \in \mathbb{R}^{n} \mid\|x\| \leq R\right\}
$$

and $r>0$ such that $x_{0} \in B_{r}$ implies $x\left(t, t_{0}, x_{0}\right) \in B_{R}, t \geq t_{0}$. For each $x_{0} \in B_{r}$ it holds $\left\|x\left(t, t_{0}, x_{0}\right)\right\| \rightarrow 0$, as $t \rightarrow \infty$. Show that $\lim _{t \rightarrow \infty}\left\|x\left(t, t_{0}, x_{0}\right)\right\|=0$ uniformly in $x_{0} \in B_{r}$. Fix an arbitrary $\varepsilon>0$. As a result of (a), there exists $\delta>0$ such that for every $t_{0} \in \mathbb{R}$ the inequality $\left\|x_{0}\right\|<\delta$ implies that $\left\|x\left(t, t_{0}, x_{0}\right)\right\|<\varepsilon, t \geq t_{0}$. For each $y \in B_{R}$, there exists $T_{y}>t_{0}$ such that $\left\|x\left(T_{y}, t_{0}, y\right)\right\|<\delta / 2$. Since $\left\|x\left(T_{y}, t_{0}, y\right)\right\|$ is continuous in $y$, there exists a neighborhood $B(y)$ of the point $y$ such that for $z \in B(y) \bigcap B_{r}$ we have $\left\|x\left(T_{y}, t_{0}, z\right)\right\|<\delta$. Hence for $z \in B(y) \bigcap B_{r}$ and $t \geq T_{y}$ we have

$$
\left\|x\left(t, t_{0}, z\right)\right\|=\left\|x\left(t-T_{y}, T_{y}, x\left(T_{y}, t_{0}, z\right)\right)\right\|<\varepsilon .
$$

The sets $B(y)$ form an open cover of the compact set $B_{r}$. Select a finite subcover $B\left(y_{1}\right), \ldots, B\left(y_{k}\right)$. Let $T(\varepsilon)=\max \left(T_{y_{1}}, \ldots, T_{y_{k}}\right)$. Then for $t \geq T(\varepsilon)$ and for all $x_{0} \in$ $B_{r}$ it holds $\left\|x\left(t, t_{0}, x_{0}\right)\right\|<\varepsilon$, which proves the uniform convergence.

17

(a) Let $\mu$ be a measure on $\mathbb{N}$ such that each positive integer has measure 1. Take arbitrary $b>a \geq 1$. By change of variables $n x \rightarrow x$ and Fubini's theorem we have

$$
\begin{gathered}
\int_{[a, b]}\left(\sum_{n=1}^{\infty} f(n x)\right) d \lambda(x)=\int_{[a, b]} \int_{\mathbb{N}} f(n x) d \mu(n) d \lambda(x)= \\
=\int_{\mathbb{N}} \int_{[a, b]} f(n x) d \lambda(x) d \mu(n)=\sum_{n=1}^{\infty} \frac{1}{n} \int_{n a}^{n b} f(x) d \lambda(x)= \\
=\int_{a}^{\infty} \sum_{\{n: n a \leq x \leq n b\}} \frac{1}{n} f(x) d \lambda(x) \leq \frac{b}{a} \int_{a}^{\infty} f(x) d \lambda(x)<+\infty
\end{gathered}
$$

because the sum $\sum_{\{n: n a \leq x \leq n b\}} \frac{1}{n}$ contains at most $\frac{x}{a}$ terms, each of which does not exceed $\frac{b}{x}$. Fubini's theorem implies the required statement.

(b) For every $\varepsilon>0$, choose $b>a>0$ large enough such that $\int_{a}^{\infty} f(x) d \lambda(x)<\varepsilon$ and $\frac{a}{b} \int_{1}^{\infty} f(x) d \lambda(x)<\varepsilon$. Then for each $T>b$ it holds

$$
\begin{aligned}
\frac{1}{T} \int_{1}^{T} x f(x) d \lambda(x) &=\frac{1}{T} \int_{1}^{a} x f(x) d \lambda(x)+\frac{1}{T} \int_{a}^{b} x f(x) d \lambda(x) \leq \\
& \leq \frac{a}{T} \int_{1}^{a} f(x) d \lambda(x)+\frac{T}{T} \int_{a}^{b} f(x) d \lambda(x) \leq \varepsilon+\varepsilon=2 \varepsilon
\end{aligned}
$$

18 Notice that $x+(\xi-x)_{+}=\xi+(x-\xi)_{+}=\max (x, \xi), x \geq 0$. Hence

$$
\begin{aligned}
x+f(x) &=\mathrm{E}\left(\xi+(x-\xi)_{+}\right)=\mathrm{E} \xi+\int_{0}^{x}(x-y) d \mathrm{P}(\xi<y)=\\
&=\mathrm{E} \xi+\int_{0}^{x} \mathrm{P}(\xi<y) d y .
\end{aligned}
$$

Therefore, there exists the derivative $(f(x)+x)^{\prime}=\mathrm{P}(\xi<x)$ almost everywhere, and

$$
\mathrm{E} e^{\xi}=\int_{0}^{+\infty} e^{x} d \mathrm{P}(\xi<x)=\int_{0}^{+\infty} e^{x} d\left((f(x)+x)^{\prime}\right) .
$$

19 Denote by $v(x)$ the number of passengers at the bus stop at a moment $x \geq 0$. If the passengers come to the stop at moments

$$
t_{1} \leq t_{2} \leq \ldots \leq t_{\nu(t)},
$$

then the waiting time of all the passengers equals

$$
\sum_{k=1}^{\nu(t)-1} k\left(t_{k+1}-t_{k}\right)+v(t)\left(t-t_{\nu(t)}\right)=\int_{0}^{t} v(x) d x .
$$

By Fubini's theorem we find

$$
\mathrm{E} \int_{0}^{t} v(x) d x=\int_{0}^{t} \mathrm{E} v(x) d x=\int_{0}^{t} \lambda x d x=\frac{1}{2} \lambda t^{2} .
$$

Answer: $\frac{1}{2} \lambda t^{2}$. 

\section{1}

1 On a circle of the unit length mark the points $k \pi, 1 \leq k \leq n_{0}$. For some $1 \leq k_{1}<$ $k_{2} \leq n_{0}$ the distance along the circle between the points $k_{1} \pi, k_{2} \pi$ does not exceed $\frac{1}{n_{0}}$, hence for $k_{0}=k_{2}-k_{1}, k_{0} \leq n_{0}$ and for some $l_{0} \in \mathbb{Z}$ it holds $\left|k_{0} \pi-l_{0}\right| \leq \frac{1}{n_{0}}$. Thus,

$$
\left|\sin l_{0}\right|=\left|\sin k_{0} \pi-\sin l_{0}\right| \leq\left|k_{0} \pi-l_{0}\right| \leq \frac{1}{n_{0}} .
$$

Since $\left|l_{0}\right|<\left|k_{0} \pi\right|+1<4 n_{0}+1$, we have $\left|l_{0} \sin l_{0}\right|<5$. It is evident that $l_{0} \rightarrow \infty$, as $n_{0} \rightarrow \infty$. Therefore, $|n \sin n| \nrightarrow+\infty$.

Answer: no, it is not.

2

(a) Suppose that for all $x$ it holds $f(x) f^{\prime \prime}(x)+2\left(f^{\prime}(x)\right)^{2}<0$. Then the function $f$ is not constant at any interval. If $f\left(x_{0}\right)=0$ then one can set $\theta=x_{0}$. Suppose that $f(x) \neq 0, x \in \mathbb{R}$. Then $f$ does not change the sign, e.g., $f(x)>0, x \in \mathbb{R}$. But in that case $\left(f^{3}\right)^{\prime \prime}=3 f\left(f f^{\prime \prime}+2\left(f^{\prime}\right)^{2}\right)<0$, so $f^{3}$ is concave, which is impossible for a positive nonconstant function.

(b) Show that, e.g., the function $G(t)=|t|^{3}$ has required property. It holds

$$
(G(f))^{\prime \prime}=3|f|\left(f f^{\prime \prime}+2\left(f^{\prime}\right)^{2}\right) .
$$

The necessity is evident.

Prove the sufficiency. Suppose that $G(f(x))$ is convex and show that

$$
|f(x)|\left(f(x) f^{\prime \prime}(x)+2\left(f^{\prime}(x)\right)^{2}\right) \geq 0, x \in \mathbb{R} .
$$

If $f(x) \neq 0$ for all $x \in \mathbb{R}$, then the required inequality holds. Otherwise the convexity of $|f(x)|^{3}$ implies that the set of zeros of $f(x)$ is convex, so this set is either a segment or a single point. If this set is a point then the inequality holds at this point due to continuity of $f^{\prime}$ and $f^{\prime \prime}$. If this set is a segment then $f^{\prime}=f^{\prime \prime}=0$ on the segment, and the inequality is evident.

3 Introduce $b_{n}=\ln \frac{2 a_{n}}{3}$. We have to show that $\lim _{n \rightarrow \infty} b_{n} \in\left(\frac{1}{4}, \frac{1}{2}\right)$. It is easy to check that $\frac{x}{2}<\ln (1+x)<x, x \in(0,1)$. Thus,

$$
\frac{1}{4}=\sum_{n \geq 2} 2^{-n-1}<\lim _{n \rightarrow \infty} b_{n}=\sum_{n \geq 2} \ln \left(1+2^{-n}\right)<\sum_{n \geq 2} 2^{-n}=\frac{1}{2},
$$

which finishes the proof.

4 Let $\left(a_{1}, a_{2}, \ldots, a_{n}\right)$ be a solution. Suppose that there are exactly $r$ distinct numbers among $a_{1}, \ldots, a_{n}$. We may assume the first $r$ numbers are distinct. Let $a_{j}$ repeats $m_{j}$ times, $1 \leq j \leq r$. After reduction of similar terms, we get

$$
m_{1} a_{1}^{k}+m_{2} a_{2}^{k}+\ldots+m_{r} a_{r}^{k}=0, \quad k=1,2, \ldots, n .
$$

Denote $y_{1}=m_{1} a_{1}, \ldots, y_{r}=m_{r} a_{r}$. We have

$$
a_{1}^{k-1} y_{1}+a_{2}^{k-1} y_{2}+\ldots+a_{r}^{k-1} y_{r}=0, \quad k=1,2, \ldots, n .
$$

The first $r$ equations form a system of linear equations with variables $y_{1}, y_{2}, \ldots$, $y_{r}$. Its determinant is the Vandermonde one for the numbers $a_{1}, a_{2}, \ldots, a_{r}$. These numbers are distinct, and hence the determinant is nonzero, so the system has a unique solution $y_{1}=y_{2}=\ldots=y_{r}=0$. Thus, $a_{1}=a_{2}=\ldots=a_{r}=0$, therefore, $r=1$ and the system has only zero solution.

Answer: $(0, \ldots, 0)$.

5 The columns of the matrix $\left(\begin{array}{l}B \\ D\end{array}\right)$ are linear combinations of the columns of the matrix $\left(\begin{array}{l}A \\ C\end{array}\right)$. Hence $B=A X$, and $D=C X$ for some matrix $X$. Thus, $X=A^{-1} B$ and $D=C X=C A^{-1} B$.

6 Write out all the couples consisting of a permutation and an element which is a fixed point of this permutation. The number of such couples equals $\sum_{k=1}^{n} k \cdot b(n, k)$.

On the other hand, for each of $n$ elements, there exist $(n-1)$ ! permutations for which this element is a fixed point. Thus, the total number of the couples equals $n \cdot(n-1) !=n !$

Answer: $n$ !

8 A linear combination of bounded solutions is a bounded solution as well, and hence $B$ is a subspace. Let $x(t)$ be a bounded solution to $(*)$. Decompose $x(0)=a_{1}+a_{2}$, where $a_{1} \in B$ and $a_{2} \in B^{\perp}$. Denote by $x_{1}(t)$ a solution to the homogeneous equation $\frac{d x}{d t}=A(t) x$ with initial condition $x_{1}(0)=a_{1}$, which is bounded by definition of the subspace $B$. Then $x_{2}(t)=x(t)-x_{1}(t)$ is a bounded solution to the equation $(*)$, and $x_{2}(0)=x(0)-a_{1}=a_{2} \in B^{\perp}$. To prove the uniqueness consider the difference of two bounded solutions to $(*)$ with initial conditions in $B^{\perp}$. It is a bounded solution to the homogeneous equation which starts in $B^{\perp}$, i.e., zero solution.

9 A solution is similar to the solution of Problem 6.

Answer: 1.

10 Denote by $g(r)$ the radius of a circle which is the image of a circle with center at the origin and radius $r$. By the maximum principle the function $g(r)$ is monotone. Therefore, either $f$ is an analytic function on $\mathbb{C}$ or it has a pole at zero.

Let $f$ be analytic on $\mathbb{C}$. If $f(0) \neq 0$ then the function $\frac{1}{f(z)}$ is bounded and analytic on $\mathbb{C}$, whence $f=$ const. If $f(0)=0$, denote by $r$ an order of zero at point 0 . The function $\frac{f(z)}{z^{r}}$ differs from 0 at zero, so by previous reasonings it is constant, i.e., $f(z)=C z^{r}, r \in \mathbb{N}$. In a similar way we get that if the function $f$ has a pole at zero then $f(z)=C z^{r}$, where $-r \in \mathbb{N}$.

Answer: $f(z)=C z^{r}, r \in \mathbb{Z}$.

11 Show that a convex set in $\mathbb{R}^{n}$ which contains a cone coincides with $\mathbb{R}^{n}$. It suffices to consider the cone $\left\{\left(x_{1}, \ldots, x_{n}\right): x_{1}^{2}+\ldots+x_{n-1}^{2} \leq r x_{n}^{2}\right\}$. For each point $\left(y_{1}, \ldots, y_{n}\right) \in \mathbb{R}^{n}$, there exists $N$ large enough such that the points $\left(y_{1}, \ldots, y_{n-1}\right.$, $-N)$ and $\left(y_{1}, \ldots, y_{n-1}, N\right)$ belong to the cone and therefore, to the convex set. Since the set is convex, it contains the point $\left(y_{1}, \ldots, y_{n-1}, y_{n}\right)$ as well. Thus, the set coincides with $\mathbb{R}^{n}$.

A projection of a convex set is a convex set, and hence the statement of the problem is equivalent to the following: if $A$ is an unbounded convex set in $\mathbb{R}^{n}$ and $A \neq \mathbb{R}^{n}$, then there exists a two-dimensional subspace $B \subset \mathbb{R}^{n}$ such that the projection of $A$ onto $B$ does not coincide with $B$. By the Separating Axis Theorem, there exists a line $\ell$ in $\mathbb{R}^{n}$ such that the projection of $A$ onto $\ell$ does not coincide with $\ell$. Then an arbitrary two-dimensional subspace $B \subset \mathbb{R}^{n}$ for which $\ell \subset B$ has the required property.

Remark 1 The statement of the problem is still correct for one-sided cones, i.e., sets which are obtained by shift and rotation of the set

$$
\left\{\left(x_{1}, \ldots, x_{n}\right): x_{1}^{2}+\ldots+x_{n-1}^{2} \leq r x_{n}^{2}, x_{n} \geq 0\right\}
$$

for some $r>0$. Since $A$ is an unbounded convex set, it contains some ray $\ell$. Let $O$ be the beginning of the ray $\ell$ and $\gamma$ be a hyperplane orthogonal to $\ell$, which intersects $\ell$ at a point different from $O$. Introduce a set $G$ of all intersection points of $\gamma$ and the rays which start at $O$ and are contained in $A$. This set is convex and does not contain any $(n-1)$-dimensional ball in $\gamma$ because $A$ does not contain a cone. Hence $G$ is contained in some $(n-2)$-dimensional subspace. The two-dimensional subspace $B$, which is orthogonal to this subspace, has the required property. Indeed, the projection of $A$ onto $B$ contains $\ell$. Show that it does not contain any other ray. Assume that it contains some ray $m$. Consider the subset of $A$ which is projected onto $m$. This set is unbounded and convex; therefore, it contains some ray. But the ray cannot intersect $\gamma$ at a point from $G$, a contradiction.

12 By the strong law of large numbers

$$
\frac{1}{n} \sum_{k=1}^{n} \gamma_{n}^{2} \stackrel{\mathrm{P} 1}{\rightarrow} \mathrm{E} \gamma_{1}^{2}=1, \text { as } n \rightarrow \infty
$$

Show that $\frac{1}{\ln n} \max _{1 \leq k \leq n} \gamma_{k}^{2} \stackrel{\mathrm{P}}{\rightarrow} 2$, as $n \rightarrow \infty$. Consider

$$
F_{n}(x)=\mathrm{P}\left(\frac{1}{\ln n} \max _{1 \leq k \leq n} \gamma_{k}^{2} \leq x\right), x \geq 0 .
$$

It suffices to prove that if $x<2$ then $F_{n}(x) \rightarrow 0$, as $n \rightarrow \infty$, and if $x \geq 2$ then $F_{n}(x) \rightarrow 1$, as $n \rightarrow \infty$

Random variables $\gamma_{1}, \ldots, \gamma_{n}$ are independent, hence

$$
F_{n}(x)=\mathrm{P}\left(\gamma_{k}^{2} \leq x \ln n, 1 \leq k \leq n\right)=\mathrm{P}^{n}\left(\gamma_{1}^{2} \leq x \ln n\right)=\left(2 \int_{0}^{\sqrt{x \ln n}} \frac{e^{-t^{2} / 2}}{\sqrt{2 \pi}} d t\right)^{n} .
$$

Show that

$$
\frac{1}{2 t} e^{-t^{2} / 2}<\int_{t}^{+\infty} e^{-s^{2} / 2} d s<\frac{1}{t} e^{-t^{2} / 2}, \quad t \geq 1 .
$$

We have

$$
\int_{t}^{+\infty} e^{-s^{2} / 2} d s<\int_{t}^{+\infty} \frac{s}{t} e^{-s^{2} / 2} d s=\frac{1}{t} e^{-t^{2} / 2} .
$$

To prove the left-hand side inequality notice that the function $y=e^{-x^{2} / 2}$ is convex for $x \geq 1\left(y^{\prime \prime}(x)=\left(x^{2}-1\right) e^{-x^{2} / 2} \geq 0\right)$, hence the area under the graph $y=e^{-x^{2} / 2}$ exceeds the area of the triangle, which is bounded by the straight lines $x=t$ and $y=0$ and the tangent to the graph at point $t$, that is $y=-t e^{-t^{2} / 2}(x-t)+e^{-t^{2} / 2}$.

For $0<x<2-\varepsilon, 0<\varepsilon<1$, and for $n$ large enough $(*)$ implies that

$$
\begin{aligned}
F_{n}(x) & \leq\left(1-2 \cdot \frac{e^{-x \ln n / 2}}{2 \sqrt{2 \pi x \ln n}}\right)^{n}<\\
&<\left(1-\frac{1}{\sqrt{4 \pi \ln n} n^{1-\varepsilon / 2}}\right)^{n}<\left(1-\frac{1}{n^{1-\varepsilon / 3}}\right)^{n} \rightarrow 0, \text { as } n \rightarrow \infty .
\end{aligned}
$$

For $x \geq 2$ due to $(*)$ we have

$$
\begin{aligned}
1 \geq F_{n}(x) & \geq\left(1-2 \cdot \frac{e^{-x \ln n / 2}}{\sqrt{2 \pi x \ln n}}\right)^{n} \geq \\
& \geq\left(1-\sqrt{\frac{1}{\pi \ln n}} \cdot \frac{1}{n}\right)^{n} \geq 1-\sqrt{\frac{1}{\pi \ln n}} \rightarrow 1, \text { as } n \rightarrow \infty .
\end{aligned}
$$

Hence $\frac{1}{\ln n} \max _{1 \leq k \leq n} \gamma_{k}^{2} \stackrel{\mathrm{P}}{\rightarrow} 2$, as $n \rightarrow \infty$, therefore,

$$
\frac{\max _{1 \leq k \leq n} \gamma_{k}^{2}}{\sum_{k=1}^{n} \gamma_{k}^{2}}: \frac{\ln n}{n} \stackrel{\mathrm{P}}{\rightarrow} 2 \text {, as } n \rightarrow \infty .
$$

1 Enumerate all the rational numbers $\left\{r_{i}, i \geq 1\right\}$. Introduce sets

$$
A_{i}^{+}=\left\{(u, v): u>r_{i}>v\right\} \text { and } A_{i}^{-}=\left\{(u, v): u<r_{i}<v\right\} .
$$

There are no $x, y, z$ such that both points $(x, y)$ and $(y, z)$ belong to one of the sets $A_{i}^{+}$or $A_{i}^{-}$. It is evident that

$$
\cup_{i \geq 1}\left(A_{i}^{+} \cup A_{i}^{-}\right) \cup A_{0}=\mathbb{R}^{2},
$$

where $A_{0}=\{(x, x): x \in \mathbb{R}\}$. Renumber the sets $A_{0}$ and $A_{i}^{+}, A_{i}^{-}, i \geq 1$, as $B_{1}$, $B_{2}, \ldots$, and put $F(x, y)=\min \left\{k \in \mathbb{N}:(x, y) \in B_{k}\right\}$. For distinct points $(x, y)$ and $(y, z)$ it holds $F(x, y) \neq F(y, z)$, so the function $F$ has required property.

Answer: a function does exist.

2 For $x=\pi$ it holds $y=1+\frac{1}{a}$. If $1 \leq a \leq 2.5$ then $1.4 \leq y \leq 2$. Hence the distance from the point $M(\pi, 1.7)$ to each of the graphs in question does not exceed $0.3<0.4$.

3 Introduce a function $g(x)=f(x) \cdot e^{-\arctan x}, x \in[-1,1]$. Since $g(-1)=$ $g(1)=0$, by Rolle's theorem there exists a point $x \in(-1,1)$ such that $g^{\prime}(x)=0$, i.e.,

$$
f^{\prime}(x) e^{-\arctan x}-f(x) e^{-\arctan x} \cdot \frac{1}{1+x^{2}}=0, \quad f(x)=\left(1+x^{2}\right) f^{\prime}(x) .
$$

4 Let $J$ be the matrix with all entries equal to 1 and $I$ be the identity matrix. We have $I=J-A-A^{\mathrm{T}}$. It holds $\mathrm{rk} J=1$. Assume that $\mathrm{rk} A<n-1$. Then $\mathrm{rk} A+\mathrm{rk} J<$ $n$, so there exists a nonzero vector $v$, for which $A v=0$ and $J v=0$. It follows that 

$$
(v, v)=v^{\mathrm{T}}\left(J-A-A^{\mathrm{T}}\right) v=v^{\mathrm{T}} J v-v^{\mathrm{T}} A v-(A v)^{\mathrm{T}} v=0,
$$

a contradiction.

5 Use the change of variables $y=\frac{\pi}{2}-x$. We get

$$
\int_{0}^{\frac{\pi}{2}} \frac{(\cos x)^{\sin x}}{(\cos x)^{\sin x}+(\sin x)^{\cos x}} d x=\int_{0}^{\frac{\pi}{2}} \frac{(\sin y)^{\cos y}}{(\cos y)^{\sin y}+(\sin y)^{\cos y}} d y
$$

whence $2 I=\int_{0}^{\frac{\pi}{2}} 1 d x=\frac{\pi}{2}$. Therefore, $I=\frac{\pi}{4}<1$.

6 Each linear operator $\varphi$ on $M_{n}(\mathbb{R})$ is determined by a collection of numbers $\varphi_{i j k l}$, $1 \leq i, j, k, l \leq n$ as follows: if $A=\left(a_{i j}\right)$ then $\varphi(A)=\left(a_{i j}^{\varphi}\right)$, where

$$
a_{i j}^{\varphi}=\sum_{k, l=1}^{n} \varphi_{i j k l} a_{k l} .
$$

The condition $\varphi\left(A^{\mathrm{T}}\right)=(\varphi(A))^{\mathrm{T}}$ means that $\varphi_{i j k l}=\varphi_{\text {jilk }}$ for all $i, j, k, l$. Hence in order to determine $\varphi$ one has to choose the coefficients $\varphi_{i j k l}$ for $i<j$ and arbitrary $k, l$, and also for $i=j$ and $k \leq l$. Thus, one has to cast

$$
\frac{n(n-1)}{2} \cdot n^{2}+n \cdot \frac{n(n+1)}{2}=\frac{n^{2}\left(n^{2}+1\right)}{2}
$$

numbers. This is the dimension of the subspace in question.

Answer: $\frac{n^{2}\left(n^{2}+1\right)}{2}$.

7 By induction on $k$ prove that $\frac{a_{k}}{e}$ is an integer for each $k \geq 1$. It holds $a_{1}=e$. Assume that $a_{k}=e \cdot b_{k}, k \leq n$, where $b_{k}$ are some integers. Then

$$
\begin{aligned}
a_{n+1} &=\sum_{j=1}^{\infty} \frac{j^{n+1}}{j !}=\sum_{j=1}^{\infty} \frac{j^{n}}{(j-1) !}=\sum_{j=0}^{\infty} \frac{(j+1)^{n}}{j !}=\\
&=\sum_{j=1}^{\infty} \sum_{m=0}^{n}\left(\begin{array}{c}
n \\
m
\end{array}\right) \frac{j^{m}}{j !}=\sum_{m=0}^{n}\left(\begin{array}{c}
n \\
m
\end{array}\right) \cdot \sum_{j=0}^{\infty} \frac{j^{m}}{j !}=e+\sum_{m=1}^{n}\left(\begin{array}{c}
n \\
m
\end{array}\right) \cdot \sum_{j=1}^{\infty} \frac{j^{m}}{j^{\prime}}=\\
&=e+\sum_{m=1}^{n}\left(\begin{array}{c}
n \\
m
\end{array}\right) a_{m}=e\left(1+\sum_{m=1}^{n}\left(\begin{array}{c}
n \\
m
\end{array}\right) b_{m}\right),
\end{aligned}
$$

so $\frac{a_{n+1}}{e}$ is an integer, and the statement is proved. Hence $a_{k} \notin \mathbb{Q}, k \geq 1$. 8 First show that $f \in C^{(\infty)}(\mathbb{R})$. To do this integrate the left-hand and right-hand sides of the equation over $y \in[0,1]$ set $z=0$, and change variables in the integrals in the right-hand side:

$$
\begin{gathered}
\int_{0}^{1}(f(x)+f(y)+f(0)) d y= \\
=\int_{0}^{1}\left(f\left(\frac{3}{7} x+\frac{6}{7} y\right)+f\left(\frac{6}{7} x-\frac{2}{7} y\right)+f\left(-\frac{2}{7} x+\frac{3}{7} y\right)\right) d y, \\
f(x)+\int_{0}^{1} f(y) d y+f(0)= \\
=\int_{\frac{3}{7} x}^{\frac{3}{7} x+1} f\left(\frac{6}{7} y\right) d y+\int_{\frac{6}{7} x}^{\frac{6}{7} x+1} f\left(-\frac{2}{7} y\right) d y+\int_{-\frac{2}{7} x}^{-\frac{2}{7} x+1} f\left(\frac{3}{7} y\right) d y .
\end{gathered}
$$

Since $f \in C(\mathbb{R})$, by the theorem on differentiating with respect to the limit of integration the right-hand side of the latter equality belongs to $C^{(1)}(\mathbb{R})$, hence $f \in C^{(1)}(\mathbb{R})$. Then the same equality implies that $f \in C^{(2)}(\mathbb{R})$, etc. Differentiate the initial equation three times with respect to $x$, and set $x=y=z$. We get

$$
f^{\prime \prime \prime}(x)=\left(\left(\frac{3}{7}\right)^{3}+\left(\frac{6}{7}\right)^{3}+\left(-\frac{2}{7}\right)^{3}\right) f^{\prime \prime \prime}(x),
$$

whence $f^{\prime \prime \prime}(x) \equiv 0, x \in \mathbb{R}$. Thus, $f$ is a polynomial of at most second degree. It is easy to verify that the functions $1, x$, and $x^{2}$ satisfy the condition of the problem. Therefore, every polynomial of at most second degree satisfies the condition. Answer: $f(x)=a x^{2}+b x+c, a, b, c \in \mathbb{R}$.

9 Let $A \subset[0,1]$ be the Cantor set, i.e., the set of the numbers $\sum_{k=1}^{\infty} \frac{\alpha_{k}}{3^{k}}$, where $\alpha_{k} \in$ $\{0,2\}$ for all $k \geq 1$. Put

$$
f\left(\sum_{k=1}^{\infty} \frac{\alpha_{k}}{3^{k}}\right)=\sum_{k=1}^{\infty} \frac{\alpha_{k}}{3^{3 k+1}} .
$$

Let $a_{1}=\sum_{k=1}^{\infty} \frac{\alpha_{k}^{(1)}}{3^{k}}$ and $a_{2}=\sum_{k=1}^{\infty} \frac{\alpha_{k}^{(2)}}{3^{k}}$. Assume that $a_{1}<a_{2}$ and the first difference between the ternary notations of the numbers $a_{1}$ and $a_{2}$ occurs at $n$th digit after point, i.e., $\alpha_{1}^{(1)}=\alpha_{1}^{(2)}, \ldots, \alpha_{n-1}^{(1)}=\alpha_{n-1}^{(2)}, \alpha_{n}^{(1)}=0$, and $\alpha_{n}^{(2)}=2$. Then

$$
a_{2}-a_{1} \geq \frac{2}{3^{n}}-\sum_{k=n+1}^{\infty} \frac{2}{3^{k}}=\frac{1}{3^{n}}
$$

and

$$
0<f\left(a_{2}\right)-f\left(a_{1}\right) \leq \sum_{k=n}^{\infty} \frac{2}{3^{3 k+1}}=\frac{9}{13} \cdot \frac{1}{3^{3 n}}<\left|a_{2}-a_{1}\right|^{3} .
$$

Hence the function $f$ satisfies the condition $\left|f\left(a_{2}\right)-f\left(a_{1}\right)\right|<\left|a_{2}-a_{1}\right|^{3}$. Since $f$ is injective and the set $A$ is uncountable, the range of $f$ is uncountable as well.

10 Take any vertex $A$ of the prismatoid and divide each face, which does not contain the vertex $A$, into triangles by its diagonals. The prismatoid can be dissected into triangular pyramids with vertex $A$, the bases of which are these triangles. The constructed pyramids are prismatoids with the same bases as initial prismatoid, because the dissection does not create new vertices. It remains to check the the statement of the problem holds true for a prismatoid, which is a triangular pyramid, in two cases: (a) three vertices belong to one of the two parallel planes and another plane contains the fourth vertex, and (b) each of the two parallel planes contains two vertices.

11 Consider sets $A_{n, k}=\left\{x \in \Omega \mid \frac{k}{n} \leq \xi(x)<\frac{k+1}{n}\right\} \in \mathscr{F}$, where $n \geq 1, k \in \mathbb{Z}$. Then for every set $A \in \mathscr{F}$ it holds

$$
\frac{k}{n} \mathrm{P}\left(A \cap A_{n, k}\right) \leq \omega\left(A \cap A_{n, k}\right) \leq \frac{k+1}{n} \mathrm{P}\left(A \cap A_{n, k}\right)
$$

and

$$
\frac{k}{n} \mathrm{P}\left(A \cap A_{n, k}\right) \leq \int_{A \cap A_{n, k}} \xi(x) d \mathrm{P}(x) \leq \frac{k+1}{n} \mathrm{P}\left(A \cap A_{n, k}\right) .
$$

Thus,

$$
\begin{aligned}
\left|\omega(A)-\int_{A} \xi(x) d \mathrm{P}(x)\right| & \leq \sum_{k \in \mathbb{Z}}\left|\omega\left(A \cap A_{n, k}\right)-\int_{A \cap A_{n, k}} \xi(x) d \mathrm{P}(x)\right| \leq \\
& \leq \frac{1}{n} \cdot \sum_{k \in \mathbb{Z}} \mathrm{P}\left(A \cap A_{n, k}\right) \leq \frac{1}{n} \rightarrow 0, \text { as } n \rightarrow \infty,
\end{aligned}
$$

whence $\omega(A)=\int_{A} \xi(x) d \mathrm{P}(x), A \in \mathscr{F}$.

12 Since the functions $f_{n}(x)$ are $2 \pi$-periodic, it suffices to deal with the interval $[0,2 \pi]$. Moreover, for each $x$ there exists $x_{0} \in\left[0, \frac{\pi}{2}\right]$ such that $\sin x_{0}=|\sin x|$ and $\cos x_{0}=|\cos x|$, hence $f_{n}\left(x_{0}\right) \geq f_{n}(x)$. Therefore, it suffices to consider $x \in$ $\left[0, \frac{\pi}{2}\right]$. Finally, the graph $y=f_{n}(x)$ has an axis of symmetry $x=\frac{\pi}{4}$. Therefore, we can consider $x \in\left[0, \frac{\pi}{4}\right]$. For each $n$, the global maximum of $f_{n}(x)$ is attained at some point $x_{n} \in\left[0, \frac{\pi}{4}\right]$. A point of global maximum satisfies the conditions

$$
f^{\prime}(x)=\ln n\left(n^{\sin x} \cdot \cos x-n^{\cos x} \cdot \sin x\right)=0
$$

and

$f^{\prime \prime}(x)=\ln ^{2} n\left(n^{\sin x} \cdot \cos ^{2} x+n^{\cos x} \cdot \sin ^{2} x\right)-\ln n\left(n^{\sin x} \cdot \sin x+n^{\cos x} \cdot \cos x\right) \leq 0$.

Since $n^{\sin x} \cdot \cos x=n^{\cos x} \cdot \sin x$, in case $\cos x \geq \sin x>\frac{1}{\ln n}$ it holds

$$
f^{\prime \prime}(x)=\ln n\left(n^{\cos x} \cdot \cos x(\ln n \cdot \sin x-1)+n^{\sin x} \cdot \sin x(\ln n \cdot \cos x-1)\right)>0,
$$

hence the point of global maximum $x_{n} \in\left[0, \frac{\pi}{4}\right]$ satisfies the condition $\sin x_{n} \leq \frac{1}{\ln n}$, so $x_{n}<\arcsin \frac{1}{\ln n} \rightarrow 0$, as $n \rightarrow \infty$

13 Verify that

$$
P_{U^{\mathrm{T}} L}\left(U^{-1} a\right)=P_{U^{\mathrm{T}} L}\left(U^{-1} P_{L} a\right) \text {, that is }\left(U^{-1} a-U^{-1} P_{L} a\right) \perp U^{\mathrm{T}} L .
$$

Indeed, for each $\ell \in L$ it holds

$$
\left(U^{-1} a-U^{-1} P_{L} a, U^{\mathrm{T}} \ell\right)=\left(U U^{-1}\left(a-P_{L} a\right), \ell\right)=\left(a-P_{L} a, \ell\right)=0
$$

by the definition of $P_{L}$. Hence

$$
\left\|P_{U^{\mathrm{T}} L}\left(U^{-1} a\right)\right\|=\left\|P_{U^{\mathrm{T}} L}\left(U^{-1} P_{L} a\right)\right\| \leq\left\|U^{-1} P_{L} a\right\| \leq\left\|U^{-1}\right\| \cdot\left\|P_{L} a\right\| .
$$

14 Let $z(t)=x(t)+i y(t), x(t), y(t) \in \mathbb{R}$. Then $x(t), y(t)$ satisfy the system of equations

$$
\left\{\begin{array}{l}
\frac{d x}{d t}=-y f(x+i y) \\
\frac{d y}{d t}=x f(x+i y)
\end{array}\right.
$$

whence $\frac{d}{d t}\left(x^{2}+y^{2}\right)=x \frac{d x}{d t}+y \frac{d y}{d t}=0$, and $x^{2}(t)+y^{2}(t)=|z(t)|^{2}=$ const. So it is convenient to use the polar coordinates. Let $z(t)=|z(0)| \cdot e^{i \varphi(t)}=r e^{i \varphi(t)}$. We have $\frac{d z}{d t}=i r e^{i \varphi} \frac{d \varphi}{d t}=i z f(z)=i r e^{i \varphi} f\left(r e^{i \varphi}\right)$, so $\frac{d \varphi}{d t}=f\left(r e^{i \varphi}\right)$. Solve the differential equation and get

$$
\int \frac{d \varphi}{f\left(r e^{i \varphi}\right)}=\int d t
$$

whence $F_{r}(2 \pi)-F_{r}(0)=t_{r}$, where $F_{r}(\varphi)$ is the primitive of $\frac{1}{f\left(r e^{i \varphi}\right)}$, and $t_{r}$ is the when the solution which comes from the point $r=r \cdot e^{i \cdot 0}$ returns to the point $r=r$. $e^{i \cdot 2 \pi}$. Notice that the function $\frac{1}{f\left(r e^{i \varphi}\right)}$ is integrable, because for each $r_{0}$ the continuous function $f$ is bounded from below on the compact set $\left\{r=r_{0}, 0 \leq \varphi \leq 2 \pi\right\}$ by some constant $M_{r_{0}}>0$. By the theorem on continuity of an integral with respect to a parameter, the function $F_{r}$ is continuous in $r$, hence $t_{r}$ is a continuous function of $r$ as well. Since the solution $z(t)=r e^{i \varphi(t)}, z(0)=r$ has a period $t_{r}$, it remains to notice that 

$$
\begin{gathered}
t_{r} \geq 2 \pi \cdot\left(\max _{\varphi} f\left(r e^{i \varphi}\right)\right)^{-1} \rightarrow \infty, \text { as } r \rightarrow 0+, \\
t_{r} \leq 2 \pi\left(\min _{\varphi} f\left(r e^{i \varphi}\right)\right)^{-1} \rightarrow 0, \text { as } r \rightarrow \infty,
\end{gathered}
$$

hence the continuity of $t_{r}$ implies that for each $T>0$ there exists $R$, for which $t_{R}=T$. The solution to the equation $\frac{d z}{d t}=i z f(z)$ with initial condition $z(0)=R$ is periodic with a period of $T$. 1 Decompose the fraction $\frac{9 n+4}{n(3 n+1)(3 n+2)}$ into a sum of partial fractions:

$$
\begin{aligned}
&\sum_{n=1}^{N} \frac{9 n+4}{n(3 n+1)(3 n+2)}=\sum_{n=1}^{N}\left(\frac{2}{n}-\frac{3}{3 n+1}-\frac{3}{3 n+2}\right)= \\
&=\sum_{n=1}^{N}\left(\frac{3}{n}-\frac{3}{3 n}-\frac{3}{3 n+1}-\frac{3}{3 n+2}\right)=3\left(\sum_{n=1}^{N} \frac{1}{n}-\sum_{n=3}^{3 N+2} \frac{1}{n}\right)= \\
&=3\left(1+\frac{1}{2}-\sum_{n=N+1}^{3 N+2} \frac{1}{n}\right)=\frac{9}{2}-3 \sum_{k=1}^{2 N+2} \frac{1}{N+k}= \\
&=\frac{9}{2}-3 \cdot \frac{1}{N} \sum_{k=1}^{2 N+2} \frac{1}{1+\frac{k}{N}} \rightarrow \frac{9}{2}-3 \int_{0}^{2} \frac{d x}{1+x}=\frac{9}{2}-3 \ln 3, \text { as } N \rightarrow \infty .
\end{aligned}
$$

Answer: $\frac{9}{2}-3 \ln 3$.

2 Estimate $\max _{1 \leq n \leq N}\{\sqrt{n}\}$. For $k^{2} \leq n<(k+1)^{2}$ it holds

$$
\{\sqrt{n}\}=\sqrt{n}-k \leq \sqrt{(k+1)^{2}-1}-k,
$$

where equality is attained for $n=(k+1)^{2}-1$. If $k<l$ then

$$
\begin{aligned}
1 &-\left\{\sqrt{(k+1)^{2}-1}\right\}=k+1-\sqrt{(k+1)^{2}-1}=\\
&=\frac{1}{k+1+\sqrt{(k+1)^{2}-1}}>\frac{1}{l+1+\sqrt{(l+1)^{2}-1}}=1-\left\{\sqrt{(l+1)^{2}-1}\right\},
\end{aligned}
$$

hence $\left\{\sqrt{(k+1)^{2}-1}\right\}$ is increasing in $k$. Thus, for $N=m^{2}-1$ we get

$$
\max _{1 \leq n \leq N}\{\sqrt{n}\}=\left\{\sqrt{m^{2}-1}\right\}=\sqrt{m^{2}-1}-m+1 .
$$

Now consider $m^{2} \leq N \leq(m+1)^{2}-1$. Since $\max _{1 \leq n \leq N}\{\sqrt{n}\}$ is increasing in $N$, it holds

$$
\sqrt{m^{2}-1}-m+1 \leq \max _{1 \leq n \leq N}\{\sqrt{n}\} \leq \sqrt{(m+1)^{2}-1}-m .
$$

Therefore,

$$
1+m-\sqrt{(m+1)^{2}-1} \leq 1-\max _{1 \leq n \leq N}\{\sqrt{n}\} \leq m-\sqrt{m^{2}-1}
$$

Also we have $m \leq \sqrt{N}<m+1$, so

$$
\begin{aligned}
&\frac{m}{m+1} \cdot(m+1)\left(m+1-\sqrt{(m+1)^{2}-1}\right) \leq \\
&\quad \leq \sqrt{N}\left(1-\max _{1 \leq n \leq N}\{\sqrt{n}\}\right)<\frac{m+1}{m} \cdot m\left(m-\sqrt{m^{2}-1}\right) .
\end{aligned}
$$

Since $m\left(m-\sqrt{m^{2}-1}\right)=\frac{m}{m+\sqrt{m^{2}-1}} \rightarrow \frac{1}{2}$ and $\frac{m}{m+1} \rightarrow 1$, as $m \rightarrow \infty$, we have $\lim _{N \rightarrow \infty} \sqrt{N}\left(1-\max _{1 \leq n \leq N}\{\sqrt{n}\}\right)=\frac{1}{2}$

Answer: $\frac{1}{2}$.

3 For $k<n$ the expansion of $\vec{x}$ in $\overrightarrow{x_{1}}, \ldots, \overrightarrow{x_{k}}$ exists not for all $\vec{x}$. For $k=n$ the expansion is unique, and for some $\vec{x}$ it contains negative coefficients. Therefore, $k \geq n+1$. The collection of $k=n+1$ vectors $\overrightarrow{x_{1}}=(1,0, \ldots, 0), \overrightarrow{x_{2}}=$ $(0,1,0, \ldots, 0), \quad \ldots, \quad \overrightarrow{x_{n}}=(0, \ldots, 0,1), \quad \vec{x}_{n+1}=(-1,-1, \ldots,-1)$ has the required property. Indeed, for every vector $\vec{x}$, one can select $a_{n+1}>0$ such that all the coordinates of the vector

$$
\vec{x}-a_{n+1} \vec{x}_{n+1}=\vec{x}+a_{n+1}(1,1, \ldots, 1)
$$

are positive, and then $\vec{x}=\sum_{i=1}^{n} a_{i} \vec{x}_{i}+a_{n+1} \vec{x}_{n+1}$, where $a_{1}, \ldots, a_{n}$ are positive.

Answer: $k=n+1$.

4 For $n=1$ every matrix is proportional to the identity matrix, and one can take $A=(1), B=(0)$, $\operatorname{rk} A+\operatorname{rk} B=1=n$. For $n \geq 2$ let $A$ be the diagonal matrix $A=\operatorname{diag}(0,1, \ldots, n-1)$, rk $A=n-1$. If a matrix $X=\left(x_{i j}\right)$ commutes with $A$ then $(i-1) x_{i j}=(j-1) x_{i j}$ for all $i, j$, whence $x_{i j}=0$ for $i \neq j$. Thus, $X$ is a diagonal matrix. Let all the entries of the matrix $B$ equal 1 . Then rk $B=1$, so rk $A+\operatorname{rk} B=n$. If the matrix $X=\operatorname{diag}\left(x_{11}, x_{22}, \ldots, x_{n n}\right)$ commutes with $B$, then for all $i, j$ the equality $x_{i i}=x_{j j}$ holds. Hence the matrix $X$ is proportional to the identity matrix. Therefore, $n \times n$ matrices $A$ and $B$ with required properties exist for every $n \in \mathbb{N}$.

Answer: for all $n \in \mathbb{N}$.

5 By taking the logarithms of both sides of the inequality we get

$$
\sum_{k=2}^{n} \frac{1}{k !} \ln k<\ln 2, n \geq 2 .
$$

The function $\ln x$ is concave on $(0,+\infty)$. By Jensen's inequality we have

$$
\sum_{k=2}^{n} \frac{1}{k !} \ln k \leq \sum_{k=2}^{n} \frac{1}{k !} \cdot \ln \frac{\sum_{k=2}^{n} \frac{1}{k !} \cdot k}{\sum_{k=2}^{n} \frac{1}{k !}}=\sum_{k=2}^{n} \frac{1}{k !} \ln \left(\frac{\sum_{k=1}^{n-1} \frac{1}{k !}}{\sum_{k=2}^{n} \frac{1}{k !}}\right)
$$

Since the sequence $\sum_{k=2}^{n} \frac{1}{k !} \ln k$ is increasing, we get

$$
\begin{aligned}
\sum_{k=2}^{n} \frac{1}{k !} \ln k &<\sum_{k=2}^{\infty} \frac{1}{k !} \ln k \leq \\
& \leq \lim _{n \rightarrow \infty} \sum_{k=2}^{n} \frac{1}{k !} \ln \left(\frac{\sum_{k=1}^{n-1} \frac{1}{k !}}{\sum_{k=2}^{n} \frac{1}{k !}}\right)=\sum_{k=2}^{\infty} \frac{1}{k !} \ln \left(\frac{\sum_{k=1}^{\infty} \frac{1}{k !}}{\sum_{k=2}^{\infty} \frac{1}{k !}}\right)=\\
&=(e-2) \ln \frac{e-1}{e-2}=(e-2) \ln \left(1+\frac{1}{e-2}\right) .
\end{aligned}
$$

It remains to show that

$$
(e-2) \ln \left(1+\frac{1}{e-2}\right)<\ln 2 \text {, or }\left(1+\frac{1}{e-2}\right)^{e-2}<2 .
$$

For $a>-1$ the function $y(x)=(1+a)^{x}$ is convex on $\mathbb{R}$, hence for $x \in(0,1)$ its graph lies under the secant $y=1+a x$, that is $(1+a)^{x}<1+a x, 0<x<1$. In particular for $a=\frac{1}{e-2}$ and $x=e-2$ we get

$$
\left(1+\frac{1}{e-2}\right)^{e-2}<1+\frac{e-2}{e-2}=2 .
$$

6 For $n \geq 0$ it holds

$$
\frac{x^{3^{n}}+\left(x^{3^{n}}\right)^{2}}{1-x^{3^{n+1}}}=\frac{1+x^{3^{n}}+x^{2 \cdot 3^{n}}}{\left(1-x^{3^{n}}\right)\left(1+x^{3^{n}}+x^{2 \cdot 3^{n}}\right)}-\frac{1}{1-x^{3^{n+1}}}=\frac{1}{1-x^{3^{n}}}-\frac{1}{1-x^{3^{n+1}}} .
$$

Therefore,

$$
\sum_{n=0}^{N} \frac{x^{3^{n}}+\left(x^{3^{n}}\right)^{2}}{1-x^{3^{n+1}}}=\frac{1}{1-x}-\frac{1}{1-x^{3^{N+1}}} .
$$

Passing to the limit as $N \rightarrow \infty$ we obtain the answer.

Answer:

$$
\sum_{n=0}^{\infty} \frac{x^{3^{n}}+\left(x^{3^{n}}\right)^{2}}{1-x^{3^{n+1}}}=\left\{\begin{array}{cc}
\frac{1}{1-x}, & |x|>1, \\
\frac{x}{1-x}, & |x|<1, \\
0, & x=-1
\end{array}\right.
$$

7 Consider sets $\mathbb{N}_{n}=\{1,2, \ldots, n\}, \mathbb{N}_{m}=\{1,2, \ldots, m\}, m \leq n$, and find the number of surjective mappings from $\mathbb{N}_{n}$ onto $\mathbb{N}_{m}$. Let $A$ be the set of all the mappings from $\mathbb{N}_{n}$ into $\mathbb{N}_{m}$, and $A_{i_{1}, \ldots, i_{k}}$ be the set of all the mappings from $\mathbb{N}_{n}$ into $\mathbb{N}_{m} \backslash\left\{i_{1}, \ldots, i_{k}\right\}$. Then the set of all surjective mappings from $\mathbb{N}_{n}$ onto $\mathbb{N}_{m}$ coincides with $A \backslash \bigcup_{i=1}^{m} A_{i}$. By the inclusion-exclusion principle

$$
\left|\bigcup_{i=1}^{m} A_{i}\right|=\sum_{k=1}^{m}(-1)^{k+1} \sum_{1 \leq i_{1}<\ldots<i_{k} \leq n}\left|A_{i_{1}} \cap \ldots \cap A_{i_{k}}\right| .
$$

A set $A_{i_{1}} \cap A_{i_{2}} \cap \ldots \cap A_{i_{k}}=A_{i_{1}, \ldots, i_{k}}$ contains $(m-k)^{n}$ mappings, because for each element of $\mathbb{N}_{n}$ there exist $m-k$ possible images. Similarly $A$ contains $m^{n}$ mappings. Hence

$$
\begin{aligned}
& \left|\bigcup_{i=1}^{m} A_{i}\right|=\sum_{k=1}^{m}(-1)^{k+1}\left(\begin{array}{c}m \\k\end{array}\right)(m-k)^{n}= \\
& =\sum_{k=0}^{m-1}(-1)^{m-k+1}\left(\begin{array}{c}m \\m-k\end{array}\right) k^{n}=\sum_{k=0}^{m-1}(-1)^{m+k+1}\left(\begin{array}{c}m \\k\end{array}\right) k^{n}, \\
& \left|A \backslash \bigcup_{i=1}^{m} A_{i}\right|=m^{n}-\sum_{k=0}^{m-1}(-1)^{k+m+1}\left(\begin{array}{c}m \\k\end{array}\right) k^{n}=\sum_{k=0}^{m}(-1)^{k+m}\left(\begin{array}{c}m \\k\end{array}\right) k^{n}
\end{aligned}
$$

On the other hand, each surjection from $\mathbb{N}_{n}$ onto $\mathbb{N}_{m}$ can be constructed as follows: first select $m$ elements that will be "representatives" of inverse images of all distinct elements from $\mathbb{N}_{m}$, and then define the images of other $n-m$ elements arbitrarily. We get a list of $\left(\begin{array}{l}n \\ m\end{array}\right) \cdot m ! \cdot m^{n-m}$ mappings which contains all surjections, but some of them might be counted several times, if the choice of "representatives" is not unique. Therefore,

$$
\sum_{k=0}^{m}(-1)^{m+k}\left(\begin{array}{l}
m \\
k
\end{array}\right) k^{n} \leq\left(\begin{array}{l}
n \\
m
\end{array}\right) \cdot m ! \cdot m^{n-m},
$$

hence

$$
\sum_{k=0}^{m}(-1)^{m+k}\left(\begin{array}{c}
m \\
k
\end{array}\right)\left(\frac{k}{m}\right)^{n} \leq\left(\begin{array}{l}
n \\
m
\end{array}\right) \cdot \frac{m !}{m^{m}} .
$$

8 Denote the vertices of $T$ by $A, B, C$. Assume that we have to construct a triangle $D E F$ similar to $T$, such that the vertices $D, E, F$ correspond to $A, B, C$, respectively. It is sufficient to find the sides of triangle $D E F$ or equivalently, to find the similarity coefficient. Since any two parabolas are similar and the similarity coefficient equals to the ratio of the distances from their centers to the directrices, it suffices to find these distances for the given parabola and the parabola with focus $C$ which passes through the points $A$ and $B$. Find the directrices of both parabolas. The distances from the points $A$ and $B$ to the directrix of the parabola are equal to the distances from these points to its focus $C$. Therefore, the directrix is a common tangent to the circles with centers $A, B$ and radii $A C, B C$. Similarly the directrix of the given parabola is a common tangent to the circles with centers $G, H$ and radii $G F, H F$, where $G$ and $H$ are arbitrary points on the parabola. Thus, using a compass and a ruler one can find the directrices of the parabolas and distances $d_{C}, d_{F}$ from their foci $C, F$ to the directrices. Next find the sides of triangle $D E F$ from the conditions $\frac{D E}{A B}=\frac{E F}{B C}=\frac{D F}{A C}=\frac{d_{F}}{d_{C}}$ and construct this triangle.

The problem might have several solutions.

9 Show that for every Lebesgue measurable set $A \subset \mathbb{R}^{2}$ there exists a Borel measurable set $B \subset A$ such that the set $E=A \backslash B$ has zero measure. If $A \neq \varnothing$ and $\lambda(A)<\infty$, then from the construction of Lebesgue measure it follows that

$$
\begin{aligned}
&\lambda(A)=\inf \left\{\sum_{k=1}^{\infty} \lambda\left(A_{k}\right) \mid \bigcup_{k=1}^{\infty} A_{k} \supset A,\left\{A_{k}\right\} \subset K\right\}= \\
&=\sup \left\{\sum_{k=1}^{\infty} \lambda\left(A_{k}\right) \mid \bigcup_{k=1}^{\infty} A_{k} \subset A, A_{i} \cap A_{j}=\varnothing, i \neq j,\left\{A_{k}\right\} \subset K\right\},
\end{aligned}
$$

where $K$ is the ring generated by sets

$$
\left(x^{\prime}, x^{\prime \prime}\right] \times\left(y^{\prime}, y^{\prime \prime}\right],-\infty<x^{\prime}<x^{\prime \prime}<\infty,-\infty<y^{\prime}<y^{\prime \prime}<\infty .
$$

Hence for $\varepsilon>0$ one can find the sets

$$
A_{k}(\varepsilon) \subset A, k \geq 1, A_{i}(\varepsilon) \cap A_{j}(\varepsilon)=\varnothing, i \neq j, A_{k}(\varepsilon) \in K,
$$

such that $\bigcup_{k} A_{k}(\varepsilon)=A(\varepsilon) \subset A$ is a Borel measurable set and

$$
\lambda(A(\varepsilon))=\sum_{k=1}^{\infty} \lambda\left(A_{k}(\varepsilon)\right)>\lambda(A)-\varepsilon,
$$

that is $\lambda(A \backslash A(\varepsilon))<\varepsilon$. Hence $B=\bigcup_{n \geq 1} A\left(\frac{1}{n}\right)$ is a Borel measurable set, $B \subset A$ and $\lambda(A \backslash B) \leq \lambda\left(A \backslash A\left(\frac{1}{n}\right)\right)<\frac{1}{n}, n \geq 1$, therefore, $\lambda(A \backslash B)=0$

For every Lebesgue measurable set $A$ consider $A^{n}=A \cap\left\{x \in \mathbb{R}^{n}:\|x\| \leq n\right\}$, $n \geq 1$. Since $\lambda\left(A^{n}\right)<\infty$, there exist Borel measurable sets $B^{n} \subset A^{n}$ such that $\lambda\left(A^{n} \backslash B^{n}\right)=0$. Put $B=\bigcup_{n \geq 1} B^{n}$. Then $B \subset A$ and $\lambda(A \backslash B)=0$.

Answer: no, it does not.

10 Notice that $d(X, A)$ is the Hilbert-Schmidt norm of the operator which corresponds to the matrix $X-A$. Hence $d(X, A)$ does not depend on the choice of orthonormal basis. First solve the problem for the most convenient basis, and then come back to the basis from the statement of the problem. Since the matrix $A$ is real and symmetric, there exists an orthonormal eigenbasis $f_{1}, \ldots, f_{n}$ such that $A f_{k}=\lambda_{k} f_{k}, 1 \leq k \leq n$. Let operators, which correspond to the matrices $A$ and $X$, are determined by the matrices $\left(\widetilde{a}_{i j}\right)$ and $\left(\widetilde{x}_{i j}\right)$ in the basis $\left\{f_{k}\right\}$. Then $\widetilde{a}_{i j}=0, i \neq j$ and $\widetilde{a}_{i i}=\lambda_{i}$. Hence

$$
d^{2}(X, A)=d^{2}(\widetilde{X}, \widetilde{A})=\sum_{i \neq j} \widetilde{x}_{i j}^{2}+\sum_{k=1}^{n}\left(\widetilde{x}_{k k}-\lambda_{k}\right)^{2} \geq \sum_{k=1}^{n}\left(\min \left(\lambda_{k}, 0\right)\right)^{2},
$$

because the positive semidefiniteness of $X$ implies that $\widetilde{x}_{k k} \geq 0$. The equality is attained if and only if $\widetilde{x}_{i j}=0, i \neq j$, and $\widetilde{x}_{k k}=\max \left(\lambda_{k}, 0\right)$. Hence $\left\{f_{k}\right\}$ is a common eigenbasis of the matrices $A$ and $X$, moreover $X f_{k}=\max \left(\lambda_{k}, 0\right) f_{k}$. Then $X f=\max \left(\lambda_{k}, 0\right) f$, for each vector $f$ from the subspace corresponding to the eigenvalue $\lambda_{k}$, in particular $X e_{k}=\max \left(\lambda_{k}, 0\right) e_{k}, 1 \leq k \leq n$. If $S$ is a matrix with columns $e_{1}, \ldots, e_{n}$, then $\left(\widetilde{a}_{i j}\right)=S^{-1} A S$ and $\left(\widetilde{x}_{i j}\right)=S^{-1} X S$. Therefore, the matrix in question is $X=S\left(\widetilde{x}_{i j}\right) S^{-1}$.

11 Let $\psi$ be a linear fractional transformation which maps the upper half-plane $\{\operatorname{Im} z>0\}$ onto the disc $\{|\omega|<1\}$, moreover $\psi\left(z_{0}\right)=0$. Then $\psi$ maps $\Omega$ onto $\{|\omega|<1\} \backslash \psi(T)$. Put $\Phi=\psi\left(\varphi^{-1}\left(\psi^{-1}\right)\right)$, where $\varphi^{-1}$ and $\psi^{-1}$ are the inverse transformations to $\varphi$ and $\psi$. Then $\Phi$ maps $\{|\omega|<1\}$ onto $\{|\omega|<1\} \backslash \psi(T)$, and

$$
\begin{aligned}
\Phi^{\prime}(0) &=\psi^{\prime}\left(\varphi^{-1} \circ \psi^{-1}(0)\right) \cdot\left(\varphi^{\prime}\left(\varphi^{-1} \circ \psi^{-1}(0)\right)\right)^{-1} \cdot\left(\psi^{\prime}\left(\psi^{-1}(0)\right)\right)^{-1}=\\
&=\psi^{\prime}\left(z_{0}\right) \cdot\left(\varphi^{\prime}\left(z_{0}\right)\right)^{-1} \cdot\left(\psi^{\prime}\left(z_{0}\right)\right)^{-1}=\left(\varphi^{\prime}\left(z_{0}\right)\right)^{-1} .
\end{aligned}
$$

Since $|\Phi(\omega)|<1$ for $|\omega|<1$ and $\Phi(0)=0$, by Schwarz lemma it holds $|\Phi(\omega)| \leq$ $|\omega|$ for $|\omega|<1$. Thus, $\left|\Phi^{\prime}(0)\right| \leq 1$, whence $\left|\varphi^{\prime}\left(z_{0}\right)\right|=\left|\Phi^{\prime}(0)\right|^{-1} \geq 1$. 12 Let the vertices of the triangle have coordinates

$$
A(\cos \alpha, \sin \alpha), B(\cos \beta, \sin \beta), C(\cos \gamma, \sin \gamma),
$$

where $\alpha, \beta, \gamma$ are independent identically distributed on $[0,2 \pi)$ random variables. Express the area of the triangle $A B C$ in terms of the areas of triangles $A O B, B O C$, and $A O C$, where $O$ is the center of the circle. For all the possible $\alpha, \beta, \gamma$ the area of the triangle $A B C$ equals

$$
\frac{1}{2}|\sin (\beta-\alpha)+\sin (\gamma-\beta)+\sin (\alpha-\gamma)| .
$$

Hence the mean value of the area is

$$
S=\frac{1}{(2 \pi)^{3}} \int_{[0,2 \pi)^{3}} \frac{1}{2}|\sin (\beta-\alpha)+\sin (\gamma-\beta)+\sin (\alpha-\gamma)| d \alpha d \beta d \gamma
$$

Notice that for $0 \leq \alpha \leq \beta \leq \gamma<2 \pi$ we have

$$
\sin (\beta-\alpha)+\sin (\gamma-\beta)+\sin (\alpha-\gamma) \geq 0,
$$

because at least two of the summands are non-negative and their sum is not less than the absolute value of the third one. Hence by symmetry of the integrand in $\alpha, \beta, \gamma$ we get $S=\frac{6}{2 \cdot(2 \pi)^{3}} I=\frac{3}{8 \pi^{3}} I$, where

$$
\begin{aligned}
I &=\iiint_{0 \leq \alpha \leq \beta \leq \gamma<2 \pi}(\sin (\beta-\alpha)+\sin (\gamma-\beta)+\sin (\alpha-\gamma)) d \alpha d \beta d \gamma=\\
&=\int_{0}^{2 \pi}\left(\int_{\beta}^{2 \pi}\left(\int_{0}^{\beta}(\sin (\beta-\alpha)+\sin (\gamma-\beta)+\sin (\alpha-\gamma)) d \alpha\right) d \gamma\right) d \beta=\\
&=\int_{0}^{2 \pi} \int_{\beta}^{2 \pi}(1-\cos \beta+\beta \sin (\gamma-\beta)-\cos (\beta-\gamma)+\cos \gamma) d \gamma d \beta=\\
&=\int_{0}^{2 \pi}((2 \pi-\beta)(1-\cos \beta)-\beta \cos \beta+\beta) d \beta=\\
&=\int_{0}^{2 \pi} 2 \pi(1-\cos \beta) d \beta=4 \pi^{2} .
\end{aligned}
$$

Answer: $\frac{3}{2 \pi}$. 1 For $n=1$ the inequality is evident. For $n \geq 2$ it holds

$$
\begin{aligned}
\sum_{k=1}^{n} \frac{2 k-1}{(k+2) !} &=\sum_{k=1}^{n}\left(\frac{2}{(k+1) !}-\frac{5}{(k+2) !}\right)=\\
&=2 \sum_{k=1}^{n} \frac{1}{(k+1) !}-5 \sum_{k=2}^{n+1} \frac{1}{(k+1) !}=\\
&=\frac{2}{2 !}-3 \sum_{k=2}^{n} \frac{1}{(k+1) !}-\frac{5}{(n+2) !}<\frac{2}{2 !}-\frac{3}{3 !}=\frac{1}{2} .
\end{aligned}
$$

2 If $n \neq 3 k+2$ then $2 n-1$ not divides by 3 and covering is impossible. For $n=3 k+2$ covering is possible if and only if the figure can be divided into $2 \times 2$ square which contains the erased cell and several $2 \times 3$ rectangles. It is possible if the cell was erased from $i$ th column, where $i$ not divides by 3 . Hence the probability is $\frac{2 k+2}{3 k+2}$.

Answer:

$$
\left\{\begin{aligned}
0 & \text { if } n \neq 3 k+2 \\
\frac{2 k+2}{3 k+2} & \text { if } n=3 k+2
\end{aligned}\right.
$$

3 Change variables in the integrals and get

$$
\frac{2}{5} \int_{0}^{1} f(x) d x+\frac{2}{5} \int_{0}^{1} f\left(\frac{3}{5} x\right) d x \geq \frac{4}{5} \int_{0}^{1} f\left(\frac{4}{5} x\right) d x .
$$

This inequality is obtained by integrating the inequality

$$
f(x)+f\left(\frac{3}{5} x\right) \geq 2 f\left(\frac{4}{5} x\right),
$$

which follows from the convexity of $f$.

4 If $f\left(x_{1}\right)=f\left(x_{2}\right)$ then $x_{1}=f\left(f\left(x_{1}\right)\right)=f\left(f\left(x_{2}\right)\right)=x_{2}$. Thus, the function $f$ is injective. Then the continuity of $f$ implies its monotonicity. Assume that $f$ is increasing. If for some $x_{0} \in \mathbb{R}$ it holds $f\left(x_{0}\right)<x_{0}$ then $f\left(f\left(x_{0}\right)\right)<f\left(x_{0}\right)<x_{0}$, and if $f\left(x_{0}\right)>x_{0}$ then $f\left(f\left(x_{0}\right)\right)>f\left(x_{0}\right)>x_{0}$, a contradiction. Thus, the only increasing function $f$ is $f(x) \equiv x$. Now assume that $f$ is decreasing. Then $-f$ is increasing and $-f(-f(x))=f(f(x))=x$. Therefore, $-f(x)=x$, i.e. $f(x)=$ $-x, x \in \mathbb{R}$.

Answer: $f(x) \equiv x$ or $f(x) \equiv-x$.

5 In some coordinate system, equations of the parabola and the circle are $y=x^{2}$ and $x^{2}+(y-R)^{2}=R^{2}$. The circle and the parabola satisfy the condition of the problem if $y=0$ is a unique nonnegative root of the equation $y+(y-R)^{2}=R^{2}$, whence $R \leq \frac{1}{2}$. Thus, we have to construct a circle with center $\left(0, \frac{1}{2}\right)$ and radius $\frac{1}{2}$. To construct the coordinate axes draw two parallel chords of the parabola. The straight line $l$, which passes through their midpoints, is parallel to $O y$. Draw a chord orthogonal to $l$. Its perpendicular bisector is the axis $O y$. We find the vertex of the parabola, so we can draw the axis $O x$. The bisector of the angle between $O x$ and $O y$ intersects the parabola at the point $(1,1)$. Now it is easy to construct the point $\left(0, \frac{1}{2}\right)$ and the required circle.

6 It holds $S^{3}=A B C \cdot D A B \cdot C D A \cdot B C D=D^{\mathrm{T}} C^{\mathrm{T}} B^{\mathrm{T}} B C D=A B C D=S$.

7 Construct a sequence $\left\{B_{n}\right\}$ of $2^{n} \times 2^{n}$ matrices, $n \geq 0$, such that $\sqrt[2^{n}]{\operatorname{det} B_{n}} \rightarrow \infty$, as $n \rightarrow \infty$. Set $B_{0}=(1)$ and

$$
B_{n+1}=\left(\begin{array}{cc}
B_{n} & B_{n} \\
-B_{n} & B_{n}
\end{array}\right), n \geq 0 .
$$

It holds

$$
\operatorname{det} B_{n+1}=\operatorname{det}\left(\begin{array}{cc}
B_{n} & B_{n} \\
-B_{n} & B_{n}
\end{array}\right)=\operatorname{det}\left(\begin{array}{cc}
B_{n} & B_{n} \\
0 & 2 B_{n}
\end{array}\right)=2^{2^{n}}\left(\operatorname{det} B_{n}\right)^{2}
$$

Let $\operatorname{det} B_{n}=2^{b_{n}}$. Then $b_{0}=0$ and $b_{n+1}=2^{n}+2 b_{n}, n \geq 1$. It is easy to verify by induction that $b_{n}=n 2^{n-1}, n \geq 0$. Therefore,

$$
\sqrt[2^{n}]{\operatorname{det} B_{n}}=2^{b_{n} / 2^{n}}=2^{n / 2} \rightarrow \infty, \text { as } n \rightarrow \infty
$$

Answer: no, it does not exist. 8 Let $x_{k} \neq x_{m}$. Put $y_{k}=y_{m}=\sqrt{x_{k} x_{m}}$ and $y_{n}=x_{n}$ for $n \neq k, n \neq m$. Then for all $n \geq \max \{k, m\}$ it holds

$$
\begin{aligned}
x_{1}+\ldots+x_{n}-n \sqrt[n]{x_{1} \cdot \ldots \cdot x_{n}} &=\left(x_{k}+x_{m}-2 \sqrt{x_{k} x_{m}}\right)+\\
+&\left(y_{1}+\ldots+y_{n}-n \sqrt[n]{y_{1} \cdot \ldots \cdot y_{n}}\right) \geq\left(\sqrt{x_{k}}-\sqrt{x_{m}}\right)^{2},
\end{aligned}
$$

hence

$$
\liminf _{n \rightarrow \infty}\left(x_{1}+\ldots+x_{n}-n \sqrt[n]{x_{1} \cdot \ldots \cdot x_{n}}\right) \geq\left(\sqrt{x_{k}}-\sqrt{x_{m}}\right)^{2}>0
$$

Answer: yes, always.

9 Since the permutation maps the identity matrix to itself, there exists a permutation $\left\{\sigma_{1}, \sigma_{2}, \ldots, \sigma_{n}\right\}$ of the set $\{1,2, \ldots, n\}$ such that the cell $(k, k)$ is mapped to the cell $\left(\sigma_{k}, \sigma_{k}\right)$ for all $1 \leq k \leq n$. For $i \neq k$ and $j \neq l$ consider a matrix, which has exactly one nonzero entry in each row and each column, and moreover two of these entries are in the cells $(i, j)$ and $(k, l)$. Since this matrix is transformed into a nonsingular one, cells $(i, j)$ and $(k, l)$ have to be mapped to cells from distinct rows and distinct columns. Hence for every $k \neq i, k \neq j$, the image of the cell $(i, j)$ cannot be in the same column or row with the cell $\left(\sigma_{k}, \sigma_{k}\right)$. Thus, the cell $(i, j)$ is mapped to one of the cells $\left(\sigma_{i}, \sigma_{j}\right)$ or $\left(\sigma_{j}, \sigma_{i}\right)$.

Let the cell $(1,2)$ be mapped to the cell $\left(\sigma_{1}, \sigma_{2}\right)$. Then for each $j \geq 2$ the cell $(1, j)$ is mapped to the cell $\left(\sigma_{1}, \sigma_{j}\right)$, otherwise the cell $\left(\sigma_{1}, \sigma_{j}\right)$ is the image of the cell $(j, 1)$,i.e., the cells $(1,2)$ and $(j, 1)$ from distinct rows and columns are mapped to cells from the same row, a contradiction. Therefore, for each $j \geq 2$ the cell $(j, 1)$ is mapped to the cell $\left(\sigma_{j}, \sigma_{1}\right)$. Prove that for every $i, j \geq 2, i \neq j$, the cell $(i, j)$ is mapped to the cell $\left(\sigma_{i}, \sigma_{j}\right)$. Indeed, if it is mapped to the cell $\left(\sigma_{j}, \sigma_{i}\right)$, then the cells $(j, 1)$ and $(i, j)$ from distinct rows and distinct columns are mapped to the cells from the same row, a contradiction. Thus, the permutation is a composition of the same permutations of rows and columns.

If the cell $(1,2)$ is mapped to the cell $\left(\sigma_{2}, \sigma_{1}\right)$, we get similarly that the permutation is a composition of the same permutations of rows and columns and of the transposition of matrix. So in both cases the permutation does not change the determinant.

10 For every positive numbers $a$ and $b$ consider

$$
f(a, b)=\sup _{u, v \in \mathbb{R}} \int_{0}^{a} \int_{0}^{b} \cos (2 x+u) \cos (2 y+v) d x d y .
$$

The properties of least upper bound imply that

$$
f\left(a_{0}, b_{0}\right) \leq \sum_{k=1}^{n} f\left(a_{k}, b_{k}\right) .
$$

Since

$$
\sup _{u \in \mathbb{R}} \int_{0}^{a} \cos (2 x+u) d x=\sup _{u \in \mathbb{R}}(\sin a \cos (a+u))=|\sin a|
$$

and similarly

$$
\sup _{v \in \mathbb{R}} \int_{0}^{b} \cos (2 y+v) d y=|\sin b|,
$$

it follows that $f(a, b)=|\sin a \sin b|, a, b>0$, whence the required inequality holds.

11 Existence of $E \xi$ means the convergence of integral

$$
\begin{aligned}
\int_{\mathbb{R}}|x|^{\alpha} \frac{e^{-x^{2} / 2}}{\sqrt{2 \pi}} d x &=\sqrt{\frac{2}{\pi}} \int_{0}^{\infty} x^{\alpha} e^{-x^{2} / 2} d x=\\
&=\sqrt{\frac{2}{\pi}}\left(\int_{0}^{1} x^{\alpha} e^{-x^{2} / 2} d x+\int_{1}^{\infty} x^{\alpha} e^{-x^{2} / 2} d x\right) .
\end{aligned}
$$

The first integral converges for $\alpha>-1$, and the second one converges for all $\alpha \in \mathbb{R}$. Answer: $\alpha>-1$.

13 The extension exists by the Hahn-Banach Theorem. Let $F_{1}, F_{2} \in X^{*}$ be two extensions of $f \in G^{*}$. Then $\frac{1}{2}\left(F_{1}+F_{2}\right)$ is an extension of $f$, and $\left\|\frac{1}{2}\left(F_{1}+F_{2}\right)\right\| \geq$ $\|f\|$. But by the triangle inequality $\left\|\frac{1}{2}\left(F_{1}+F_{2}\right)\right\| \leq \frac{1}{2}\left(\left\|F_{1}\right\|+\left\|F_{2}\right\|\right)=\|f\|$, whence

$$
\left\|F_{1}\right\|=\left\|F_{2}\right\|=\left\|\frac{1}{2}\left(F_{1}+F_{2}\right)\right\| .
$$

Since $X^{*}$ is strictly normed, it follows that $F_{1}=F_{2}$.

14 It holds

$$
R(0)=0, \quad R(z)=\int_{0}^{z}\left(s-1+\frac{1}{s+1}\right) d s=\int_{0}^{z} \frac{s^{2}}{1+s} d s,
$$

whence

$$
\begin{aligned}
|R(i x)|=\left|\int_{0}^{i x} \frac{s^{2}}{1+s} d s\right| &=\left|\int_{0}^{x} \frac{-t^{2}}{1+i t} i d t\right| \\
& \leq\left|\int_{0}^{x} \frac{t^{2}}{|1+i t|} d t\right| \leq\left|\int_{0}^{x} t^{2} d t\right|=\frac{|x|^{3}}{3} .
\end{aligned}
$$

15 It holds

$$
S^{3}=A B C \cdot D A B \cdot C D A \cdot B C D=D^{\mathrm{T}} C^{\mathrm{T}} B^{\mathrm{T}} B C D=A B C D=S,
$$

hence $S^{3}-S=(S+I)\left(S^{2}-S\right)=0$, where $I$ is the identity matrix. Because $S=D^{\mathrm{T}} D$ is positive semidefinite, $S+I$ is positive definite and there exists $(S+I)^{-1}$. Therefore, $S^{2}-S=(S+I)^{-1}\left(S^{3}-S\right)=0$.

16 A couple $(x, y)$ is a stationary point of $F$ for some $\lambda, \mu$, if maximum or minimum of $\|y-x\|$ is attained at that point under the condition $f_{e}(x)=f_{-e}(x)=1$. Geometrically this means that the vector $y-x$ is orthogonal to ellipses $\left\{f_{e}(x)=1\right\}$ and $\left\{f_{-e}(x)=1\right\}$. Consider basis $(e, g)$ in $\mathbb{R}^{2}$, where $g \perp e$ and $\|g\|=\|e\|$. Define $A$ such that $A e=\frac{e}{2\|e\|}$ and $A g=\frac{g}{3\|g\|}$. Then radius vectors of edges of the minor axis of the ellipse $\left\{f_{e}(x)=1\right\}$ are $x_{1}=e$ and $x_{2}=-3 e$, and corresponding vectors for the ellipse $\left\{f_{-e}(x)=1\right\}$ are $y_{1}=-e$ and $y_{2}=3 e$. From that vectors one can form 4 couples $\left(x_{i}, y_{j}\right), i, j=1,2$. Because for that couples $\left\|x_{i}-y_{j}\right\| \leq 6\|e\|$ and the distance between the points of the ellipses $-e+3 g, e-3 g$ equals $\sqrt{40}\|e\|>6\|e\|$, the maximum is attained at some other couple $\left(x_{0}, y_{0}\right)$ and for the couple which is symmetric to the latter couple with respect to the straight line that contains the vector $e$. At last, there are two more couples $(x, y)$, for which $x=y$, they are the intersection points of the ellipses.

18 Let be known that the croupier chooses a number from the interval $[a, a+k-1]$. Put $d(k)=\frac{1}{2}+\frac{1}{2 k} \operatorname{odd}(k)$, where

$$
\operatorname{odd}(k)=\left\{\begin{array}{l}
1 \text { if } k \text { is odd } \\
0 \text { if } k \text { is even }
\end{array}\right.
$$

Notice that the numbers $a, a+1, \ldots, a+k-1$ are chosen with prior probability $\frac{1}{k}$. Prove by induction on $k$ that the player, who guesses the number first, will win with probability at least $d(k)$ and his rival will win with probability at least $1-d(k)$. For $k=1$, the statement is evident. Suppose that the statement is proved for $k \leq n-1$ and show it for $k=n$.

Let the first player call the number $a$. Then he/she either wins at once (with probability $\frac{1}{n}$ ), or plays the role of the second player in the situation with $n-1$ numbers. By the induction hypothesis he/she will win with probability at least

$$
\frac{1}{n}+\frac{n-1}{n}(1-d(n-1))=\frac{1}{2}+\frac{1}{2 n}(1-\operatorname{odd}(n-1))=d(n) .
$$

Now, consider chances to win of the second player. If the first one calls some number $c$ out of the interval $[a, a+n-1]$, then the second player plays the role of the first and improves his/her chances. Suppose that the first player calls the number $a+m-1,1 \leq m \leq n$. Then the second one either loses at once (with probability $\frac{1}{n}$ ), or plays the role of the first in the situation with $m-1$ or $n-m$ numbers and by the induction hypothesis he/she will win with probability at least 

$$
\begin{aligned}
\frac{m-1}{n} d(m-1) &+\frac{n-m}{n} d(n-m)=\\
&=\frac{n-1}{2 n}+\frac{1}{2 n}(\operatorname{odd}(m-1)+\operatorname{odd}(n-m)) \geq \\
& \geq \frac{n-1}{2 n}+\frac{1}{2 n}(1-\operatorname{odd}(n))=1-d(n) .
\end{aligned}
$$

By the principle of mathematical induction the statement is proved. The result of the problem is obtained for $k=2004$ and $a=1$.

20 If the sequence $\left\{x_{n}\right\}$ exists then for $n \geq 2005$, it holds

$$
\left\|x_{1}+\ldots+x_{n}\right\|^{2}=\sum_{i=1}^{n}\left\|x_{i}\right\|^{2}+2 \sum_{1 \leq i<j \leq n}\left(x_{i}, x_{j}\right)<n-\frac{n(n-1)}{2004} \leq 0
$$

a contradiction.

Answer: there is no such sequence. 

\section{5}

1 Necessity. Let $x_{n} \rightarrow a$, as $n \rightarrow \infty$. Then

$$
\lim _{m \rightarrow \infty}\left|x_{n}-x_{m}\right|=\left|x_{n}-a\right| \text { and } \lim _{n \rightarrow \infty} \limsup _{m \rightarrow \infty}\left|x_{n}-x_{m}\right|=\lim _{n \rightarrow \infty}\left|x_{n}-a\right|=0 .
$$

Sufficiency. Show that $\left\{x_{n}: n \geq 1\right\}$ is a Cauchy sequence. For every $\varepsilon>0$, select $N \in \mathbb{N}$ such that

$$
\limsup _{m \rightarrow \infty}\left|x_{m}-x_{N}\right|<\frac{\varepsilon}{2},
$$

hence

$$
\exists M \in \mathbb{N}: \forall m \geq M \quad\left|x_{m}-x_{N}\right|<\frac{\varepsilon}{2} .
$$

From here for all $m_{1}, m_{2} \geq M$, it holds

$$
\left|x_{m_{1}}-x_{m_{2}}\right| \leq\left|x_{m_{1}}-x_{N}\right|+\left|x_{m_{2}}-x_{N}\right|<\varepsilon .
$$

We proved that a given sequence is a Cauchy one, and by the Cauchy Criterion the sequence converges.

Answer: yes, it is true.

2 Let $A=\left(a_{i j}\right), B=\left(b_{i j}\right)$, and $C=\left(c_{i j}\right), 1 \leq i \leq k, 1 \leq j \leq l$. Then

$$
\begin{aligned}
&\operatorname{tr}\left(A\left(A^{\mathrm{T}}-B^{\mathrm{T}}\right)+B\left(B^{\mathrm{T}}-C^{\mathrm{T}}\right)+C\left(C^{\mathrm{T}}-A^{\mathrm{T}}\right)\right)= \\
&=\sum_{i, j}\left(a_{i j}^{2}-a_{i j} b_{i j}+b_{i j}^{2}-b_{i j} c_{i j}+c_{i j}^{2}-c_{i j} a_{i j}\right) .
\end{aligned}
$$

It remains to notice that for every $a, b, c \in \mathbb{R}$

$$
a^{2}+b^{2}+c^{2}-a b-a c-b c=\frac{1}{2}\left((a-b)^{2}+(b-c)^{2}+(c-a)^{2}\right) \geq 0 .
$$

3 Let $\tan \alpha=\frac{p}{q}$. Divide each square of the board into rectangles of size $\frac{1}{p} \times \frac{1}{q}$. Then the ball will move along the diagonals of the rectangles only. Assume that the ball will never fall into a hole. Sooner or later it will move along the diagonal of some rectangle at least twice in the same direction, because there exists a finite number of diagonals and directions. But then the movement of the ball will be periodic. Inspecting the movement of the ball, we get that in a period after the start the ball has to return to the corner, from which it was shot, and fall into the hole. We come to a contradiction.

4 It is easy to verify that the function

$$
f(x):=\lim _{n \rightarrow \infty} \sqrt{1+\sqrt{x+\sqrt{x^{2}+\ldots+\sqrt{x^{n}}}}}
$$

is well-defined for $x \geq 0$ (because the sequence under the limit is nondecreasing and bounded). For $x<y$, it holds

$$
f(x) \leq \sqrt{1+\sqrt{x+\lim _{n \rightarrow \infty} \sqrt{y^{2}+\ldots+\sqrt{y^{n}}}}}<f(y),
$$

i.e., $f$ is increasing and the equation $f(x)=2$ has at most one solution.

Show that $f(4)=2$. Indeed, for $a, b \geq 0$ and $c \geq 1$, we have

$$
|\sqrt{c+a}-\sqrt{c+b}|=\frac{|a-b|}{\sqrt{c+a}+\sqrt{c+b}} \leq \frac{1}{2}|a-b| .
$$

Therefore, the sequences

$$
A_{n}=\sqrt{1+\sqrt{4+\ldots+\sqrt{4^{n-1}+\sqrt{4^{n}}}}}, \quad B_{n}=\sqrt{1+\sqrt{4+\ldots+\sqrt{4^{n-1}+\sqrt{4^{n}}+1}}}
$$

satisfy $\left|A_{n}-B_{n}\right| \leq \frac{1}{2^{n}}$, hence $\lim _{n \rightarrow \infty}\left(A_{n}-B_{n}\right)=0$. But

$$
\sqrt{4^{n-1}+\sqrt{4^{n}}+1}=2^{n-1}+1=\sqrt{4^{n-1}}+1
$$

whence

$$
B_{n}=B_{n-1}=\ldots=B_{1}=\sqrt{1+\sqrt{4}+1}=2,
$$

therefore, $f(4)=\lim _{n \rightarrow \infty} A_{n}=\lim _{n \rightarrow \infty} B_{n}=2$.

Answer: $x=4$.

5 At the plane consider an equilateral triangle with the center at the origin, and let $A, B, C$ be $2 \times 2$ matrices which correspond to symmetries of the plane with respect to the altitudes of the triangle. Then the matrices have no common eigenvector, and moreover the matrix $A B=B C=C A$ corresponds to the rotation $120^{\circ}$ around the origin.

Answer: yes, there exist.

6 Transform the product of cosines into a sum, using repeatedly the formula

$$
\cos \alpha \cos \beta=\frac{1}{2}(\cos (\alpha+\beta)+\cos (\alpha-\beta)) .
$$

The expression under the integral equals

$$
\frac{1}{2^{2003}} \sum \cos (2 x \pm 3 x \pm 4 x \pm \ldots \pm 2005 x),
$$

where the summation is performed over all $2^{2003}$ possible choices of signs. The integral $\int_{-\pi}^{\pi} \cos k x d x$ vanishes for $k \in \mathbb{Z} \backslash\{0\}$ and equals $2 \pi$ for $k=0$. Hence the integral is nonnegative. It remains to notice that there exists at least one summand, for which $k=2 \pm 3 \pm 4 \pm \ldots \pm 2005=0$. It does exist because

$$
(2-3-4+5)+(6-7-8+9)+\ldots+(2002-2003-2004+2005)=0
$$

7 Let $f(x)=x^{3}-3 x, a_{1}=-2, a_{3}=-1, b_{1}=1$, and $b_{3}=2$. Then $f\left(a_{1}\right)=$ $f\left(b_{1}\right), f\left(a_{3}\right)=f\left(b_{3}\right)$, and $f^{\prime}\left(c_{1}\right)=f^{\prime}\left(c_{3}\right)=0$. Take $a_{1}<a_{2}<a_{3}$ and $b_{1}<b_{2}<$ $b_{3}$ such that $f\left(b_{2}\right)>f\left(a_{2}\right)$, hence it should hold $f^{\prime}\left(c_{2}\right)>0$. But $f^{\prime}(x)=3\left(x^{2}-1\right)$, and the conditions $f^{\prime}\left(c_{1}\right)=f^{\prime}\left(c_{3}\right)=0, c_{1} \leq c_{2} \leq c_{3}$ imply that $f^{\prime}\left(c_{2}\right) \leq 0$. Thus, in this case required $c_{i}, 1 \leq i \leq 3$, do not exist.

Answer: no, not always.

8 Alongside with a point $(x, y, z)$, the required set $S$ consider points $(\pm x, \pm y, \pm z)$. If at least two of numbers $x, y, z$ are nonzero, then four or eight above mentioned points belong to $S$. Hence the number of points of $S$ that do not lie on coordinate axes is divisible by 4 . Let $k \leq R<k+1$. Then there are $6 k+1$ points on the axes, and hence $2005 \equiv 6 k+1(\bmod 4)$, and $k$ should be even. On the other hand, it is not difficult to show that $7 \leq R<8$ and $k=7$, a contradiction.

9 Let the points $A_{1}, A_{2}$, and $A_{3}$ have coordinates $(1,0),(0,1)$, and $(x, y)$, respectively. Suppose that a required polynomial $P$ exists, and consider $Q(x, y)=$ $P(1,0,0,1, x, y)$. The triangle $A_{1} A_{2} A_{3}$ is negative if $A_{3}$ lies in the first quadrant and positive if $A_{3}$ lies in the other quadrants. Hence for each fixed positive $y$, the polynomial $Q(x, y)$ changes the sign with the vertex $A_{3}$ passing over the point $(0, y)$. Then $Q(0, y)=0, y>0$, and $Q(x, y)=Q_{1}(x, y) \cdot x$. In a similar way $Q_{1}(x, y)=Q_{2}(x, y) \cdot y$, and $Q(x, y)=x y \cdot Q_{2}(x, y)$. Now $Q_{2}(x, y)$ changes the sign with passing over the points $(0, y), y<0$, or $(x, 0), x<0$, hence

$$
Q_{2}(x, y)=x y \cdot R(x, y), Q(x, y)=(x y)^{2} R(x, y) .
$$

For $x y \neq 0$, the polynomial $R(x, y)$ has the same sign as $Q(x, y)$; therefore, by the same reasoning we obtain $R(x, y)=(x y)^{2} S(x, y)$, etc., where the degrees of the polynomials $Q, R, S, \ldots$ are decreasing all the time, a contradiction.

10 Show that $f$ is uniformly continuous. By the Arzela-Ascoli theorem the set $K$ is equicontinuous, i.e.,

$$
\forall \varepsilon>0 \exists \delta>0 \quad \forall x \in K \quad \forall t_{1}, t_{2} \in[0,1]:\left|t_{1}-t_{2}\right|<\delta \Rightarrow\left|x\left(t_{1}\right)-x\left(t_{1}\right)\right|<\varepsilon .
$$

Let $t_{1}, t_{2} \in[0,1],\left|t_{1}-t_{2}\right|<\delta$, and let $x_{1}, x_{2}$ be the functions at which the values $f\left(t_{1}\right), f\left(t_{2}\right)$ are attained, respectively (such functions exist because the function $g_{t}(x)=x(t)+x(1-t)$ is continuous on the compact set $K$ for each $\left.t \in[0,1]\right)$. Then

$$
\begin{aligned}
f\left(t_{1}\right)-f\left(t_{2}\right) &=x_{1}\left(t_{1}\right)+x_{1}\left(1-t_{1}\right)-x_{2}\left(t_{2}\right)-x_{2}\left(1-t_{2}\right) \leq \\
& \leq\left(x_{1}\left(t_{1}\right)+x_{1}\left(1-t_{1}\right)\right)-\left(x_{2}\left(t_{1}\right)+x_{2}\left(1-t_{1}\right)\right)+\\
&+\left|x_{2}\left(t_{1}\right)-x_{2}\left(t_{2}\right)\right|+\left|x_{2}\left(1-t_{1}\right)-x_{2}\left(1-t_{2}\right)\right|<2 \varepsilon
\end{aligned}
$$

(here we use that $g_{t_{1}}\left(x_{1}\right) \leq g_{t_{1}}\left(x_{2}\right)$ ). Similarly, we get the inequality $f\left(t_{2}\right)-f\left(t_{1}\right)<$ $2 \varepsilon$. Hence $\left|t_{1}-t_{2}\right|<\delta$ implies $\left|f\left(t_{1}\right)-f\left(t_{2}\right)\right|<2 \varepsilon$, and the uniform continuity is proved.

11 In case $\lambda=0$, a sequence $\left\{a_{n}\right\}$ can be arbitrary.

If $0<|\lambda|<1$ then $\left|a_{n+1}\right|<|\lambda| \cdot\left|a_{n}\right|+\frac{1}{|\lambda|}, n \geq 1$, whence by induction on $n$ we obtain

$$
\left|a_{n}\right|<|\lambda|^{n-1}\left|a_{1}\right|+|\lambda|^{n-2}+\ldots+|\lambda|+1+\frac{1}{|\lambda|} \leq\left|a_{1}\right|+\frac{1}{|\lambda|(1-|\lambda|)} .
$$

In case $|\lambda|=1$ put $a_{n}=\frac{n}{2} \lambda^{n}$, and in case $|\lambda|>1$ put $a_{n}=\lambda^{n}$. In both cases the sequences are unbounded and satisfy the condition of the problem.

Answer: $0<|\lambda|<1$.

12 Let $K \neq 0$ be a supercompact operator. Then for some $x \in X$ it holds $y=K x \neq$ 0 , and the image of a bounded set $M=\{\alpha x \mid \alpha \in(0,1)\}$ is the set $K(M)=\{\alpha y \mid \alpha \in$ $(0,1)\}$, which is not closed, hence it is not compact. We come to a contradiction. 13 Because $A^{2}=A A^{\mathrm{T}}=I$, the matrix $A$ is nonsingular. Hence $A=A^{\mathrm{T}}$, i.e., $A$ is self-adjoint. Then $A=U B U^{\mathrm{T}}$, where $U$ is some orthogonal matrix and $B$ is a diagonal matrix with eigenvalues of the matrix $A$ on the diagonal. Because $A^{2}=I$, its eigenvalues satisfy the equation $\lambda^{2}=1$, thus they are equal to $\pm 1$.

14 Let $C_{n}$ be the circle in $\mathbb{R}^{2}$ with center $(0, n)$ and radius $n$,

$$
X=\bigcup_{n \geq 1} C_{n} \backslash(0,1)^{2}
$$

endowed with Euclidean metrics, and $B$ be the intersection of $X$ and the disc in $\mathbb{R}^{2}$ with center at the origin and radius 2 . Then $X$ is a connected unbounded space, but there is no connected proper subset of $X$ that contains $B$.

Answer: no, not always.

15 For $\alpha_{1}<\alpha_{2}$, it holds

$$
\int_{\mathbb{R}^{+}} \exp \left(\alpha_{1}(x+1)^{t}\right) d \mu(x)<\int_{\mathbb{R}^{+}} \exp \left(\alpha_{2}(x+1)^{t}\right) d \mu(x),
$$

and hence it suffices to consider $0<\alpha<1$. Let $\alpha<\beta<1$. Then $\frac{\alpha(x+1)^{t}}{\beta x^{t}} \rightarrow \frac{\alpha}{\beta}<1$ as $x \rightarrow+\infty$, and hence there exists $c>0$ such that $\alpha(x+1)^{t} \leq \beta x^{t}$ for all $x \geq c$. Thus,

$$
\begin{aligned}
\int_{\mathbb{R}^{+}} \exp (\alpha(x)&\left.+1)^{t}\right) d \mu(x) \leq \\
& \leq \int_{0}^{c} \exp \left(\alpha(x+1)^{t}\right) d \mu(x)+\int_{c}^{\infty} \exp \left(\beta x^{t}\right) d \mu(x) \leq \\
& \leq \exp \left(\alpha(c+1)^{t}\right) \int_{0}^{\infty} \exp \left(0 \cdot x^{t}\right) d \mu(x)+\int_{0}^{\infty} \exp \left(\beta x^{t}\right) d \mu(x)<\infty .
\end{aligned}
$$

18 It is not difficult to construct a function $f \in C^{(1)}\left(\left[x_{0}, x_{n}\right]\right)$ such that

$$
f\left(x_{i}\right)=y_{i}, i=0, \ldots, n,
$$

and moreover the derivative of $f$ is positive and bounded away from zero at $\left[x_{0}, x_{n}\right]$, that is $f^{\prime}(x) \geq \delta$ for some $\delta>0$ and all $x \in\left[x_{0}, x_{n}\right]$. For each $\varepsilon>0$, there exists a polynomial $P_{\varepsilon}$ such that $\max _{x \in\left[x_{0}, x_{n}\right]}\left|f^{\prime}(x)-P_{\varepsilon}(x)\right|<\varepsilon$. Set

$$
Q_{\varepsilon}(x):=y_{0}+\int_{x_{0}}^{x} P_{\varepsilon}(t) d t, x \in\left[x_{0}, x_{n}\right] .
$$

Then $\max _{x \in\left[x_{0}, x_{n}\right]}\left|f(x)-Q_{\varepsilon}(x)\right|<\varepsilon\left(x_{n}-x_{0}\right)$. Let $R_{\varepsilon}$ by a Lagrange polynomial which interpolates the values $y_{i}-Q_{\varepsilon}\left(x_{i}\right)$ at points $x_{i}, i=0, \ldots, n$. Consider the transformation which maps a collection of numbers $\left(z_{0}, \ldots, z_{n}\right)$ to the derivative of the Lagrange interpolating polynomial $R^{\prime}=R^{\prime}\left(z_{0}, \ldots, z_{n}\right)$, where $R\left(x_{i}\right)=z_{i}$, $i=0, \ldots, n$. This transformation is a linear transformation of finite-dimensional spaces (because deg $R^{\prime} \leq n$ ); therefore, it is continuous and there exists $C>0$ such that

$$
\max _{x \in\left[x_{0}, x_{n}\right]}\left|R^{\prime}(x)\right| \leq C \cdot \max _{0 \leq k \leq n}\left|z_{k}\right| .
$$

Choose $\varepsilon>0$ such that $C\left(x_{n}-x_{0}\right) \varepsilon+\varepsilon<\delta$. Then for

$$
R_{\varepsilon}=R\left(y_{0}-Q_{\varepsilon}\left(x_{0}\right), \ldots, y_{n}-Q_{\varepsilon}\left(x_{n}\right)\right)
$$

it holds $\max _{x \in\left[x_{0}, x_{n}\right]}\left|R_{\varepsilon}^{\prime}(x)\right|+\varepsilon \leq \delta$. Hence the polynomial $Q_{\varepsilon}+R_{\varepsilon}$ is required, because

$$
\forall x \in\left[x_{0}, x_{n}\right] \quad Q_{\varepsilon}^{\prime}(x)+R_{\varepsilon}^{\prime}(x)>P_{\varepsilon}(x)+\varepsilon-\delta>f^{\prime}(x)-\delta>0 .
$$

1 The condition of the problem is equivalent to each of the following statements: the polynomial $\left(x^{3}+x^{2}+x+1\right)^{n}+x^{n}$ is divisible by $Q(x)=x^{4}+x^{3}+x^{2}+x+1$, or $\left(-x^{4}\right)^{n}+x^{n}$ is divisible by $Q(x)$, or $(-1)^{n} x^{3 n}+1$ is divisible by $Q(x)$. We will denote by $\equiv$ the congruence modulo $Q(x)$, that is $P_{1}(x) \equiv P_{2}(x)$ if $P_{1}(x)-P_{2}(x)$ is divisible by $Q(x)$. It holds $x^{5} \equiv 1, x^{6} \equiv x, x^{7} \equiv x^{2}, \ldots$, hence if $3 n=5 a+b$, where $0 \leq b \leq 4$, then $(-1)^{n} x^{3 n}+1 \equiv(-1)^{n} x^{b}+1$. It remains to notice that for $b \leq 4$ the congruence $(-1)^{n} x^{b}+1 \equiv 0$ holds if and only if we have simultaneously $b=0$ and $n$ is odd, i.e. $n=10 k-5, k \in \mathbb{N}$.

Answer: $n=10 k-5, k \in \mathbb{N}$.

2 The condition of the problem implies that the points $(z+1) z^{3},(z+1)^{2} z^{2}$, $(z+1)^{3} z$, and $(z+1)^{4}$ are the vertices of an inscribed quadrangle as well. Let $z_{0}$ and $z_{0}^{\prime}$ be centers of the circles which pass through the first three and the last three of the latter points. Then $z_{0}^{\prime}=\frac{z+1}{z} \cdot z_{0}$. Notice that $z \neq 0$ because all the points are distinct. Since the quadrangle is inscribed, $z_{0}^{\prime}=z_{0}=\frac{z+1}{z} \cdot z_{0}$, so $z_{0}=0$. Thus

$$
\left|(z+1) z^{3}\right|=\left|(z+1)^{2} z^{2}\right|,
$$

hence $|z|=|z+1|$, that is $\operatorname{Re} z=-\frac{1}{2}$.

Answer: $\operatorname{Re} z=-\frac{1}{2}$.

3 Suppose that $\left(x_{i}, x_{j}\right)<-\frac{1}{n}$ for all $1 \leq i<j \leq n+1$. Then

$\left\|x_{1}+x_{2}+\ldots+x_{n+1}\right\|^{2}=\sum_{i=1}^{n+1}\left\|x_{i}\right\|^{2}+2 \sum_{1 \leq i<j \leq n+1}\left(x_{i}, x_{j}\right)<n+1-\frac{n(n+1)}{n}=0$,

a contradiction.

(O) Springer International Publishing AG 2017

V. Brayman and A. Kukush, Undergraduate Mathematics We construct an example of $n+1$ unit vectors in $\mathbb{R}^{n}$ such that $\left(x_{i}, x_{j}\right)=-\frac{1}{n}$, $1 \leq i<j \leq n+1$. Use induction on $n$. For $n=1$, put $x_{1}=1, x_{2}=-1$. Assume that the example has been already constructed for $n<k$. Consider $n=k$. Let $e_{1}, \ldots, e_{k}$ be an orthobasis in $\mathbb{R}^{k}$. By inductive hypothesis there exist unit vectors $y_{1}, \ldots, y_{k}$ in $(k-1)$-dimensional space with the basis $\left\{e_{1}, \ldots, e_{k-1}\right\}$ such that $\left(y_{i}, y_{j}\right)=-\frac{1}{k-1}$, $1 \leq i<j \leq k$. It is easy to verify that the vectors

$$
x_{i}=\frac{\sqrt{k^{2}-1}}{k} y_{i}-\frac{1}{k} e_{k}, 1 \leq i \leq k, x_{k+1}=e_{k}
$$

satisfy the required condition, and by the principle of mathematical induction there exists a required example for all $n \geq 1$.

Answer: $-\frac{1}{n}$

4 Take an arbitrary column vector $x \in \mathbb{R}^{m}, x \neq 0$. Then for the column vector $y=\left(\begin{array}{c}-A x \\ x\end{array}\right) \in \mathbb{R}^{m+n}$ it holds $y \neq 0$, and the positive definiteness implies that

$$
y^{\mathrm{T}}\left(\begin{array}{cc}
I_{m} & A \\
A^{\mathrm{T}} & B
\end{array}\right) y=x^{\mathrm{T}}\left(B-A^{\mathrm{T}} A\right) x>0,
$$

so the matrix $B-A^{\mathrm{T}} A$ is positive definite.

5 Consider $2 \times 2$ matrices $A_{x}=\left(\begin{array}{cc}1 & x \\ x & -1\end{array}\right), x \in \mathbb{R}$. We have that $A_{x}^{2}=\left(\begin{array}{cc}1+x^{2} & 0 \\ 0 & 1+x^{2}\end{array}\right)$ commutes with every $2 \times 2$ matrix, while $A_{x} A_{y}=\left(\begin{array}{ccc}1+x y & y-x \\ x-y & 1+x y\end{array}\right) \neq A_{y} A_{x}$ for $x \neq y$. Answer: yes, there exists.

6 Since the function $f:(0,+\infty) \rightarrow \mathbb{R}$ is concave, the sequence $g(n)=f(n+$ 1) $-f(n)$ decreases. If $g\left(n_{0}\right) \leq 0$ then $g(n) \leq 0$ for all $n \geq n_{0}$, hence the condition $\lim _{x \rightarrow+\infty} f(x)=+\infty$ fails. Therefore, $g(n)>0, n \geq 1$, so there exists a finite limit

$$
\lim _{n \rightarrow+\infty} g(n)=\lim _{n \rightarrow+\infty}(f(n+1)-f(n))=a \geq 0 .
$$

By Stolz-Cesaro theorem it holds $\lim _{n \rightarrow+\infty} \frac{f(n)}{n}=a$, hence $a=0$. For every $\varepsilon>0$, there exists $N_{0}=N_{0}(\varepsilon)$ such that for all $n \geq N_{0}$, it holds $f(n)<f(n+1)<$ $f(n)+\varepsilon$. Take an integer $K>f\left(N_{0}\right)$. Let $N_{1}$ be the least number greater than $N_{0}$, for which $f\left(N_{1}\right) \geq K$ (there exists such a number because $\lim _{x \rightarrow+\infty} f(x)=+\infty$ ). Then $K-\varepsilon<f\left(N_{1}-1\right)<K$ and $\{f(n)\}>1-\varepsilon$. Since $\varepsilon>0$ is arbitrary, $\sup _{n \in \mathbb{N}}\{f(n)\}=1$.

Remark 1 In a similar way one can prove that the sequence of the fractional parts $\{f(n)\}, n \geq 1$, is dense in $[0,1]$.

7 Consider the function $f(x)=\left((|x|+3) \ln ^{2}(|x|+3)\right)^{-1}: \mathbb{R} \rightarrow(0,1)$. This function is continuous. For $n \geq 3$ it holds 

$$
a_{n}=\int_{-n}^{n} f(x) d x=2 \int_{3}^{n+3} \frac{d x}{x \ln ^{2} x}=\frac{2}{\ln 3}-\frac{2}{\ln (n+3)} \rightarrow \frac{2}{\ln 3}, \text { as } n \rightarrow \infty,
$$

and

$$
\begin{aligned}
\left|b_{n}\right| &=\int_{-n}^{n} f(x)|\ln f(x)| d x=2 \int_{3}^{n+3} \frac{\ln x+2 \ln \ln x}{x \ln ^{2} x} d x>\\
&>2 \int_{3}^{n+3} \frac{\ln x}{x \ln ^{2} x} d x=2 \ln \ln (n+3)-2 \ln \ln 3 \rightarrow \infty, \text { as } n \rightarrow \infty .
\end{aligned}
$$

Answer: yes, there exists.

8 Introduce new variables $s=3 a+5 b, t=2 a+2 b$. There exist infinitely many couples of integers $(s, t)$ such that $P(s-t)+P(s+t)=0$, because distinct couples $(a, b)$ correspond to distinct couples $(s, t)$. By Taylor's formula it holds

$$
P(s-t)+P(s+t)=2\left(P(s)+\frac{1}{2} P^{\prime \prime}(s) t^{2}+\frac{1}{4 !} P^{(4)}(s) t^{4}+\ldots\right)=0 .
$$

For $s$ with absolute value large enough, all the coefficients have the same sign and all the powers of $t$ are even, thus, there are no roots. Hence there exists $s_{0}$ that corresponds to infinitely many values of $t$, which implies

$$
P\left(s_{0}\right)+\frac{1}{2} P^{\prime \prime}\left(s_{0}\right) t^{2}+\frac{1}{4 !} P^{(4)}\left(s_{0}\right) t^{4}+\ldots \equiv 0,
$$

in particular $P\left(s_{0}\right)=0$.

9 For $z>0$ it holds $\frac{1}{z}=\int_{0}^{1} x^{z-1} d x$, the integral is proper Riemann integral for $z \geq 1$ and improper one for $0<z<1$. Then

$$
\begin{aligned}
\sum_{i, j=1}^{n} \frac{a_{i} a_{j}}{a_{i}^{2}+a_{j}^{2}} &=\sum_{i, j=1}^{n} a_{i} a_{j} \int_{0}^{1} x^{a_{i}^{2}+a_{j}^{2}-1} d x=\\
&=\int_{0}^{1} x^{-1} \sum_{i, j=1}^{n} a_{i} a_{j} x^{a_{i}^{2}+a_{j}^{2}} d x=\int_{0}^{1} x^{-1}\left(\sum_{i=1}^{n} a_{i} x^{a_{i}^{2}}\right)^{2} d x \geq 0 .
\end{aligned}
$$

Remark 1 If the numbers $\left|a_{i}\right|, 1 \leq i \leq n$, are distinct then the power functions $x^{a_{i}^{2}}$, $0 \leq x \leq 1,1 \leq i \leq n$, are linear independent, hence the inequality is strict.

Remark 2 For the functions $f_{i}(t)=t^{a_{i}^{2}-1 / 2}, t \in(0,1)$, from the space $L_{2}([0,1])$, it holds

$$
\left(f_{i}, f_{j}\right)=\int_{0}^{1} t^{a_{i}^{2}-1 / 2} \cdot t^{a_{j}^{2}-1 / 2} d t=\frac{1}{a_{i}^{2}+a_{j}^{2}}
$$

Hence the matrix $B=\left(\frac{1}{a_{i}^{2}+a_{j}^{2}}\right)_{i, j=1}^{n}$ is positive semidefinite as the Gram matrix of the set of functions $f_{1}, \ldots, f_{n}$, hence for all numbers $a_{1}, a_{2}, \ldots, a_{n} \in \mathbb{R} \backslash\{0\}$ and $b_{1}, b_{2}, \ldots, b_{n} \in \mathbb{R}$, a more general inequality $\sum_{i, j=1}^{n} \frac{b_{i} b_{j}}{a_{i}^{2}+a_{j}^{2}} \geq 0$ holds.

11 See solution of Problem 7.

Remark 1 If $f$ is a probability density function then $\int_{-\infty}^{\infty} f(x) \ln f(x) d x$ is called the entropy of $f$. Problem 11 is about the existence of a probability density function with infinite entropy.

\section{4}

(a) It suffices to verify that the distribution of the random variable $\min (\xi, \eta)$ has no atoms, i.e., for each $x \in \mathbb{R}$ it holds $\mathrm{P}(\min (\xi, \eta)=x)=0$. This is correct because

$$
\mathrm{P}(\min (\xi, \eta)=x) \leq \mathrm{P}(\xi=x)+\mathrm{P}(\eta=x)=0
$$

(b) Similar to (a), for each Borel measurable set $B$ with $\lambda(B)=0$ (here $\lambda$ is Lebesgue measure), it holds

$$
\mathrm{P}(\min (\xi, \eta) \in B) \leq \mathrm{P}(\xi \in B)+\mathrm{P}(\eta \in B)=0,
$$

because $\xi$ and $\eta$ have probability density functions. Therefore, the probability distribution of the random variable $\min (\xi, \eta)$ is absolutely continuous with respect to Lebesgue measure, and by Radon-Nikodym theorem the latter random variable has a probability density function.

15 For $\alpha \in(0,1)$, put $x(\alpha)=\left(x_{1}(\alpha), \frac{\alpha}{2}, \frac{\alpha}{3}, \ldots, \frac{\alpha}{n}, \ldots\right)$ and select a number $x_{1}(\alpha)$ such that

$$
\left(x_{1}(\alpha)\right)^{2}+\alpha^{2}\left(\frac{1}{4}+\frac{1}{9}+\ldots\right)=\left(x_{1}(\alpha)\right)^{2}+\left(\frac{\pi^{2}}{6}-1\right) \alpha^{2}=1 .
$$

Then $\|x(\alpha)\|_{2}=1, \alpha \in(0,1)$, and for $\alpha \neq \beta$, it holds

$$
\sum_{n=1}^{\infty}\left|x_{n}(\alpha)-x_{n}(\beta)\right| \geq|\alpha-\beta| \sum_{n=2}^{\infty} \frac{1}{n}=\infty .
$$

16 The random variable $\frac{\xi \eta}{\xi^{2}+\eta^{2}}$ is well-defined because

$$
\mathrm{P}\left(\xi^{2}+\eta^{2}=0\right)=\mathrm{P}(\xi=0, \eta=0)=\mathrm{P}^{2}(\xi=0)=0 .
$$

Random variables $\xi$ and $\eta$ are independent and identically distributed, therefore, by Fubini's theorem it holds 

$$
\begin{aligned}
\mathrm{E} \frac{\xi \eta}{\xi^{2}+\eta^{2}} &=\mathrm{E} \int_{0}^{+\infty} \xi \eta \exp \left(-t\left(\xi^{2}+\eta^{2}\right)\right) d t=\\
&=\int_{0}^{+\infty} \mathrm{E} \xi \eta \exp \left(-t\left(\xi^{2}+\eta^{2}\right)\right) d t=\int_{0}^{+\infty}\left(\mathrm{E} \xi \exp \left(-t \xi^{2}\right)\right)^{2} d t \geq 0
\end{aligned}
$$

The conditions of Fubini's theorem are fulfilled due to the inequality

$$
\mathrm{E} \int_{0}^{+\infty}|\xi \eta| e^{-t\left(\xi^{2}+\eta^{2}\right)} d t=\mathrm{E} \frac{|\xi \eta|}{\xi^{2}+\eta^{2}} \leq \frac{1}{2} \text {. }
$$

Remark 1 The inequality cannot be replaced with a strict one, because in the case where $\xi$ is symmetrically distributed, it holds $\mathrm{E}_{\frac{\xi \eta}{\xi^{2}+\eta^{2}}}=0$.

17 Consider $n$-dimensional simplex $K \subset \mathbb{R}^{n}$ with vertices $v_{1}, \ldots, v_{n+1}$, that is the convex hull of the vertices

$$
K=\operatorname{conv}\left(v_{1}, \ldots, v_{n+1}\right)=\left\{\alpha_{1} v_{1}+\ldots+\alpha_{n+1} v_{n+1} \mid \alpha_{i} \geq 0, \alpha_{1}+\ldots+\alpha_{n+1}=1\right\},
$$

where the points $v_{1}, \ldots, v_{n+1}$ do not lie in a proper subset of $\mathbb{R}^{n}$. Let $\lambda<n$. For each point $x=\beta_{1} v_{1}+\ldots+\beta_{n+1} v_{n+1} \in K$, there exists an index $j$ such that $\beta_{j} \leq \frac{1}{n+1}$. Let the line that passes through the points $x$ and $v_{j}$ intersects the face of the simplex opposite to $v_{j}$ at the point $\widetilde{v}_{j}$. Then

$$
\frac{\left\|\widetilde{v}_{j}-x\right\|}{\left\|v_{j}-x\right\|}=\frac{\beta_{j}}{1-\beta_{j}} \leq \frac{1}{n}<\frac{1}{\lambda} .
$$

Then the image of $v_{j}$ under the homothety with center $x$ and coefficient $\left(-\frac{1}{\lambda}\right)$ lies on the extension of the segment with endpoints $x$ and $\widetilde{v}_{j}$ behind the point $\widetilde{v}_{j}$, that is the image is out of $K$. Hence the image of $K$ under the homothety with center $x$ and coefficient $(-\lambda)$ does not contain the point $v_{j}$.

Thus, $\lambda \geq n$. For $\lambda=n$, and for each convex compact set $K \subset \mathbb{R}^{n}$ we will find a point $x \in K$ such that the image of $K$ under the homothety with center $x$ and coefficient $(-n)$ contains $K$. We may assume that $K$ has positive ( $n$-dimensional) volume, otherwise $K$ lies in a proper subset of $\mathbb{R}^{n}$ and the problem is reduced to the case of lower dimension. For arbitrary points $v_{1}, \ldots, v_{n+1}$ in the compact $K$, consider the volume of the set $\operatorname{conv}\left(v_{1}, \ldots, v_{n+1}\right)$. It is a continuous function on the compact $K^{n+1}$, hence it attains its (positive) maximum at some points $v_{1}^{*}, \ldots, v_{n+1}^{*}$, which are vertices of a $n$-dimensional simplex.

Show that the point

$$
x=\frac{1}{n+1} \sum_{i=1}^{n+1} v_{i}^{*}
$$

has required property. Under the homothety with center $x$ and coefficient $(-n)$, the hyperplane which contains the points $v_{1}^{*}, \ldots, v_{i-1}^{*}, v_{i+1}^{*}, \ldots, v_{n+1}^{*}$ is mapped to the parallel hyperplane $\pi_{i}$ that contains the point $v_{i}^{*}$. There is no point $y \in K$ lying on the other side of $\pi_{i}$ compared with $v_{1}^{*}, \ldots, v_{n+1}^{*}$, otherwise we can replace $v_{i}^{*}$ with $y$ in the collection $v_{1}^{*}, \ldots, v_{n+1}^{*}$ and obtain greater volume. Hence $K$ belongs to the simplex which is the intersection of $n+1$ semi-spaces with boundaries $\pi_{i}, 1 \leq i \leq$ $n+1$ that contain the points $v_{1}^{*}, \ldots, v_{n+1}^{*}$. But this simplex is just an image of $K$ under the latter homothety.

Answer: $\lambda=n$.

18 Denote by $I$ the identity operator in $X$. Because $\left\|I-T_{n}\right\| \leq 2, n \geq 1$, it suffices to prove the convergence

$$
\lim _{n \rightarrow \infty}\left\|f-T_{n} f\right\|_{X}=0
$$

for a dense in $X$ set of functions. Put

$$
(x)_{+}=\left\{\begin{array}{l}
x, x \geq 0 \\
0, x<0
\end{array}\right.
$$

Since the linear hull of functions $\left\{1, x,(x-a)_{+}, a \in(0,1)\right\}$ contains all the piecewise linear continuous functions on $[0,1]$, it is dense in $X$. Hence it suffices to prove that

$$
\forall a \in(0,1) \quad \lim _{n \rightarrow \infty}\left\|(x-a)_{+}-T_{n}(x-a)_{+}\right\|_{X}=0 .
$$

The operator $T_{n}$ is nonnegative, therefore, $T_{n}(x-a)_{+} \geq T_{n}(x-a)$ and $T_{n}(x-a)_{+} \geq$ $T_{n} 0=0$, thus, $T_{n}(x-a)_{+} \geq\left(T_{n}(x-a)\right)_{+}$. Since $(b)_{+}-(c)_{+} \leq(b-c)_{+}$for all $b, c \in \mathbb{R}$, we have

$$
(x-a)_{+}-T_{n}(x-a)_{+} \leq(x-a)_{+}-\left(T_{n}(x-a)\right)_{+} \leq\left((x-a)-T_{n}(x-a)\right)_{+},
$$

hence $\left((x-a)_{+}-T_{n}(x-a)_{+}\right)_{+} \leq\left((x-a)-T_{n}(x-a)\right)_{+}$, and

$$
\left\|\left((x-a)_{+}-T_{n}(x-a)_{+}\right)_{+}\right\|_{X} \leq\left\|\left((x-a)-T_{n}(x-a)\right)_{+}\right\|_{X} \rightarrow 0, \text { as } n \rightarrow \infty .
$$

To accomplish the proof we use the following lemma.

Lemma 1 Let functions $g, h \in X=L_{1}([0,1])$ satisfy $g(x) \geq 0, h(x) \geq 0$, $x \in[0,1]$, and $\|h\|_{X} \leq\|g\|_{X}$. Then

$$
\|g-h\|_{X} \leq 2\left\|(g-h)_{+}\right\|_{X}
$$

Proof It is easy to verify under conditions of the lemma, it holds

$$
\left\|(g-h)_{+}\right\|_{X}-\left\|(h-g)_{+}\right\|_{X}=\|g\|_{X}-\|h\|_{X} \geq 0,
$$

hence $\|g-h\|_{X}=\left\|(g-h)_{+}\right\|_{X}+\left\|(h-g)_{+}\right\|_{X} \leq 2\left\|(g-h)_{+}\right\|_{X}$.

Now, we apply the lemma to $g(x) \equiv(x-a)_{+}$and $h=T_{n} g$ (by the condition of the problem $h(x) \geq 0, x \in[0,1]$, and $\|h\|_{X}=\left\|T_{n} g\right\|_{X} \leq\|g\|_{X}$, that is conditions of the lemma are fulfilled). According to the lemma,

$\left\|(x-a)_{+}-T_{n}(x-a)_{+}\right\|_{X} \leq 2\left\|\left((x-a)_{+}-T_{n}(x-a)_{+}\right)_{+}\right\|_{X} \rightarrow 0$, as $n \rightarrow \infty$, and the statement of the problem is proved. 1 It holds

$$
\prod_{k=1}^{n} \frac{(k+p)(k+q)}{(k+r)(k+s)}=\frac{r ! s !}{p ! q !} \cdot \frac{(n+p) !}{(n+r) !} \cdot \frac{(n+q) !}{(n+s) !} .
$$

After canceling out common factors in numerator and denominator of the fraction $\frac{(n+p) !}{(n+r) !}$, it is easy to show that

$$
\lim _{n \rightarrow \infty} \frac{(n+p) ! n^{r-p}}{(n+r) !}=1
$$

and in a similar way that $\lim _{n \rightarrow \infty} \frac{(n+q) ! n^{s-q}}{(n+s) !}=1$. Therefore,

$$
\lim _{n \rightarrow \infty} \prod_{k=1}^{n} \frac{(k+p)(k+q)}{(k+r)(k+s)}=\frac{r ! s !}{p ! q !} \cdot \lim _{n \rightarrow \infty} n^{p+q-r-s},
$$

whence the answer follows.

Answer: $\left\{\begin{array}{cc}0 & \text { if } p+q<r+s ; \\ \frac{r ! s !}{p ! q !} & \text { if } p+q=r+s ; \\ +\infty & \text { if } p+q>r+s\end{array}\right.$

2 Because $k\left(\begin{array}{c}2 n \\ k\end{array}\right)=k \cdot \frac{(2 n) !}{k !(2 n-k) !}=2 n \cdot \frac{(2 n-1) !}{(k-1) !(2 n-k) !}=2 n\left(\begin{array}{c}2 n-1 \\ k-1\end{array}\right)$, the sum

$$
\sum_{k=1}^{n} k\left(\begin{array}{c}
2 n \\
k
\end{array}\right)=\sum_{k=1}^{n} 2 n\left(\begin{array}{c}
2 n-1 \\
k-1
\end{array}\right)=n \cdot 2 \sum_{i=0}^{n-1}\left(\begin{array}{c}
2 n-1 \\
i
\end{array}\right)=n \sum_{i=0}^{2 n-1}\left(\begin{array}{c}
2 n-1 \\
i
\end{array}\right)=n 2^{2 n-1}
$$

is divisible by 8 , for all $n \geq 2$. 3 We describe a winning strategy for the second player. If the first player places a number $a$ to cell $(i, j)$ of the matrix, then the second player in return places the number $101-a$ to cell $(11-i, j)$ (it is clear that he/she can always do that). In the finally formed matrix, the sum of the 1 st and 10 th rows is equal to the sum of the 2nd and 9th rows, hence the matrix is singular.

Answer: the second player has a winning strategy.

4 Show that the function $f$ is bounded at $[1,+\infty)$. Indeed, for all $x \geq 1$, by Newton-Leibniz formula it holds

$f(x)=f(1)+\int_{1}^{x} f^{\prime}(t) d t \leq f(1)+\int_{1}^{x} \frac{1}{t^{4}} d t=f(1)+\frac{1}{3}\left(1-\frac{1}{x^{3}}\right)<f(1)+\frac{1}{3}$.

Therefore, if $f$ is unbounded at $(0,+\infty)$, then as a result of the monotonicity $f(x) \rightarrow-\infty$, as $x \rightarrow 0+$. Further, consider an integer $k$ such that $\frac{\pi}{2}-2 \pi k<f(1)$. By the intermediate value theorem there exist points $0<s<t<1$ for which $f(s)=-\frac{\pi}{2}-2 \pi k$ and $f(t)=\frac{\pi}{2}-2 \pi k$. For $x \in[s, t]$, it holds $-\frac{\pi}{2}-2 \pi k \leq$ $f(x) \leq \frac{\pi}{2}-2 \pi k, \cos f(x) \geq 0$, hence $f^{\prime}(x) \leq 1, x \in[s, t]$. But by the mean value theorem there exists $\theta \in[s, t]$ such that $f^{\prime}(\theta)=\frac{f(t)-f(s)}{t-s}=\frac{\pi}{t-s}>\pi>1$, a contradiction.

5 We will construct a required polynomial of the form

$$
P(x)=x^{2007}+c Q(x)+\sum_{i=1}^{1003}\left(a_{i} x+b_{i}\right) Q_{i}(x),
$$

where $Q(x)=\prod_{j=1}^{1003}(x-j)^{2}$ and $Q_{i}(x)=\prod_{\substack{j=1 \\ j \neq i}}^{1003}(x-j)^{2}, 1 \leq i \leq 1003$. Notice that for each $1 \leq i \leq 1003$, it holds $Q(i)=Q^{\prime}(i)=0$ and $Q_{j}(i)=Q_{j}^{\prime}(i)=0$ if $j \neq i$. Hence

$$
\begin{gathered}
P(i)=i^{2007}+\left(a_{i} \cdot i+b_{i}\right) Q_{i}(i), \\
P^{\prime}(i)=2007 i^{2006}+a_{i} Q_{i}(i)+\left(a_{i} \cdot i+b_{i}\right) Q_{i}^{\prime}(i),
\end{gathered}
$$

and since $Q_{i}(i) \neq 0$, one can choose constants $a_{i}$ and $b_{i}$ in such a way that

$$
P(i)=2 i, P^{\prime}(i)=0, i=1,2, \ldots, 1003 .
$$

Now, one can choose $c$ such that

$$
P\left(\frac{1}{2}\right)>2007, P\left(\frac{3}{2}\right)>2007, \ldots, P\left(\frac{2005}{2}\right)>2007 . \quad(* *)
$$

The conditions $(*)$ and $(* *)$ imply that the polynomial $P$ of degree 2007 has exactly 2006 extreme points, namely $x_{1}<1<x_{2}<2<\ldots<x_{1003}<1003$, and the polynomial is monotone at each of the intervals

$$
\left(-\infty, x_{1}\right),\left(x_{1}, 1\right),\left(1, x_{2}\right), \ldots,(1003,+\infty)
$$

Taking into account that $P(i)=2 i$ and $P\left(x_{i}\right)>2007,1 \leq i \leq 1003$, it is easy to verify that the polynomial $P$ has required properties.

Answer: yes, there exists.

6 Let the minute hand makes an angle $\varphi \in[0 ; 2 \pi)$ with the vertical position. At that moment, the hour hand makes one of the angles

$$
\psi_{k}=\frac{\varphi+2 \pi k}{12}(\bmod 2 \pi), k \in \mathbb{Z},
$$

with the vertical position. The hands intersect along a segment of length

$$
\max \left(0, \cos \left(\psi_{k}-\varphi\right)\right),
$$

hence the area in question equals $\frac{1}{2} \int_{0}^{2 \pi} \rho^{2}(\varphi) d \varphi$, where

$$
\rho(\varphi)=\max \left(0, \max _{k} \cos \left(\psi_{k}-\varphi\right)\right), \varphi \in \mathbb{R} .
$$

Notice that

$$
\rho(\varphi)=\cos \left(\min _{k}\left|\frac{\varphi+2 \pi k}{12}-\varphi\right|\right)=\cos \left(\min _{k}\left|\frac{11 \varphi}{12}-\frac{\pi k}{6}\right|\right) .
$$

This function is periodic with period of $\frac{2 \pi}{11}$. Since for $\varphi \in\left[-\frac{\pi}{11}, \frac{\pi}{11}\right]$ it holds $\rho(\varphi)=$ $\cos \left(\frac{11 \varphi}{12}\right)$, the area of the figure formed by intersections of hands equals

$$
\begin{gathered}
\frac{1}{2} \int_{0}^{2 \pi} \rho^{2}(\varphi) d \varphi=\frac{11}{2} \int_{-\frac{\pi}{11}}^{\frac{\pi}{11}} \rho^{2}(\varphi) d \varphi=\frac{11}{2} \int_{-\frac{\pi}{11}}^{\frac{\pi}{11}} \cos ^{2}\left(\frac{11 \varphi}{12}\right) d \varphi= \\
=\frac{11}{4} \int_{-\frac{\pi}{11}}^{\frac{\pi}{11}}\left(1+\cos \left(\frac{11 \varphi}{6}\right)\right) d \varphi=\left.\left(\frac{11 \varphi}{4}+\frac{3}{2} \sin \left(\frac{11 \varphi}{6}\right)\right)\right|_{-\frac{\pi}{11}} ^{\frac{\pi}{11}}=\frac{\pi+3}{2} .
\end{gathered}
$$

Answer: $\frac{\pi+3}{2}$.

7 Look at the behavior of expression $x_{1}^{3}+x_{2}^{3}$ when we move the variables $x_{1} \leq x_{2}$ together, that is, replace them with $x_{1}+\varepsilon$ and $x_{2}-\varepsilon, 0<\varepsilon \leq \frac{x_{2}-x_{1}}{2}$, and when we move them apart, that is, replace them with $x_{1}-\varepsilon$ and $x_{2}+\varepsilon, \varepsilon>0$. After moving together, the product $x_{1} x_{2}$ is not decreasing, and after moving apart, it is not increasing. Hence the sum of cubes of two variables 

$$
x_{1}^{3}+x_{2}^{3}=\left(x_{1}+x_{2}\right)\left(\left(x_{1}+x_{2}\right)^{2}-3 x_{1} x_{2}\right)
$$

is not decreasing as two variables move apart, if $x_{1}+x_{2} \geq 0$, and as they move together, if $x_{1}+x_{2} \leq 0$.

We will move apart couples of variables $x_{i}, x_{j}$, for which $x_{i}+x_{j} \geq 0$, until one of the variables coincides with an end of the segment $[-1 ; 2]$. That can be done while at least one couple of variables with nonnegative sum remains in the interval $(-1 ; 2)$. After that we get one of the next two situations:

(1) some variables are equal to 2 or $-1$, and the rest of the variables have negative pairwise sums (and moreover each of the variables in the interval $(-1 ; 2)$ has negative sum with -1), or

(2) some variables are equal to 2 or $-1$, and a single variable in the interval $(-1 ; 2)$ has nonnegative sum with $-1$.

In the first case, let $x$ be the mean value of all the variables in $[-1 ; 2)$. Then $x<0$ because all pairwise sums are negative. We will move together couples of variables from the interval $[-1 ; 2)$ as described above, until one of the variables in the couple equals $x$.

Thus, we can start with an arbitrary collection of points $x_{1}, \ldots, x_{10}$ and use the described above moving together and apart (which preserve the sum of elements and not decrease the sum of cubes) to reach one of the following collections:

(1) $k$ variables are at a point $x \in[-1 ; 0]$ and $10-k$ variables are at the point 2 $(k=0, \ldots, 10)$

(2) $k$ variables are at the point $-1$, a single variable equals $x \in[1 ; 2]$, and $9-k$ variables are at the point $2(k=0, \ldots, 9)$.

From the conditions $x_{1}+\ldots+x_{10}=10$ and $x \in[-1 ; 0]$ (or $x \in[1 ; 2]$ ), we obtain that either $k=4$ or $k=5$ for collections of the first type and $k=3$ for collections of the second type. It remains to examine the following collections:

$$
\begin{gathered}
\left(-\frac{1}{2},-\frac{1}{2},-\frac{1}{2},-\frac{1}{2}, 2,2,2,2,2,2\right), \quad(0,0,0,0,0,2,2,2,2,2), \\
(-1,-1,-1,1,2,2,2,2,2,2) .
\end{gathered}
$$

The maximal value of the sum of cubes is equal to $47.5$ and attained at the first collection.

Answer: $47.5$.

8 Show that $a_{n}$ equals to the number $b_{n}$ of permutations of $n$ elements which consist only of the cycles of length 1 and 2 . For $n \geq 3$, either the $n$th element of a permutation is fixed, and then one can determine the permutation of other elements in $b_{n-1}$ ways, or the $n$th element forms a cycle of length 2 with some of $n-1$ other elements, and then one can determine the permutation of other elements in $b_{n-2}$ ways. Hence $b_{n}=b_{n-1}+(n-1) b_{n-2}, n \geq 2$, and equalities $a_{1}=b_{1}=1, a_{2}=b_{2}=2$ imply that $a_{n}=b_{n}$ for all $n \geq 1$. Fix an arbitrary odd number $p$ and check that $a_{p}-1=b_{p}-1$ is divisible by $p$, or equivalently, the number of permutations with cycles of length 1 and 2 which contain at least one cycle of length 2 is divisible by $p$. For that purpose we show that for each $1 \leq k \leq \frac{p-1}{2}$, the number of permutations which contain exactly $k$ cycles of length 2 is divisible by $p$. To determine such a permutation one has to choose $2 k$ elements which will form the cycles of length 2 , and divide them into couples, while an order of couples and of elements in couples does not matter. Therefore, the number of permutations with $k$ cycles of length 2 equals

$$
\left(\begin{array}{c}
p \\
2 k
\end{array}\right) \cdot \frac{(2 k) !}{2^{k} \cdot k !}=\frac{p}{2^{k}} \cdot\left(\begin{array}{c}
p-k \\
k
\end{array}\right) \cdot \frac{(p-1) !}{(p-k) !},
$$

hence it is divisible by $p$, because the number $p$ is odd.

9 For each integer $k$, consider $n \times n$ matrices of the form $A_{k}=B_{k} C B_{-k}$, where

$$
B_{k}=\left(\begin{array}{cccccc}
1 & k & 0 & \ldots & 0 & 0 \\
0 & 1 & 0 & \ldots & 0 & 0 \\
0 & 0 & 1 & \ldots & 0 & 0 \\
\ldots & \ldots & \ldots & \ldots & \ldots \\
0 & 0 & 0 & \ldots & 1 & 0 \\
0 & 0 & 0 & \ldots & 0 & 1
\end{array}\right), \quad C=\left(\begin{array}{cccccc}
0 & 1 & 0 & \ldots & 0 & 0 \\
0 & 0 & 1 & \ldots & 0 & 0 \\
0 & 0 & 0 & \ldots & 0 & 0 \\
\ldots & \ldots & \ldots & \ldots & \ldots \\
0 & 0 & 0 & \ldots & 0 & 1 \\
1 & 0 & 0 & \ldots & 0 & 0
\end{array}\right)
$$

(here $C$ is a permutation matrix). Then the matrix $A_{k}$ has integer entries and since $B_{-k}=B_{k}^{-1}$ and $C^{n}=I$, it holds

$$
A_{k}^{n}=\left(B_{k} C B_{-k}\right)^{n}=\left(B_{k} C B_{k}^{-1}\right)^{n}=B_{k} C^{n} B_{k}^{-1}=B_{k} B_{k}^{-1}=I .
$$

It remains to notice that $A_{k}=\left(\begin{array}{cc}k & 1-k^{2} \\ 1 & -k\end{array}\right)$ for $n=2$, and

$$
A_{k}=\left(\begin{array}{cccccc}
0 & 1 & k & 0 & \ldots & 0 \\
0 & 0 & 1 & 0 & \ldots & 0 \\
0 & 0 & 0 & 1 & \ldots & 0 \\
\ldots & \ldots & \ldots & \ldots & \ldots & \ldots \\
0 & 0 & 0 & 0 & \ldots & 1 \\
1 & -k & 0 & 0 & \ldots & 0
\end{array}\right), n \geq 3
$$

that is all the matrices $A_{k}$, which correspond to different values of $k$, are distinct. Answer: $n \geq 2$.

10 Consider a continuous function $f(x)=x+\ln x, x \in(0,1]$. Since $\lim _{x \rightarrow 0+} f(x)=$ $-\infty$ and $f(1)=1$, by the intermediate value theorem there exists $a \in(0,1)$ such that $f(a)=0$. It is clear that $\sin a \neq 0, f^{\prime}(a)=1+\frac{1}{a} \neq 0$, hence it holds $\frac{\sin x}{x+\ln x} \sim \frac{\sin a}{(x-a) f^{\prime}(a)}$, as $x \rightarrow a$. Since the integral $\int_{a}^{1} \frac{d x}{x-a}$ is divergent, the initial integral is divergent as well.

Answer: the integral is divergent. 11 Let $B_{N}=\{x \in \mathbb{R}: f(x) \geq N\}, N \geq 0$. Show that there exists $M$ such that $\lambda\left(B_{M}\right)<\infty$. Indeed, otherwise there exists a set $A_{0} \subset B_{1}, \lambda\left(A_{0}\right)=1$, and for $n \geq 1$, one can select subsequently the sets $A_{n}$ such that $A_{n} \subset B_{2^{n}} \backslash\left(\bigcup_{i=0}^{n-1} A_{i}\right)$ and $\lambda\left(A_{n}\right)=\frac{1}{2^{n}}$. Put $A=\bigcup_{n=0}^{\infty} A_{n}$. Then $\lambda(A)=\sum_{n=0}^{\infty} \lambda\left(A_{n}\right)=2$, but by the choice of the sets $A_{n}$ it holds

$$
\int_{A} f(x) d \lambda(x)=\sum_{n=0}^{\infty} \int_{A_{n}} f(x) d \lambda(x) \geq \sum_{n=0}^{\infty} 2^{n} \lambda\left(A_{n}\right)=\sum_{n=0}^{\infty} \frac{2^{n}}{2^{n}}=+\infty
$$

a contradiction with the condition of the problem.

Show that a constant $M$, for which $\lambda\left(B_{M}\right)<+\infty$, and the function

$$
g(x)=\left\{\begin{array}{r}
f(x), x \in B_{M}, \\
0, x \notin B_{M}
\end{array}\right.
$$

have required properties. Indeed, by the construction $f(x) \leq g(x)+M$ for all $x \in$ $\mathbb{R}$, and $g$ is integrable because $\int_{\mathbb{R}} g d \lambda=\int_{B_{M}} f d \lambda<+\infty$.

12 Notice that $\xi$ can be expanded as $\xi=m+\sigma \zeta$, where $\zeta \sim N(0 ; 1)$ is a standard normal random variable. Hence

$$
f(\sigma)=\frac{1}{\sqrt{2 \pi}} \int_{\mathbb{R}} \frac{e^{-x^{2} / 2}}{1+e^{m+\sigma x}} d x=\frac{1}{\sqrt{2 \pi}} \int_{0}^{\infty} p(x, \sigma) e^{-x^{2} / 2} d x
$$

where

$$
p(x, \sigma)=\frac{1}{1+e^{m+\sigma x}}+\frac{1}{1+e^{m-\sigma x}} .
$$

Fix an arbitrary $x>0$ and study the monotonicity of $p(x, \sigma)$ as a function in $\sigma$. We have

$$
\begin{aligned}
p_{\sigma}^{\prime}(x, \sigma) &=-\frac{x e^{m+\sigma x}}{\left(1+e^{m+\sigma x}\right)^{2}}+\frac{x e^{m-\sigma x}}{\left(1+e^{m-\sigma x}\right)^{2}}=-\frac{x e^{m+\sigma x}}{\left(1+e^{m+\sigma x}\right)^{2}}+\frac{x e^{m+\sigma x}}{\left(e^{\sigma x}+e^{m}\right)^{2}}=\\
&=x e^{m+\sigma x} \cdot \frac{-\left(e^{\sigma x}+e^{m}\right)^{2}+\left(1+e^{m+\sigma x}\right)^{2}}{\left(1+e^{m+\sigma x}\right)^{2}\left(e^{\sigma x}+e^{m}\right)^{2}}=\frac{x e^{m+\sigma x}\left(e^{2 \sigma x}-1\right)\left(e^{2 m}-1\right)}{\left(1+e^{m+\sigma x}\right)^{2}\left(e^{\sigma x}+e^{m}\right)^{2}} .
\end{aligned}
$$

Hence for all $x>0$, the sign of $p_{\sigma}^{\prime}(x, \sigma)$ coincides with the sign of the expression $e^{2 m}-1$. Therefore,

$$
f^{\prime}(\sigma)=\frac{1}{\sqrt{2 \pi}} \int_{0}^{\infty} p_{\sigma}^{\prime}(x, \sigma) e^{-x^{2} / 2} d x\left\{\begin{array}{l}
<0 \text { if } m<0 \\
=0 \text { if } m=0 \\
>0 \text { if } m>0
\end{array}\right.
$$

Answer: $f$ decreases if $m<0$; is a constant if $m=0$; increases if $m>0$.

13 Put $C=A+B-I$ and

$$
D=A^{-1}+B^{-1}-\frac{1}{2}\left(A^{-1} B^{-1}+B^{-1} A^{-1}\right)=\frac{1}{2}\left(A^{-1} C B^{-1}+B^{-1} C A^{-1}\right) .
$$

Then for every vector $x$ it holds

$$
\begin{aligned}
(D x, x) &=\frac{1}{2}\left(\left(A^{-1} C B^{-1} x, x\right)+\left(B^{-1} C A^{-1} x, x\right)\right) \\
&=\frac{1}{2}\left(\left(C B^{-1} x, A^{-1} x\right)+\left(A^{-1} x, C B^{-1} x\right)\right)=\left(C B^{-1} x, A^{-1} x\right) .
\end{aligned}
$$

Notice that the matrices $A B^{-1}=A B^{-1 / 2} \cdot B^{-1 / 2}$ and $B^{-1 / 2} A B^{-1 / 2}$ have common eigenvalues, and moreover the latter matrix is positive definite. Then all the eigenvalues of $A B^{-1}$ are positive. Let $x \neq 0$ be an eigenvector of $A B^{-1}$ that corresponds to an eigenvalue $\lambda>0$. Then $B^{-1} x=\lambda A^{-1} x,(D x, x)=\lambda\left(C A^{-1} x, A^{-1} x\right)>0$. Therefore, the matrix $D$ cannot be negative definite.

Answer: no, it is impossible.

Remark 1 Let $A=\left(\begin{array}{ll}1 & 2 \\ 2 & 5\end{array}\right)$ as $B=\left(\begin{array}{ll}5 & 2 \\ 2 & 1\end{array}\right)$. Then the matrix $C=\left(\begin{array}{ll}5 & 4 \\ 4 & 5\end{array}\right)$ is positive definite, but the matrix $D=\left(\begin{array}{cc}-3 & 8 \\ 8 & -3\end{array}\right)$ has eigenvalues of both signs. Thus, under the conditions of the problem the matrix $D$ is not necessarily positive definite.

14 Let $\operatorname{deg} P=n$. Assume that $\max _{\{|z|=1\}}|P(z)|<1$. Then for $|z|=1$, it holds $|P(z)|<\left|z^{n}\right|$, and by Rouche's theorem the polynomials $z^{n}$ and $z^{n}-P(z)$ have the same number of roots in the domain $\{z \in \mathbb{C}:|z|<1\}$ (here the roots are counted according to their multiplicity). But the polynomial $z^{n}$ has $n$ roots in the latter domain and the polynomial $z^{n}-P(z)$ has at most $n-1$ roots there, a contradiction. 

\section{8}

1 It suffices to consider $0<a, b, c<\frac{\pi}{2}$. For $0<x<\frac{\pi}{2}$, put $f(x)=\ln \left(\frac{\sin x}{x}\right)$ and define by continuity $f(0)=0$. Because for $0<x<\frac{\pi}{2}$ it holds

$$
f^{\prime}(x)=\cot x-\frac{1}{x}<0, \quad f^{\prime \prime}(x)=-\frac{1}{\sin ^{2} x}+\frac{1}{x^{2}}<0,
$$

the function $f$ is decreasing and concave. Hence

$$
\begin{aligned}
f(\pi / 6) &=f(a)+f(b)+f(c) \geq f(0)+f(a+b)+f(c) \geq \\
& \geq f(0)+f(0)+f(a+b+c)=f(a+b+c),
\end{aligned}
$$

therefore, $a+b+c \geq \frac{\pi}{6}$. As $a, b \rightarrow 0+$, it holds $c \rightarrow \frac{\pi}{6}$, thus, the required infimum equals $\frac{\pi}{6}$.

Answer: $\frac{\pi}{6}$.

2 Assume that $f$ is not uniformly continuous. Then for some $\varepsilon>0$, there exist $x_{n} \in \mathbb{R}$ and $\delta_{n}>0, n \geq 1$, such that $\delta_{n} \rightarrow 0$ as $n \rightarrow 0$ and $\left|f\left(x_{n}+\delta_{n}\right)-f\left(x_{n}\right)\right| \geq \varepsilon$, $n \geq 1$. Fix an arbitrary $N \in \mathbb{N}$ and choose $n \geq 1$ such that

$$
\sup _{x \in \mathbb{R}}\left|f\left(x-\delta_{n}\right)-2 f(x)+f\left(x+\delta_{n}\right)\right| \leq \frac{\varepsilon}{N} .
$$

Without loss of generality we may assume that $f\left(x_{n}+\delta_{n}\right)-f\left(x_{n}\right) \geq \varepsilon$. Inequality (*) implies that for all $x \in \mathbb{R}$, it holds

$$
f\left(x+\delta_{n}\right)-f(x) \geq f(x)-f\left(x-\delta_{n}\right)-\frac{\varepsilon}{N},
$$

whence by induction on $k$ we get

$$
f\left(x_{n}+(k+1) \delta_{n}\right)-f\left(x_{n}+k \delta_{n}\right) \geq \varepsilon\left(1-\frac{k}{N}\right), \quad 0 \leq k \leq N-1 .
$$

Adding the latter inequalities we obtain

$$
f\left(x_{n}+N \delta_{n}\right)-f\left(x_{n}\right) \geq \varepsilon\left(\frac{N}{N}+\frac{N-1}{N}+\ldots+\frac{1}{N}\right)=\frac{(N+1) \varepsilon}{2},
$$

hence $f$ cannot be bounded, a contradiction.

Answer: yes, it follows.

3 Since $f$ is even, it holds $f^{\prime}(0)=f^{\prime \prime \prime}(0)=0$. Introduce new variable $y=\sqrt{x}$, and use L'Hospital's Rule several times. We get

$$
\begin{aligned}
g^{\prime}(x) &=\lim _{x \rightarrow 0+} \frac{f(\sqrt{x})-f(0)}{x}=\lim _{y \rightarrow 0+} \frac{f(y)-f(0)}{y^{2}}=\\
&=\lim _{y \rightarrow 0+} \frac{f^{\prime}(y)}{2 y}=\lim _{y \rightarrow 0+} \frac{f^{\prime}(y)-f^{\prime}(0)}{2 y}=\frac{f^{\prime \prime}(0)}{2}
\end{aligned}
$$

and

$$
\begin{aligned}
g^{\prime \prime}(x) &=\lim _{x \rightarrow 0+} \frac{g^{\prime}(x)-g^{\prime}(0)}{x}=\lim _{x \rightarrow 0+} \frac{\frac{f^{\prime}(\sqrt{x})}{2 \sqrt{x}}-\frac{f^{\prime \prime}(0)}{2}}{x}=\\
&=\lim _{y \rightarrow 0+} \frac{f^{\prime}(y)-f^{\prime \prime}(0) y}{2 y^{3}}=\lim _{y \rightarrow 0+} \frac{f^{\prime \prime}(y)-f^{\prime \prime}(0)}{6 y^{2}}=\\
&=\lim _{y \rightarrow 0+} \frac{f^{\prime \prime \prime}(y)}{12 y}=\lim _{y \rightarrow 0+} \frac{f^{\prime \prime \prime}(y)-f^{\prime \prime \prime}(0)}{12 y}=\frac{1}{12} f^{(4)}(0) .
\end{aligned}
$$

Answer: $g^{\prime \prime}(0)=\frac{1}{12} f^{(4)}(0)$

4 Put $I_{k}=\left[2^{2^{k}}, 2^{2^{k+1}}\right), k \geq 1$. Then $[4,+\infty)=\bigcup_{k \geq 1} I_{k}$. If $x \in I_{k}$ then $x^{2} \in I_{k+1}$ and $f\left(x^{2}\right)-f(x)=\left[\log _{2} \log _{2} x\right]^{-2}=\frac{1}{k^{2}}$. Hence for $x \in I_{k}$, from condition (a) we get subsequently

$$
\begin{aligned}
f(x)=f\left(x^{2}\right)-\frac{1}{k^{2}} &=f\left(x^{4}\right)-\frac{1}{k^{2}}-\frac{1}{(k+1)^{2}}=\ldots=\\
&=f\left(x^{2^{n}}\right)-\frac{1}{k^{2}}-\ldots-\frac{1}{(k+n-1)^{2}}, n \geq 1 .
\end{aligned}
$$

Let $c=\lim _{t \rightarrow+\infty} f(t)$. Then $x^{2^{n}} \rightarrow \infty$, and $f\left(x^{2^{n}}\right) \rightarrow c$ as $n \rightarrow \infty$. We pass to the limit in $(*)$ and get 

$$
f(x)=c-\sum_{m \geq k} \frac{1}{m^{2}}, \quad x \in I_{k}, k \geq 1 .
$$

For arbitrary $4 \leq x_{1} \leq x_{2}$, if $x_{1} \in I_{k_{1}}$ and $x_{2} \in I_{k_{2}}$ then $k_{1} \leq k_{2}$. Thus, relation (**) implies that $f\left(x_{1}\right) \leq f\left(x_{2}\right)$, that is, the function $f$ is nondecreasing.

5 Let $x_{1}, \ldots, x_{m}$ be distinct roots of $P(x)$ with multiplicities $k_{1}, \ldots, k_{m}$, respectively, $k_{1}+\ldots+k_{m}=n$. Polynomials $P(x)$ and $P^{\prime}(x)$ are divisible by the polynomial $Q(x)=\left(x-x_{1}\right)^{k_{1}-1} \ldots\left(x-x_{k}\right)^{k_{m}-1}$ of degree $n-m$. Then $R(x)=n P(x)-x P^{\prime}(x)$ is divisible by $Q(x)$ as well, hence either $R(x)$ is identical zero or $\operatorname{deg} R(x) \geq$ $\operatorname{deg} Q=n-m$. But

$$
R(x)=n P(x)-x P^{\prime}(x)=p_{n-1} x^{n-1}+2 p_{n-2} x^{n-2}+\ldots+n p_{0},
$$

therefore, $R(x)$ can be identical zero if and only if $p_{n-1}=p_{n-2}=\ldots=p_{0}=0$, i.e., $P(x) \equiv x^{n}$. Since $P(x)$ has $m \geq 2$ distinct roots, we get a contradiction. Thus, $\operatorname{deg} R \geq n-m$, hence at least one of the coefficients $p_{n-1}, \ldots, p_{n-m}$ is nonzero.

6 We use a natural inner product $(u, v)=u^{T} \cdot \bar{v}, u, v \in \mathbb{C}^{n}$. Denote by $L$ a linear hull of the columns of the matrix $A$ and by $\bar{L}$ a linear hull of vectors which are complex conjugate to the columns of $A$. Then $\operatorname{dim} L=\operatorname{dim} \bar{L}=\operatorname{rk} A$ and $L \perp \bar{L}$, whence $\operatorname{rk} A \leq \min \left(\frac{n}{2}, k\right)$.

It remains to give an example of a matrix with rank equal to $\min \left(\left[\frac{n}{2}\right], k\right)$. Consider

$$
A=\left(\begin{array}{ccccc}
1 & 0 & 0 & \ldots & 0 \\
i & 0 & 0 & \ldots & 0 \\
0 & 1 & 0 & \ldots & 0 \\
0 & i & 0 & \ldots & 0 \\
\ldots & \ldots & \ldots & \ldots & \ldots
\end{array}\right)
$$

i.e. $A=\left(a_{j l}\right)$, where

$$
a_{j l}= \begin{cases}1 & \text { if } l=2 j-1,1 \leq j \leq\left[\frac{n}{2}\right] \\ i & \text { if } l=2 j, 1 \leq j \leq\left[\frac{n}{2}\right] \\ 0, & \text { otherwise. }\end{cases}
$$

Then $A^{\mathrm{T}} \cdot A=0$ and $\mathrm{rk} A=\min \left(\left[\frac{n}{2}\right], k\right)$.

Answer: $\min \left(\left[\frac{n}{2}\right], k\right)$.

7 Let

$$
f(x)=\left\{\begin{array}{cc}
e^{-1 / x^{2}} \sin \left(e^{1 / x^{2}}\right), & x \neq 0, \\
0, & x=0 .
\end{array}\right.
$$

Then $f \in C(\mathbb{R})$, moreover $f \in C^{(\infty)}(\mathbb{R} \backslash\{0\})$ and

$$
|f(x)| \leq e^{-1 / x^{2}}=o\left(x^{n}\right), \text { as } x \rightarrow 0,
$$

for each $n \geq 1$. For $x \neq 0$, the derivative equals

$$
f^{\prime}(x)=\frac{2}{x^{3}} \cdot\left(e^{-1 / x^{2}} \sin \left(e^{1 / x^{2}}\right)-\cos \left(e^{1 / x^{2}}\right)\right) .
$$

Since $f^{\prime}$ is unbounded at each neighborhood of zero, it holds that $f \notin C^{(1)}(\mathbb{R})$.

Answer: no, it is not.

8 In case $n \leq 2$, every matrix $A$ satisfies the condition of the problem, because for all matrices it holds $M^{S}=M^{\mathrm{T}}$.

In case $n \geq 3$, as a result of the linearity it suffices to verify the condition only for matrices $B$ with a single nonzero entry $b_{i j}$. In particular if $i+j$ is even, then we get that $a_{k i}=0$ for all $k \neq i$, and $a_{j j}=a_{i i}$. Hence for $n \geq 4$, it holds $A=\operatorname{diag}(\alpha, \beta, \alpha, \beta, \ldots)$, where $\alpha$ and $\beta$ are some real numbers, and for $n=3$ the matrix $A$ takes a form $\left(\begin{array}{lll}\alpha & \gamma & 0 \\ 0 & \beta & 0 \\ 0 & \delta & \alpha\end{array}\right)$. It is easy to verify that for $n \geq 4$ all such matrices $A$ satisfy the condition, and for $n=3$ only the following matrices do: $A=\left(\begin{array}{lll}\alpha & 0 & 0 \\ 0 & \beta & 0 \\ 0 & 0 & \alpha\end{array}\right)$. Answer: $A$ is arbitrary for $n \leq 2 ; A=\operatorname{diag}(\alpha, \beta, \alpha, \beta, \ldots), \alpha, \beta \in \mathbb{R}$ for $n \geq 3$.

9 For an unmarked point $(x, y)$, define $R(x, y)$ in the same manner as for marked ones, i.e., $R(x, y)$ is the number of marked points $\left(x^{\prime}, y^{\prime}\right)$ with $x^{\prime} \geq x$ and $y^{\prime} \geq y$. Then for each marked point, it holds

$$
R(x, y)-R(x, y+1)-R(x+1, y)+R(x+1, y+1)=1 .
$$

Indeed, on the left-hand side all the points except $(x, y)$ are counted with plus and minus sign the same number of times, and the point $(x, y)$ is counted only in $R(x, y)$. Hence if the point $(x, y)$ is marked then among the numbers $R(x, y), R(x, y+1)$, $R(x+1, y)$, and $R(x+1, y+1)$ there exists at least one odd number, moreover that number corresponds to a marked point, because for an unmarked point $(a, b)$, it is evident that $R(a, b)=0$.

Divide the set of points with positive integer coordinates into subsets of the form

$$
\{(2 r-1,2 s-1),(2 r-1,2 s),(2 r, 2 s-1),(2 r, 2 s)\},(r, s) \in \mathbb{N},
$$

and consider those of them, which contain at least one marked point. We will get at least $n / 4$ such subsets and by the condition of the problem in each such subset the point $(2 r-1,2 s-1)$ is marked. Thus, each such subset contains a marked point $(x, y)$ with odd value of $R(x, y)$. This proves the required statement. 11 For arbitrary number $a \geq 0$, as a result of independence of $\xi$ and $\xi^{2}$ it holds

$$
\begin{aligned}
\mathrm{P}(|\xi|<a) &=\mathrm{P}\left(|\xi|<a, \xi^{2}<a^{2}\right)=\\
&=\mathrm{P}(|\xi|<a) \mathrm{P}\left(\xi^{2}<a^{2}\right)=\mathrm{P}^{2}(|\xi|<a),
\end{aligned}
$$

whence either $\mathrm{P}(|\xi|<a)=0$ or $\mathrm{P}(|\xi|<a)=1$. Thus, the cumulative distribution function of random variable $|\xi|$ has a single jump point $a_{0}$, hence $|\xi|=a_{0}$, almost surely, and $\cos \xi=\cos |\xi|=\cos a_{0}$, almost surely.

13 Since $f$ is even, it holds $f^{\prime}(0)=f^{\prime \prime \prime}(0)=\ldots=f^{(2 k-1)}(0)=0$. Introduce the function

$$
F(x)=f(x)-\left(f(0)+\frac{1}{2} f^{\prime \prime}(0) x^{2}+\ldots+\frac{1}{(2 k) !} f^{(2 k)}(0) x^{2 k}\right), x \in \mathbb{R} .
$$

Then $F$ is $2 k$ times differentiable and even function on $\mathbb{R}$, moreover $F^{(n)}(0)=0$, $0 \leq n \leq 2 k$. Therefore,

$$
g(x)=f(0)+\frac{1}{2} f^{\prime \prime}(0) x+\ldots+\frac{1}{(2 k) !} f^{(2 k)}(0) x^{k}+G(x),
$$

where $G(x)=F(\sqrt{x}), x \geq 0$.

Show that the function $G$ is $k$ times differentiable at $x=0$ and $G^{(k)}(0)=0$. This will imply that $g$ will be $k$ times differentiable as well, and moreover

$$
g^{(k)}(0)=\frac{k !}{(2 k) !} f^{(2 k)}(0) .
$$

By Taylor's formula with Peano's form of remainder for each $0 \leq j \leq 2 k-1$, it holds $F^{(j)}(x)=o\left(x^{2 k-j}\right)$, as $x \rightarrow 0+$. Therefore,

$$
F^{(j)}(\sqrt{x})=o\left(x^{k-\frac{j}{2}}\right), \text { as } x \rightarrow 0+, 0 \leq j \leq k \leq 2 k-1
$$

For $x>0$ and $1 \leq i \leq k$, we get

$$
G^{(i)}(x)=\sum_{1 \leq j \leq i} c_{j i} F^{(j)}(\sqrt{x}) x^{-i+\frac{j}{2}}=\sum_{1 \leq j \leq i} c_{j i} \cdot o\left(x^{k-i}\right), \text { as } x \rightarrow 0+,
$$

where $c_{j i}$ are some real numbers. Hence for $0 \leq i \leq k-1$, it holds $G^{(i)}(x)=o(x)$, as $x \rightarrow 0+$. This makes it possible to prove subsequently for $i=1, \ldots, k$, that there exists $G^{(i)}(0)=0$. Indeed, assume that there exists $G^{(i-1)}(0)=0$. Then

$$
G^{(i)}(0)=\lim _{x \rightarrow 0+} \frac{G^{(i-1)}(x)-G^{(i-1)}(0)}{x}=\lim _{x \rightarrow 0+} \frac{o(x)-0}{x}=0 .
$$

Answer: $g^{(k)}(0)=\frac{k !}{(2 k) !} f^{(2 k)}(0)$

14 Denote $\Delta=\max _{1 \leq i, j \leq n}\left|a_{i j}-b_{i j}\right|$. The real matrices $A$ and $B$ are symmetric, hence we have the following bound (here $x$ is a column vector and $\|x\|$ is Euclidean norm):

$$
\begin{gathered}
\left|\lambda_{\min }(A)-\lambda_{\min }(B)\right|=\left|\min _{\|x\|=1} x^{\mathrm{T}} A x-\min _{\|x\|=1} x^{\mathrm{T}} B x\right|= \\
=\left|\max _{\|x\|=1} x^{\mathrm{T}}(-A) x-\max _{\|x\|=1} x^{\mathrm{T}}(-B) x\right| \leq \\
\leq \max _{\|x\|=1}\left|x^{\mathrm{T}}(-A) x-x^{\mathrm{T}}(-B) x\right|=\max _{\|x\|=1}\left|x^{\mathrm{T}}(B-A) x\right| \leq \\
\leq \max _{\|x\|=1}\left|\sum_{i, j=1}^{n}\left(b_{i j}-a_{i j}\right) x_{i} x_{j}\right| \leq \Delta \cdot \max _{\|x\|=1} \sum_{i, j=1}^{n}\left|x_{i} x_{j}\right|= \\
=\Delta \cdot \max _{\|x\|=1}\left(\sum_{i=1}^{n}\left|x_{i}\right|\right)^{2} \leq \Delta \cdot \max _{\|x\|=1}\left(n \cdot \sum_{i=1}^{n} x_{i}^{2}\right)=n \Delta .
\end{gathered}
$$

15 Show that for all $x \in \mathbb{R}^{2}$, the matrix $J(x)=D f(x)+D f(x)^{\mathrm{T}}$ is positive definite. Indeed, $J(x)$ is a symmetric matrix and according to Problem 14 the function $\lambda(x)=$ $\lambda_{\min }(J(x))$ is continuous on $\mathbb{R}^{2}$. But by the condition of the problem $\lambda(0)=2>0$ and $\lambda(x) \neq 0, x \in \mathbb{R}^{2}$. Hence for each $x \in \mathbb{R}^{2}$, it holds $\lambda(x)>0$, that is, $J(x)$ is positive definite.

Suppose that $f(y)=f(z)$, for some $y \neq z$. For the function

$$
g(t)=(z-y)^{\mathrm{T}} f(y+t(z-y)), t \in[0,1],
$$

it holds $g(0)=g(1)$. By Rolle's theorem there exists $\theta \in(0,1)$ such that $g^{\prime}(\theta)=0$. Denote $x_{\theta}=y+\theta(z-y)$. Then

$$
\begin{aligned}
&g^{\prime}(\theta)=(z-y)^{\mathrm{T}} D f\left(x_{\theta}\right)(z-y)= \\
&=\frac{1}{2}(z-y)^{\mathrm{T}}\left(D f\left(x_{\theta}\right)+D f\left(x_{\theta}\right)^{\mathrm{T}}\right)(z-y)= \\
&=\frac{1}{2}(z-y)^{\mathrm{T}}\left(J\left(x_{\theta}\right)\right)(z-y)=0,
\end{aligned}
$$

and because of $z-y \neq 0$, we come to a contradiction with the fact that the matrix $J\left(x_{\theta}\right)$ is positive definite. Thus, $f$ is an injection.

Answer: yes, it is.

16 Let $F$ be the cumulative distribution function of $\xi$. Then $F$ is a continuous bijection of $\mathbb{R}$ onto $(0,1)$, moreover the random variable $F(\xi)$ is uniformly distributed on $(0,1)$. But then the random variable $1-F(\xi)$ is uniformly distributed on $(0,1)$ as well and one can take $f(x)=F(x)$ and $g(x)=1-f(x), x \in \mathbb{R}$. It remains to notice that $f$ does not coincide with $g$ because $f$ is increasing and $g$ is decreasing. Answer: yes, it is true.

17 Prove, e.g., that $\lambda(\{x: f(x) \geq 1\})=0$. Denote $B=\{x: f(x) \geq 1\}$. Suppose that $\lambda(B)=\varepsilon>0$. Since

$$
\int_{[0,1]} f^{2 n-1}(x) d \lambda(x)=\int_{(0,1)} f^{2 n-1}(x) d \lambda(x) \rightarrow 0, \text { as } n \rightarrow \infty
$$

for every closed set $F \subset[0,1]$ it holds

$$
\int_{F} f^{2 n-1}(x) d \lambda(x)=\int_{[0,1]} f^{2 n-1}(x) d \lambda(x)-\int_{[0,1] \backslash F} f^{2 n-1}(x) d \lambda(x) \rightarrow 0, \text { as } n \rightarrow \infty .
$$

The regularity of Lebesgue measure on $[0,1]$ implies that there exists a closed set $F \subset B$ such that $\lambda(F) \geq \frac{\varepsilon}{2}$. Then

$$
\int_{F} f^{2 n-1}(x) d \lambda(x) \geq \int_{F} 1 d \lambda(x) \geq \frac{\varepsilon}{2} \not \rightarrow 0, \text { as } n \rightarrow \infty
$$

a contradiction. In a similar way $\lambda(\{x: f(x) \leq-1\})=0$.

18 Notice that $M$ is a group with respect to the multiplication of matrices. Find the order of the group. The first column of a nonsingular $3 \times 3$ matrix over $\mathbb{Z}_{2}$ can be chosen in $2^{3}-1=7$ ways (it can be any nonzero vector), the second one in $2^{3}-2=6$ ways (it can be any vector that is linearly independent with the first one), and the third one in $2^{3}-2^{2}=4$ ways (it can be any vector which is linearly independent with the first two vectors). Hence $M$ consists of $7 \cdot 6 \cdot 4=168=8 \cdot 3 \cdot 7$ matrices. By Sylow's theorem $M$ has elements of order 3 and 7 . Moreover since $\left(\begin{array}{lll}1 & 0 & 1 \\ 0 & 1 & 1 \\ 1 & 1 & 1\end{array}\right)^{4}=I$ but $\left(\begin{array}{lll}1 & 0 & 1 \\ 0 & 1 & 1 \\ 1 & 1 & 1\end{array}\right)^{2} \neq I$, the group $M$ has an element of order 4 . Thus, the least number $n$ such that $A^{n}=I$ for all $A \in M$, is a divisor of $8 \cdot 3 \cdot 7$ and is divisible by $4 \cdot 3 \cdot 7$.

Show that $n=4 \cdot 3 \cdot 7=84$. It suffices to prove that $M$ has no element of order 8 , that is, $A^{8}=I$ and $A \in M$ imply that $A^{4}=I$. Over $\mathbb{Z}_{2}$, it holds

$$
A^{8}-I=\left(A^{4}-I\right)\left(A^{4}+I\right)=\left(A^{4}-I\right)^{2}=\ldots=(A-I)^{8}=0 .
$$

Hence the polynomial $p(t)=(t-1)^{8}$ annihilates the matrix $A$. But the minimal polynomial of $A$ is a divisor of $p(t)$ of at most third degree. Therefore, the minimal polynomial of $A$ is a divisor of the polynomial $(t-1)^{3}$. Hence $(t-1)^{4}$ annihilates the matrix $A$, that is $(A-I)^{4}=A^{4}-I=0$. Thus, $n=4 \cdot 3 \cdot 7=84$.

Answer: $n=84$. 1 A quadrilateral $A B C D$ is circumscribed if and only if $A D+B C=A B+C D$. Let $R$ be the radius of the circle circumscribed about the quadrilateral $A B C D$, $\angle A B C=\beta$ and $\angle A B D=\varphi \leq \beta$. Then $\angle D B C=\beta-\varphi$, and by the sine law

$$
A D+B C-A B-C D=B C-A B+2 R(\sin \varphi-\sin (\beta-\varphi))=: f(\varphi) .
$$

The function $f$ is continuous on $[0, \beta]$ and by the triangle inequality $f(0)=B C-$ $A B-A C<0$ and $f(\beta)=B C-A B+A C>0$. By the intermediate value theorem there exists $\varphi_{0}$ in $0<\varphi_{0}<\beta$ such that $f\left(\varphi_{0}\right)=0$, and the corresponding quadrilateral $A B C D$ is circumscribed.

Alternative solution. In the case $A B \neq B C$ consider a branch of the hyperbola with foci $A$ and $C$ that passes through the point $B$. This branch intersects the circumcircle of the triangle $A B C$ for the second time at a point $D$ such that $A D-C D=A B-B C$, that is, $A D+B C=A B+C D$, and the quadrangle $A B C D$ is circumscribed.

In the case $A B=B C$ the bisector of $A C$ intersects the circumcircle of the triangle $A B C$ in the required point $D$.

Answer: yes, there exists.

2 For $n=1$, the polynomial $x^{2}+x-1$ has irrational roots $\frac{-1 \pm \sqrt{5}}{2}$, and is irreducible in $\mathbb{Q}[x]$. For $n \geq 2$, it holds

$$
F_{n} x^{n+1}+F_{n+1} x^{n}-1=\left(x^{2}+x-1\right)\left(F_{n} x^{n-1}+F_{n-1} x^{n-2}+\ldots+F_{2} x+F_{1}\right),
$$

thus the polynomial is not irreducible.

Answer: $n=1$. 3 (a) The inequality is just an identity. Indeed, it holds

$$
\begin{aligned}
\frac{\cos A}{\sin B \sin C}+\frac{\cos B}{\sin A \sin C}+\frac{\cos C}{\sin A \sin B} &=\frac{\cos A \sin A+\cos B \sin B+\cos C \sin C}{\sin A \sin B \sin C}=\frac{\sin 2 A+\sin 2 B+\sin 2 C}{2 \sin A \sin B \sin C}=\\
&=\frac{2 \sin (A+B) \cos (A-B)-2 \sin (A+B) \cos (A+B)}{2 \sin A \sin B \sin (A+B)}=\frac{2 \sin A \sin B}{\sin A \sin B}=2
\end{aligned}
$$

(b) By the Cauchy-Schwarz inequality

$$
\begin{aligned}
&\left(\frac{\cos A}{\sqrt{\sin B \sin C}}+\frac{\cos B}{\sqrt{\sin A \sin C}}+\frac{\cos C}{\sqrt{\sin A \sin C}}\right)^{2} \leq \\
&\leq\left(\frac{\cos A}{\sin B \sin C}+\frac{\cos B}{\sin A \sin C}+\frac{\cos C}{\sin A \sin C}\right)(\cos A+\cos B+\cos C)= \\
&\quad=2(\cos A+\cos B+\cos C) \leq \sqrt{3},
\end{aligned}
$$

because the cosine function is concave on $\left[0, \frac{\pi}{2}\right]$, and by Jensen's inequality it holds $\cos A+\cos B+\cos C \leq 3 \cos \frac{A+B+C}{3}=3 \cos \frac{\pi}{3}=\frac{\sqrt{3}}{2}$.

4 The trace of a product of two matrices does not depend on the order of multipliers, therefore,

$\operatorname{tr} A B C=\operatorname{tr}(A B) C=\operatorname{tr} C(A B)=\operatorname{tr} C A B=\operatorname{tr}(C A) B=\operatorname{tr} B(C A)=\operatorname{tr} B C A$,

and $n=\operatorname{tr} I=\operatorname{tr}(A B C+B C A+C A B)=3 \operatorname{tr} A B C$. Thus, $n$ has to be divisible by 3 . For $n=3$, one can take

$$
A_{1}=\left(\begin{array}{lll}
0 & 1 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right), \quad B_{1}=\left(\begin{array}{lll}
0 & 0 & 0 \\
0 & 0 & 1 \\
0 & 0 & 0
\end{array}\right), \quad C_{1}=\left(\begin{array}{lll}
0 & 0 & 0 \\
0 & 0 & 0 \\
1 & 0 & 0
\end{array}\right) .
$$

For $n=3 k, k>1$, one can construct $A, B, C$ as block matrices: $A_{k}=$ diag $\left(A_{1}, \ldots, A_{1}\right), B_{k}=\operatorname{diag}\left(B_{1}, \ldots, B_{1}\right), C_{k}=\operatorname{diag}\left(C_{1}, \ldots, C_{1}\right)$.

Answer: $n=3 k, k \in \mathbb{N}$.

5 Denote $K=\{(x(t), y(t)) \mid t \in \mathbb{R}\} \subset \mathbb{R}^{2}$. If $\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right) \in K$ and $x_{1}+y_{1} \leq$ $x_{2}+y_{2}$, then $x_{1} \leq x_{2}$ and $y_{1} \leq y_{2}$, moreover $x_{1}+y_{1}=x_{2}+y_{2}$ implies that $x_{1}=x_{2}, y_{1}=y_{2}$. Hence on the set $L=\{x+y \mid(x, y) \in K\} \subset \mathbb{R}$ the following nondecreasing functions are well-defined: $f: x+y \mapsto x$ and $g: x+y \mapsto y$, where $(x, y) \in K$. Put $z(t)=x(t)+y(t), t \in \mathbb{R}$. Then $x(t)=f(z(t)), y(t)=g(z(t))$, $t \in \mathbb{R}$.

It remains to extend $f$ and $g$ from $L$ to $\mathbb{R}$ in such a way that the extensions will be nondecreasing. Show that $f, g: L \rightarrow \mathbb{R}$ are Lipschitz continuous with the Lipschitz constant equal to 1 . Indeed, for $z_{1}=x_{1}+y_{1}<z_{2}=x_{2}+y_{2}$ it holds

$$
\begin{aligned}
\left|z_{2}-z_{1}\right| &=x_{2}+y_{2}-x_{1}-y_{1}=\\
&=\left|x_{2}-x_{1}\right|+\left|y_{2}-y_{1}\right|=\left|f\left(z_{2}\right)-f\left(z_{1}\right)\right|+\left|g\left(z_{2}\right)-g\left(z_{1}\right)\right| .
\end{aligned}
$$

Hence $f$ and $g$ can be extended first by continuity to the closure $\bar{L}$, and then in a linear way to each of the intervals that compose $\mathbb{R} \backslash \bar{L}$ preserving the monotonicity of $f$ and $g$.

6 Denote $y_{n}=\frac{1}{n} \sum_{k=1}^{n} x_{k}, n \geq 1$. Then $x_{k}=k y_{k}-(k-1) y_{k-1}, k \geq 1$, where $y_{0}=0$, and

$$
\frac{1}{n^{p}} \sum_{k=1}^{n} k^{p-1} x_{k}=-\frac{1}{n^{p}} \sum_{k=1}^{n-1}\left((k+1)^{p-1}-k^{p-1}\right) k y_{k}+y_{n} .
$$

Put $c_{n k}=\frac{p}{(p-1) n^{p}}\left((k+1)^{p-1}-k^{p-1}\right) k, 1 \leq k \leq n-1, n \geq 1$. Then $c_{n k} \geq 0$, for each $k$ it holds $c_{n k} \rightarrow 0, n \rightarrow \infty$, and by Stolz-Cesaro theorem

$$
\begin{aligned}
\lim _{n \rightarrow \infty} \sum_{k=1}^{n-1} c_{n k} &=\lim _{n \rightarrow \infty} \frac{p}{(p-1) n^{p}} \sum_{k=1}^{n-1}\left((k+1)^{p-1}-k^{p-1}\right) k=\\
&=\lim _{n \rightarrow \infty} \frac{p}{p-1} \cdot \frac{\left((n+1)^{p-1}-n^{p-1}\right) n}{(n+1)^{p}-n^{p}}=1 .
\end{aligned}
$$

This implies by Toeplitz's theorem that $\lim _{n \rightarrow \infty} \sum_{k=1}^{n-1} c_{n k} y_{k}=\lim _{n \rightarrow \infty} y_{n}$, hence

$$
\lim _{n \rightarrow \infty} \frac{1}{n^{p}} \sum_{k=1}^{n} k^{p-1} x_{k}=\lim _{n \rightarrow \infty}\left(-\frac{p-1}{p} \sum_{k=1}^{n} c_{n k} y_{k}+y_{n}\right)=\frac{1}{p} \lim _{n \rightarrow \infty} y_{n} .
$$

7 The function $K(t)=t e^{-t}, t \geq 0$ increases on $[0 ; 1]$ and decreases on $[1 ;+\infty)$. Define $a=a(n)$ by the condition $K(a)=K((n-1) a)$, that is

$$
a e^{-a}=(n-1) a e^{-(n-1) a}, \quad a=\frac{1}{n-2} \ln (n-1) .
$$

Show that the supremum in question equals $K(a)=\frac{\ln (n-1)}{(n-2)(n-1)^{\frac{1}{n-2}}}$ and is attained at points $x_{1}^{*}<x_{2}^{*}<\ldots<x_{n}^{*}$ under the condition

$$
x_{2}^{*}-x_{1}^{*}=x_{3}^{*}-x_{2}^{*}=\ldots=x_{n}^{*}-x_{n-1}^{*}=a .
$$

Indeed, for all $i \neq j$ it holds $\left|x_{i}^{*}-x_{j}^{*}\right|=|i-j| a \in[a ;(n-1) a]$ and $K\left(\left|x_{i}^{*}-x_{j}^{*}\right|\right) \geq$ $K(a)$, whence $\inf _{i \neq j} K\left(\left|x_{i}^{*}-x_{j}^{*}\right|\right)=K(a)$. Assume that $\inf _{i \neq j} K\left(\left|x_{i}-x_{j}\right|\right)>K(a)$ for some points $x_{1}<x_{2}<\ldots<x_{n}$. Then it holds $x_{2}-x_{1}>a, x_{3}-x_{2}>a, \ldots$, $x_{n}-x_{n-1}>a$, whence $x_{n}-x_{1}>(n-1) a$. But then $K\left(\left|x_{n}-x_{1}\right|\right)<K((n-1) a)=$ $K(a)$, a contradiction. Answer: $\frac{\ln (n-1)}{(n-2)(n-1)^{\frac{1}{n-2}}}$.

8 Put $f\left(\frac{p}{q}\right)=\frac{1}{q}$, where $p \in \mathbb{Z}, q \in \mathbb{N}$, and $\frac{p}{q}$ is a fraction in lowest terms. Show that the function $f$ has the required properties. Indeed, for $x=\frac{p}{q}$ and $y=\frac{r}{s}$, and $x \neq y$, it holds

$$
|x-y|=\frac{|p s-q r|}{q s} \geq \frac{1}{q s}=f(x) f(y) .
$$

Now, let $x=\frac{p}{q}$ be a fraction in lowest terms. From the Euclidean Algorithm it follows that there exist numbers $r \in \mathbb{Z}$ and $s \in \mathbb{N}$ such that $|p s-q r|=1$. Then

$$
|p(s+n q)-q(r+n p)|=1,
$$

and for each $y_{n}=\frac{r+n p}{s+n q}, n \geq 1$, it holds $f(x) f\left(y_{n}\right)=\left|x-y_{n}\right|$

Answer: yes, there exists.

9 For $n=2$, it is obvious that there exists a required enumeration. For $n=3$, adjacent indices can be attributed only to permutations that can be obtained from each other by a cyclic shift, hence it is easy to check that there is no required enumeration.

Construct an example for $n \geq 4$. We call an enumeration $\sigma_{1}, \ldots, \sigma_{n}$ ! of permutations of the set $\{1, \ldots, n\}$ :

(A)-enumeration if for all $1 \leq k \leq n$ it holds

$$
\sigma_{l+1}(k) \neq \sigma_{l}(k), 1 \leq l \leq n !-1, \text { and } \sigma_{1}(k) \neq \sigma_{n !}(k)
$$

(B)-enumeration if for all $1 \leq k \leq n-1$ it holds

$$
\sigma_{l+1}(k+1) \neq \sigma_{l}(k), 1 \leq l \leq n !-1, \text { and } \sigma_{1}(k+1) \neq \sigma_{n !}(k)
$$

We will show that there exists a required enumeration, i.e., an $(A)$-enumeration of permutations of the set $\{1, \ldots, n\}$, for all $n \geq 4$. At the same time we will show that there exists a $(B)$-enumeration for all $n \geq 3$.

Lemma 1 Assume that there exist a $(B)$-enumeration $\sigma_{1}, \ldots, \sigma_{(n-1) !}$ of permutations of the set $\{1, \ldots, n-1\}$. Then there exists an $(A)$-enumeration $\tau_{1}, \ldots, \tau_{n}$ ! of permutations of the set $\{1, \ldots, n\}$.

Proof Let $\tau_{n(l-1)+1}=\left(\sigma_{l}(1), \ldots, \sigma_{l}(n-1), n\right)$ for all $1 \leq l \leq(n-1) !$, and permutations with indices from $n(l-1)+2$ to $n l$ are obtained by subsequent left circular shifts by one position of the permutation $\tau_{n(l-1)+1}$. Then $\tau_{1}, \ldots, \tau_{n !}$ is an $(A)$-enumeration. Indeed, the condition holds by the construction for all adjacent permutations except, possibly, permutations with indices $n l$ and $n l+1,1 \leq l<(n-1)$ !, or indices $n !$ and 1. But the permutation $\tau_{n l}$ has a form $\left(n, \sigma_{l}(1), \sigma_{l}(2), \ldots, \sigma_{l}(n-1)\right)$, and the permutation $\tau_{n l+1}$ has a form $\left(\sigma_{l+1}(1), \sigma_{l+1}(2), \ldots, \sigma_{l+1}(n-1), n\right)$. Therefore, the condition holds for those permutations as well, because the permutations $\sigma_{l}$ and $\sigma_{l+1}$ are adjacent under the $(B)$-enumeration.

Lemma 2 Assume that $n \geq 4$ and there exist an $(A)$-enumeration $\sigma_{1}, \ldots, \sigma_{(n-1)}$ ! of permutations of the set $\{1, \ldots, n-1\}$. Then there exists a $(B)$-enumeration $\tau_{1}, \ldots, \tau_{n}$ ! of permutations of the set $\{1, \ldots, n\}$.

Proof Let $\tau_{n(l-1)+1}=\left(n, \sigma_{l}(1), \ldots, \sigma_{l}(n-1)\right)$ for all $1 \leq l \leq(n-1) !$, and permutations $\tau_{n l}, \tau_{n(l-1)+2}, \tau_{n(l-1)+3}, \ldots, \tau_{n l-1}$ (exactly in this order) are obtained by subsequent left circular shifts by one position of the permutation $\tau_{n(l-1)+1}$. Then $\tau_{1}, \ldots, \tau_{n !}$ is a $(B)$-enumeration. Indeed, the condition holds by the construction for all adjacent permutations except, possibly, permutations with indices $n l$ and $n l+1$, $1 \leq l<(n-1)$ !, or indices $n$ ! and 1 . But the permutations $\tau_{n l}$ and $\tau_{n l+1}$ have a form $\left(\sigma_{l}(1), \sigma_{l}(2), \ldots, \sigma_{l}(n-1), n\right)$ and $\left(n, \sigma_{l+1}(1), \sigma_{l+1}(2), \ldots, \sigma_{l+1}(n-1)\right)$, respectively. Hence the condition holds for those permutations as well, because the permutations $\sigma_{l}$ and $\sigma_{l+1}$ are adjacent under the $(A)$-enumeration.

Notice that for both $n=3$ and $n=4$, there exist $(B)$-enumerations of permutations of the set $\{1, \ldots, n\}$ :

$$
\begin{gathered}
(1,2,3),(3,2,1),(2,1,3),(1,3,2),(2,3,1),(3,1,2) ; \\
(1,2,3,4),(2,3,4,1),(3,4,1,2),(4,1,2,3),(3,1,2,4),(1,2,4,3), \\
(2,4,3,1),(4,3,1,2),(2,3,1,4),(3,1,4,2),(1,4,2,3),(4,2,3,1), \\
(2,1,3,4),(1,3,4,2),(3,4,2,1),(4,2,1,3),(3,2,1,4),(2,1,4,3), \\
(1,4,3,2),(4,3,2,1),(1,3,2,4),(3,2,4,1),(2,4,1,3),(4,1,3,2) .
\end{gathered}
$$

Therefore, by Lemma 1 for $n=4$ there exists an $(A)$-enumeration of permutations of the set $\{1, \ldots, n\}$, and further due to Lemmas 1 and 2 we get by induction on $n$ that there exist $(A)$ - and $(B)$-enumerations of permutations of the set $\{1, \ldots, n\}$ for all $n \geq 4$.

Answer: $n=2$ or $n \geq 4$.

15 The function $f(a)=\int_{\mathbb{R}} x e^{a x} d \mu(x)$ is well-defined because

$$
\left|x e^{a x}\right| \leq\left(e^{x}+e^{-x}\right) e^{a x}=e^{(a+1) x}+e^{(a-1) x} .
$$

By theorem on the continuity of a Lebesgue integral with respect to a parameter, it holds that $f \in C(\mathbb{R})$. For some $\varepsilon>0$, we have $\mu([\varepsilon ;+\infty))>0$. On the right-hand side of the equality

$$
f(a)=\int_{(-\infty ; 0)} x e^{a x} d \mu(x)+\int_{[0 ;+\infty)} x e^{a x} d \mu(x),
$$

the first summand tends to 0 , as $a \rightarrow+\infty$, by Lebesgue's dominated convergence theorem, and the second summand is greater than or equal to $\varepsilon e^{a \varepsilon} \mu([\varepsilon ;+\infty))$, which tends to infinity as $a \rightarrow+\infty$. Hence $f(a) \rightarrow+\infty$, as $a \rightarrow+\infty$. In a similar way $f(a) \rightarrow-\infty$, as $a \rightarrow-\infty$. By the intermediate value theorem, there exists $a_{0}$ such that $f\left(a_{0}\right)=0$. For $a_{1}<a_{2}$ and all $x \in \mathbb{R} \backslash\{0\}$, it holds $x\left(e^{a_{2} x}-e^{a_{1} x}\right)>0$, whence

$$
f\left(a_{2}\right)-f\left(a_{1}\right)=\int_{\mathbb{R}} x\left(e^{a_{2} x}-e^{a_{1} x}\right) d \mu(x)>0 .
$$

Thus, $f$ is increasing and a required value $a_{0}$ is unique.

16 Denote by $\tau_{i}$ the number of members equal to $i$ in the sequence $\left\{x_{n}, n \geq 0\right\}$. Then $\tau_{i}, n \geq 0$ are independent identically distributed random variables. It can be verified by straightforward calculation of probabilities $\mathrm{P}\left(\tau_{0}=k_{0}, \ldots, \tau_{n}=k_{n}\right)$. Furthermore,

$$
\mathrm{P}\left(\tau_{0}=k\right)=p^{k-1}(1-p), k \geq 1 .
$$

Define $S_{l}=\sum_{j=0}^{l} \tau_{j}, l \geq 0$, and $\sigma_{n}=\max \left\{k: S_{k} \leq n\right\}, n \geq 1$. From the strong law of large numbers it follows that $\frac{S_{l}}{l} \rightarrow \mathrm{E} \tau_{0}$, almost surely, as $l \rightarrow \infty$. It implies that $\frac{\sigma_{n}}{n} \stackrel{P 1}{\rightarrow}\left(\mathrm{E} \tau_{0}\right)^{-1}$, as $n \rightarrow \infty$. Now,

$$
\frac{1}{n} \sum_{k=0}^{n} \xi_{x_{k}}=\frac{1}{n} \sum_{l=0}^{\sigma_{n}} \xi_{l} \tau_{l}+\frac{1}{n} r_{n},
$$

where $\left|r_{n}\right| \leq\left|\xi_{\sigma_{n}+1}\right| \tau_{\sigma_{n}+1}$. Since $\mathrm{E}\left|\xi_{0}\right|<+\infty$, we get $\frac{1}{n} r_{n} \stackrel{\text { P1 }}{\rightarrow} 0$, as $n \rightarrow \infty$. Due to the independence of $\xi$ and $\tau$ and the strong law of large numbers, we have

$$
\frac{1}{n} \sum_{l=0}^{\sigma_{n}} \xi_{l} \tau_{l}=\frac{\sigma_{n}}{n} \cdot \frac{1}{\sigma_{n}} \sum_{l=0}^{\sigma_{n}} \xi_{l} \tau_{l} \stackrel{\mathrm{P} 1}{\rightarrow} 0, \text { as } n \rightarrow \infty .
$$

17 Denote $S_{k}=\sum_{i=1}^{k} X_{i}$, and $T_{k}=\sum_{i=1}^{k} X_{i}^{2}, 1 \leq k \leq 2 n$. Then it holds

$$
\begin{aligned}
\mathrm{E} Y_{2 n}^{2}=\mathrm{E} \frac{S_{2 n}^{2}}{T_{2 n}} &=1+2 n(2 n-1) \mathrm{E} \frac{X_{1} X_{n}}{T_{2 n}}=1+\frac{2 n(2 n-1)}{n^{2}} \mathrm{E} \frac{S_{n}\left(S_{2 n}-S_{n}\right)}{T_{2 n}} \leq \\
& \leq 1+4 \mathrm{E} \frac{\left|S_{n}\right|\left|S_{2 n}-S_{n}\right|}{T_{2 n}} \leq 1+4 \mathrm{E} \frac{\left|S_{n}\right|\left|S_{2 n}-S_{n}\right|}{\sqrt{T_{n}} \sqrt{T_{2 n}-T_{n}}}=\\
&=1+4 \mathrm{E} \frac{\left|S_{n}\right|}{\sqrt{T_{n}}} \cdot \mathrm{E} \frac{\left|S_{2 n}-S_{n}\right|}{\sqrt{T_{2 n}-T_{n}}}=1+4\left(\mathrm{E} \frac{\left|S_{n}\right|}{\sqrt{T_{n}}}\right)^{2}=1+4\left(\mathrm{E} Y_{n}\right)^{2} .
\end{aligned}
$$

1 Consider piecewise linear functions $F_{\alpha}:[0,1] \rightarrow \mathbb{R}, \alpha \in(0,1)$, defined as

$$
F_{\alpha}(x)=\left\{\begin{array}{cc}
\frac{x}{\alpha}, & 0 \leq x \leq \alpha \\
\frac{1-x}{1-\alpha}, & \alpha<x \leq 1
\end{array}\right.
$$

The graphs of all the functions $F_{\alpha}$ pass through the points $(0,0)$ and $(1,0)$. For $\alpha<\beta$, the graphs of the functions $F_{\alpha}$ and $F_{\beta}$, besides these two points, intersect at a single point $\left(\frac{\beta}{1-\alpha+\beta}, \frac{1}{1-\alpha+\beta}\right)$. Hence for every $\alpha<\beta<\gamma$, the graphs of the functions $F_{\alpha}$, $F_{\beta}$, and $F_{\gamma}$ have exactly two common points. It remains to put $f_{\alpha}=F_{g(\alpha)}, \alpha \in \mathbb{R}$, where $g: \mathbb{R} \rightarrow(0,1)$ is an arbitrary bijection.

Answer: yes, there exists.

2 Introduce a sequence $b_{n}=\frac{a_{n}}{n}, n \geq 1$. By the condition of the problem

$$
\left[n(n+1) b_{n}\right]=\left[n(n+1) b_{n+1}\right], n \geq 1 .
$$

Therefore, $\left|n(n+1) b_{n}-n(n+1) b_{n+1}\right|<1$, i.e., $\left|b_{n}-b_{n+1}\right|<\frac{1}{n(n+1)}, \quad n \geq 1$. Hence for all $n \geq 1, p \geq 1$, it holds

$$
\begin{aligned}
\mid b_{n+p} &-b_{n}|\leq| b_{n+1}-b_{n}|+\ldots+| b_{n+p}-b_{n+p-1} \mid<\\
&<\frac{1}{n(n+1)}+\ldots+\frac{1}{(n+p-1)(n+p)}=\frac{1}{n}-\frac{1}{n+p}<\frac{1}{n} \rightarrow 0, \text { as } n \rightarrow \infty .
\end{aligned}
$$

Thus, $\left\{b_{n}\right\}$ is a Cauchy sequence, so it converges. Denote $c=\lim _{n \rightarrow \infty} b_{n}$. For every $n \geq 1$, we have $\left|b_{n}-c\right|=\lim _{p \rightarrow \infty}\left|b_{n+p}-b_{n}\right| \leq \frac{1}{n}$. Hence 

$$
\left|b_{n}-c\right| \leq\left|b_{n}-b_{n+1}\right|+\left|b_{n+1}-c\right|<\frac{1}{n(n+1)}+\frac{1}{n+1}=\frac{1}{n} .
$$

Therefore, $\left|\frac{a_{n}}{n}-c\right|<\frac{1}{n}$, i.e., $\left|a_{n}-c n\right|<1, n \geq 1$.

Alternative solution. For each $n \geq 1$, the inequality $\left|a_{n}-c n\right|<1$ is equivalent to the double inequality

$$
c_{n}:=\frac{a_{n}-1}{n}<c<C_{n}:=\frac{a_{n}+1}{n} .
$$

Show that all the intervals $\left(c_{n}, C_{n}\right), n \geq 1$, have a common point. The condition of the problem implies that $\left|n a_{n+1}-(n+1) a_{n}\right|<1, n \geq 1$, hence

$$
\begin{gathered}
c_{n+1}-c_{n}=\frac{a_{n+1}-1}{n+1}-\frac{a_{n}-1}{n}=\frac{n a_{n+1}-(n+1) a_{n}+1}{n(n+1)}>0, \\
C_{n+1}-C_{n}=\frac{a_{n+1}+1}{n+1}-\frac{a_{n}+1}{n}=\frac{n a_{n+1}-(n+1) a_{n}-1}{n(n+1)}<0 .
\end{gathered}
$$

Thus, $\left[c_{n+1}, C_{n+1}\right] \subset\left[c_{n}, C_{n}\right], n \geq 1$, moreover

$$
C_{n}-c_{n}=\frac{2}{n} \rightarrow 0, \text { as } n \rightarrow \infty .
$$

Therefore, by the nested intervals theorem there exists a unique point $c$ that belongs to all the segments $\left[c_{n}, C_{n}\right]$. Finally, since

$$
c \in\left[c_{n+1}, C_{n+1}\right] \subset\left(c_{n}, C_{n}\right),
$$

the number $c$ belongs to all the intervals $\left(c_{n}, C_{n}\right), n \geq 1$, as well.

3 Introduce a function

$$
\begin{aligned}
h(x)=f(x)(a g(b)-b g(a)) &-g(x)(a f(b)-b f(a))-\\
&-\left(\frac{f}{g}\right)(x)(a-b) g(a) g(b), x \in[a, b] .
\end{aligned}
$$

The function $h$ is continuous on $[a, b]$ and differentiable on $(a, b)$, and moreover $h(a)=h(b)=b f(a) g(b)-a f(b) g(a)$. Therefore, by Rolle's theorem there exists $x \in(a, b)$ such that

$$
h^{\prime}(x)=f^{\prime}(x)(a g(b)-b g(a))-g^{\prime}(x)(a f(b)-b f(a))-\left(\frac{f}{g}\right)^{\prime}(x)(a-b) g(a) g(b)=0 .
$$

4 For $k=1$, all the elements of the set $S\left(A_{1}\right)$ are polynomials of $A_{1}$, therefore, each two of them commute. Hence there is no matrix $A_{1}$ for which $S\left(A_{1}\right)=M_{n}(\mathbb{R})$. Give an example of required matrices for $k=2$. Denote by $I_{i j}$ the $n \times n$ matrix, the only nonzero entry of which equals 1 and is located in $i$ th row and $j$ th column. Put

$$
A_{1}=I_{11}, \quad A_{2}=\left(\begin{array}{ccccc}
0 & 1 & 0 & \ldots & 0 \\
0 & 0 & 1 & \ldots & 0 \\
\hdashline 0 & \ldots & \ldots & \cdots & \cdots \\
1 & 0 & 0 & \ldots & 0
\end{array}\right)
$$

(here $A_{2}$ is a permutation matrix).

It is easy to verify that $A_{2}^{n-i+1} A_{1} A_{2}^{n-j+1}=I_{i j}$. Thus, the set $S\left(A_{1}, A_{2}\right)$ contains all the matrices $I_{i j}, 1 \leq i, j \leq n$, which form a basis in $M_{n}(\mathbb{R})$. Hence $S\left(A_{1}, A_{2}\right)=$ $M_{n}(\mathbb{R})$.

Answer: $k=2$.

5 The sequence $\left\{x_{n}\right\}$ is increasing, hence it has a finite or infinite limit. If $\lim _{n \rightarrow \infty} x_{n}=$ $x<+\infty$, then $x=x+e^{-x}$, a contradiction. Therefore, $\lim _{n \rightarrow \infty} x_{n}=+\infty$. We use twice the Stolz-Cesaro theorem and obtain

$$
\lim _{n \rightarrow \infty} \frac{x_{n}}{\ln n}=\lim _{n \rightarrow \infty} \frac{x_{n+1}-x_{n}}{\ln (n+1)-\ln n}=\lim _{n \rightarrow \infty} \frac{e^{-x_{n}}}{\ln \left(1+\frac{1}{n}\right)}=\lim _{n \rightarrow \infty} \frac{n}{e^{x_{n}}}=\lim _{n \rightarrow \infty} \frac{1}{e^{x_{n+1}}-e^{x_{n}}} .
$$

It remains to notice that

$$
\lim _{n \rightarrow \infty}\left(e^{x_{n+1}}-e^{x_{n}}\right)=\lim _{n \rightarrow \infty}\left(e^{x_{n}+e^{-x_{n}}}-e^{x_{n}}\right)=\lim _{n \rightarrow \infty} \frac{e^{e^{-x_{n}}}-1}{e^{-x_{n}}}=\lim _{t \rightarrow 0} \frac{e^{t}-1}{t}=1 .
$$

Answer: 1.

6 We check that for each polynomial $P(x, y)$, there exists a polynomial $Q(x, y)$ of a form

$$
\sum_{i, j=0}^{n-1} c_{i j} x^{i} y^{j}
$$

such that $\operatorname{deg} Q \leq \operatorname{deg} P$ and $Q(i, j)=P(i, j)$ for all $1 \leq i, j \leq n$. Indeed, one can rewrite $P(x, y)$ as $\sum_{l \geq 0} P_{l}(x) y^{l}$ and replace each polynomial $P_{l}(x)$ with its remainder after division by $\prod_{i=1}^{n}(x-i)$, and then rewrite the result as $\sum_{l=0}^{n-1} x^{l} \widetilde{P}_{l}(y)$ and replace each polynomial $\widetilde{P}_{l}(y)$ with its remainder after division by $\prod_{j=1}^{n}(y-j)$. Thus, it suffices to consider only the polynomials $P(x, y)$ of the form $(*)$.

Notice that for arbitrary numbers $a_{i j}$, there exists an interpolating polynomial of the form $(*)$, namely 

$$
P(x, y)=\sum_{\substack{i_{0}, j_{0}=1}}^{n} a_{i_{0} j_{0}} \cdot \prod_{\substack{i=0 \\ i \neq i_{0}}}^{n} \frac{x-i}{i_{0}-i} \cdot \prod_{\substack{j=0 \\ j \neq j_{0}}}^{n} \frac{y-j}{j_{0}-j} .
$$

Thus, $k \leq 2 n-2$. On the other hand, both the set of polynomials of the form $(*)$ and the set of collections of real numbers $\left\{a_{i j}, 1 \leq i, j \leq n\right\}$ are real linear spaces of the same dimension $n^{2}$. Hence, the relation $(* *)$ determines a one-to-one correspondence between the collections of numbers $\left\{a_{i j}\right\}$ and the polynomials of the form $(*)$. For instance, a unique polynomial of the form $(*)$, which corresponds to the numbers $a_{i j} \equiv i^{n-1} j^{n-1}$, is equal to $x^{n-1} y^{n-1}$ and has a degree $2 n-2$. Therefore, $k=2 n-2$.

Answer: $k=2 n-2$.

7 Denote by $A_{i j}$ the matrix obtained from $A$ by erasing of $i$ th row and $j$ th column, $1 \leq i, j \leq n$. In particular $A_{i i}=A_{i}$. Since $\operatorname{det} A=0$ and $\operatorname{det} A_{1}=2010 \neq 0$, the last $n-1$ columns of the matrix $A$ are linearly independent and the first column is their linear combination with some coefficients $\lambda_{2}, \ldots, \lambda_{n}$. Because $A$ is symmetric, the first row is a linear combination of the last $n-1$ rows with the same coefficients $\lambda_{2}, \ldots, \lambda_{n}$, and since $A$ has integer entries, it holds $\lambda_{2}, \ldots, \lambda_{n} \in \mathbb{Q}$. Fix $i>1$ and consider the matrix $A_{i 1}$. Its first row is a linear combination of other rows of this matrix and of $i$ th row of $A$ with deleted first entry. Hence it is easy to verify that $\operatorname{det} A_{i 1}=(-1)^{i-1} \lambda_{i} \operatorname{det} A_{11}$. In a similar way

$$
\operatorname{det} A_{i}=\operatorname{det} A_{i i}=(-1)^{i-1} \lambda_{i} \operatorname{det} A_{i 1}=\lambda_{i}^{2} \operatorname{det} A_{11}=2010 \lambda_{i}^{2} .
$$

Notice that $\operatorname{det} A_{i}=2010 \lambda_{i}^{2}$ is an integer as the determinant of a matrix with integer entries. If $\lambda_{i}$ is not an integer, then $\lambda_{i}$ equals to an irreducible fraction $\frac{s}{t}$ with $s \in \mathbb{Z}$ and $t \in \mathbb{N}, t>1$. Then $2010=2 \cdot 3 \cdot 5 \cdot 67$ is not divisible by $t^{2}$, hence $2010 \lambda_{i}^{2}=\frac{2010 s^{2}}{t^{2}}$ is not an integer, a contradiction. Therefore, $\lambda_{i}$ is an integer and $\operatorname{det} A_{i}=2010 \lambda_{i}^{t^{2}}$ is divisible by 2010.

Alternative solution. The condition of the problem implies that rk $A=n-1$. Let $\widetilde{A}=\left(\widetilde{a}_{i j}\right)_{i, j=1}^{n}$ be the adjugate matrix, that is $\widetilde{a}_{i j}=(-1)^{i+j} \operatorname{det} A_{j i}$, where $A_{i j}$ is the matrix obtained from $A$ by erasing of $i$ th row and $j$ th column, $1 \leq i, j \leq n$. Then $A \widetilde{A}=\operatorname{det} A \cdot I=0$, where $I$ is the identity $n \times n$ matrix. Well-known inequality $\operatorname{rk} A B \geq \operatorname{rk} A+\operatorname{rk} B-n$ implies that

$$
0=\operatorname{rk} A \widetilde{A} \geq \operatorname{rk} A+\operatorname{rk} \widetilde{A}-n=\operatorname{rk} \widetilde{A}-1 .
$$

Hence $\operatorname{rk} \widetilde{A} \leq 1$, i.e., all the rows of $\widetilde{A}$ are proportional to each other. Let $\widetilde{a}_{i 1}=\mu_{i} \widetilde{a}_{11}$. Then $\widetilde{a}_{i i}=\mu_{i} \tilde{a}_{i 1}=\mu_{i}^{2} \widetilde{a}_{11}$. But all $\widetilde{a}_{i j}$ are integers, hence $\mu_{i} \in \mathbb{Q}$. Now,

$$
\operatorname{det} A_{i}=\widetilde{a}_{i i}=\mu_{i}^{2} \widetilde{a}_{11}=2010 \mu_{i}^{2},
$$

and the solution can be finished similar to the previous one. 8 Suppose that the linear span of vectors $\vec{v}_{i j}, 1 \leq i<j \leq n$, does not coincide with $\mathbb{R}^{n}$. Then there exists a nonzero vector $\vec{x}=\left(x_{1}, \ldots, x_{n}\right)$ orthogonal to all the vectors $\vec{v}_{i j}$, that is

$$
\forall 1 \leq i<j \leq n \quad\left(\vec{x}, \vec{v}_{i j}\right)=\sum_{k=1}^{n} x_{k} \delta_{i j k}=0
$$

Therefore, one can place the numbers $x_{1}, \ldots, x_{n}$ at the points $P_{1}, \ldots, P_{n}$ in such a way that the sum of numbers on each line $P_{i} P_{j}$ equals zero. Denote $S=x_{1}+\ldots+$ $x_{n}$. Fix an arbitrary point $P_{i}, 1 \leq i \leq n$, and consider all the lines $P_{i} P_{j}, j \neq i$. Assume that there are exactly $k_{i}$ distinct lines among them. Notice that $k_{i}>1$ by the condition of the problem. The sum of numbers on each lines $P_{i} P_{j}$ equals zero. The sum of these sums over $k_{i}$ distinct lines equals $S+\left(k_{i}-1\right) x_{i}=0$, because the summand $x_{i}$ is counted $k_{i}$ times, and each of the summands $x_{j}, j \neq i$, is counted once. Hence $x_{i}=-\frac{S}{k_{i}-1}, 1 \leq i \leq n$. Thus, the coordinates of nonzero vector $\vec{x}$ are either all positive or all negative. Therefore, the sum of numbers on each line $P_{i} P_{j}$ is nonzero, a contradiction.

Alternative solution. Assume that there are exactly $m$ distinct lines among the lines $P_{i} P_{j}$. Introduce $n \times m$ matrix $A$, columns of which are vectors $\vec{v}_{i j}$ corresponding to distinct lines. Set $B=A A^{\mathrm{T}}=\left(b_{i j}\right)_{i, j=1}^{n}$. It is not difficult to verify that $b_{i j}$ is the number of lines passing through both points $P_{i}$ and $P_{j}$. Thus, $b_{i j}=1$ for $i \neq j$, and $b_{i i}=k_{i}$, where $k_{i}>1$ is the number of lines passing through the point $P_{i}$, $1 \leq i \leq n$. If the determinant of $B$ is nonzero, then $\operatorname{rk} A \geq \operatorname{rk} B=n$, and the linear span of the columns of $A$ coincides with $\mathbb{R}^{n}$. It remains to compute det $B$. Subtract the first row from all the rest ones, and then subtract from the first row all the other rows multiplied by $\frac{1}{k_{2}-1}, \ldots, \frac{1}{k_{n}-1}$, respectively. We get

$$
\begin{aligned}
& \operatorname{det} B=\left|\begin{array}{ccccc}k_{1} & 1 & 1 & \ldots & 1 \\1 & k_{2} & 1 & \ldots & 1 \\1 & 1 & k_{3} & \ldots & 1 \\\ldots & \ldots & \ldots & \ldots & \ldots \\1 & 1 & 1 & \ldots & k_{n}\end{array}\right|=\left|\begin{array}{ccccc}k_{1} & 1 & 1 & \ldots & 1 \\1-k_{1} & k_{2}-1 & 0 & \ldots & 0 \\1-k_{1} & 0 & k_{3}-1 & \ldots & 0 \\\ldots & \ldots & \ldots & \ldots & \ldots \\1-k_{1} & 0 & 0 & \ldots & k_{n}-1\end{array}\right|= \\
& =\left|\begin{array}{ccccc}k_{1}+\left(k_{1}-1\right)\left(\frac{1}{k_{2}-1}+\ldots+\frac{1}{k_{n}-1}\right) & 0 & 0 & \ldots & 0 \\1-k_{1} & k_{2}-1 & 0 & \ldots & 0 \\1-k_{1} & 0 & k_{3}-1 & \ldots & 0 \\\ldots & \ldots & \ldots & \ldots & \ldots \\1-k_{1} & 0 & 0 & \ldots k_{n}-1\end{array}\right|= \\
& =\left(k_{1}+\left(k_{1}-1\right)\left(\frac{1}{k_{2}-1}+\ldots+\frac{1}{k_{n}-1}\right)\right)\left(k_{2}-1\right) \cdot \ldots \cdot\left(k_{n}-1\right)= \\
& =\left(k_{1}-1\right)\left(k_{2}-1\right) \cdot \ldots \cdot\left(k_{n}-1\right)\left(1+\frac{1}{k_{1}-1}+\frac{1}{k_{2}-1}+\ldots+\frac{1}{k_{n}-1}\right)>0,
\end{aligned}
$$

which finishes the proof. 9 Introduce an ellipse $E_{1}$ with foci $(x, 2-x)$ and $(2-x, x), 0<x<1$, that touches coordinate axes at the points $\left(x^{2}-2 x+2,0\right)$ and $\left(0, x^{2}-2 x+2\right)$, and an ellipse $E$ equal to $E_{1}$ with foci $(-\sqrt{2}(1-x), 0)$ and $(\sqrt{2}(1-x), 0)$. If $x=0$, the ellipses becomes a couple of disjoint segments, and if $x=1$, the ellipses becomes a couple of intersecting circles. It is not difficult to verify that the minimal distance between the points of $E_{1}$ and $E$ is a continuous function of $x$. Denote by $x_{*}$ the infimum of $0<x<1$ for which the ellipses intersect. By the continuity, for $x=x_{*}$ the ellipses $E_{1}$ and $E$ touch. Reflect the ellipse $E_{1}$ with respect to the axes and the origin to obtain three more required ellipses $E_{2}, E_{3}$, and $E_{4}$ (see Fig. 1).

Suppose that there exist equal ellipses $E, E_{1}, E_{2}, E_{3}$ that pairwise touch each other. It is easy to verify that one of the ellipses lies inside a triangle with vertices at the tangent points of three other ellipses. But then the major axis of this ellipse is less than the largest side of the triangle, which is a chord of another ellipse. Thus, the major axes of ellipses cannot be equal, a contradiction.

Answer: $N=4$.

Fig. 1 Touching ellipses

![](https://cdn.mathpix.com/cropped/2022_10_28_1f3b279542cbcf365e2cg-097.jpg?height=469&width=477&top_left_y=1132&top_left_x=1212)

10 For every $0<\varepsilon<1$, there exists $C=C(\varepsilon)$ such that for all $x \geq C$, it holds

$$
\frac{1-\varepsilon}{x^{3}}<f(x)<\frac{1+\varepsilon}{x^{3}} .
$$

Hence for all $c \geq C$, the integral $\int_{c}^{+\infty}(x-c) f(x) d x$ is well defined, moreover

$$
\frac{1-\varepsilon}{2}=(1-\varepsilon) c \int_{c}^{+\infty} \frac{x-c}{x^{3}} d x \leq c \int_{c}^{+\infty}(x-c) f(x) d x \leq(1+\varepsilon) c \int_{c}^{+\infty} \frac{x-c}{x^{3}} d x=\frac{1+\varepsilon}{2} .
$$

Since $0<\varepsilon<1$ is arbitrary, the limit in question equals $\frac{1}{2}$. Answer: $\frac{1}{2}$. 11 By Jensen's inequality $f(\alpha \sin x) \leq \frac{1+\alpha}{2} f(\sin x)+\frac{1-\alpha}{2} f(-\sin x)$. Therefore,

$$
\begin{aligned}
&\int_{-\pi / 2}^{\pi / 2} f(\alpha \sin x) d x \leq \\
&\quad \leq \frac{1+\alpha}{2} \int_{-\pi / 2}^{\pi / 2} f(\sin x) d x+\frac{1-\alpha}{2} \int_{-\pi / 2}^{\pi / 2} f(-\sin x) d x=\int_{-\pi / 2}^{\pi / 2} f(\sin x) d x .
\end{aligned}
$$

12 Introduce a set $A=\{(x, f(x)) \mid x \in \mathbb{R}\} \subset \mathbb{R}^{2}$. It is Borel measurable because

$$
A=\{(x, y) \mid y-f(x) \in\{0\}\}=\varphi^{-1}(\{0\}),
$$

where the set $\{0\}$ is closed, and the function $\varphi(x, y)=y-f(x): \mathbb{R}^{2} \rightarrow \mathbb{R}$ is Borel measurable as a difference of continuous function $\pi_{2}(x, y)=y$ and the function $f\left(\pi_{1}(x, y)\right)=f(x)$, which is a composition of continuous function $\pi_{1}(x, y)=x$ and Borel measurable function $f$. We have

$$
\mathrm{P}(\{(\xi, \eta) \in A\})=\mathrm{P}(\{(\xi, f(\xi)) \in A\})=1,
$$

i.e., $(\xi, \eta) \in A$ almost surely. Hence $\eta=f(\xi)$ almost surely.

15 Define

$$
\|x\|=\sqrt{\left(x_{1}-\sum_{k=2}^{\infty} k^{2} x_{k}\right)^{2}+x_{2}^{2}+x_{3}^{2}+\ldots+x_{k}^{2}+\ldots, x \in X .}
$$

It is not difficult to verify that it is indeed a norm on $X$. Consider the sequence

$$
x(n)=(n, \underbrace{0, \ldots, 0}_{n-2}, \frac{1}{n}, 0, \ldots), n \geq 2 .
$$

Then $\|x(n)\|=\frac{1}{n} \rightarrow 0$, as $n \rightarrow \infty$, but $f(x(n))=n \rightarrow \infty$, as $n \rightarrow \infty$. Thus, $f$ is discontinuous.

Answer: yes, there exists.

Remark 1 One can give other examples of a required norm:

$$
\|x\|=\sqrt{\int_{0}^{1}\left(\sum_{k=1}^{\infty} x_{k} t^{k-1}\right)^{2} d t}, x \in X ; \quad\|x\|=\int_{0}^{1}\left|\sum_{k=1}^{\infty} x_{k} t^{k-1}\right| d t, x \in X .
$$

18 The Gram determinant $\Gamma\left(\xi_{1}, \ldots, \xi_{n}\right)$ is equal to a squared volume of a parallelepiped determined by the vectors $\xi_{1}, \ldots, \xi_{n}$. Hence 

$$
\Gamma\left(\xi_{1}, \ldots, \xi_{n}\right)=\left\|\xi_{1}\right\|^{2} \cdot d_{2}^{2} \cdot \ldots \cdot d_{n}^{2},
$$

where $d_{k}$ is the length of component of $\xi_{k}$ which is orthogonal to the subspace generated by the vectors $\xi_{1}, \ldots, \xi_{k-1}$.

Prove that for each $1 \leq k \leq n$,

$$
\mathbf{P}\left(\Gamma\left(\xi_{1}, \ldots, \xi_{k}\right) \neq 0\right)=1 \text { and } \mathrm{E} \Gamma\left(\xi_{1}, \ldots, \xi_{k}\right)=n(n-1) \cdot \ldots \cdot(n-k+1)
$$

For $k=1$, these statements are obvious. Assume that they hold for some $k$, $1 \leq k \leq n-1$. Consider $\Gamma\left(\xi_{1}, \ldots, \xi_{k+1}\right)=\Gamma\left(\xi_{1}, \ldots, \xi_{k}\right) \cdot d_{k+1}^{2}$. By the inductive hypothesis $\xi_{1}, \ldots, \xi_{k}$ are linearly independent with probability 1. Therefore, for fixed $\xi_{1}, \ldots, \xi_{k}$, the random variable $d_{k+1}^{2}$ has $\chi_{n-k}^{2}$ distribution, i.e., the same distribution as a squared norm of a Gaussian vector of dimension $n-k$ with zero mean and unit covariance matrix. Thus, $d_{k+1}^{2} \neq 0$ almost surely, and $\mathrm{P}\left(\Gamma\left(\xi_{1}, \ldots, \xi_{k+1}\right) \neq 0\right)=1$. Next,

$$
\begin{aligned}
\mathrm{E} \Gamma\left(\xi_{1}, \ldots, \xi_{k+1}\right) &=\mathrm{E}\left(\mathrm{E} \Gamma\left(\xi_{1}, \ldots, \xi_{k+1}\right) / \xi_{1}, \ldots, \xi_{k}\right)=\\
&=\mathrm{E} \Gamma\left(\xi_{1}, \ldots, \xi_{k}\right) \mathrm{E}\left(d_{k+1}^{2} / \xi_{1}, \ldots, \xi_{k}\right)=\\
&=\mathrm{E} \Gamma\left(\xi_{1}, \ldots, \xi_{k}\right)(n-k)=n(n-1) \cdot \ldots \cdot(n-k) .
\end{aligned}
$$

By the principle of mathematical induction the statement is proved. For $k=n$, it follows that $\mathrm{E} \Gamma\left(\xi_{1}, \ldots, \xi_{n}\right)=n$ !

Alternative solution. Let $\xi_{i}=\left(\xi_{i 1}, \xi_{i 2}, \ldots, \xi_{i n}\right), 1 \leq i \leq n$. Put $A=\left(\xi_{i j}\right)_{i, j=1}^{n}$. Then $A A^{\mathrm{T}}=\left(\left(\xi_{i}, \xi_{j}\right)\right)_{i, j=1}^{n}$, therefore, $\Gamma\left(\xi_{1}, \ldots, \xi_{n}\right)=\operatorname{det}\left(A A^{\mathrm{T}}\right)=(\operatorname{det} A)^{2}$. Hence

$$
\begin{aligned}
\mathrm{E} \Gamma\left(\xi_{1}, \ldots, \xi_{n}\right) &=\mathrm{E}(\operatorname{det} A)^{2}=\mathrm{E}\left(\sum_{\sigma \in S_{n}}(-1)^{\operatorname{sgn}(\sigma)} \xi_{1 \sigma(1)} \cdot \ldots \cdot \xi_{n \sigma(n)}\right)^{2}=\\
&=\sum_{\sigma, \tau \in S_{n}}(-1)^{\operatorname{sgn}(\sigma)+\operatorname{sgn}(\tau)} \mathrm{E}\left(\xi_{1 \sigma(1)} \cdot \ldots \cdot \xi_{n \sigma(n)} \xi_{1 \tau(1)} \cdot \ldots \cdot \xi_{n \tau(n)}\right) .
\end{aligned}
$$

Since zero mean random vectors $\xi_{1}, \ldots, \xi_{n}$ are stochastically independent and have unit covariance matrix, it holds

$$
\begin{aligned}
& \mathrm{E}\left(\xi_{1 \sigma(1)} \cdot \ldots \cdot \xi_{n \sigma(n)} \xi_{1 \tau(1)} \cdot \ldots \cdot \xi_{n \tau(n)}\right)= \\
& =\mathrm{E}\left(\xi_{1 \sigma(1)} \xi_{1 \tau(1)}\right) \cdot \ldots \cdot \mathrm{E}\left(\xi_{n \sigma(n)} \xi_{n \tau(n)}\right)=\left\{\begin{array}{l}1, \sigma=\tau, \\0, \sigma \neq \tau,\end{array}\right.
\end{aligned}
$$

therefore, $\mathrm{E} \Gamma\left(\xi_{1}, \ldots, \xi_{n}\right)=\sum_{\sigma=\tau \in S_{n}} 1=n !$

Answer: $n$ ! 

\section{1}

1 Let $f_{1}(x)=x^{2}$ and $f_{2}(x)=\frac{1}{2}\left(x^{2}+g(x)\right), x \in[0,1]$, where the function $g$ is defined as follows: $g(0)=0, g\left(\frac{1}{n}\right)=\frac{1}{n^{2}}, n \geq 1$, and $g$ is linear on the segment $\left[\frac{1}{n+1}, \frac{1}{n}\right]$ for each $n \geq 1$. It is not difficult to verify that $g$ is convex, hence the functions $f_{1}$ and $f_{2}$ are strictly convex and satisfy the condition of the problem. Answer: yes, there exist.

2 Since there exists a continuous inverse map $\varphi^{-1}: \mathbb{R} \rightarrow \mathbb{R}$, the equality $\varphi^{(n)}(t)=$ $f(\varphi(t)), t \in \mathbb{R}$, holds true for a unique continuous function $f(t)=\varphi^{(n)}\left(\varphi^{-1}(t)\right)$.

3 Notice that a symmetric matrix always has an eigenbasis. Let $\lambda_{1}, \ldots, \lambda_{n}>0$ be eigenvalues of the matrix $A$. Then $\lambda_{1} \lambda_{2} \ldots \lambda_{n}=\operatorname{det} A \geq 1$, because the determinant of $A$ is a positive integer. On the other hand, $\lambda_{1}+\lambda_{2}+\ldots+\lambda_{n}=\operatorname{tr} A \leq n$. By the AM-GM inequality it holds

$$
1 \geq \frac{\lambda_{1}+\ldots+\lambda_{n}}{n} \geq \sqrt[n]{\lambda_{1} \ldots \lambda_{n}} \geq 1,
$$

hence the equality is attained in the AM-GM inequality. Therefore, $\lambda_{1}=\ldots=\lambda_{n}=$ 1 and $A$ is the identity matrix.

Alternative solution. Show that the matrix $A$ is diagonal. Indeed, suppose that $a_{i j} \neq 0$ for some $i \neq j$. Then $a_{i j}=a_{j i}=1$. Introduce a vector $\vec{v}=\overrightarrow{e_{i}}-\overrightarrow{e_{j}}$, where $\overrightarrow{e_{1}}, \ldots, \overrightarrow{e_{n}}$ is a standard basis in $\mathbb{R}^{n}$. Then $\vec{v} \mathrm{~T} A \vec{v}=a_{i i}+a_{j j}-2 \leq 0$, a contradiction because the matrix $A$ is positive definite. Since the eigenvalues of $A$ are positive, there are no zeros on its main diagonal, and finally $A$ is the identity matrix.

Answer: $A$ is the identity matrix.

4 Let the ellipses $e(A, B, C), e(B, C, A)$, and $e(C, A, B)$ share a common point D. Then 

$$
\left\{\begin{array}{l}
A D+B D=A C+B C \\
B D+C D=A B+A C \\
A D+C D=A B+B C
\end{array}\right.
$$

whence $A D=B C, B D=A C, C D=A B$. Therefore, the triangles $A B C, D C B$, $C D A$, and $B A D$ are equal. Let $A_{0} B_{0} C_{0}$ be the triangle for which $A B C$ is the medial triangle. Due to the obtained equalities $D$ is either a vertex of the triangle $A_{0} B_{0} C_{0}$, or it belongs simultaneously to all its sides, which is impossible. Without loss of generality assume that $D$ and $B_{0}$ coincide. Then $A B C D$ is a parallelogram with equal diagonals, i.e., a rectangle, whence $\angle A B C=90^{\circ}$. On the other hand, if $A B C$ is a right triangle $\left(\angle A B C=90^{\circ}\right)$, then it is evident that each of the ellipses $e(A, B, C), e(B, C, A)$, and $e(C, A, B)$ passes through a vertex $D$ of a rectangle $A B C D$, since $D$ is symmetric to $C$ with respect to the bisector of $A B$, symmetric to $A$ with respect to the bisector of $B C$, and symmetric to $B$ with respect to midpoint of $A C$.

Answer: $A B C$ is an arbitrary right triangle.

5 It is easy to verify that $x=0, x=1$, and $x=2$ satisfy the equation. Show that there are no other roots. Introduce a function

$$
f(x)=9^{x}+4^{x}+2^{x}-8^{x}-6^{x}-1
$$

and suppose that it has at least 4 zeros. By Rolle's theorem if a function $g(x)$ has at least $n$ zeros $x_{1}<x_{2}<\ldots<x_{n}$, then the function $D_{a} g(x):=a^{x}\left(g(x) a^{-x}\right)^{\prime}$ has at least $n-1$ zeros $y_{1}, \ldots, y_{n-1}$, where $x_{1}<y_{1}<x_{2}<\ldots<x_{n-1}<y_{n-1}<x_{n}$. Hence the function $D_{8} D_{6} D_{1} f(x)$ should have at least one zero. But the function

$D_{8} D_{6} D_{1} f(x)=\ln \frac{9}{8} \cdot \ln \frac{9}{6} \cdot \ln 9 \cdot 9^{x}+\ln \frac{4}{8} \cdot \ln \frac{4}{6} \cdot \ln 4 \cdot 4^{x}+\ln \frac{2}{8} \cdot \ln \frac{2}{6} \cdot \ln 2 \cdot 2^{x}$

is always positive, a contradiction.

Answer: $0,1,2$.

6 By the monotone convergence theorem, it is easy to show that $x_{n} \rightarrow 0$, as $n \rightarrow \infty$. Next, by the Stolz-Cesaro theorem it holds

$$
\begin{aligned}
n x_{n}=\frac{n}{\frac{1}{x_{n}}} \sim \frac{1}{\frac{1}{x_{n}}-\frac{1}{x_{n-1}}}=\frac{x_{n-1} x_{n}}{x_{n-1}-x_{n}}=\\
\quad=\frac{x_{n-1}\left(x_{n-1}-x_{n-1}^{2}\right)}{x_{n-1}^{2}}=1-x_{n-1} \rightarrow 1, \text { as } n \rightarrow \infty .
\end{aligned}
$$

Transform the expression in question and use the Stolz-Cesaro theorem once again: 

$$
\begin{aligned}
\frac{n^{2} x_{n}-n}{\ln n}=\frac{n x_{n} \cdot\left(n-\frac{1}{x_{n}}\right)}{\ln n} \sim \frac{n-\frac{1}{x_{n}}}{\ln n} \sim \frac{1-\frac{1}{x_{n}}+\frac{1}{x_{n-1}}}{\ln n-\ln (n-1)}=\frac{1-\frac{1}{x_{n}}+\frac{1}{x_{n-1}}}{-\ln \left(1-\frac{1}{n}\right)} \sim \\
\sim n\left(1-\frac{1}{x_{n-1}-x_{n-1}^{2}}+\frac{1}{x_{n-1}}\right)=-\frac{n x_{n-1}}{1-x_{n-1}} \rightarrow-1, \text { as } n \rightarrow \infty
\end{aligned}
$$

Answer: $-1$.

7 Introduce a function $f(x)=\left\{\begin{array}{cc}-1, & x<-1, \\ x, & -1 \leq x<2009, \\ 2009, & x \geq 2009,\end{array}\right.$ and check that $f$ is 2010-positive.

Let $x_{1}, \ldots, x_{2010}$ be such that $x_{1}+\ldots+x_{2010} \geq 0$. Notice that $f$ is convex on $(-\infty, 2009]$. If all $x_{i} \leq 2009$ then by Jensen's inequality it holds

$$
\frac{1}{2010}\left(f\left(x_{1}\right)+\ldots+f\left(x_{2010}\right)\right) \geq f\left(\frac{x_{1}+\ldots+x_{2010}}{2010}\right) \geq f(0)
$$

If there exists some $x_{i} \geq 2009$ then

$$
\frac{1}{2010}\left(f\left(x_{1}\right)+\ldots+f\left(x_{2010}\right)\right) \geq \frac{1}{2010}(2009+2009 \cdot(-1)) \geq 0=f(0) .
$$

On the other side, consider $x_{1}=\ldots=x_{2010}=-1$ and $x_{2011}=2010$. We have $x_{1}+\ldots+x_{2011}=0$, but

$$
\frac{1}{2011}\left(f\left(x_{1}\right)+\ldots+f\left(x_{2011}\right)\right)=-\frac{1}{2011}<0=f(0) .
$$

Thus, $f$ is not 2011-positive.

Answer: yes, there exists.

8 Show that

$$
\lim _{n \rightarrow \infty} \frac{1}{\sqrt{n}} \sum_{k=0}^{n} \sqrt{\left(\begin{array}{l}
n \\
k
\end{array}\right) p^{k}(1-p)^{n-k}}=0
$$

for every $0<p<1$ (in particular for $p=\frac{1}{2}$ we obtain the given sequence $\left\{a_{n}\right\}$ ).

Fix $0<\delta<1$ and consider separately index sets

$$
\{0 \leq k \leq n:|k-n p| \leq \delta n\} \text { and }\{0 \leq k \leq n:|k-n p|>\delta n\} .
$$

There exist at most $2 \delta n+1$ integers $k$ for which $|k-n p| \leq \delta n$, hence by the QM-AM inequality it holds 

$$
\begin{aligned}
&\left(\sum_{|k-n p| \leq \delta n} \sqrt{\left(\begin{array}{l}
n \\
k
\end{array}\right) p^{k}(1-p)^{n-k}}\right)^{2} \leq \\
&\leq(2 \delta n+1) \sum_{|k-n p| \leq \delta n}\left(\begin{array}{l}
n \\
k
\end{array}\right) p^{k}(1-p)^{n-k} \leq 2 \delta n+1 .
\end{aligned}
$$

Again by the QM-AM inequality we get

$$
\begin{aligned}
& \left(\sum_{|k-n p|>\delta n} \sqrt{\left(\begin{array}{l}n \\k\end{array}\right) p^{k}(1-p)^{n-k}}\right)^{2} \leq n \sum_{|k-n p|>\delta n}\left(\begin{array}{l}n \\k\end{array}\right) p^{k}(1-p)^{n-k}= \\
& =n \sum_{\left|\frac{k}{n}-p\right|>\delta}\left(\begin{array}{l}n \\k\end{array}\right) p^{k}(1-p)^{n-k}<\frac{n}{\delta^{2}} \sum_{\left|\frac{k}{n}-p\right|>\delta}\left(\begin{array}{l}n \\k\end{array}\right) p^{k}(1-p)^{n-k}\left(\frac{k}{n}-p\right)^{2} \leq \\
& \leq \frac{n}{\delta^{2}} \sum_{k=0}^{n}\left(\begin{array}{l}n \\k\end{array}\right) p^{k}(1-p)^{n-k}\left(\frac{k}{n}-p\right)^{2}=\frac{p(1-p)}{\delta^{2}} .
\end{aligned}
$$

The latter equality is a corollary of well-known identities

$$
\begin{gathered}
\sum_{k=0}^{n}\left(\begin{array}{l}
n \\
k
\end{array}\right) p^{k}(1-p)^{n-k}=1, \quad \sum_{k=0}^{n} k\left(\begin{array}{l}
n \\
k
\end{array}\right) p^{k}(1-p)^{n-k}=n p, \\
\sum_{k=0}^{n} k(k-1)\left(\begin{array}{l}
n \\
k
\end{array}\right) p^{k}(1-p)^{n-k}=n(n-1) p^{2}
\end{gathered}
$$

Thus,

$$
\begin{aligned}
& \frac{1}{\sqrt{n}} \sum_{k=0}^{n} \sqrt{\left(\begin{array}{l}n \\k\end{array}\right) p^{k}(1-p)^{n-k}} \leq \sqrt{\frac{2 \delta n+1}{n}}+\frac{\sqrt{p(1-p)}}{\delta \sqrt{n}} \leq \\
& \leq \sqrt{2 \delta}+\frac{1}{\sqrt{n}}+\frac{\sqrt{p(1-p)}}{\delta \sqrt{n}} .
\end{aligned}
$$

It remains to notice that for every $\varepsilon>0$ one can choose $\delta>0$ such that $\sqrt{2 \delta}<\frac{\varepsilon}{3}$, and choose $N \geq 1$ such that for all $n \geq N$ it holds $\frac{1}{\sqrt{n}}<\frac{\varepsilon}{3}, \frac{\sqrt{p(1-p)}}{\delta \sqrt{n}}<\frac{\varepsilon}{3}$.

Answer: 0.

9 It holds

$$
\arctan (\xi+\eta) \geq \arctan \xi, \quad \text { almost surely, }
$$

hence $\mathrm{E} \arctan (\xi+\eta) \geq \mathrm{E} \arctan \xi$ (the expectations exist because $\arctan x$ is bounded). But $\xi+\eta$ is identically distributed with $\xi$, therefore, in fact the equality is attained in the latter inequality. In view of $(*)$ we have $\arctan (\xi+\eta)=\arctan \xi$ almost surely, whence $\xi+\eta=\xi$ almost surely, and $\eta=0$ almost surely.

11 Show the following: if $g_{n}(x) \stackrel{\lambda}{\rightarrow} g(x)$, as $n \rightarrow \infty$, and a function $\varphi: \mathbb{R} \rightarrow \mathbb{R}$ is uniformly continuous on $\mathbb{R}$, then $\varphi\left(g_{n}(x)\right) \stackrel{\lambda}{\rightarrow} \varphi(g(x))$, as $n \rightarrow \infty$. Indeed, fix $\varepsilon>0$. There exists $\delta=\delta(\varepsilon)$ such that the inequality $|t-s| \leq \delta$ implies the inequality $|\varphi(t)-\varphi(s)|<\varepsilon$. Then as $n \rightarrow \infty$, it holds

$$
\lambda\left(\left\{x:\left|\varphi\left(g_{n}(x)\right)-\varphi(g(x))\right|>\varepsilon\right\}\right) \leq \lambda\left(\left\{x:\left|g_{n}(x)-g(x)\right|>\delta\right\}\right) \rightarrow 0 .
$$

To prove the statement of the problem consider

$$
g_{n}(x)=\left(f_{n}(x)\right)^{2011}, n \geq 1, \quad g(x)=(f(x))^{2011},
$$

and notice that the function $\varphi(t)=\sqrt[2011]{t}, t \in \mathbb{R}$, is uniformly continuous on $\mathbb{R}$.

12 Let $X$ be a ring with 2011 elements. Since 2011 is a prime number, the additive group of the ring is the cyclic group of order 2011. Fix a nonzero element $x \in X$. Then $X=\{0, x, 2 x, \ldots, 2010 x\}$. Consider two cases:

(1) $x^{2}=0$. Then for arbitrary $y, z \in X$ it holds $y=i x, z=j x$, whence $y z=$ $i j x^{2}=0$, thus, it is a ring with the zero product property.

(2) $x^{2}=k x$ for some integer $k \in\{1, \ldots, 2010\}$. There exists $l \in\{1, \ldots, 2010\}$ such that $k l-1$ is divisible by 2011. Consider $a=l x$. Then $a^{2}=l^{2} x^{2}=l^{2} k x=l x=a$, and $X=\{0, a, 2 a, \ldots, 2010 a\}$, so the mapping $f: X \ni i a \mapsto i \in \mathbb{Z}_{2011}$ is an isomorphism.

Answer: there are two rings: the ring with the zero product property and $\mathbb{Z}_{2011}$.

13 See solution of the Problem 8.

Answer: 0.

14 Denote by $B_{i}$ the $n \times n$ matrix the only nonzero entry of which equals 1 and is located in $i$ th row and $i$ th column. Put $\overrightarrow{v_{1}}=\left(\sqrt{x_{1}}, \sqrt{x_{2}}, \ldots, \sqrt{x_{n}}\right)$, then $\left\|\overrightarrow{v_{1}}\right\|=1$. One can extend $\overrightarrow{v_{1}}$ to an orthobasis $\overrightarrow{v_{1}}, \overrightarrow{v_{2}}, \ldots, \overrightarrow{v_{n}}$ of the space $\mathbb{R}^{n}$. Introduce a matrix $S$ whose columns are vectors $\overrightarrow{v_{1}}, \overrightarrow{v_{2}}, \ldots, \overrightarrow{v_{n}}$. Then the matrices $A_{i}=S^{\mathrm{T}} B_{i} S$, $1 \leq i \leq n$, have required properties, because

$$
\begin{gathered}
A_{i}^{2}=S^{\mathrm{T}} B_{i} S S^{\mathrm{T}} B_{i} S=S^{\mathrm{T}} B_{i}^{2} S=S^{\mathrm{T}} B_{i} S=A_{i}, \\
A_{i} A_{j}=S^{\mathrm{T}} B_{i} S S^{\mathrm{T}} B_{j} S=S^{\mathrm{T}} B_{i} B_{j} S=0, i \leq j,
\end{gathered}
$$

and $\left(A_{i}\right)_{11}=x_{i}$.

Answer: yes, there exist.

15 The function $g(t)=t_{+}$is convex on $\mathbb{R}$, hence by Jensen's inequality

$$
\mathrm{E}\left(\xi_{1}+\xi_{2}+\xi_{3}-K\right)_{+} \geq\left(\mathrm{E}\left(\xi_{1}+\xi_{2}+\xi_{3}\right)-K\right)_{+}=(2-K)_{+} .
$$

The equality is attained, e.g., under the condition $\xi_{1}+\xi_{2}+\xi_{3}=2$ almost surely. It remains to construct an example of random variables $\xi_{1}, \xi_{2}, \xi_{3}$ satisfying the latter condition. Let the random vector $\left(\xi_{1}, \xi_{2}, \xi_{3}\right)$ be uniformly distributed in the triangle with vertices $(1,1,0),(0,1,1)$, and $(1,0,1)$. Then $\xi_{1}+\xi_{2}+\xi_{3} \equiv 2$ and it is not difficult to verify that each of the random variables $\xi_{1}, \xi_{2}, \xi_{3}$ has the required probability density function.

Answer: $f(K)=(2-K)_{+}, K \geq 0$.

16 Let $x_{0}$ be a point at which the function $d(T x, x)$ attains its minimal value on the compact set $X$. Suppose that $x_{0}$ is not a fixed point of the mapping $T$. For $x=x_{0}$ and $y=T x_{0}$, it holds $\frac{1}{2} d\left(x_{0}, T x_{0}\right)<d\left(x_{0}, T x_{0}\right)$, hence $d\left(T^{2} x_{0}, T x_{0}\right)<d\left(x, T x_{0}\right)$, a contradiction.

Now, suppose that there exist two distinct fixed points $x_{0}$ and $y_{0}$. Then $0=$ $\frac{1}{2} d\left(x_{0}, T x_{0}\right)<d\left(x_{0}, y_{0}\right)$, hence $d\left(x_{0}, y_{0}\right)<d\left(x_{0}, y_{0}\right)$, a contradiction. 

\section{2}

2012

1 Notice that $a_{i j}=-a_{j i}, i, j=0,1, \ldots, 2012$, i.e., the matrix $A=\left(a_{i j}\right)_{i, j=0}^{2012}$ is skew-symmetric, $A^{\mathrm{T}}=-A$. Therefore,

$$
\operatorname{det} A=\operatorname{det} A^{\mathrm{T}}=\operatorname{det}(-A)=(-1)^{2013} \operatorname{det} A=-\operatorname{det} A,
$$

whence $\operatorname{det} A=0$.

Answer: $\operatorname{det}\left(a_{i j}\right)=0$.

2 Let a function $f:[a, b] \rightarrow \mathbb{R}$ be monotone, and $F$ be one of its primitive functions. Show that $f \in C([a, b])$. Indeed, let $x_{0} \in[a, b)$. By Lagrange's mean value theorem for each $0<h<b-x_{0}$ there exists a real number $\theta_{h} \in\left(x_{0}, x_{0}+h\right)$ such that $\frac{F\left(x_{0}+h\right)-F\left(x_{0}\right)}{h}=f\left(\theta_{h}\right)$. Since $f$ is monotone, there exists the right-hand limit $f\left(x_{0}+\right)$. Thus $f\left(x_{0}\right)=F^{\prime}\left(x_{0}\right)=\lim _{h \rightarrow 0+} f\left(\theta_{h}\right)=f\left(x_{0}+\right)$. Similarly, for $x_{0} \in(a, b]$ it holds $f\left(x_{0}\right)=f\left(x_{0}-\right)$. Therefore, $f \in C([a, b])$, whence by Cantor's theorem $f$ is uniformly continuous on $[a, b]$.

Answer: yes, it is.

3 Consider a Cartesian coordinate system, in which the ellipse has a canonical equation $\frac{x^{2}}{a^{2}}+\frac{y^{2}}{b^{2}}=1, a>b>0$ (see Fig. 1). The foci $F_{1}$ and $F_{2}$ have coordinates $(\pm c, 0)$, where $c=\sqrt{a^{2}-b^{2}}$. Take any point $A(x, y)$ of the ellipse that is not lying on the $x$-axis, and denote by $I$ the incenter of the triangle $F_{1} F_{2} A$. Then $I$ lies on the angle bisector $A L$ of the triangle. Put $F_{1} A=u$ and $F_{2} A=v, u+v=2 a$. Let the point $L$ have coordinates $\left(x_{L}, 0\right)$. By the angle bisector theorem $F_{1} L: F_{2} L=u: v$, therefore,

$$
F_{1} L=x_{L}+c=F_{1} F_{2} \cdot \frac{u}{u+v}=\frac{2 c u}{u+v},
$$

(c) Springer International Publishing AG 2017

V. Brayman and A. Kukush, Undergraduate Mathematics

Competitions (1995-2016), Problem Books in Mathematics,

DOI 10.1007/978-3-319-58673-1_40 whence

$$
x_{L}=\frac{c(u-v)}{u+v}=\frac{c\left(u^{2}-v^{2}\right)}{(u+v)^{2}}=\frac{c\left(u^{2}-v^{2}\right)}{4 a^{2}} .
$$

Next, $u^{2}-v^{2}=(x+c)^{2}+y^{2}-(x-c)^{2}-y^{2}=4 c x$, hence $x_{L}=\frac{c^{2} x}{a^{2}}$.

Fig. 1 The locus of incenters of triangles $F_{1} F_{2} A$, for which the vertex $A$ lies on a given ellipse with the foci $F_{1}$ and $F_{2}$

![](https://cdn.mathpix.com/cropped/2022_10_28_1f3b279542cbcf365e2cg-107.jpg?height=436&width=572&top_left_y=655&top_left_x=1102)

Again by the angle bisector theorem the point $I\left(x_{I}, y_{I}\right)$ divides the segment $A L$ in the ratio

$$
A F_{1}: F_{1} L=u: \frac{2 c u}{u+v}=(u+v): 2 c=a: c,
$$

whence

$$
x_{I}=\frac{c x+a x_{L}}{a+c}=\frac{c x}{a}, \quad y_{I}=\frac{c y}{a+c} .
$$

Thus, the point $I$ belongs to an ellipse which can be obtained from the initial one after scaling along $x$-axes by a factor of $\frac{c}{a}$ and along $y$-axes by a factor of $\frac{c}{a+c}$. Answer: the locus is an ellipse, the major axis of which is $F_{1} F_{2}$, the points $F_{1}$ and $F_{2}$ omitted.

4 By the AM-GM inequality

$$
\frac{1}{x_{1}^{x_{2}}}+\frac{1}{x_{2}^{x_{3}}}+\ldots+\frac{1}{x_{n}^{x_{1}}} \geq \frac{n}{\sqrt[n]{x_{1}^{x_{2}} x_{2}^{x_{3}} \ldots x_{n}^{x_{1}}}}
$$

By the weighted AM-GM inequality

$$
\left(x_{1}^{x_{2}} x_{2}^{x_{3}} \ldots x_{n}^{x_{1}}\right)^{\frac{1}{x_{1}+\ldots+x_{n}}} \leq \frac{x_{1} x_{2}+x_{2} x_{3}+\ldots+x_{n} x_{1}}{x_{1}+\ldots+x_{n}},
$$

hence

$$
\sqrt[n]{x_{1}^{x_{2}} x_{2}^{x_{3}} \ldots x_{n}^{x_{1}}} \leq\left(\frac{x_{1} x_{2}+x_{2} x_{3}+\ldots+x_{n} x_{1}}{x_{1}+\ldots+x_{n}}\right)^{\frac{x_{1}+\ldots+x_{n}}{n}} \leq \frac{x_{1} x_{2}+x_{2} x_{3}+\ldots+x_{n} x_{1}}{x_{1}+\ldots+x_{n}}
$$

(indeed, the expression in parentheses does not exceed 1 and the exponent is not less than 1).

Thus,

$$
\begin{aligned}
& \frac{1}{x_{1}^{x_{2}}}+\frac{1}{x_{2}^{x_{3}}}+\ldots+\frac{1}{x_{n}^{x_{1}}} \geq \frac{n\left(x_{1}+\ldots+x_{n}\right)}{x_{1} x_{2}+x_{2} x_{3}+\ldots+x_{n} x_{1}} \geq \\
& \geq \frac{n^{2}}{x_{1} x_{2}+x_{2} x_{3}+\ldots+x_{n} x_{1}} \geq \frac{n^{2}}{x_{1}^{2}+\ldots+x_{n}^{2}} .
\end{aligned}
$$

5 Let $n \geq 2$ and $m=[\sqrt{n}]$, where $[x]$ denotes the integer part of $x$. Each of $m$ numbers $x_{1}, \ldots, x_{m}$ is a nonnegative integer and does not exceed $n$. Hence these numbers can be chosen in $(n+1)^{m}$ ways at most. Next, the sum of numbers $x_{m+1}, \ldots, x_{n}$ is less than or equal to $m$, otherwise

$$
x_{1}+2 x_{2}+\ldots+n x_{n} \geq(m+1)\left(x_{m+1}+\ldots+x_{n}\right) \geq(m+1)^{2}>n .
$$

Thus, each choice of numbers $x_{m+1}, \ldots, x_{n}$ corresponds to a map from $\{1, \ldots, m\}$ to $\{m+1, \ldots, n, n+1\}$ which takes the value $k$ at $x_{k}$ points, $m+1 \leq k \leq n$, and the value $n+1$ at $m-\left(x_{m+1}+\ldots+x_{n}\right)$ points. It is clear that different choices of the numbers $x_{m+1}, \ldots, x_{n}$ correspond to different functions. Therefore, the number of ways to make a choice does not exceed $(n-m+1)^{m} \leq n^{m}$. Thus, a total number of solutions satisfies

$$
p(n) \leq(n+1)^{m} n^{m} \leq(n+1)^{2 m} \leq \exp (2 \sqrt{n} \ln (n+1))=O\left(e^{\varepsilon n}\right), \text { as } n \rightarrow \infty,
$$

for each $\varepsilon>0$. Hence $p(n)=O\left(\alpha^{n}\right)$, as $n \rightarrow \infty$, for all $\alpha>1$.

6 For $x=y=0$, we have $f(0)=0$. For $x=0$, we get $f(-y)=f(y)$, for all $y \in \mathbb{R}$, i.e., $f$ is an even function. Substitute $y=n x$ and get

$$
f((n+1) x)=2 f(x)+2 f(n x)-f((n-1) x), n \in \mathbb{N},
$$

whence it is easy to show by induction on $n \geq 1$ that $f(n x)=n^{2} f(x)$, for all $x \in \mathbb{R}$, $n \in \mathbb{N}$. For every $x \in \mathbb{R}$ and $n, k \in \mathbb{N}$, it holds $f(n x)=k^{2} f\left(\frac{n}{k} x\right)=n^{2} f(x)$, whence $f\left(\frac{n}{k} x\right)=\left(\frac{n}{k}\right)^{2} f(x)$. Since $f$ is even we get

$$
f(r x)=r^{2} f(x), x \in \mathbb{R}, r \in \mathbb{Q} .
$$

For an arbitrary $x \neq 0$, consider a sequence $\left\{r_{n}\right\}$ of rational numbers that converges to $\frac{1}{x}$. Then $f\left(r_{n} x\right)=r_{n}^{2} f(x) \rightarrow \frac{1}{x^{2}} f(x)$, as $n \rightarrow \infty$. But $f$ is continuous at the point 1 , hence $f\left(r_{n} x\right) \rightarrow f(1)$, as $n \rightarrow \infty$. Therefore, $\frac{1}{x^{2}} f(x)=f(1)$, that is $f(x)=$ $x^{2} f(1)$, for all $x \neq 0$. For $x=0$, the latter equality still holds. Thus, $f(x)=a x^{2}$, $x \in \mathbb{R}$, where $a=f(1)$. It remains to check that these functions satisfy the conditions of the problem.

Answer: $f(x)=a x^{2}, x \in \mathbb{R}$, where $a \in \mathbb{R}$ is arbitrary. Fig. 2 The set of points $\left(x_{1}, x_{2}\right)$ which satisfy the inequality $\left|x_{2}\right| \leq \frac{1}{3}\left|x_{1}\right|$

![](https://cdn.mathpix.com/cropped/2022_10_28_1f3b279542cbcf365e2cg-109.jpg?height=255&width=569&top_left_y=347&top_left_x=1103)

7 Introduce a sequence $b_{n}=\sqrt{2\left(a_{n}^{2}-1\right)}$. It holds $b_{1}=0$ and $a_{n+1}=3 a_{n}+2 b_{n}$, $n \geq 1$. We claim that $b_{n+1}=4 a_{n}+3 b_{n}, n \geq 1$. Indeed, $a_{n} \geq 0, b_{n} \geq 0$, and

$$
\begin{aligned}
b_{n+1}^{2}=2 a_{n+1}^{2}-2 &=2\left(3 a_{n}+2 b_{n}\right)^{2}-2 a_{n}^{2}+b_{n}^{2}=\\
&=16 a_{n}^{2}+9 b_{n}^{2}+24 a_{n} b_{n}=\left(4 a_{n}+3 b_{n}\right)^{2} .
\end{aligned}
$$

Therefore, $\left(\begin{array}{l}a_{1} \\ b_{1}\end{array}\right)=\left(\begin{array}{l}1 \\ 0\end{array}\right)$ and $\left(\begin{array}{l}a_{n+1} \\ b_{n+1}\end{array}\right)=\left(\begin{array}{ll}3 & 2 \\ 4 & 3\end{array}\right)\left(\begin{array}{l}a_{n} \\ b_{n}\end{array}\right), n \geq 1$, whence all the numbers $a_{n}$ and $b_{n}, n \geq 2$, are positive integers. Then

Fix an arbitrary $n \geq 1$. Introduce matrices $A=\left(\begin{array}{ll}3 & 2 \\ 4 & 3\end{array}\right)$ and $B=A^{n}=\left(\begin{array}{ll}p & q \\ r & s\end{array}\right)$.

$$
\left(\begin{array}{l}
a_{n+1} \\
b_{n+1}
\end{array}\right)=A^{n}\left(\begin{array}{l}
a_{1} \\
b_{1}
\end{array}\right)=B\left(\begin{array}{l}
1 \\
0
\end{array}\right),\left(\begin{array}{l}
a_{2 n+1} \\
b_{2 n+1}
\end{array}\right)=A^{2 n}\left(\begin{array}{l}
a_{1} \\
b_{1}
\end{array}\right)=B^{2}\left(\begin{array}{l}
1 \\
0
\end{array}\right),
$$

whence $a_{n+1}=p, a_{2 n+1}=p^{2}+q r$. It remains to notice that

$$
\operatorname{det} B=p s-q r=(\operatorname{det} A)^{n}=1 .
$$

Therefore, $p$ and $q r$ are relatively prime, hence $a_{n+1}$ and $a_{2 n+1}$ are relatively prime as well.

Remark 1 One can show that $a_{n+2}=6 a_{n+1}-a_{n}, n \geq 1$. The characteristic equation $\lambda^{2}=6 \lambda-1$ has roots $3 \pm \sqrt{8}$, hence $a_{n}=\alpha(3-\sqrt{8})^{n}+\beta(3+\sqrt{8})^{n}$, where $\alpha$ and $\beta$ can be found from the initial conditions $a_{1}=1, a_{2}=3$. Then

$$
a_{n}=\frac{1}{2}\left((3-\sqrt{8})^{n-1}+(3+\sqrt{8})^{n-1}\right), n \geq 1 .
$$

Now, it is easy to check that $a_{2 n+1}=2 a_{n+1}^{2}-1$, therefore, $a_{n+1}$ and $a_{2 n+1}$ are relatively prime.

8 Put $A=\left(\begin{array}{cc}\frac{1}{\sqrt{2}} & 0 \\ 0 & \sqrt{3}\end{array}\right)$, det $A=\sqrt{\frac{3}{2}}>1$. Find for which vectors $x=\left(\begin{array}{l}x_{1} \\ x_{2}\end{array}\right) \in \mathbb{R}^{2}$ the inequality $\|A x\| \leq \frac{\sqrt{3}}{2}\|x\|$ holds. This inequality is equivalent to 

$$
\frac{1}{2} x_{1}^{2}+3 x_{2}^{2} \leq \frac{3}{4}\left(x_{1}^{2}+x_{2}^{2}\right), \text { or }\left|x_{2}\right| \leq \frac{1}{3}\left|x_{1}\right| .
$$

The set of points $\left(x_{1}, x_{2}\right)$ which satisfy the latter inequality is shaded on Fig. 2. Since $\tan \frac{\varphi}{2}=\frac{1}{3}$ then $\tan \varphi=\frac{3}{4}$, whence $\varphi>\frac{\pi}{6}$.

Put $B=(1+\varepsilon)\left(\begin{array}{cc}\cos \varphi & -\sin \varphi \\ \sin \varphi & \cos \varphi\end{array}\right)$, where $\varepsilon>0$ is such that $(1+\varepsilon)^{5} \cdot \frac{\sqrt{3}}{2} \leq 1$. We have $\operatorname{det} B=(1+\varepsilon)^{2}>1$. Show that the matrices $A$ and $B$ have required properties.

For a fixed $u_{0} \in \mathbb{R}^{2}$, introduce a vector sequence $u_{i}, i \geq 1$, as follows: $u_{i}=$ $M_{i} u_{i-1}$, where $M_{i}=A$ if $u_{i-1}$ belongs to the shaded set on Fig. 2, and $M_{i}=B$ otherwise. Put $i_{0}=0$, and let $1 \leq i_{1}<i_{2}<\ldots<i_{k}<\ldots$ be the sequence of indices for which $M_{i}=A$. To obtain a vector $B x$ one should rotate a vector $x$ by the angle $\varphi$ counterclockwise and increase the length of the resulting vector in $1+\varepsilon$ times. Since $\varphi>\frac{\pi}{6}$, then for each $x \in \mathbb{R}^{2}$ at least one of the vectors $x, B x, B^{2} x, \ldots, B^{5} x$ belongs to the shaded set on Fig. 2. Thus, $i_{1} \leq 5, i_{2}-i_{1} \leq 5, i_{3}-i_{2} \leq 5, \ldots$ Therefore, the inequality $(1+\varepsilon)^{5} \cdot \frac{\sqrt{3}}{2} \leq 1$ implies $\left\|u_{0}\right\| \geq\left\|u_{i_{1}}\right\| \geq\left\|u_{i_{2}}\right\| \geq\left\|u_{i_{3}}\right\| \geq \ldots$ Then for $i_{k}<i \leq i_{k+1}, k \geq 0$, it holds $\left\|u_{i}\right\| \leq(1+\varepsilon)^{5}\left\|u_{i_{k}}\right\| \leq(1+\varepsilon)^{5}\left\|u_{0}\right\|$.

Answer: yes, there exist.

9 Let $1 \leq k \leq n-1$. Rewrite the inequality in question:

$$
\begin{gathered}
\frac{1}{n-k} \sum_{j=k}^{n} \frac{n-j}{j} \leq 1, \quad \frac{1}{n-k} \sum_{j=k}^{n-1}\left(\frac{n}{j}-1\right) \leq 1, \quad \frac{1}{n-k} \sum_{j=k}^{n-1} \frac{n}{j} \leq 2, \\
\sum_{j=k}^{n-1} \frac{1}{j} \leq \frac{2(n-k)}{n}=2-\frac{2 k}{n} .
\end{gathered}
$$

Since $n \geq 3$ by the condition of the problem, the inequality holds for $k=n-1$ and fails for $k=1$, because $\frac{1}{n-1} \leq \frac{2}{n}$ and $1+\frac{1}{2}+\ldots+\frac{1}{n-1}+\frac{2}{n}>1+\frac{n}{n}=2$. Hence there exists the required number $k^{*}$, and moreover $2 \leq k^{*} \leq n-1$. Also notice that for each fixed $k$ the inequality fails for $n$ large enough, hence $k^{*} \rightarrow \infty$, as $n \rightarrow \infty$.

The number $k^{*}$ satisfies

$$
2-\frac{2 k^{*}}{n} \geq \sum_{j=k^{*}}^{n-1} \frac{1}{j}, \quad 2-\frac{2 k^{*}-2}{n}<\sum_{j=k^{*}-1}^{n-1} \frac{1}{j},
$$

hence $\frac{1}{k^{*}-1}>\frac{2}{n}$, that is $k^{*}<\frac{n}{2}+1$.

Show that a sequence $\left\{\frac{k^{*}(n)}{n}\right\}$ has a unique limit point. Consider an arbitrary subsequence of numbers $\left\{n^{\prime}\right\}$, for which $\frac{k^{*}\left(n^{\prime}\right)}{n^{\prime}} \rightarrow u \in\left[0, \frac{1}{2}\right]$, as $n^{\prime} \rightarrow \infty$. Because $k^{*}\left(n^{\prime}\right) \rightarrow \infty$, as $n^{\prime} \rightarrow \infty$, the inequalities $(*)$ imply that 

$$
\sum_{j=k^{*}\left(n^{\prime}\right)}^{n^{\prime}-1} \frac{1}{j} \rightarrow 2-2 u, \text { as } n^{\prime} \rightarrow \infty .
$$

On the other hand, since

$$
\left|\sum_{j=k}^{n-1} \frac{1}{j}-\sum_{j=k}^{n-1} \int_{j}^{j+1} \frac{d t}{t}\right| \leq \sum_{j=k}^{n-1}\left(\frac{1}{j}-\frac{1}{j+1}\right)<\frac{1}{k} \text { and } \sum_{j=k}^{n-1} \int_{j}^{j+1} \frac{d t}{t}=\int_{k}^{n} \frac{d t}{t}=\ln \frac{n}{k},
$$

it follows that

$$
\lim _{n^{\prime} \rightarrow \infty} \sum_{j=k^{*}\left(n^{\prime}\right)}^{n^{\prime}-1} \frac{1}{j}=\lim _{n^{\prime} \rightarrow \infty} \ln \frac{n^{\prime}}{k^{*}\left(n^{\prime}\right)}=-\ln u
$$

Thus, $2-2 u=-\ln u$. The function $f(x)=2-2 x+\ln x$ is increasing on $\left(0, \frac{1}{2}\right]$, $f(0+)=-\infty$, and $f\left(\frac{1}{2}\right)=1-\ln 2>0$. Hence this function has a unique zero $x_{0}$ on the interval $\left(0, \frac{1}{2}\right)$. We have $f(u)=0$, so $u=x_{0}$ is the only possible limit point of the sequence $\left\{\frac{k^{*}(n)}{n}\right\}$, hence $\frac{k^{*}(n)}{n} \rightarrow x_{0}$, as $n \rightarrow \infty$.

10 Fix $x_{1}, x_{2} \in X$ and $y_{1}, y_{2} \in Y$ such that $x_{1} \neq x_{2}, y_{1} \neq y_{2}$. Introduce a continuous function

$$
f(x, y)=\rho\left(x, x_{1}\right) \sigma\left(y, y_{1}\right), x \in X, y \in Y .
$$

Suppose that $f(x, y)=g(x)+h(y)$, for all $x \in X$ and $y \in Y$. Substitute $y=y_{1}$ and get $f\left(x, y_{1}\right) \equiv 0$, whence $g(x) \equiv-h\left(y_{1}\right)$. In a similar way substitute $x=x_{1}$ and get $h(y) \equiv-g\left(x_{1}\right)$. Hence $f(x, y) \equiv-h\left(y_{1}\right)-g\left(x_{1}\right)=$ const. But $f\left(x_{1}, y_{1}\right)=0$ and $f\left(x_{2}, y_{2}\right)=\rho\left(x_{2}, x_{1}\right) \sigma\left(y_{2}, y_{1}\right)>0$, a contradiction.

Answer: yes, always.

11 If $\xi=0$ or $\eta=0$ then $u(\xi+\eta)-u(\xi)-u(\eta)+u(0)=0$. Since by the condition of the problem it holds $\mathrm{P}(\xi=0$ or $\eta=0)=1$, we have

$$
\mathrm{E}(u(\xi+\eta)-u(\xi)-u(\eta)+u(0))=0
$$

12 The general solution to the equation has a form

$$
x(t)=c e^{-a t}+e^{-a t} \int_{0}^{t} e^{a u} f(u) d u, t \in \mathbb{R},
$$

where $c$ is an arbitrary constant. Denote $f_{\infty}=\lim _{t \rightarrow+\infty} f(t)$.

(a) If $a>0$ then the first summand on the right-hand side of $(*)$ tends to zero, as $t \rightarrow+\infty$, and one can apply L'Hospital's Rule to the second summand, because $e^{a t} \rightarrow+\infty$, as $t \rightarrow+\infty$ 

$$
\lim _{t \rightarrow+\infty} \frac{1}{e^{a t}} \int_{0}^{t} e^{a u} f(u) d u=\lim _{t \rightarrow+\infty} \frac{\frac{d}{d t} \int_{0}^{t} e^{a u} f(u) d u}{\frac{d}{d t} e^{a t}}=\lim _{t \rightarrow+\infty} \frac{e^{a t} f(t)}{a e^{a t}}=\frac{f_{\infty}}{a} .
$$

Thus, the general solution $(*)$ tends to $\frac{f_{\infty}}{a}$. Answer: for $a>0$, the limit equals $\frac{1}{a} \lim _{t \rightarrow+\infty} f(t)$.

(b) Let $a<0$. Notice that the continuous function $f$ is bounded on $[0,+\infty)$, hence the improper integral $\int_{0}^{+\infty} e^{a u} f(u) d u$ converges absolutely. Rewrite the solution in a form

$$
x(t)=\left(c+\int_{0}^{\infty} e^{a u} f(u) d u\right) e^{-a t}-\frac{1}{e^{a t}} \int_{t}^{+\infty} e^{a u} f(u) d u, t \in \mathbb{R} .
$$

As $t \rightarrow+\infty$, we have an indeterminate form $0 / 0$ in the second summand. By L'Hospital's Rule we get

$$
\lim _{t \rightarrow+\infty} \frac{1}{e^{a t}} \int_{t}^{+\infty} e^{a u} f(u) d u=\lim _{t \rightarrow+\infty} \frac{-e^{a t} f(t)}{a e^{a t}}=\frac{f_{\infty}}{-a} .
$$

Therefore, the solution has a finite limit, as $t \rightarrow+\infty$, only for $c=-\int_{0}^{+\infty} e^{a u} f(u) d u$, and in this case the limit equals $\frac{f_{\infty}}{a}$.

13 Because $A^{2}=0$, the matrix $A$ has a single eigenvalue 0 of multiplicity 3 . Moreover, the Jordan form of $A$ does not equal to the cell $J_{3}(0)$, i.e., the cells in the Jordan form of $A$ are $J_{2}(0)$ or $J_{1}(0)$. Hence $\operatorname{rk}(A) \leq 1$. In a similar way $\operatorname{rk}(B) \leq 1$. Therefore, $\operatorname{rk}(A+B) \leq 2$, and 0 is an eigenvalue of the matrix $A+B$. Moreover $\operatorname{tr}(A+B)=0$, i.e., the sum of all the eigenvalues of $A+B$ (repeated according to their multiplicities) equals 0 . Hence the set of eigenvalues of $A+B$ has a form $\{0, \lambda,-\lambda\}$ for some $\lambda \in \mathbb{C}$. Every such set is indeed a collection of eigenvalues of $A+B$ for the matrices

$$
A=\left(\begin{array}{lll}
0 & \lambda & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right), \quad B=\left(\begin{array}{lll}
0 & 0 & 0 \\
\lambda & 0 & 0 \\
0 & 0 & 0
\end{array}\right)
$$

Answer: $\{0, \lambda,-\lambda\}$, where $\lambda \in \mathbb{C}$ is arbitrary.

14 Show that $g_{n} \stackrel{\lambda}{\rightarrow} 0$, as $n \rightarrow \infty$. Suppose that it is false. Then there exists a sequence $n_{k} \rightarrow \infty$ and $\varepsilon>0$ such that $\lambda\left(\left\{g_{n_{k}} \geq \varepsilon\right\}\right) \geq \varepsilon, k \geq 1$. The function $f(x)=\frac{x^{2}}{1+x}$ is increasing on $[0,+\infty)$, because $f^{\prime}(x)=\frac{x^{2}+2 x}{(1+x)^{2}}>0$. Hence 

$$
\int_{X} \frac{g_{n_{k}}^{2}}{1+g_{n_{k}}} d \lambda \geq \varepsilon \cdot \frac{\varepsilon^{2}}{1+\varepsilon} \not \rightarrow 0, \text { as } k \rightarrow \infty,
$$

a contradiction.

Then $\int_{X} g_{n} I_{\left\{g_{n}<1\right\}} d \lambda \rightarrow 0$, as $n \rightarrow \infty$, by Lebesgue dominated convergence theorem (with the majorant 1), and $\int_{X} g_{n} I_{\left\{g_{n} \geq 1\right\}} d \lambda \rightarrow 0$, as $n \rightarrow \infty$, because $g_{n} \leq \frac{2 g_{n}^{2}}{1+g_{n}}$ on the set $\left\{x \mid g_{n}(x) \geq 1\right\}$.

15 Fix $a \in[0,1]$ and an orthobasis $e_{0}, e_{1}, \ldots$ in $l_{2}$. Then the vectors

$$
v_{k}=\sqrt{a} e_{0}+\sqrt{1-a} e_{k}, k \geq 1,
$$

satisfy the condition of the problem.

Conversely, assume that vectors $v_{k}, k \geq 1$, satisfy the condition of the problem. Prove that $a \in[0,1]$. Let $a=x+i y$, where $x, y \in \mathbb{R}$. By the Cauchy-Schwarz inequality it holds

$$
\left|\left(v_{1}+\ldots+v_{n}, v_{n+1}+\ldots+v_{2 n}\right)\right| \leq\left\|v_{1}+\ldots+v_{n}\right\| \cdot\left\|v_{n+1}+\ldots+v_{2 n}\right\|
$$

Notice that

$$
\left\|v_{1}+\ldots+v_{n}\right\|^{2}=\sum_{k=1}^{n}\left\|v_{k}\right\|^{2}+2 \sum_{k<l} \operatorname{Re}\left(v_{k}, v_{l}\right)=n+n(n-1) x
$$

and in a similar way $\left\|v_{n+1}+\ldots+v_{2 n}\right\|^{2}=n+n(n-1) x$. Hence

$$
n^{2}|x+i y| \leq n+n(n-1) x \text {, i.e., } \sqrt{x^{2}+y^{2}} \leq \frac{1}{n}+\frac{n-1}{n} x .
$$

As $n \rightarrow \infty$, we get $\sqrt{x^{2}+y^{2}} \leq x$. Therefore, $x \geq 0$ and $y=0$. Thus, $a=x$. Since $\left|\left(v_{1}, v_{2}\right)\right| \leq 1$ we have $x \leq 1$.

Answer: $a \in[0,1]$.

16 We perform the proof in several steps.

(1) Similarly to the solution of Problem 6, we get that $f(0)=0$ and $f$ is even.

(2) Show that for all $x, y \in \mathbb{R}$ and $n \in \mathbb{Z}$ it holds

$$
f(n x+y)=n^{2} f(x)+n \Delta+f(y),
$$

where $\Delta=\Delta(x, y)=f(x+y)-f(x)-f(y)$. For $n=0$ and $n=1$ the equality is evident. Next, $f(k x+y+x)+f(k x+y-x)=2 f(k x+y)+2 f(x)$. Thus, if (*) holds for $n=k-1$ and $n=k$, then 

$$
\begin{aligned}
&f((k+1) x+y)=2 f(k x+y)+2 f(x)-f((k-1) x+y)= \\
&\quad=\left(2 k^{2}+2-(k-1)^{2}\right) f(x)+(2 k-(k-1)) \Delta+(2-1) f(y)= \\
&\quad=(k+1)^{2} f(x)+(k+1) \Delta+f(y),
\end{aligned}
$$

i.e., $(*)$ holds for $n=k+1$ as well. Similarly if $(*)$ holds for $n=k$ and $n=k+1$, then $(*)$ holds for $n=k-1$ as well. Therefore, by the principle of mathematical induction $(*)$ holds for all $n \in \mathbb{Z}$.

(3) We substitute $y=0$ in $(*)$ and obtain $f(n x)=n^{2} f(x)$, for all $x \in \mathbb{R}$ and $n \in \mathbb{N}$. Hence for all $x, y \in \mathbb{R}$ and $n \in \mathbb{Z}$, it holds

$$
\begin{aligned}
\Delta(n x, y) &=f(n x+y)-f(n x)-f(y)=\\
&=n^{2} f(x)+n \Delta(x, y)+f(y)-n^{2} f(x)-f(y)=n \Delta(x, y) .
\end{aligned}
$$

(4) Steps (2) and (3) imply that

$$
f(q x+y)=q^{2} f(x)+q \Delta(x, y)+f(y)
$$

for every $x, y \in \mathbb{R}$ and $q \in \mathbb{Q}$. Indeed, if $q=\frac{m}{n}, m \in \mathbb{Z}, n \in \mathbb{N}$, then

$$
f\left(\frac{m}{n} x+y\right)=m^{2} f\left(\frac{x}{n}\right)+m \Delta\left(\frac{x}{n}, y\right)+f(y)=\frac{m^{2}}{n^{2}} f(x)+\frac{m}{n} \Delta(x, y)+f(y) .
$$

(5) To prove the statement of the problem it suffices to show that

$$
\frac{f(x+y)}{x+y}=\frac{f(x)}{x}+\frac{f(y)}{y}, \text { for all } x \neq 0, y \neq 0, x+y \neq 0 . \quad(* *)
$$

To this purpose take a sequence $\left\{q_{n}, n \geq 1\right\} \subset \mathbb{Q}$ such that $q_{n} \rightarrow-\frac{y}{x}$, as $n \rightarrow \infty$. We have

$$
f\left(q_{n} x+y\right)=q_{n}^{2} f(x)+q_{n}(f(x+y)-f(x)-f(y))+f(y) .
$$

Pass to the limit in the equality as $n \rightarrow \infty$. Since $f$ is continuous at zero, we get

$$
0=f(0)=\frac{y^{2}}{x^{2}} f(x)-\frac{y}{x}(f(x+y)-f(x)-f(y))+f(y),
$$

or equivalently,

$$
\frac{(x+y) y}{x^{2}} f(x)+\frac{x+y}{x} f(y)=\frac{y}{x} f(x+y),
$$

which implies $(* *)$.

Remark 1 The conditions of the problem imply that $f(x) \equiv a x^{2}, x \in \mathbb{R}$, for some $a \in \mathbb{R}$. Indeed, consider the function $\varphi$ from the statement of the problem. The function satisfies $\varphi(x+y) \equiv \varphi(x)+\varphi(y)$. Then either $\varphi(x)=a x$ for some fixed $a \in \mathbb{R}$, or the graph of $\varphi$ is dense in $\mathbb{R}^{2}$. If $\varphi(x)=a x$ then $f(x)=a x^{2}$. Show that the graph of $\varphi$ cannot be dense in $\mathbb{R}^{2}$. The function $f$ is continuous at zero, hence there exists $\delta>0$ such that $|f(x)| \leq 1$ for all $x \in(-\delta, \delta)$. Therefore, $|\varphi(x)| \leq \frac{1}{|x|}$ for all $x \in(0, \delta)$, and the graph of $\varphi$ is not dense in $\mathbb{R}^{2}$. 

\section{3}

1 Substitute $x=1$ and get $f(2)=1$. Because $f(1)=2$ and $f(2)=1$, by the intermediate value theorem for each $y \in[1,2]$ there exists $z \in[1,2]$ such that $f(z)=$ $y$. Substitute $x=z$ and get $f(y) \cdot y=2$, i.e. $f(y)=\frac{2}{y}, y \in[1,2]$. It is easy to check that this function satisfies the conditions of the problem.

Answer: $f(x)=\frac{2}{x}, x \in[1,2]$

2 If there exists a finite ring with required properties, then the squares of its elements are pairwise distinct (indeed, otherwise there exists an element which is not a square of any element of the ring). But by the condition of the problem there exists some $y \neq 0$ such that $y^{2}=0$, and $0^{2}=0$ (here 0 is the zero element of the ring), a contradiction.

Answer: no, there is no such a ring.

3 Let the angles of triangle $A B C$ be such that $\tan A+\tan C=2 \tan B$. Then

$$
\tan B=-\tan (A+C)=\frac{\tan A+\tan C}{\tan A \tan C-1}=\frac{2 \tan B}{\tan A \tan C-1},
$$

hence $\tan A \tan C=3$. Thus, both numbers $\tan A$ and $\tan C$ are positive, therefore, $\tan B$ is positive as well, and the triangle is acute. Since the function $y=\tan x$ is increasing on $\left[0, \frac{\pi}{2}\right)$, we have that $B$ is a middle angle of the triangle. The Law of Sines implies that sines of the angles form an arithmetic progression as well. The function $y=\sin x$ is increasing on $\left[0, \frac{\pi}{2}\right)$, hence $\sin B$ is the middle term of the progression, i.e., $\sin A+\sin C=2 \sin B$. Next, the function $y=\tan x$ is convex on $\left[0, \frac{\pi}{2}\right)$, and the function $y=\sin x$ is concave on $\left[0, \frac{\pi}{2}\right)$. Therefore,

$$
\sin B=\frac{\sin A+\sin C}{2} \leq \sin \frac{A+C}{2} \text { and } \tan B=\frac{\tan A+\tan C}{2} \geq \tan \frac{A+C}{2},
$$

whence $B \leq \frac{A+C}{2}$ and $B \geq \frac{A+C}{2}$. Thus, $B=\frac{A+C}{2}=\frac{\pi-B}{2}$, that is $B=\frac{\pi}{3}$. From $\tan A+\tan C=2 \sqrt{3}$ and $\tan A \tan C=3$ we derive that $\tan A=\tan C=\sqrt{3}$, and finally $A=B=C=\frac{\pi}{3}$.

Answer: each angle equals $\frac{\pi}{3}$, i.e., the triangle is equilateral.

4 Introduce a function

$$
f(x)=\sqrt{x_{1}+\sqrt{x_{2}+\sqrt{\ldots+\sqrt{x_{n}+x}}}}, \quad x \geq 0 .
$$

By Lagrange's mean value theorem there exists $\theta \in(0, c)$ for which $f(c)-f(0)=$ $c f^{\prime}(\theta)$. We have

$$
\begin{aligned}
&f^{\prime}(\theta)=\frac{1}{2 \sqrt{x_{1}+\sqrt{x_{2}+\sqrt{\ldots+\sqrt{x_{n}+\theta}}}}} \times \\
&\times \frac{1}{2 \sqrt{x_{2}+\sqrt{\ldots+\sqrt{x_{n}+\theta}}}} \cdot \ldots \cdot \frac{1}{2 \sqrt{x_{n}+\theta}}
\end{aligned}
$$

hence $f^{\prime}(\theta)<\frac{1}{2^{n} \sqrt{x_{1} \cdot \ldots \cdot x_{n}}}$ and $f(c)<f(0)+\frac{c}{2^{n} \sqrt{x_{1} \cdot \ldots \cdot x_{n}}}$.

5 Show that at least one of the matrices $A$ and $B$ is nonsingular. Indeed, if the matrix $A$ is singular, by elementary transformations of rows one can nullify its first row, hence there exists some nonsingular matrix $U$ such that the first row of the matrix $U A$ is zero. Similarly if the matrix $B$ is singular, then there exists some nonsingular matrix $V$ such that the first row of the matrix $B V$ is zero. Notice the following: if $X, Y$ is a solution of the equation $A X+Y B=U^{-1} C V^{-1}$, then $U A(X V)+(U Y) B V=C$, i.e., the equation $U A X+Y B V=C$ has a solution for every matrix $C$ as well. But for arbitrary matrices $X$ and $Y$, the first row of $U A X$ and the first column of $Y B V$ are zero. Hence if the entry in the first row and the first column of $C$ is nonzero, the equation $U A X+Y B V=C$ has no solution, a contradiction.

If the matrix $A$ is nonsingular, then the equation $A^{2013} X+Y B^{2013}=C$ has a solution $X=A^{-2013} C$ and $Y=0$, and if $B$ is nonsingular, then one can put $X=0$ and $Y=C B^{-2013}$, respectively.

6 It suffices to prove that the set $A=\{x \in \mathbb{R} \mid f(x)+g(x)<0\}$ is at most countable. Fix some $\delta>0$, and show that the set $A_{\delta}=\{x \in \mathbb{R} \mid f(x)+g(x)<-\delta\}$ is at most countable. Take any $x, y \in A_{\delta}$. Then $f(x)+g(x)<-\delta, f(y)+g(y)<-\delta$, and at least one of the inequalities $f(x)+g(y)>0$ or $f(y)+g(x)>0$ holds.

If $f(x)+g(y)>0$ then $f(x)-f(y)>-(f(y)+g(y))>\delta$, and if $f(y)+$ $g(x)>0$ then $f(x)-f(y)<f(x)+g(x)<-\delta$. Thus, for arbitrary distinct points $x, y \in A_{\delta}$, it holds $|f(x)-f(y)|>\delta$. Hence each of the sets $B_{k, \delta}=\{x \in \mathbb{R} \mid f(x) \in$ $[k \delta, k \delta+\delta)\}, k \in \mathbb{Z}$, contains no more than one point from $A_{\delta}$. Since $\cup_{k \in \mathbb{Z}} B_{k, \delta}=\mathbb{R}$, the set $A_{\delta}$ is at most countable for each $\delta>0$. Hence the set $A=\cup_{n \geq 1} A_{1 / n}$ is at most countable as well.

7 Fix a positive integer $N$, and estimate the number of positive integers that are less than $N$ and cannot be represented as a sum of two good numbers. Squares of positive integers have remainders 0 or 1 after division by 4 , hence no number of the form $4 j+3$ is a sum of two squares.

Let $n^{k}+m^{l}<N$, where $k \geq 3$ and $l \geq 2$. We have $n \leq N^{1 / 3}, m \leq N^{1 / 2}$, and it holds $k, l \leq \log _{2} N$ for $n, m \geq 2$ (one can assume that $k=3$ for $n=1$ and $l=2$ for $m=1)$. Thus, there exist at most $N^{5 / 6}\left(\log _{2} N\right)^{2}$ numbers which are less than $N$ and can be represented as a sum of two good non-square numbers. Consider the numbers of the form $4 j+3$ which are less than $N$. There are at least $\left[\frac{N}{4}\right]-N^{5 / 6}\left(\log _{2} N\right)^{2}$ numbers among them which cannot be represented as a sum of two good ones, and $\left[\frac{N}{4}\right]-N^{5 / 6}\left(\log _{2} N\right)^{2} \rightarrow \infty$, as $N \rightarrow \infty$.

Answer: it is infinite.

8 We say that a matrix $M=\left(\begin{array}{cc}a & b \\ c & d\end{array}\right)$ is of type $\mathrm{I}$ if it holds

$$
\quad|a|>|c|, \quad|b|>|d|, \quad a c \geq 0, \quad b d \geq 0,
$$

and of type II if it holds

$$
|a|<|c|, \quad|b|<|d|, \quad a c \leq 0, \quad b d \leq 0 .
$$

In particular, the matrix $A=\left(\begin{array}{ll}1 & 2 \\ 0 & 1\end{array}\right)$ is of type I, and the matrix $B=\left(\begin{array}{cc}1 & 0 \\ -2 & 1\end{array}\right)$ is of type II.

Consider the matrix $A M=\left(\begin{array}{cc}a+2 c & b+2 d \\ c & d\end{array}\right)$. If the matrix $M$ is of type I then

$$
\begin{array}{cc}
|a+2 c|=|a|+2|c|>|c|, & |b+2 d|=|b|+2|d|>|d|, \\
(a+2 c) c=a c+2 c^{2} \geq 0, & (b+2 d) d=b d+d^{2} \geq 0,
\end{array}
$$

and if the matrix $M$ is of type II then

$$
\begin{gathered}
|a+2 c|=2|c|-|a|>|c|, \quad|b+2 d|=2|d|-|b|>|d|, \\
(a+2 c) c=2 c^{2}-(-a c) \geq 0, \quad(b+2 d) d=d^{2}-(-b d) \geq 0
\end{gathered}
$$

i.e. in both cases $A M$ is of type I. In a similar way one can verify that if $M$ is of type either I or II, then the matrix $B M=\left(\begin{array}{cc}a & b \\ -2 a+c & -2 b+d\end{array}\right)$ is of type II. Hence every product $X_{1} X_{2} \ldots X_{n}$, where each multiplier $X_{i}$ equals either $A$ or $B$, is of type I in case $X_{1}=A$ and of type II in case $X_{1}=B$. Therefore, the product cannot be equal to the identity matrix, which is neither of type I nor of type II.

Answer: no, it can not. 9 For all $x \in \mathbb{R}$, it holds

$$
\begin{aligned}
&\sum_{n=0}^{\infty} \frac{\cos \left(\frac{\pi n}{2}-x\right)}{n !} x^{n}=\cos x \sum_{n=0}^{\infty} \frac{\cos \frac{\pi n}{2}}{n !} x^{n}+\sin x \sum_{n=0}^{\infty} \frac{\sin \frac{\pi n}{2}}{n !} x^{n}= \\
&=\cos x \sum_{k=0}^{\infty} \frac{(-1)^{k}}{(2 k) !} x^{2 k}+\sin x \sum_{k=0}^{\infty} \frac{(-1)^{k}}{(2 k+1) !} x^{2 k+1}=\cos ^{2} x+\sin ^{2} x=1 .
\end{aligned}
$$

In particular for $x=\frac{1}{2}$ we obtain the sum of the series in question.

Answer: 1.

10 Suppose that there exists a finite ring with required properties. Then the squares of its elements are pairwise distinct (otherwise there exists an element which cannot be represented as the square of an element of the ring). Fix an arbitrary nonzero element $x$ of the ring. Consider a sequence $x, x^{2}, x^{4}, x^{8}, \ldots$ Since the ring is finite, some terms of the sequence are equal. Let $x^{2^{k+n}}=x^{2^{k}}$. Because the squares of all the elements of the ring are distinct, we get $x^{2^{n}}=x$. Denote $a=x^{2^{n}-1} \neq 0$. Then

$$
a^{2}=\left(x^{2^{n}-1}\right)^{2}=x^{2^{n}} \cdot x^{2^{n}-2}=x \cdot x^{2^{n}-2}=a,
$$

but by the condition of the problem there exists an element $b \neq a$ such that $b^{2}=a$ as well, a contradiction.s

Answer: no, there is no such a ring.

12 It was shown in the solution of Problem 5 that at least one of the matrices $A$ and $B$ is nonsingular. Hence one of the numbers $k_{0}(A)$ and $k_{0}(B)$ equals 0 , and the other is at most $n$.

13 Let

$$
C_{x y}=\underset{x \in \mathbb{R}}{\operatorname{ess} \sup }(\underset{y \in \mathbb{R}}{\operatorname{ess} \sup } f(x, y)), \quad C_{y x}=\underset{y \in \mathbb{R}}{\operatorname{ess} \sup }(\underset{x \in \mathbb{R}}{\operatorname{ess} \sup } f(x, y)) .
$$

Denote by $\lambda_{1}$ the Lebesgue measure on $\mathbb{R}$ and by $\lambda_{2}$ the Lebesgue measure on $\mathbb{R}^{2}$. We claim that $C_{x y}=C_{y x}=C$, where $C=$ ess sup $f(x, y)$ is essential supremum with respect to the measure $\lambda_{2}$.

$$
(x, y) \in \mathbb{R}^{2}
$$

For a fixed $a \in \mathbb{R}$, introduce sets

$$
A_{a}=\{(x, y) \mid f(x, y)>a\}, \quad A_{a, x}=\{y \mid f(x, y)>a\}, x \in \mathbb{R} .
$$

Then

$$
\lambda_{2}\left(A_{a}\right)=\int_{\mathbb{R}} \lambda_{1}\left(A_{a, x}\right) d \lambda_{1}(x) .
$$

If $C<\infty$ then for $a=C$ it holds $\lambda_{2}\left(A_{C}\right)=0$, whence $\lambda_{1}\left(A_{C, x}\right)=0$ for almost all $x \in \mathbb{R}$. Then ess sup $f(x, y) \leq C$ for almost all $x \in \mathbb{R}$, hence $C_{x y} \leq C$. It is evident that for $C=\infty$, the inequality $C_{x y} \leq C$ holds as well.

For every $a<C$, we have $\lambda_{2}\left(A_{C}\right)>0$. Therefore, there exists a set $B \subset \mathbb{R}$ such that $\lambda_{1}(B)>0$ and $\lambda_{1}\left(A_{a, x}\right)>0, x \in B$. Then ess $\sup f(x, y)>a, x \in B$, whence $C_{x y}>a$. Because $a<C$ is arbitrary, it holds $C_{x y} \geq C$.

The inequalities $C_{x y} \leq C$ and $C_{x y} \geq C$ imply that $C_{x y}=C$. Similarly $C_{y x}=C$, hence $C_{x y}=C_{y x}$.

Answer: yes, it is always true.

![](https://cdn.mathpix.com/cropped/2022_10_28_1f3b279542cbcf365e2cg-120.jpg?height=54&width=1255&top_left_y=819&top_left_x=435)
if $\varphi=p^{n} \cdot \frac{q}{r}$, where $q$ and $r$ are polynomials which are not divisible by $p$. Then

$$
\begin{gathered}
\varphi^{\prime}=n p^{n-1} p^{\prime} \cdot \frac{q}{r}+p^{n} \cdot \frac{q^{\prime} r-q r^{\prime}}{r^{2}}=p^{n-1} \cdot \frac{n p^{\prime} q r+p\left(q^{\prime} r-q r^{\prime}\right)}{r^{2}}, \\
\frac{\varphi^{\prime}}{\varphi}=\frac{n p^{\prime} q r+p\left(q^{\prime} r-q r^{\prime}\right)}{p q r} .
\end{gathered}
$$

Hence for $n \neq 0$, it holds $\operatorname{ord}_{p}\left(\varphi^{\prime}\right)=n-1$ and $\operatorname{ord}_{p}\left(\frac{\varphi^{\prime}}{\varphi}\right)=-1$, and for $n=0$, we have $^{2} \operatorname{ord}_{p}\left(\varphi^{\prime}\right) \geq 0$.

Let $\varphi$ and $\psi$ be nonconstant rational functions, for which $\psi^{\prime}=\frac{\varphi^{\prime}}{\varphi}$. Consider an irreducible polynomial $p$, which is a divisor of numerator or denominator of the function $\varphi$. Then $\operatorname{ord}_{p}(\varphi) \neq 0$, hence $\operatorname{ord}_{p}\left(\psi^{\prime}\right)=\operatorname{ord}_{p}\left(\frac{\varphi^{\prime}}{\varphi}\right)=-1$. But if $\operatorname{ord}_{p}(\psi)=$ $k \neq 0$, then $\operatorname{ord}_{p}\left(\psi^{\prime}\right)=k-1 \neq-1$, and if $\operatorname{ord}_{p}(\psi)=0$, then $\operatorname{ord}_{p}(\psi) \geq 0$. In both cases we get a contradiction with $\operatorname{ord}_{p}\left(\psi^{\prime}\right)=-1$.

Answer: no, they do not exist.

15 For a finite measure $\mu$ on Borel sigma-algebra $\mathscr{B}\left(\mathbb{R}^{2}\right)$, denote by supp $\mu$ the support of $\mu$, that is the minimal closed set $F$ for which $\mu(F)=\mu\left(\mathbb{R}^{2}\right)$.

Rewrite the statement of the problem as follows: if supp $P$ does not belong to any straight line, then there exists a bounded Borel measurable set $A$ such that the support of the restricted measure $\mathrm{P}_{A}(B)=\mathrm{P}(A \cap B), B \in \mathscr{B}\left(\mathbb{R}^{2}\right)$, does not belong to any straight line as well.

By the condition of the problem the support of $\mathrm{P}$ contains at least three noncollinear points $u, v, w$. It is not difficult to verify that supp $\mathrm{P}_{A}=\operatorname{supp} \mathrm{P} \cap A$ for an arbitrary closed set $A$. Let $A$ be a closed disc with center in the origin and radius $R=\max \{\|u\|,\|v\|,\|w\|\}$. Then the support of the measure $\mathrm{P}_{A}$ contains all three points $u, v, w$ hence it does not belong to any straight line.

Answer: yes, it is.

16 Show that partial sums of the series $\sum_{n=1}^{\infty}(-1)^{S_{n}} a_{n}$ form a Cauchy sequence in $L_{2}(\Omega, \mathscr{F}, \mathrm{P})$. This will imply that the series converges in $L_{2}(\Omega, \mathscr{F}, \mathrm{P})$, hence in probability as well. Denote $T_{k, k+m}=\sum_{n=k}^{k+m}(-1)^{S_{n}} a_{n}$. It holds

$$
\mathrm{E} T_{k, k+m}^{2}=\sum_{i, j=k}^{k+m} a_{i} a_{j} \mathrm{E}(-1)^{S_{i}+S_{j}} .
$$

Next, $\mathrm{E}(-1)^{S_{i}+S_{j}}=\mathrm{E}(-1)^{S_{i}-S_{j}}=\mathrm{E}(-1)^{S_{i-j \mid}}$ (here $S_{0}=0$ ). The sum $S_{n}$ has the binomial distribution with parameters $n, p=\frac{2}{3}$, and $q=\frac{1}{3}$. Hence

$$
\begin{aligned}
& \mathrm{E}(-1)^{S_{n}}=\sum_{k=0}^{n}(-1)^{k}\left(\begin{array}{l}n \\k\end{array}\right) p^{k} q^{n-k}=(q-p)^{n}=\left(-\frac{1}{3}\right)^{n}, \\
& \mathrm{E} T_{k, k+m}^{2}=\sum_{i, j=k}^{k+m} a_{i} a_{j}\left(-\frac{1}{3}\right)^{|i-j|}=\sum_{i=k}^{k+m} a_{i}^{2}+2 \sum_{k \leq i<j \leq k+m} a_{i} a_{j}\left(-\frac{1}{3}\right)^{j-i} \leq \\
& \leq \sum_{i=k}^{k+m} a_{i}^{2}+2 \sum_{s=1}^{m} \sum_{k \leq i \leq k+m-s}\left|a_{i} a_{i+s}\right| \frac{1}{3^{s}} \text {. }
\end{aligned}
$$

By the Cauchy-Schwarz inequality

$$
\sum_{k \leq i \leq k+m-s}\left|a_{i} a_{i+s}\right| \leq\left(\sum_{k \leq i \leq k+m-s} a_{i}^{2}\right)^{1 / 2}\left(\sum_{k \leq i \leq k+m-s} a_{i+s}^{2}\right)^{1 / 2} \leq \sum_{i=k}^{\infty} a_{i}^{2},
$$

therefore,

$$
E T_{k, k+m}^{2} \leq \sum_{i=k}^{\infty} a_{i}^{2}\left(1+2 \sum_{s=1}^{\infty} \frac{1}{3^{s}}\right)=2 \sum_{i=k}^{\infty} a_{i}^{2} \rightarrow 0 \text {, as } k \rightarrow \infty .
$$

Thus, the partial sums of the series $\sum_{n=1}^{\infty}(-1)^{S_{n}} a_{n}$ indeed form a Cauchy sequence in $L_{2}(\Omega, \mathscr{F}, \mathrm{P})$, which proves the claim. 

\section{4}

1 For $\pi / 2 \leq x \leq \pi$, it holds $\cos x \leq 0<e^{-x^{2} / 2}$, hence it remains to consider $0<x<\pi / 2$. After taking the logarithm on both sides the inequality turns to $\ln \cos x<-x^{2} / 2$, or equivalently $x^{2} / 2+\ln \cos x<0,0<x<\pi / 2$. The function $f(x)=x^{2} / 2+\ln \cos x$ is decreasing on $[0, \pi / 2)$, because $f^{\prime}(x)=x-\tan x<0$, $0<x<\pi / 2$. Therefore, $f(x)<f(0)=0,0<x<\pi / 2$.

2 The value of initial polynomial $x^{k}(x-2)^{2 n}$ at $x=1$ equals 1 . This property is preserved after all the allowed steps. Thus, it is impossible to get a polynomial of the form $x^{l}(x-2)^{2 m+1}$, which equals $-1$ at $x=1$.

Answer: no, it is impossible.

3 For arbitrary points $P, Q, R, S$ in the space, define $f(P, Q, R, S)=(\overrightarrow{S P}, \overrightarrow{S Q}$, $\overrightarrow{S R})$ (here $\left(\overrightarrow{a_{1}}, \overrightarrow{a_{2}}, \overrightarrow{a_{3}}\right)=\overrightarrow{a_{1}} \cdot\left(\overrightarrow{a_{2}} \times \overrightarrow{a_{3}}\right)$ is a scalar triple product). Then

$$
V_{P Q R S}=\frac{1}{6}|f(P, Q, R, S)|,
$$

and it suffices to prove that $f\left(A_{1}, B_{1}, M_{C}, M_{D}\right)=f\left(A_{2}, B_{2}, M_{C}, M_{D}\right)$. Notice that

$$
\begin{aligned}
f &\left(A_{1}, B_{1}, M_{C}, M_{D}\right)+f\left(A_{2}, B_{1}, M_{C}, M_{D}\right)=\\
&=\left(\overrightarrow{M_{D} A_{1}}+\overrightarrow{M_{D} A_{2}}, \overrightarrow{M_{D} B_{1}}, \overrightarrow{M_{D} M_{C}}\right)=\left(2 \overrightarrow{M_{D} M_{A}}, \overrightarrow{M_{D} M_{B}}+\overrightarrow{M_{B} B_{1}}, \overrightarrow{M_{D} M_{C}}\right)=\\
\quad &=2\left(\overrightarrow{M_{D} M_{A}}, \overrightarrow{M_{D} M_{B}}, \overrightarrow{M_{D} M_{C}}\right)=2 f\left(M_{A}, M_{B}, M_{C}, M_{D}\right) .
\end{aligned}
$$

We used the equality $\left(\overrightarrow{M_{D} M_{A}}, \overrightarrow{M_{B} B_{1}}, \overrightarrow{M_{D} M_{C}}\right)=0$, which is true because the vector $\overrightarrow{M_{B} B_{1}}$ lies in the plane $A C D$ that is parallel to the plane $M_{A} M_{C} M_{D}$, hence it is a linear combination of vectors $\overrightarrow{M_{D} M_{A}}$ and $\overrightarrow{M_{D} M_{C}}$. Similarly

$$
f\left(A_{2}, B_{2}, M_{C}, M_{D}\right)+f\left(A_{2}, B_{1}, M_{C}, M_{D}\right)=2 f\left(M_{A}, M_{B}, M_{C}, M_{D}\right) .
$$

Subtract the obtained equalities to get the required equality.

4 Consider a polynomial $P(x)=x^{4}+\beta x^{3}-\alpha x+1$. Choose $\alpha>0$ and $\beta>0$ which have the following properties:

1) for all $x>0$, it holds $x^{4}+1-\alpha x \geq 0$, and there exists a number $x_{1}>0$ such that $x_{1}^{4}+1-\alpha x_{1}=0$, and

2) for all $x>0$, it holds $x^{4}+1+\beta x^{3} \geq 0$, and there exists a number $x_{2}<0$ such that $x_{2}^{4}+1+\beta x_{2}^{3}=0$.

It suffices to put $\alpha=\min _{x>0} \frac{x^{4}+1}{x}$ and $\beta=-\max _{x<0} \frac{x^{4}+1}{x^{3}}$. It is easy to check that the minimal and the maximal values exist and are attained at unique points $x_{1}>0$ and $x_{2}<0$, respectively.

The polynomial $P(x)$ has no real roots, because

1) if $x \geq 0$ then $P(x)=\left(x^{4}+1-\alpha x\right)+\beta x^{3}>0$, and

2) if $x<0$ then $P(x)=\left(x^{4}+1+\beta x^{3}\right)-\alpha x>0$.

It is evident that each of the polynomials $x^{4}+\beta x^{3}-\alpha x$ and $\beta x^{3}-\alpha x+1$ has a real root. Finally, each of the polynomials $x^{4}-\alpha x+1$ and $x^{4}+\beta x^{3}+1$ has a real root due to the choice of $\alpha$ and $\beta$.

Answer: yes, there exists.

5 Consider $t_{1}, t_{2} \in\left(0, \frac{\pi}{2}\right)$ such that $x_{1}=\cos ^{2} t_{1}$ and $x_{2}=\cos ^{2} t_{2}$. Then

$$
\begin{gathered}
\sqrt{x_{1}\left(1-x_{2}\right)}+\sqrt{x_{2}\left(1-x_{1}\right)}=\cos t_{1} \sin t_{2}+\cos t_{2} \sin t_{1}=\sin \left(t_{1}+t_{2}\right), \\
x_{1}-x_{2}=\frac{1}{2}\left(\cos 2 t_{1}-\cos 2 t_{2}\right)=\sin \left(t_{1}+t_{2}\right) \sin \left(t_{1}-t_{2}\right), \\
\max \left(\left|2 x_{1}-1\right|,\left|2 x_{2}-1\right|\right)=\max \left(\left|\cos 2 t_{1}\right|,\left|\cos 2 t_{2}\right|\right),
\end{gathered}
$$

hence the inequality in question turns into

$$
\sin \left(t_{1}+t_{2}\right) \max \left(\left|\cos 2 t_{1}\right|,\left|\cos 2 t_{2}\right|\right) \geq\left|\sin \left(t_{1}+t_{2}\right) \sin \left(t_{1}-t_{2}\right)\right| .
$$

Since $t_{1}+t_{2} \in(0, \pi)$, we have $\sin \left(t_{1}+t_{2}\right)>0$. Thus the inequality is equivalent to

$$
\max \left(\left|\cos 2 t_{1}\right|,\left|\cos 2 t_{2}\right|\right) \geq\left|\sin \left(t_{1}-t_{2}\right)\right| .
$$

Because $2 t_{1}, 2 t_{2} \in(0, \pi)$ and $\left(2 t_{1}, 2 t_{2}\right) \neq\left(\frac{\pi}{2}, \frac{\pi}{2}\right)$, there exists $s \in\left(0, \frac{\pi}{4}\right)$ such that

$$
\max \left(\left|\cos 2 t_{1}\right|,\left|\cos 2 t_{2}\right|\right)=\cos 2 s .
$$

Then $t_{1}, t_{2} \in\left[s, \frac{\pi}{2}-s\right],\left|t_{1}-t_{2}\right| \leq \frac{\pi}{2}-2 s$ and $\left|\sin \left(t_{1}-t_{2}\right)\right| \leq \sin \left(\frac{\pi}{2}-2 s\right)=$ $\cos 2 s$, which finishes the proof. The equality holds if and only if either $t_{1}=s$ and $t_{2}=\frac{\pi}{2}-s$ or $t_{1}=\frac{\pi}{2}-s$ and $t_{2}=s$ for some $s \in\left(0, \frac{\pi}{4}\right)$, i.e. $x_{1} \in\left(0, \frac{1}{2}\right) \cup\left(\frac{1}{2}, 1\right)$ and $x_{2}=1-x_{1}$.

6 Let $P$ be $n \times n$ matrix. The set of vectors $H=\operatorname{Im} P=\left\{P x, x \in \mathbb{R}^{n}\right\}$ is a subspace of $\mathbb{R}^{n}$, and $H \neq\{0\}$, because $P$ is not zero matrix. If $y=P x \in H$ then $P y=P^{2} x=$ $P x=y$. Hence $H \neq \mathbb{R}^{n}$ because $P$ is not the identity matrix. Show that there exists a matrix $Q$ such that $Q x \in H$ for all $x \in \mathbb{R}^{n}, Q y=y$ for all $y \in H$ and $Q \neq P$.

Since $P$ is neither zero matrix nor the identity one, there exist vectors $a, b \in \mathbb{R}^{n}$ such that $P a \neq 0$ and $(I-P)^{\mathrm{T}} b \neq 0$. Then the matrix $Q=P+P a b^{\mathrm{T}}(I-P)$ has above-mentioned properties. Indeed, for all $x \in \mathbb{R}^{n}$ it holds $Q x=P\left(x+a b^{\mathrm{T}}(I-\right.$ $P) x) \in H$, and for all $y=P x \in H$ we have $Q y=P y+P a b^{\mathrm{T}}(I-P) P x=y+P a b^{\mathrm{T}}$ $\left(P-P^{2}\right) x=y$. Finally $Q \neq P$, because $Q-P$ is a product of nonzero column vector $P a$ by nonzero row vector $b^{\mathrm{T}}(I-P)=\left((I-P)^{\mathrm{T}} b\right)^{\mathrm{T}}$.

By the construction for all $x \in \mathbb{R}^{n}$ it holds $P x \in H$ and $Q x \in H$, hence $Q(Q x)=$ $Q x, Q(P x)=P x$, and $P(Q x)=Q x$. Thus, $Q^{2}=Q, Q P=P$, and $P Q=Q$, whence $P Q=(Q P) Q$ but $Q P=P \neq Q=P Q$.

Answer: yes, always.

7 For $0<\delta<\frac{1}{2}$ put $\alpha(\delta)=\max _{x \in[0,1]} f(x)-\max _{x \in A(\delta)} f(x)$, where

$$
A(\delta)=\left\{x \in[0,1]:\left|x-x_{0}\right| \geq \delta\right\}, \quad B(\delta)=\left\{x \in[0,1]:\left|x-x_{0}\right| \leq \delta\right\} .
$$

The condition of the problem implies that $\alpha(\delta)>0$. Denote $\|g\|=\max _{x \in[0,1]}|g(x)|$. If $\|g\|=0$ then $g(x)=0, x \in[0,1]$, and the statement of the problem is evident. We assume further that $\|g\|>0$.

Lemma 1 For $0<t<\frac{\alpha(\delta)}{2\|g\|}$, the maximum of the function $f(\cdot)+t g(\cdot)$ is attained on the segment $\left[x_{0}-\delta, x_{0}+\delta\right]$.

Proof We have

$$
\begin{aligned}
&\max _{x \in B(\delta)}(f(x)+t g(x))-\max _{x \in A(\delta)}(f(x)+t g(x)) \geq \\
&\geq \max _{x \in B(\delta)} f(x)-t\|g\|-\max _{x \in A(\delta)} f(x)-t\|g\|=\alpha(\delta)-2 t\|g\|>0 .
\end{aligned}
$$

Fix an arbitrary $\varepsilon>0$ and choose $\delta \in\left(0, \frac{1}{2}\right)$ such that $\left|g(x)-g\left(x_{0}\right)\right| \leq \varepsilon$ for all $x \in B(\delta)$. The lemma implies that for small enough $t>0$, it holds

$$
\begin{aligned}
\frac{\varphi(t)-\varphi(0)}{t} &=\frac{1}{t}\left(\max _{x \in[0,1]}(f(x)+t g(x))-\max _{x \in[0,1]} f(x)\right)=\\
&=\frac{1}{t}\left(\max _{x \in B(\delta)}(f(x)+t g(x))-f\left(x_{0}\right)\right) \leq \\
& \leq \frac{1}{t}\left(f\left(x_{0}\right)+t \max _{x \in B(\delta)} g(x)-f\left(x_{0}\right)\right)=\max _{x \in B(\delta)} g(x) \leq g\left(x_{0}\right)+\varepsilon .
\end{aligned}
$$

Thus, $\limsup _{t \rightarrow 0+} \frac{\varphi(t)-\varphi(0)}{t} \leq g\left(x_{0}\right)$.

On the other hand,

$$
\begin{aligned}
\frac{\varphi(t)-\varphi(0)}{t}=\frac{1}{t}\left(\max _{x \in[0,1]}(f(x)+t g(x))-\right.&\left.\max _{x \in[0,1]} f(x)\right) \\
& \geq \frac{1}{t}\left(f\left(x_{0}\right)+t g\left(x_{0}\right)-f\left(x_{0}\right)\right)=g\left(x_{0}\right) .
\end{aligned}
$$

Hence $\liminf _{t \rightarrow 0+} \frac{\varphi(t)-\varphi(0)}{t} \geq g\left(x_{0}\right)$. Therefore, $\varphi_{+}^{\prime}(0)=\lim _{t \rightarrow 0+} \frac{\varphi(t)-\varphi(0)}{t}=g\left(x_{0}\right)$. Notice that $\varphi(-t)=\max _{x \in[0,1]}(f(x)+t \cdot(-g(x)))$. Hence $\varphi_{-}^{\prime}(0)=-\left(-g\left(x_{0}\right)\right)=$ $g\left(x_{0}\right)$. Because $\varphi_{+}^{\prime}(0)=\varphi_{-}^{\prime}(0)=g\left(x_{0}\right)$, there exists $\varphi^{\prime}(0)=g\left(x_{0}\right)$.

8 By substitution $t=\pi-x$ we get that the integral equals $\int_{0}^{\pi}(\sin t)^{-\cos t} d t$. Show that $(\sin t)^{-\cos t} \sim \frac{1}{t}$, as $t \rightarrow 0+$. By the limit comparison test this will imply that the integral diverges to $+\infty$. It holds

$$
\frac{(\sin t)^{\cos t}}{t} \sim(\sin t)^{\cos t-1}=e^{(\cos t-1) \ln \sin t}, \text { as } t \rightarrow 0+.
$$

Since $1-\cos t=2 \sin ^{2} \frac{t}{2} \sim \frac{t^{2}}{2} \sim \frac{1}{2} \sin ^{2} t$, as $t \rightarrow 0+$, it holds that

$$
(\cos t-1) \ln \sin t \sim-\frac{1}{2} \sin ^{2} t \cdot \ln \sin t=\left|\begin{array}{l}
u=\sin t \\
u \rightarrow 0+
\end{array}\right|=-\frac{1}{2} u^{2} \cdot \ln u \rightarrow 0, \text { as } t \rightarrow 0+\text {, }
$$

hence $e^{(\cos t-1) \ln \sin t} \rightarrow e^{0}=1$, as $t \rightarrow 0+$. Thus, $(\sin t)^{-\cos t} \sim \frac{1}{t}$, as $t \rightarrow 0+$, which finishes the proof.

Answer: $+\infty$.

9 The set $\mathscr{P}_{2014}$ of polynomials of degree at most 2014 is a finite-dimensional subspace of $L_{2014}([0,2014], \lambda)$, where $\lambda$ is Lebesgue measure on $\mathbb{R}$. In particular this set is closed. Hence the condition of the problem implies that the polynomial $P$ also has degree at most 2014. Consider two norms on $\mathscr{P}_{2014}$ : the $L_{2014}$-norm $\|f\|_{2014}$ and the norm $\|f\|=\|f(0)\|+\left\|f^{\prime}\right\|_{2014}$. Every two norms on a finite-dimensional space are equivalent, hence the convergence of $P_{n}$ to $P$ in the first norm implies the convergence of $P_{n}$ to $P$ in the second norm, whence $\left\|P_{n}^{\prime}-P^{\prime}\right\|_{2014} \rightarrow 0$, as $n \rightarrow \infty$. Answer: yes, it is.

10 If $f_{n}(x, y) \stackrel{\lambda_{2}}{\rightarrow} 0$, as $n \rightarrow \infty$, then there exists $\varepsilon>0$ such that $\lambda_{2}$-measure of the $\operatorname{set} A_{n}=\left\{(x, y) \in[0,1]^{2}:\left|f_{n}(x, y)\right| \geq \varepsilon\right\}$ is not less than $\varepsilon$ for infinitely many indices $n$

By the property of product measure there exists $y_{n} \in[0,1]$ such that for the cross section $A_{n, y_{n}}=\left\{x \in[0,1]:\left(x, y_{n}\right) \in A_{n}\right\}$, it holds $\lambda_{1}\left(A_{n, y_{n}}\right) \geq \lambda_{2}\left(A_{n}\right)$. Consider $g_{n}(x) \equiv y_{n}, n \geq 1$. Then for all $x \in A_{n, y_{n}}$, it holds $\left|f_{n}\left(x, g_{n}(x)\right)\right|=\left|f_{n}\left(x, y_{n}\right)\right| \geq \varepsilon$, and $\lambda_{1}\left(A_{n, y_{n}}\right) \geq \lambda_{2}\left(A_{n}\right) \geq \varepsilon$ for infinitely many $n$. Thus, $f_{n}\left(x, g_{n}(x)\right) \stackrel{\lambda_{1}}{\rightarrow} 0$, as $n \rightarrow \infty$, a contradiction.

13 For all $k \geq 4$, it holds

$$
\begin{array}{r}
x_{k+1}=x_{k}-x_{k-1}+x_{k-2}+\varepsilon_{k+1}+\frac{1}{2} \varepsilon_{k}=\left(x_{k-1}-x_{k-2}+x_{k-3}+\varepsilon_{k}+\frac{1}{2} \varepsilon_{k-1}\right)- \\
-x_{k-1}+x_{k-2}+\varepsilon_{k+1}+\frac{1}{2} \varepsilon_{k}=x_{k-3}+\varepsilon_{k+1}+\frac{3}{2} \varepsilon_{k}+\frac{1}{2} \varepsilon_{k-1} .
\end{array}
$$

Consider the sequences $y_{k}=x_{4 k+1}, k \geq 0$, and $\xi_{k}=\varepsilon_{4 k+1}+\frac{3}{2} \varepsilon_{4 k}+\frac{1}{2} \varepsilon_{4 k-1}, k \geq$ 1. We have $y_{k}=y_{k-1}+\xi_{k}, k \geq 1$. Assume that $\mathrm{E} x_{k}^{2} \leq C$ for all $k \geq 1$. Then it is evident that $\mathrm{E} y_{k}^{2} \leq C, k \geq 0$. Since $\left\{\xi_{k}, k \geq 1\right\}$ is a sequence of independent random variables such that $\mathbb{E} \xi_{k}=0$ and $\operatorname{Var} \xi_{k}=\frac{7}{2}, k \geq 1$, for random variables $y_{k}-y_{0}=\sum_{i=1}^{k} \xi_{i}, k \geq 1$, it holds $\mathrm{E}\left(y_{k}-y_{0}\right)=0$, and

$$
\mathrm{E}\left(y_{k}-y_{0}\right)^{2}=\operatorname{Var}\left(y_{k}-y_{0}\right)=\sum_{i=1}^{k} \operatorname{Var} \xi_{i}=\frac{7}{2} k \rightarrow \infty, \text { as } k \rightarrow \infty
$$

On the other hand, $\mathrm{E}\left(y_{k}-y_{0}\right)^{2} \leq 2 \mathrm{E} y_{0}^{2}+2 \mathrm{E} y_{k}^{2} \leq 4 C, k \geq 1$, a contradiction. Answer: no, there is no such sequences.

14 Let $P$ be $n \times n$ matrix and $I$ be the identity $n \times n$ matrix. Then $I-P$ is neither zero matrix nor the identity one, and $(I-P)^{2}=I-2 P+P^{2}=I-P$. We apply the reasonings from solution of Problem 6 to the matrix $I-P$ instead of $P$, and get that there exists a matrix $Q$ such that $Q^{2}=Q, Q(I-P)=I-P,(I-P) Q=Q$, and $Q \neq I-P$. The matrix $Q$ has required properties because it satisfies the relations $Q P=Q+P-I$ and $P Q=0$, whence $Q(P Q)=0=P Q$ and $(P Q) P=0 \neq Q P$. Answer: yes, always. 

\section{5}

1 Let the endpoints of the segment have coordinates $\left(a, \frac{1}{a}\right)$ and $\left(b, \frac{8}{b}\right)$. Then the middle point has coordinates $\left(\frac{a+b}{2}, \frac{1}{2 a}+\frac{4}{b}\right)$. If this points lies on the hyperbola $y=\frac{1}{x}$ then $\frac{a+b}{2}\left(\frac{1}{2 a}+\frac{4}{b}\right)=1$, whence $\frac{1}{4}+\frac{b}{4 a}+2+\frac{2 a}{b}=1, \frac{b}{4 a}+\frac{2 a}{b}=-\frac{5}{4},\left(\frac{b}{a}\right)^{2}+5 \frac{b}{a}+$ $8=0$. But the quadratic equation $x^{2}+5 x+8=0$ has no real roots, a contradiction. Answer: no, it is impossible.

2 Introduce a polynomial $P(t)=\left(1+t x_{1}\right)\left(1+t x_{2}\right) \ldots\left(1+t x_{n}\right)$. It has a degree at most $n$. The system implies that $P(k)=k+1,1 \leq k \leq n$, and moreover $P(0)=1$. Hence the polynomial $P(t)-t-1$ of degree at most $n$ has $n+1$ roots, therefore, it is identically zero. Thus, $\left(1+t x_{1}\right)\left(1+t x_{2}\right) \ldots\left(1+t x_{n}\right) \equiv 1+t$. Hence one of the numbers $x_{1}, \ldots, x_{n}$ equals 1 and the rest of them equal 0 .

Answer: $(1,0, \ldots, 0)$ and all permutations of these numbers.

3 Assume that there exists a continuous and non-monotone function $f$, which satisfy $(f(z)-f(y))(f(y)-f(x)) \geq 0$ for all $0 \leq x<y<z \leq 1$ such that $z-y=y-$ $x$. In particular for each $u, v \in[0,1]$ if $f(u) \leq f(v)$ then $f(u) \leq f\left(\frac{u+v}{2}\right) \leq f(v)$. Without loss of generality we may assume that $f(0) \leq f(1)$ (otherwise consider the function $-f$ instead of $f$ ). Then we get subsequently

$$
f(0) \leq f\left(\frac{1}{2}\right) \leq f(1), f(0) \leq f\left(\frac{1}{4}\right) \leq f\left(\frac{1}{2}\right) \leq f\left(\frac{3}{4}\right) \leq f(1), \ldots
$$

It is not difficult to prove by induction on $n$ that

$$
f(0) \leq f\left(\frac{1}{2^{n}}\right) \leq f\left(\frac{2}{2^{n}}\right) \leq \ldots \leq f\left(\frac{2^{n}-1}{2^{n}}\right) \leq f(1) \text { for all } n \geq 0 .
$$

Show that the function $f$ is nondecreasing on $[0,1]$. For arbitrary numbers $0 \leq$ $a<b \leq 1$, put $a_{n}=\frac{\left[2^{n} a\right]}{2^{n}}, b_{n}=\frac{\left[2^{n} b\right]}{2^{n}}, n \geq 0$. Since $\left[2^{n} a\right] \leq\left[2^{n} b\right]$, it holds $f\left(a_{n}\right) \leq$ $f\left(b_{n}\right)$. Because $a_{n} \rightarrow a$ and $b_{n} \rightarrow b$, as $n \rightarrow \infty$, by continuity of $f$ we get 

$$
f(a)=\lim _{n \rightarrow \infty} f\left(a_{n}\right) \leq \lim _{n \rightarrow \infty} f\left(b_{n}\right)=f(b) .
$$

Thus, $f$ is monotone on $[0,1]$, and we come to a contradiction.

4 A bijection on the finite set $X$ is a permutation of its elements. We decompose a permutation $f$ into cycles. Consider a set $A \subset X$ such that $f(A)=A$. Then for each element $a \in A$ the set $A$ contains the elements $f(a), f(f(a))$, etc. Thus, the cycle which contains $a$ is a subset of $A$. Hence $A$ is a union of some cycles of the permutation $f$. On the other hand, if $A$ is a union of some cycles of the permutation $f$ then it is clear that $f(A)=A$. Therefore, if the permutation $f$ consists of $k$ cycles, then there exist exactly $2^{k}$ sets $A$ for which $f(A)=A$.

5 If numbers $n$ and $k$ are odd, then Bilbo might have chosen either $x=1$ and $y=-1$ or $x=2$ and $y=-2$, and in both cases he tells Gollum the numbers 0 and 0 . Similarly if numbers $n$ and $k$ are even, then Bilbo might have chosen either $x=1$ and $y=1$ or $x=1$ and $y=-1$, and in both cases he tells Gollum the numbers 2 and 2 . Therefore, if the numbers $n$ and $k$ have the same parity, then Gollum cannot determine $x y$.

From now on we assume that $n$ is odd and $k$ is even. Let Bilbo had chosen the numbers $x \leq y$ and told Gollum the numbers $a=x^{n}+y^{n}$ and $b=x^{k}+y^{k}$. Then $x^{n}=\frac{a}{2}-t, y^{n}=\frac{a}{2}+t$ for some $t \geq 0$. Hence

$$
x=\sqrt[n]{\frac{a}{2}-t}, \quad y=\sqrt[n]{\frac{a}{2}+t}, \quad \text { and } x^{k}+y^{k}=\sqrt[n]{\left(\frac{a}{2}-t\right)^{k}}+\sqrt[n]{\left(\frac{a}{2}+t\right)^{k}} .
$$

Since $x y=\sqrt[n]{\frac{a^{2}}{4}-t^{2}}$ is a decreasing function of $t$ for $t \geq 0$, Gollum can determine $x y$ if and only if there is no $b$ such that the equality

$$
\sqrt[n]{\left(\frac{a}{2}-t\right)^{k}}+\sqrt[n]{\left(\frac{a}{2}+t\right)^{k}}=b
$$
![](https://cdn.mathpix.com/cropped/2022_10_28_1f3b279542cbcf365e2cg-128.jpg?height=476&width=1000&top_left_y=1774&top_left_x=562)

Fig. 1 The graph of the function $f(x)=\sqrt[n]{(1-x)^{k}}+\sqrt[n]{(1+x)^{k}}$ holds for two distinct values $t \geq 0$. It is evident that the latter condition holds if $a=0$. For $a \neq 0$, after the change of variables $t=\frac{|a|}{2} s, b=\sqrt[n]{\left(\frac{a}{2}\right)^{k}} c$ the equality takes a form $\sqrt[n]{(1-s)^{k}}+\sqrt[n]{(1+s)^{k}}=c$. Thus, Gollum can determine $x y$, if the function $f(s)=\sqrt[n]{(1-s)^{k}}+\sqrt[n]{(1+s)^{k}}$ attains each value at most once on $s \geq$ 0 . Determining intervals of monotonicity we get that for $k / n<1$ the continuous function $f(s)$ decreases on $(0,1)$ and increases on $(1,+\infty)$, and for $k / n>1$, it increases on $[0,+\infty)$ (see Fig. 1). Thus, Gollum wins if and only if $k>n$. Answer: either $n$ is odd, $k$ is even and $n<k$, or $k$ is odd, $n$ is even and $k<n$.

6 Introduce a function

$$
f(x)=\frac{\sum_{i=0}^{n} a_{i} e^{a_{i} x}}{\sum_{i=0}^{n} e^{a_{i} x}}, x \in \mathbb{R} .
$$

Its derivative

$$
f^{\prime}(x)=\frac{\sum_{i=0}^{n} a_{i}^{2} e^{a_{i} x} \cdot \sum_{i=0}^{n} e^{a_{i} x}-\left(\sum_{i=0}^{n} a_{i} e^{a_{i} x}\right)^{2}}{\left(\sum_{i=0}^{n} e^{a_{i} x}\right)^{2}}
$$

is positive for every $x \in \mathbb{R}$. Indeed, by the Cauchy-Schwarz inequality the numerator is nonnegative and turns to 0 only if the vectors

$$
\left(a_{0} e^{a_{0} x / 2}, \ldots, a_{n} e^{a_{n} x / 2}\right) \text { and }\left(e^{a_{0} x / 2}, \ldots, e^{a_{n} x / 2}\right)
$$

are proportional. But this condition fails because not all the numbers $a_{0}, \ldots, a_{n}$ coincide. Hence the function $f$ is strictly increasing on $\mathbb{R}$.

Put $m=\min \left\{a_{i}, 0 \leq i \leq n\right\}$ and $M=\max \left\{a_{i}, 0 \leq i \leq n\right\}$. Then $f(x) \rightarrow m$, as $x \rightarrow-\infty$, and $f(x) \rightarrow M$, as $x \rightarrow+\infty$. Therefore, a strictly monotone continuous function $f$ attains each value from the interval $(m, M)$ exactly once. It remains to notice that the equation from the statement of the problem can be written in a form $f(x)=\frac{1}{2^{n}} \sum_{i=0}^{n}\left(\begin{array}{l}n \\ i\end{array}\right) a_{i}$, and $m<\frac{1}{2^{n}} \sum_{i=0}^{n}\left(\begin{array}{l}n \\ i\end{array}\right) a_{i}<M$, because not all the numbers $a_{0}, \ldots, a_{n}$ coincide.

7 First we show that $\operatorname{det}\left(\begin{array}{cc}A & B \\ C^{\mathrm{T}} & 0\end{array}\right)=\operatorname{det}\left(\begin{array}{cc}A & B \\ C^{\mathrm{T}} & 1\end{array}\right)$. Indeed, in the decomposition of the

![](https://cdn.mathpix.com/cropped/2022_10_28_1f3b279542cbcf365e2cg-129.jpg?height=59&width=1255&top_left_y=2017&top_left_x=435)
$\operatorname{det} A=0$, hence the determinant does not depend on the choice of $d$. Denote by $I$ the identity $n \times n$ matrix and by $O$ the zero $n \times 1$ vector. Then

$$
\left(\begin{array}{cc}
A & B \\
C^{\mathrm{T}} & 1
\end{array}\right)=\left(\begin{array}{cc}
I & B \\
O^{\mathrm{T}} & 1
\end{array}\right)\left(\begin{array}{cc}
A-B C^{\mathrm{T}} & O \\
C^{\mathrm{T}} & 1
\end{array}\right),
$$

hence

![](https://cdn.mathpix.com/cropped/2022_10_28_1f3b279542cbcf365e2cg-130.jpg?height=86&width=1140&top_left_y=616&top_left_x=487)

This implies the statement of the problem.

8 Since $f_{n} \rightarrow 0\left(\bmod \lambda_{1}\right)$, as $n \rightarrow \infty$ (where $\lambda_{1}$ is Lebesgue measure on $\left.\mathbb{R}\right)$, it suffices to check whether the sequence converges to 0 in Lebesgue measure. We prove that there is no convergence in the measure. To this purpose we show that

$$
\begin{aligned}
\lambda_{1}\left(\left\{x \in \mathbb{R}:\left|f_{n}(x)\right| \geq \frac{1}{2}\right\}\right) &=\lambda_{1}\left(\left\{x \in[0, n]:|\sin \pi x| \geq \frac{1}{\sqrt[n]{2}}\right\}\right)=\\
&=n\left(1-\frac{2}{\pi} \arcsin \frac{1}{\sqrt[n]{2}}\right) \not \rightarrow 0 .
\end{aligned}
$$

Indeed, for $t \in[0,1]$, it holds $\arcsin t \leq \frac{\pi}{2} t$. Hence

$$
n\left(1-\frac{2}{\pi} \arcsin \frac{1}{\sqrt[n]{2}}\right) \geq n\left(1-\frac{1}{\sqrt[n]{2}}\right)=\frac{2^{1 / n}-1}{2^{1 / n} \cdot 1 / n} \rightarrow \ln 2>0, \text { as } n \rightarrow \infty .
$$

Answer: the sequence does not converge in the measure.

9 Since $B \subset f\left(\mathbb{R}^{2}\right) \subset \mathbb{R}$ is a compact set, the numbers $y_{*}=\min B$ and $y^{*}=$ $\max B$ are well defined, moreover $y_{*}=f\left(x_{*}\right)$ and $y^{*}=f\left(x^{*}\right)$ for some $x_{*}, x^{*} \in \mathbb{R}^{2}$. Consider

$$
\left[x_{*}, x^{*}\right]=\left\{(1-t) x_{*}+t x^{*}, t \in[0,1]\right\} \subset \mathbb{R}^{2}, \quad A=\left[x_{*}, x^{*}\right] \cap f^{-1}(B) .
$$

Then $\left[x_{*}, x^{*}\right]$ is a compact set, the set $f^{-1}(B)$ is closed in $\mathbb{R}^{2}$ as inverse image of a closed set under a continuous mapping, and the set $A$ is compact as a closed subset of a compact set. It remains to show that $f(A)=B$. Introduce a function

$$
g(t)=f\left((1-t) x_{*}+t x^{*}\right), t \in[0,1] .
$$

Then $g(0)=f\left(x_{*}\right)=y_{*}, g(1)=f\left(x^{*}\right)=y^{*}$ and $g \in C([0,1])$. Hence $g$ attains all the values from $\left[y_{*}, y^{*}\right]$ on the segment $[0,1]$. Thus, $f$ attains all the values from $\left[y_{*}, y^{*}\right]$ on $\left[x_{*}, x^{*}\right]$. Therefore, $B \subset f\left(\left[x_{*}, x^{*}\right]\right)$, whence $f(A)=f\left(\left[x_{*}, x^{*}\right] \cap\right.$ $\left.f^{-1}(B)\right)=B$. 10 Put $m=\mathrm{E} \xi$ and $\delta=\frac{1}{3 C}$. Consider an arbitrary random variable $\xi \in K_{C}$. Since

$$
\mathrm{P}(|\xi-m|<\delta)=\int_{m-\delta}^{m+\delta} p(x) d x \leq 2 C \delta=\frac{2}{3},
$$

it holds $\mathrm{P}(|\xi-m| \geq \delta) \geq \frac{1}{3}$. Then

$$
\operatorname{Var} \xi=\mathrm{E}(\xi-m)^{2} \geq \mathrm{E}(\xi-m)^{2} \mathrm{I}_{\{|\xi-m| \geq \delta\}} \geq \delta^{2} \mathrm{P}(|\xi-m| \geq \delta) \geq \frac{\delta^{2}}{3}=\frac{1}{27 C^{2}}
$$

Thus, one can take $a(C)=\frac{1}{27 C^{2}}$.

12 Introduce a function $f(a)=\frac{\mathrm{E} e^{2 a \xi}}{\mathrm{E} e^{a \xi}}, a \geq 0$. Show that the function is continuous. For $0 \leq a \leq c$, it holds

$$
e^{a \xi} \leq e^{c \xi} \mathrm{I}_{\{\xi \geq 0\}}+\mathrm{I}_{\{\xi<0\}} \leq e^{c \xi}+1 \in L(\Omega, \mathscr{F}, \mathrm{P}) .
$$

Hence the function $g(a)=\mathrm{E} e^{a \xi}$ is continuous on $[0, c]$ for every $c>0$. Because $g(a)>0$ for $a \geq 0$, the function $f(a)=\frac{g(2 a)}{g(a)}$ is continuous as well.

Show that $f(a) \rightarrow+\infty$, as $a \rightarrow+\infty$. Notice that by the continuity of measure from below it holds $\lim _{n \rightarrow \infty} \mathrm{P}\left(\xi>\frac{1}{n}\right)=\mathrm{P}(\xi>0)>0$, hence for some $N \geq 1$ it holds $\mathrm{P}\left(\xi>\frac{1}{N}\right)>0$. Then

$$
f(a)=\frac{\mathrm{E} e^{2 a \xi}}{\mathrm{E} e^{a \xi}} \geq \mathrm{E} e^{a \xi} \geq e^{\frac{a}{N}} \mathrm{P}\left(\xi>\frac{1}{N}\right) \rightarrow+\infty, a \rightarrow+\infty
$$

Thus, $f \in C([0,+\infty), f(0)=1$, and $f(a) \rightarrow+\infty$, as $a \rightarrow+\infty$. Therefore, by the intermediate value theorem there exists a number $\sigma>0$ such that $f(\sigma)=2$.

14 We parametrize the equation:

$$
\left\{\begin{array}{l}
y=x u-\sqrt{1+u^{2}} \\
d y=u d x
\end{array}\right.
$$

Then for the variables $u$ and $x$, we have $\left(x-\frac{u}{\sqrt{1+u^{2}}}\right) d u=0$, whence

$$
\left\{\begin{array} { l } 
{ u = c , c \in \mathbb { R } , } \\
{ y = c x - \sqrt { 1 + c ^ { 2 } } }
\end{array} \text { or } \left\{\begin{array}{l}
x=\frac{u}{\sqrt{1+u^{2}}}, \\
y=\frac{u^{2}}{\sqrt{1+u^{2}}}-\sqrt{1+u^{2}}=-\frac{1}{\sqrt{1+u^{2}}},
\end{array}\right.\right.
$$

i.e., $y=c x-\sqrt{1+c^{2}}, c \in \mathbb{R}$, or $y^{2}+x^{2}=1, y<0$.

One can draw tangent lines from the points $\left(x_{1}, y_{1}\right)$ and $\left(x_{2}, y_{2}\right)$ to the lower half of the unit circle, besides the points of tangency will belong to the third and the fourth quadrants, respectively. It is easy to verify that the equation of each tangent line to the lower half of the unit circle has a form $y=c x-\sqrt{1+c^{2}}$ for some $c \in \mathbb{R}$. Hence the graph of a required solution can be constructed as a union of two segments of tangent lines and an arc of the unit circle as shown on Fig. 2.

Fig. 2 The graph of a solution to the differential equation which connects the points $\left(x_{1}, y_{1}\right)$ and $\left(x_{2}, y_{2}\right)$

![](https://cdn.mathpix.com/cropped/2022_10_28_1f3b279542cbcf365e2cg-132.jpg?height=447&width=642&top_left_y=590&top_left_x=1034)



\section{6}

1 If $n$ is divisible by 9 , then $\cos \frac{n \pi}{9}=\pm 1$ and the expression equals $4-\sqrt[3]{5}$. If $n$ is divisible by 3 and is not divisible by 9 , then $\cos \frac{n \pi}{9}=\pm \frac{1}{2}$ and the expression equals $1+\sqrt[3]{4}$. Let $n$ be not divisible by 3 . Rewrite the expression as

$$
2+2 \cos \frac{2 n \pi}{9}+\sqrt[3]{1-6 \cos \frac{2 n \pi}{9}} .
$$

Since $\cos 3 \alpha=4 \cos ^{3} \alpha-3 \cos \alpha$, it holds for $\alpha=\frac{2 n \pi}{9}$ that

$$
4 \cos ^{3} \frac{2 n \pi}{9}=\cos \frac{6 n \pi}{9}+3 \cos \frac{2 n \pi}{9}=-\frac{1}{2}+3 \cos \frac{2 n \pi}{9} \text {, }
$$

whence

$$
2+2 \cos \frac{2 n \pi}{9}+\sqrt[3]{1-6 \cos \frac{2 n \pi}{9}}=2+2 \cos \frac{2 n \pi}{9}+\sqrt[3]{-8 \cos ^{3} \frac{2 n \pi}{9}}=2 .
$$

It remains to notice that $4-\sqrt[3]{5}>2$ and $1+\sqrt[3]{4}>2$.

Answer: 2.

2 For each infinite increasing geometric progression which consists of positive integers its first term and the denominator are positive integers as well. Hence the set of all such progressions is countable. We enumerate all such progressions and choose a number $c_{n}>10^{n}$ from the $n$th progression. Introduce a set $A=\mathbb{N} \backslash\left\{c_{1}, c_{2}, \ldots\right\}=$ $\left\{a_{1}, a_{2}, \ldots\right\}$. It does not contain any infinite increasing geometric progression, and

$$
\frac{\max \left\{k: a_{k} \leq n\right\}}{n} \geq \frac{n-[\lg n]}{n} \rightarrow 1, \text { as } n \rightarrow \infty .
$$

(c) Springer International Publishing AG 2017 Alternative solution. Denote

$$
B=\left\{b q^{b+1}, b, q \in \mathbb{N}, q \geq 2\right\} \text { and } A=\mathbb{N} \backslash B=\left\{a_{1}, a_{2}, \ldots\right\}
$$

Each infinite increasing geometric progression which consists of positive integers has a positive integer first term $b$ and a positive integer denominator $q \geq 2$, hence it contains the number $b q^{b+1}$. Therefore, the set $A$ contains no infinite increasing geometric progressions. Estimate a number of elements of $B$ which do not exceed a positive integer $n$. If $b q^{b+1} \leq n$ then $2^{b}<b q^{b+1} \leq n$, whence $b<\log _{2} n$ and $q^{2} \leq b q^{b+1} \leq n$, whence $q \leq \sqrt{n}$. Thus, the set $B$ contains less than $\sqrt{n} \log _{2} n$ numbers, so the set $A$ contains more than $n-\sqrt{n} \log _{2} n$ numbers that do not exceed $n$. Therefore,

$$
\frac{\max \left\{k: a_{k} \leq n\right\}}{n}>\frac{n-\sqrt{n} \log _{2} n}{n} \rightarrow 1, \text { as } n \rightarrow \infty
$$

Answer: yes, there exists.

3 Without loss of generality we may assume that $\angle B_{1} A_{1} C_{1}=90^{\circ}$ and either $\angle B A C=90^{\circ}$ or $\angle A B C=90^{\circ}$. Introduce a Cartesian coordinate system in such a way that $\overrightarrow{A_{1} B_{1}}=(1,0,0)$ and $\overrightarrow{A_{1} C_{1}}=(0,1,0)$. Then for some $x, y \in \mathbb{R}$, it holds $\overrightarrow{A B}=(1,0, x), \overrightarrow{A C}=(0,1, y)$, and $\overrightarrow{B C}=(-1,1, y-x)$

Let $\angle B A C=90^{\circ}$. Then the triangle $A B C$ is a right isosceles triangle if $|\overrightarrow{A B}|=$ $|\overrightarrow{A C}|$ and $(\overrightarrow{A B}, \overrightarrow{A C})=0$, i.e., $1+x^{2}=1+y^{2}$ and $x y=0$. Thus, $x=y=0$ and $A B=A_{1} B_{1}$.

Let $\angle A B C=90^{\circ}$. Then the triangle $A B C$ is a right isosceles triangle if $|\overrightarrow{A B}|=$ $|\overrightarrow{B C}|$ and $(\overrightarrow{A B}, \overrightarrow{B C})=0$, i.e.,

$$
\left\{\begin{array} { l } 
{ 1 + x ^ { 2 } = 2 + ( y - x ) ^ { 2 } } \\
{ - 1 + x ( y - x ) = 0 , }
\end{array} \quad \left\{\begin{array}{l}
y^{2}-2 x y=-1 \\
x y-x^{2}=1
\end{array}\right.\right.
$$

Since $x \neq 0$ and $y^{2}-x y-x^{2}=0$, it holds $(y / x)^{2}-y / x-1=0$. Hence $y=$ $\frac{1}{2}(1 \pm \sqrt{5}) x$. Denote $\varphi=\frac{1}{2}(1+\sqrt{5})$. For $y=\frac{1}{2}(1-\sqrt{5}) x$ the system of equations has no solution, and for $y=\varphi x$ it is easy to find solutions of the system: $x=\sqrt{\varphi}, y=\varphi \sqrt{\varphi}$ and $x=-\sqrt{\varphi}, y=-\varphi \sqrt{\varphi}$. Thus, $A B=\sqrt{1+x^{2}}=$ $\sqrt{1+\varphi}=\varphi=\varphi A_{1} B_{1}$.

Answer: 1 and $\frac{1+\sqrt{5}}{2}$.

4 Put $y_{n}=\frac{1}{x_{n}}$. Then $y_{0}=1$, and for $n \geq 0$, it holds

$$
\frac{1}{y_{n+1}}=\frac{1}{y_{n}}-\frac{1}{2016 y_{n}^{2}}, \quad \text { or equivalently } \quad y_{n+1}=\frac{2016 y_{n}^{2}}{2016 y_{n}-1},
$$

whence

$$
y_{n+1}-y_{n}=\frac{1}{2016-\frac{1}{y_{n}}} \text {. }
$$

It is clear that $x_{n} \in(0,1)$ for all $n \geq 1$. Hence $y_{n}>1$ for all $n \geq 1$. Thus,

$$
y_{1}-y_{0}=\frac{1}{2015} \text { and } \frac{1}{2016}<y_{n+1}-y_{n}=\frac{y_{n}}{2016 y_{n}-1}<\frac{1}{2015}, n \geq 1 .
$$

By induction on $n$ we get

$$
1+\frac{n}{2016}<y_{n}<1+\frac{n}{2015}, n \geq 2 .
$$

Therefore, $y_{2015}<2<y_{2016}$, that is $x_{2016}<\frac{1}{2}<x_{2015}$.

5 (a) Let $A$ be the identity matrix and $B$ be a block-diagonal matrix with 1008 blocks of the form $\left(\begin{array}{cc}0 & -1 \\ 1 & 0\end{array}\right)$ on the diagonal. Then $a A+b B$ is a block-diagonal matrix with 1008 blocks of the form $\left(\begin{array}{cc}a & -b \\ b & a\end{array}\right)$ on the diagonal. Therefore, $\operatorname{det}(a A+b B)=$ $\left(a^{2}+b^{2}\right)^{1008} \neq 0$ if $a^{2}+b^{2} \neq 0$.

(b) If $A$ or $B$ is singular, one can put $a=1, b=0$ or $a=0, b=1$, respectively. Let both matrices $A$ and $B$ be nonsingular. Since $\operatorname{det}(-A)=(-1)^{2017} \operatorname{det} A=-\operatorname{det} A$, there exists a number $\alpha \in\{-1 ; 1\}$ such that $\operatorname{det}(\alpha A)>0$. Similarly there exists a number $\beta \in\{-1 ; 1\}$ such that $\operatorname{det}(\beta B)<0$. Then the function

$$
f(t)=\operatorname{det}(t \alpha A+(1-t) \beta B), t \in \mathbb{R},
$$

is continuous, $f(0)=\operatorname{det}(\beta B)<0$, and $f(1)=\operatorname{det}(\alpha A)>0$. Thus, by the intermediate value theorem there exists a real number $0<t_{0}<1$, for which $f\left(t_{0}\right)=0$, and one can put $a=t_{0} \alpha, b=\left(1-t_{0}\right) \beta$.

Answer: (a) no, (b) yes.

6 Prove that

$$
\sqrt{x}>\frac{m}{n}+\frac{1}{m n}=\frac{m^{2}+1}{m n} .
$$

Since $\sqrt{x}>\frac{m}{n}$, it holds $n^{2} x>m^{2}$. But $n^{2} x \neq m^{2}+1$ and $n^{2} x \neq m^{2}+2$, because the numbers $m^{2}+1$ and $m^{2}+2$ are not divisible by 7 for all integer $m$. Hence $n^{2} x \geq m^{2}+3$

Suppose that $\sqrt{x} \leq \frac{m^{2}+1}{m n}$. Then $m^{2} n^{2} x \leq\left(m^{2}+1\right)^{2}=m^{4}+2 m^{2}+1$, hence $m^{4}+2 m^{2}+1 \geq m^{2}\left(m^{2}+3\right)=m^{4}+3 m^{2}$, i.e., $1 \geq m^{2}$, whence $m=1$. But then $n^{2} x \leq 4$ and $x$ is not divisible by 7 , a contradiction. Therefore, if $\sqrt{x}>\frac{m}{n}$ then $\sqrt{x}>\frac{m}{n}+\frac{1}{m n}=\frac{m^{2}+1}{m n}$. This implies

$$
\sqrt{x}>\frac{m^{2}+1}{m n}+\frac{1}{\left(m^{2}+1\right) m n}=\frac{m^{4}+2 m^{2}+2}{m^{3} n+m n} .
$$

7 Introduce a polynomial

$$
P(y)=\left(x_{1}+y\right)\left(x_{2}+y\right) \ldots\left(x_{n}+y\right)-c .
$$

It holds $P\left(y_{1}\right)=P\left(y_{2}\right)=\ldots=P\left(y_{n}\right)=0$, besides $P$ is a polynomial of degree $n$ with leading coefficient 1 . Therefore,

$$
P(y)=\left(y-y_{1}\right)\left(y-y_{2}\right) \ldots\left(y-y_{n}\right)=\left(x_{1}+y\right)\left(x_{2}+y\right) \ldots\left(x_{n}+y\right)-c .
$$

Hence

$$
P(-y)=\left(x_{1}-y\right)\left(x_{2}-y\right) \ldots\left(x_{n}-y\right)-c=\left(-y-y_{1}\right)\left(-y-y_{2}\right) \ldots\left(-y-y_{n}\right),
$$

thus,

$$
\left(y+y_{1}\right)\left(y+y_{2}\right) \ldots\left(y+y_{n}\right)=\left(y-x_{1}\right)\left(y-x_{2}\right) \ldots\left(y-x_{n}\right)-(-1)^{n} c .
$$

Substitute $y=x_{i}$ and get $\left(x_{i}+y_{1}\right)\left(x_{i}+y_{2}\right) \ldots\left(x_{i}+y_{n}\right)=(-1)^{n+1} c$.

Answer: $(-1)^{n+1} c$.

8 Let the numbers written on the opposite faces of the die be $a_{1}$ and $a_{2}, b_{1}$ and $b_{2}, c_{1}$ and $\left(c_{2}\right)$, respectively. If the cubic die satisfies the condition of the problem, then the sum of the numbers written on each pair of adjacent faces is integer. A total sum of the written numbers equals $S=\left(a_{1}+b_{1}\right)+\left(b_{2}+c_{1}\right)+\left(c_{2}+a_{2}\right)$, hence the sum is integer as well. But expectation of the sum of numbers on the two upper faces equals $\frac{S}{3}$. Hence it should hold $\frac{S}{3}=\frac{21}{6}$, i.e., $S=10.5$. Thus, $S$ is not integer, we get a contradiction.

Answer: no, it is impossible.

9 The sets $A=\{x \in[0,1]: f(x)=0\}$ and $B=\left\{x \in[0,1]: f(x)=f^{\prime}(x)=0\right\}$ are closed, hence Borel measurable. It is clear that $B \subset A$. Show that all the limit points of $A$ belong to $B$. Indeed, assume that $\left\{x_{n}, n \geq 1\right\} \subset A \backslash\{x\}$ and $x_{n} \rightarrow x$, as $n \rightarrow \infty$. Then $x \in A$, because the set $A$ is closed. Hence $f\left(x_{n}\right)=0, n \geq 1$, and $f(x)=0$, whence $f^{\prime}(x)=\lim _{n \rightarrow \infty} \frac{f\left(x_{n}\right)-f(x)}{x_{n}-x}=0$. Thus, $x \in B$. Therefore, the set $A \backslash B$ contains only isolated points of $A$. Then $A \backslash B$ is at most countable, because for each $n \geq 1$, there exists finitely many points of the set $A$, in $\frac{1}{n}$-neighborhood of which there is no other point of $A$. Therefore, $\lambda_{1}(A \backslash B)=0$, and $\lambda_{1}(A)=\lambda_{1}(B)+$ $\lambda_{1}(A \backslash B)=\lambda_{1}(B)$ 11 Since the matrices $A(t)$ are skew-symmetric, that is $A(t)^{\mathrm{T}}=-A(t), t \in \mathbb{R}$, it holds that

$$
\begin{aligned}
\frac{d}{d t}(X(t) x, X(t) x) &=(A(t) X(t) x, X(t) x)+(X(t) x, A(t) X(t) x)=\\
&=(A(t) X(t) x, X(t) x)+\left(A(t)^{\mathrm{T}} X(t) x, X(t) x\right)=0
\end{aligned}
$$

for all $x \in \mathbb{R}^{n}$ and $t \in \mathbb{R}$. Whence $\|X(t) x\|^{2}=(X(t) x, X(t) x)=(x, x)=\|x\|^{2}$, i.e., the mapping $X(t): \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ is an isometry. Thus, the mapping $X(t): S_{r}(0) \rightarrow$ $S_{r}(0)$ is a bijection for each $t \in \mathbb{R}$ and each $r>0$, where $S_{r}(0)$ is a sphere in $\mathbb{R}^{n}$ with center in the origin and radius $r$.

Consider a sequence of points $y_{t}=X(t)^{-1} y, t \in \mathbb{N}$. Let $r=\|y\|$. Since $\left\{y_{t}, t \geq 1\right\} \subset S_{r}(0)$ and $S_{r}(0)$ is a compact set, there exists a subsequence $\left\{y_{t_{i}}, i \geq\right.$ $1\}$ which converges to some $z \in S_{r}(0)$. Then

$$
\left\|X\left(t_{i}\right) z-y\right\|=\left\|X\left(t_{i}\right) z-X\left(t_{i}\right) X\left(t_{i}\right)^{-1} y\right\|=\left\|z-y_{t_{i}}\right\| \rightarrow 0, \text { as } i \rightarrow \infty .
$$

12 For all $a, b \in \mathbb{R}$, it holds

$$
\int_{0}^{1} p(x, a, b) d x=\exp (f(a, b)) \cdot \int_{0}^{1} \exp \left(a x+b x^{2}+g(x)\right) d x=1
$$

(hereafter all the integrals are proper Riemann integrals). Hence

$$
f(a, b)=-\ln \left(\int_{0}^{1} \exp \left(a x+b x^{2}+g(x)\right) d x\right),
$$

therefore, $f \in C^{(1)}\left(\mathbb{R}^{2}\right)$. We differentiate the identity $\int_{0}^{1} p(x, a, b) d x=1$ with respect to $a$ and $b$. We get

$$
\int_{0}^{1}\left(x+f_{a}^{\prime}(a, b)\right) p(x, a, b) d x=0, \quad \int_{0}^{1}\left(x^{2}+f_{b}^{\prime}(a, b)\right) p(x, a, b) d x=0
$$

Let $\xi=\xi(a, b)$ be a random variable with probability density function $p(x, a, b)$. Then

$$
\begin{aligned}
\mathrm{E} \xi &=\int_{0}^{1} x p(x, a, b) d x=-\int_{0}^{1} f_{a}^{\prime}(a, b) p(x, a, b) d x=-f_{a}^{\prime}(a, b), \\
\mathrm{E} \xi^{2} &=\int_{0}^{1} x^{2} p(x, a, b) d x=-\int_{0}^{1} f_{b}^{\prime}(a, b) p(x, a, b) d x=-f_{b}^{\prime}(a, b),
\end{aligned}
$$

whence $\operatorname{Var} \xi=\mathrm{E} \xi^{2}-(\mathrm{E} \xi)^{2}=-f_{b}^{\prime}(a, b)-\left(-f_{a}^{\prime}(a, b)\right)^{2}>0$, i.e., $\left(f_{a}^{\prime}(a, b)\right)^{2}+$ $f_{b}^{\prime}(a, b)<0$ (notice that $\operatorname{Var} \xi>0$, because if $\operatorname{Var} \xi=0$, the random variable $\xi$ would have no probability density). 13 Show that the limit in question equals zero. Since $K\left(1-\frac{1}{3^{k}}\right)=1-\frac{1}{2^{k}}$, it holds

$$
K(x) \leq 1-\frac{1}{2^{k}}, \quad \text { for } 1-\frac{1}{3^{k-1}} \leq x \leq 1-\frac{1}{3^{k}}, \quad k \geq 1 .
$$

Hence

$$
I_{n}:=n \int_{[0,1]} K^{n}(x) d \lambda_{1} \leq \sum_{k \geq 1} n\left(1-\frac{1}{2^{k}}\right)^{n} \frac{2}{3^{k}} .
$$

For a fixed $k$, the sequence $a_{n}=n\left(1-\frac{1}{2^{k}}\right)^{n}, n \geq 1$, satisfies

$$
\frac{a_{n+1}}{a_{n}}=\frac{n+1}{n}\left(1-\frac{1}{2^{k}}\right) \geq 1 \Leftrightarrow n \leq 2^{k}-1,
$$

hence the sequence is not decreasing for $n \leq 2^{k}-1$ and not increasing for $n \geq 2^{k}$. Thus, $a_{n} \leq a_{2^{k}}<2^{k}$, for all $n \in \mathbb{N}$.

For each $k_{0}$ it holds

$$
\begin{aligned}
I_{n} & \leq \sum_{1 \leq k \leq k_{0}} n\left(1-\frac{1}{2^{k}}\right)^{n} \frac{2}{3^{k}}+\sum_{k>k_{0}} n\left(1-\frac{1}{2^{k}}\right)^{n} \frac{2}{3^{k}}<\\
&<\sum_{1 \leq k \leq k_{0}} n\left(1-\frac{1}{2^{k}}\right)^{n} \frac{2}{3^{k}}+\sum_{k>k_{0}} \frac{2^{k+1}}{3^{k}} .
\end{aligned}
$$

For each $\varepsilon>0$, one can choose a number $k_{0}$ such that the second sum is less than $\varepsilon / 2$. Since for each fixed $k$, it holds $n\left(1-\frac{1}{2^{k}}\right)^{n} \rightarrow 0$, as $n \rightarrow \infty$, for given $k_{0}$ and for all $n$ large enough the first sum is less than $\varepsilon / 2$ as well.

Alternative solution. For each $j \in \mathbb{N}$, introduce a function

$$
K_{j}(x)= \begin{cases}1-\frac{1}{2^{j}}, & 0 \leq x \leq 1-\frac{2}{3^{j}}, \\ 1+\frac{3^{j}}{2^{j+1}}(x-1), & 1-\frac{2}{3^{j}} \leq x \leq 1 .\end{cases}
$$

It is not difficult to verify that $K(x) \leq K_{j}(x), x \in[0,1]$, and

$$
\begin{aligned}
n \int_{[0,1]} K_{j}^{n}(x) d \lambda_{1} &=n\left(1-\frac{2}{3^{j}}\right)\left(1-\frac{1}{2^{j}}\right)^{n}+\\
&+\frac{n}{n+1} \frac{2^{j+1}}{3^{j}}\left(1-\left(1-\frac{1}{2^{j}}\right)^{n+1}\right) \rightarrow \frac{2^{j+1}}{3^{j}}, \text { as } n \rightarrow \infty .
\end{aligned}
$$

Thus,

$$
\limsup _{n \rightarrow \infty} n \int_{[0,1]} K^{n}(x) d \lambda_{1} \leq \lim _{n \rightarrow \infty} n \int_{[0,1]} K_{j}^{n}(x) d \lambda_{1}=\frac{2^{j+1}}{3^{j}} .
$$

Because $j \geq 1$ is arbitrary, the upper limit equals 0 .

Answer: 0.

14 Find a matrix $X_{0}$ of the form $X_{0}=A^{\mathrm{T}} Y$ for which $A X_{0}=I$ (here $Y$ is an $m \times m$ matrix). It holds $A A^{\mathrm{T}} Y=I$. Notice that $A A^{\mathrm{T}}$ is the Gram matrix of the rows of $A$, and it is nonsingular because rk $A=m$. Hence $Y=\left(A A^{\mathrm{T}}\right)^{-1}$ and $X_{0}=A^{\mathrm{T}}\left(A A^{\mathrm{T}}\right)^{-1}$.

Show that $X_{0}$ is a required matrix. If $X$ is an arbitrary solution of the equation $A X=I$, then $A\left(X-X_{0}\right)=O$, where $O$ is zero matrix of size $m \times m$. Then $(X-$ $\left.X_{0}\right)^{\mathrm{T}} A^{\mathrm{T}}=O$. Therefore, $\left(X-X_{0}\right)^{\mathrm{T}} X_{0}=\left(X-X_{0}\right)^{\mathrm{T}} A^{\mathrm{T}}\left(A A^{\mathrm{T}}\right)^{-1}=O$. Thus, the columns of the matrix $X-X_{0}$ are orthogonal to the corresponding columns of $X_{0}$. Thus, the sum of squared norms of the columns of $X=X_{0}+\left(X-X_{0}\right)$ is not less than the sum of squared norms of the columns of $X_{0}$. The equality is attained at $X=X_{0}$ only.

Answer: $X=A^{\mathrm{T}}\left(A A^{\mathrm{T}}\right)^{-1}$.